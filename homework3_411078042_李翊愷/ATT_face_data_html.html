<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>ATT_face_data</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>




<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
    @font-face {
 font-family: "Fira Mono";
 font-weight: normal;
 font-style: normal;
 src: local('"Fira Mono"'), url('fonts/fira.ttf') format('truetype');
}
div#notebook {
 font-family: sans-serif;
 font-size: 13pt;
 line-height: 170%;
 color: #cdd2e9;
 -webkit-font-smoothing: antialiased !important;
 padding-top: 25px !important;
}
body,
div.body {
 font-family: sans-serif;
 font-size: 13pt;
 color: #a2b0c7;
 background-color: #262931;
 background: #262931;
 -webkit-font-smoothing: antialiased !important;
}
body.notebook_app {
 padding: 0;
 background-color: #262931;
 background: #262931;
 padding-right: 0px !important;
 overflow-y: hidden;
}
a {
 font-family: sans-serif;
 color: #a2b0c7;
 -webkit-font-smoothing: antialiased !important;
}
a:hover,
a:focus {
 color: #d8dcee;
 -webkit-font-smoothing: antialiased !important;
}
div#maintoolbar {
 position: absolute;
 width: 90%;
 margin-left: -10%;
 padding-right: 8%;
 float: left;
 background: transparent !important;
}
#maintoolbar {
 margin-bottom: -3px;
 margin-top: 0px;
 border: 0px;
 min-height: 27px;
 padding-top: 2px;
 padding-bottom: 0px;
}
#maintoolbar .container {
 width: 75%;
 margin-right: auto;
 margin-left: auto;
}
.list_header,
div#notebook_list_header.row.list_header {
 font-size: 14pt;
 color: #d8dcee;
 background-color: transparent;
 height: 35px;
}
i.fa.fa-folder {
 display: inline-block;
 font: normal normal normal 14px "FontAwesome";
 font-family: "FontAwesome" !important;
 text-rendering: auto;
 -webkit-font-smoothing: antialiased;
 font-size: 18px;
 -moz-osx-font-smoothing: grayscale;
}
#running .panel-group .panel .panel-heading {
 font-size: 14pt;
 color: #a2b0c7;
 padding: 8px 8px;
 background: #2e3642;
 background-color: #2e3642;
}
#running .panel-group .panel .panel-heading a {
 font-size: 14pt;
 color: #a2b0c7;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
 font-size: 14pt;
 color: #a2b0c7;
}
#running .panel-group .panel .panel-body .list_container .list_item {
 background: #384152;
 background-color: #384152;
 padding: 2px;
 border-bottom: 2px solid rgba(80,92,133,.22);
}
#running .panel-group .panel .panel-body .list_container .list_item:hover {
 background: #384152;
 background-color: #384152;
}
#running .panel-group .panel .panel-body {
 padding: 2px;
}
button#refresh_running_list {
 border: none !important;
}
button#refresh_cluster_list {
 border: none !important;
}
div.running_list_info.toolbar_info {
 font-size: 15px;
 padding: 4px 0 4px 0;
 margin-top: 5px;
 margin-bottom: 8px;
 height: 24px;
 line-height: 24px;
 text-shadow: none;
}
.list_placeholder {
 font-weight: normal;
}
#tree-selector {
 padding: 0px;
 border-color: transparent;
}
#project_name > ul > li > a > i.fa.fa-home {
 color: #4c8be2;
 font-size: 17pt;
 display: inline-block;
 position: static;
 padding: 0px 0px;
 font-weight: normal;
 text-align: center;
 vertical-align: text-top;
}
.fa-folder:before {
 color: #4c8be2;
}
.fa-arrow-up:before {
 font-size: 14px;
}
.fa-arrow-down:before {
 font-size: 14px;
}
span#last-modified.btn.btn-xs.btn-default.sort-action:hover .fa,
span#sort-name.btn.btn-xs.btn-default.sort-action:hover .fa {
 color: #4c8be2;
}
.folder_icon:before {
 display: inline-block;
 font: normal normal normal 14px/1 FontAwesome;
 font-size: inherit;
 text-rendering: auto;
 -webkit-font-smoothing: antialiased;
 -moz-osx-font-smoothing: grayscale;
 content: "\f07b";
 color: #4c8be2;
}
.notebook_icon:before {
 display: inline-block;
 font: normal normal normal 14px/1 FontAwesome;
 font-size: inherit;
 text-rendering: auto;
 -webkit-font-smoothing: antialiased;
 -moz-osx-font-smoothing: grayscale;
 content: "\f02d";
 position: relative;
 color: #48a667 !important;
 top: 0px;
}
.file_icon:before {
 display: inline-block;
 font: normal normal normal 14px/1 FontAwesome;
 font-size: inherit;
 text-rendering: auto;
 -webkit-font-smoothing: antialiased;
 -moz-osx-font-smoothing: grayscale;
 content: "\f15b";
 position: relative;
 top: 0px;
 color: #899ab8 !important;
}
#project_name a {
 display: inline-flex;
 padding-left: 7px;
 margin-left: -2px;
 text-align: -webkit-auto;
 vertical-align: baseline;
 font-size: 18px;
}
div#notebook_toolbar div.dynamic-instructions {
 font-family: sans-serif;
 font-size: 17px;
 color: #546379;
}
span#login_widget > .button,
#logout {
 font-family: "Proxima Nova", sans-serif;
 color: #a2b0c7;
 background: transparent;
 background-color: transparent;
 border: 2px solid #3a4452;
 font-weight: normal;
 box-shadow: none;
 text-shadow: none;
 border-radius: 3px;
 margin-right: 10px;
 padding: 2px 7px;
}
span#login_widget > .button:hover,
#logout:hover {
 color: #4c8be2;
 background-color: transparent;
 background: transparent;
 border: 2px solid #4c8be2;
 background-image: none;
 box-shadow: none !important;
 border-radius: 3px;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus,
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
 color: #e4e8ee;
 background-color: #a2b0c7;
 background: #a2b0c7;
 border-color: #a2b0c7;
 background-image: none;
 box-shadow: none !important;
 border-radius: 2px;
}
body > #header #header-container {
 padding-bottom: 0px;
 padding-top: 4px;
 box-sizing: border-box;
 -moz-box-sizing: border-box;
 -webkit-box-sizing: border-box;
}
body > #header {
 background: #262931;
 background-color: #262931;
 position: relative;
 z-index: 100;
}
.list_container {
 font-size: 13pt;
 color: #a2b0c7;
 border: none;
 text-shadow: none !important;
}
.list_container > div {
 border-bottom: 1px solid rgba(80,92,133,.22);
 font-size: 13pt;
}
.list_header > div,
.list_item > div {
 padding-top: 6px;
 padding-bottom: 2px;
 padding-left: 0px;
}
.list_header > div .item_link,
.list_item > div .item_link {
 margin-left: -1px;
 vertical-align: middle;
 line-height: 22px;
 font-size: 13pt;
}
.item_icon {
 color: #4c8be2;
 font-size: 13pt;
 vertical-align: middle;
}
.list_item input:not([type="checkbox"]) {
 padding-right: 0px;
 height: 1.75em;
 width: 25%;
 margin: 0px 0 0;
 margin-top: 0px;
}
.list_header > div .item_link,
.list_item > div .item_link {
 margin-left: -1px;
 vertical-align: middle;
 line-height: 1.5em;
 font-size: 12pt;
 display: inline-table;
 position: static;
}
#button-select-all {
 height: 34px;
 min-width: 55px;
 z-index: 0;
 border: none !important;
 padding-top: 0px;
 padding-bottom: 0px;
 margin-bottom: 0px;
 margin-top: 0px;
 left: -3px;
 border-radius: 0px !important;
}
#button-select-all:focus,
#button-select-all:active:focus,
#button-select-all.active:focus,
#button-select-all.focus,
#button-select-all:active.focus,
#button-select-all.active.focus {
 background-color: #3a4452 !important;
 background: #3a4452 !important;
}
button#tree-selector-btn {
 height: 34px;
 font-size: 12.0pt;
 border: none;
 left: 0px;
 border-radius: 0px !important;
}
input#select-all.pull-left.tree-selector {
 margin-left: 7px;
 margin-right: 2px;
 margin-top: 2px;
 top: 4px;
}
input[type="radio"],
input[type="checkbox"] {
 margin-top: 1px;
 line-height: normal;
}
.delete-button {
 border: none !important;
}
i.fa.fa-trash {
 font-size: 13.5pt;
}
.list_container a {
 font-size: 16px;
 color: #a2b0c7;
 border: none;
 text-shadow: none !important;
 font-weight: normal;
 font-style: normal;
}
div.list_container a:hover {
 color: #d8dcee;
}
.list_header > div input,
.list_item > div input {
 margin-right: 7px;
 margin-left: 12px;
 vertical-align: baseline;
 line-height: 22px;
 position: relative;
 top: -1px;
}
div.list_item:hover {
 background-color: rgba(80,92,133,.05);
}
.breadcrumb > li {
 font-size: 12.0pt;
 color: #a2b0c7;
 border: none;
 text-shadow: none !important;
}
.breadcrumb > li + li:before {
 content: "/\00a0";
 padding: 0px;
 color: #a2b0c7;
 font-size: 18px;
}
#project_name > .breadcrumb {
 padding: 0px;
 margin-bottom: 0px;
 background-color: transparent;
 font-weight: normal;
 margin-top: -2px;
}
ul#tabs a {
 font-family: sans-serif;
 font-size: 13.5pt;
 font-weight: normal;
 font-style: normal;
 text-shadow: none !important;
}
.nav-tabs {
 font-family: sans-serif;
 font-size: 13.5pt;
 font-weight: normal;
 font-style: normal;
 background-color: transparent;
 border-color: transparent;
 text-shadow: none !important;
 border: 2px solid transparent;
}
.nav-tabs > li > a:active,
.nav-tabs > li > a:focus,
.nav-tabs > li > a:hover,
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:focus,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
 color: #4c8be2;
 background-color: transparent;
 border-color: transparent;
 border-bottom: 2px solid transparent;
}
.nav > li.disabled > a,
.nav > li.disabled > a:hover {
 color: #546379;
}
.nav-tabs > li > a:before {
 content: "";
 position: absolute;
 width: 100%;
 height: 2px;
 bottom: -2px;
 left: 0;
 background-color: #4c8be2;
 visibility: hidden;
 -webkit-transform: perspective(0)scaleX(0);
 transform: perspective(0)scaleX(0);
 -webkit-transition: ease 220ms;
 transition: ease 220ms;
 -webkit-font-smoothing: antialiased !important;
}
.nav-tabs > li > a:hover:before {
 visibility: visible;
 -webkit-transform: perspective(1)scaleX(1);
 transform: perspective(1)scaleX(1);
}
.nav-tabs > li.active > a:before {
 content: "";
 position: absolute;
 width: 100%;
 height: 2px;
 bottom: -2px;
 left: 0;
 background-color: #4c8be2;
 visibility: visible;
 -webkit-transform: perspective(1)scaleX(1);
 transform: perspective(1)scaleX(1);
 -webkit-font-smoothing: subpixel-antialiased !important;
}
div#notebook {
 font-family: sans-serif;
 font-size: 13pt;
 padding-top: 4px;
}
.notebook_app {
 background-color: #262931;
}
#notebook-container {
 padding: 13px 2px;
 background-color: #262931;
 min-height: 0px;
 box-shadow: none;
 width: 90%;
 margin-right: auto;
 margin-left: auto;
}
div#ipython-main-app.container {
 width: 90%;
 margin-right: auto;
 margin-left: auto;
 margin-right: auto;
 margin-left: auto;
}
.container {
 width: 90%;
 margin-right: auto;
 margin-left: auto;
}
div#menubar-container {
 width: 100%;
 width: 90%;
}
div#header-container {
 width: 90%;
}
.notebook_app #header,
.edit_app #header {
 box-shadow: none !important;
 background-color: #262931;
 border-bottom: 2px solid rgba(80,92,133,.22);
}
#header,
.edit_app #header {
 font-family: sans-serif;
 font-size: 13pt;
 box-shadow: none;
 background-color: #262931;
}
#header .header-bar,
.edit_app #header .header-bar {
 background: #262931;
 background-color: #262931;
}
body > #header .header-bar {
 width: 100%;
 background: #262931;
}
span.checkpoint_status,
span.autosave_status {
 font-size: small;
 display: none;
}
#menubar,
div#menubar {
 background-color: #262931;
 padding-top: 0px !important;
}
#menubar .navbar,
.navbar-default {
 background-color: #262931;
 margin-bottom: 0px;
 margin-top: 0px;
}
.navbar {
 border: none;
}
div.navbar-text,
.navbar-text,
.navbar-text.indicator_area,
p.navbar-text.indicator_area {
 margin-top: 8px !important;
 margin-bottom: 0px;
 color: #4c8be2;
}
.navbar-default {
 font-family: sans-serif;
 font-size: 13pt;
 background-color: #262931;
 border-color: #343d4b;
 line-height: 1.5em;
 padding-bottom: 0px;
}
.navbar-default .navbar-nav > li > a {
 font-family: sans-serif;
 font-size: 13pt;
 color: #a2b0c7;
 display: block;
 line-height: 1.5em;
 padding-top: 14px;
 padding-bottom: 11px;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
 color: #d8dcee !important;
 background-color: rgba(80,92,133,.22) !important;
 border-color: #343d4b !important;
 line-height: 1.5em;
 transition: 80ms ease;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
 color: #e4e8ee;
 background-color: #384251;
 border-color: #384251;
 line-height: 1.5em;
}
.navbar-nav > li > .dropdown-menu {
 margin-top: 0px;
}
.navbar-nav {
 margin: 0;
}
div.notification_widget.info,
.notification_widget.info,
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus,
div#notification_notebook.notification_widget.btn.btn-xs.navbar-btn,
div#notification_notebook.notification_widget.btn.btn-xs.navbar-btn:hover,
div#notification_notebook.notification_widget.btn.btn-xs.navbar-btn:focus {
 color: #899ab8 !important;
 background-color: transparent !important;
 border-color: transparent !important;
 padding-bottom: 0px !important;
 margin-bottom: 0px !important;
 font-size: 9pt !important;
 z-index: 0;
}
div#notification_notebook.notification_widget.btn.btn-xs.navbar-btn {
 font-size: 9pt !important;
 z-index: 0;
}
.notification_widget {
 color: #4c8be2;
 z-index: -500;
 font-size: 9pt;
 background: transparent;
 background-color: transparent;
 margin-right: 3px;
 border: none;
}
.notification_widget,
div.notification_widget {
 margin-right: 0px;
 margin-left: 0px;
 padding-right: 0px;
 vertical-align: text-top !important;
 margin-top: 6px !important;
 background: transparent !important;
 background-color: transparent !important;
 font-size: 9pt !important;
 border: none;
}
.navbar-btn.btn-xs:hover {
 border: none !important;
 background: transparent !important;
 background-color: transparent !important;
 color: #a2b0c7 !important;
}
div.notification_widget.info,
.notification_widget.info {
 display: none !important;
}
.edit_mode .modal_indicator:before {
 display: none;
}
.command_mode .modal_indicator:before {
 display: none;
}
.item_icon {
 color: #4c8be2;
}
.item_buttons .kernel-name {
 font-size: 13pt;
 color: #4c8be2;
}
.running_notebook_icon:before {
 color: #48a667 !important;
 font: normal normal normal 15px/1 FontAwesome;
 font-size: 15px;
 text-rendering: auto;
 -webkit-font-smoothing: antialiased;
 -moz-osx-font-smoothing: grayscale;
 content: "\f10c";
 vertical-align: middle;
 position: static;
 display: inherit;
}
.item_buttons .running-indicator {
 padding-top: 4px;
 color: #48a667;
 font-family: sans-serif;
 text-rendering: auto;
 -webkit-font-smoothing: antialiased;
}
#notification_trusted {
 font-family: sans-serif;
 border: none;
 background: transparent;
 background-color: transparent;
 margin-bottom: 0px !important;
 vertical-align: bottom !important;
 color: #546379 !important;
 cursor: default !important;
}
#notification_area,
div.notification_area {
 float: right !important;
 position: static;
 cursor: pointer;
 padding-top: 6px;
 padding-right: 4px;
}
div#notification_notebook.notification_widget.btn.btn-xs.navbar-btn {
 font-size: 9pt !important;
 z-index: 0;
 margin-top: -5px !important;
}
#modal_indicator {
 float: right !important;
 color: #4c8be2;
 background: #262931;
 background-color: #262931;
 margin-top: 8px !important;
 margin-left: 0px;
}
#kernel_indicator {
 float: right !important;
 color: #4c8be2;
 background: #262931;
 background-color: #262931;
 border-left: 2px solid #4c8be2;
 padding-top: 0px;
 padding-bottom: 4px;
 margin-top: 10px !important;
 margin-left: -2px;
 padding-left: 5px !important;
}
#kernel_indicator .kernel_indicator_name {
 font-size: 17px;
 color: #4c8be2;
 background: #262931;
 background-color: #262931;
 padding-left: 5px;
 padding-right: 5px;
 margin-top: 4px;
 vertical-align: text-top;
 padding-bottom: 0px;
}
.kernel_idle_icon:before {
 display: inline-block;
 font: normal normal normal 22px/1 FontAwesome;
 font-size: 22px;
 text-rendering: auto;
 -webkit-font-smoothing: antialiased;
 cursor: pointer;
 margin-left: 0px !important;
 opacity: 0.7;
 vertical-align: bottom;
 margin-top: 1px;
 content: "\f1db";
}
.kernel_busy_icon:before {
 display: inline-block;
 font: normal normal normal 22px/1 FontAwesome;
 font-size: 22px;
 -webkit-animation: pulsate 2s infinite ease-out;
 animation: pulsate 2s infinite ease-out;
 text-rendering: auto;
 -webkit-font-smoothing: antialiased;
 cursor: pointer;
 margin-left: 0px !important;
 vertical-align: bottom;
 margin-top: 1px;
 content: "\f111";
}
@-webkit-keyframes pulsate {
 0% {
  -webkit-transform: scale(1.0,1.0);
  opacity: 0.8;
 }
 8% {
  -webkit-transform: scale(1.0,1.0);
  opacity: 0.8;
 }
 50% {
  -webkit-transform: scale(0.75,0.75);
  opacity: 0.3;
 }
 92% {
  -webkit-transform: scale(1.0,1.0);
  opacity: 0.8;
 }
 100% {
  -webkit-transform: scale(1.0,1.0);
  opacity: 0.8;
 }
}
div.notification_widget.info,
.notification_widget.info,
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus,
div#notification_notebook.notification_widget.btn.btn-xs.navbar-btn,
div#notification_notebook.notification_widget.btn.btn-xs.navbar-btn:hover,
div#notification_notebook.notification_widget.btn.btn-xs.navbar-btn:focus {
 color: #899ab8;
 background-color: #262931;
 border-color: #262931;
}
#notification_area,
div.notification_area {
 float: right !important;
 position: static;
}
.notification_widget,
div.notification_widget {
 margin-right: 0px;
 margin-left: 0px;
 padding-right: 0px;
 vertical-align: text-top !important;
 margin-top: 6px !important;
 z-index: 1000;
}
#kernel_logo_widget,
#kernel_logo_widget .current_kernel_logo {
 display: none;
}
div#ipython_notebook {
 display: none;
}
i.fa.fa-icon {
 -webkit-font-smoothing: antialiased;
 -moz-osx-font-smoothing: grayscale;
 text-rendering: auto;
}
.fa {
 display: inline-block;
 font: normal normal normal 10pt/1 "FontAwesome", sans-serif;
 text-rendering: auto;
 -webkit-font-smoothing: antialiased;
 -moz-osx-font-smoothing: grayscale;
}
.dropdown-menu {
 font-family: sans-serif;
 font-size: 13pt;
 box-shadow: none;
 padding: 0px;
 text-align: left;
 border: none;
 background-color: #384251;
 background: #384251;
 line-height: 1;
}
.dropdown-menu:hover {
 font-family: sans-serif;
 font-size: 13pt;
 box-shadow: none;
 padding: 0px;
 text-align: left;
 border: none;
 background-color: #384251;
 box-shadow: none;
 line-height: 1;
}
.dropdown-menu > li > a {
 font-family: sans-serif;
 font-size: 12.0pt;
 display: block;
 padding: 10px 20px 9px 10px;
 color: #a2b0c7;
 background-color: #384251;
 background: #384251;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
 color: #d8dcee;
 background-color: #343d4b;
 background: #343d4b;
 border-color: #343d4b;
 transition: 200ms ease;
}
.dropdown-menu .divider {
 height: 1px;
 margin: 0px 0px;
 overflow: hidden;
 background-color: rgba(80,92,133,.45);
}
.dropdown-submenu > .dropdown-menu {
 display: none;
 top: 2px !important;
 left: 100%;
 margin-top: -2px;
 margin-left: 0px;
 padding-top: 0px;
 transition: 200ms ease;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
 font-family: sans-serif;
 font-size: 12.0pt;
 font-weight: normal;
 color: #546379;
 padding: none;
 display: block;
 clear: both;
 white-space: nowrap;
}
.dropdown-submenu > a:after {
 color: #a2b0c7;
 margin-right: -16px;
 margin-top: 0px;
 display: inline-block;
}
.dropdown-submenu:hover > a:after,
.dropdown-submenu:active > a:after,
.dropdown-submenu:focus > a:after,
.dropdown-submenu:visited > a:after {
 color: #4c8be2;
 margin-right: -16px;
 display: inline-block !important;
}
div.kse-dropdown > .dropdown-menu,
.kse-dropdown > .dropdown-menu {
 min-width: 0;
 top: 94%;
}
.btn,
.btn-default {
 font-family: sans-serif;
 color: #a2b0c7;
 background: #3a4452;
 background-color: #3a4452;
 border: 2px solid #3a4452;
 font-weight: normal;
 box-shadow: none;
 text-shadow: none;
 border-radius: 3px;
 font-size: initial;
}
.btn:hover,
.btn:active:hover,
.btn.active:hover,
.btn-default:hover,
.open > .dropdown-toggle.btn-default:hover,
.open > .dropdown-toggle.btn:hover {
 color: #4c8be2;
 border: 2px solid #363f4c;
 background-color: #363f4c;
 background: #363f4c;
 background-image: none;
 box-shadow: none !important;
 border-radius: 3px;
}
.btn:active,
.btn.active,
.btn:active:focus,
.btn.active:focus,
.btn:active.focus,
.btn.active.focus,
.btn-default:focus,
.btn-default.focus,
.btn-default:active,
.btn-default.active,
.btn-default:active:hover,
.btn-default.active:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn:focus,
.open > .dropdown-toggle.btn.focus,
.open > .dropdown-toggle.btn-default:hover,
.open > .dropdown-toggle.btn-default:focus,
.open > .dropdown-toggle.btn-default.hover,
.open > .dropdown-toggle.btn-default.focus {
 color: #4c8be2;
 border: 2px solid #363f4c;
 background-color: #363f4c !important;
 background: #363f4c !important;
 background-image: none;
 box-shadow: none !important;
 border-radius: 3px;
}
.btn-default:active:hover,
.btn-default.active:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.btn-default:active.focus,
.btn-default.active.focus {
 color: #157bff !important;
 background-color: #3a4452;
 border-color: #33517c !important;
 transition: 2000ms ease;
}
.btn:focus,
.btn.focus,
.btn:active:focus,
.btn.active:focus,
.btn:active,
.btn.active,
.btn:active.focus,
.btn.active.focus {
 color: #157bff !important;
 outline: none !important;
 outline-width: 0px !important;
 background: #33517c !important;
 background-color: #33517c !important;
 border-color: #33517c !important;
 transition: 200ms ease !important;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
 font-size: 13pt;
 background: transparent;
 background-color: transparent;
 border: 0px solid #2e3642;
 border-bottom: 2px solid transparent;
 margin-left: 5px;
 padding-top: 4px !important;
}
.item_buttons > .btn:hover,
.item_buttons > .btn-group:hover,
.item_buttons > .input-group:hover,
.item_buttons > .btn.active,
.item_buttons > .btn-group.active,
.item_buttons > .input-group.active,
.item_buttons > .btn.focus {
 margin-left: 5px;
 background: #2a313c;
 padding-top: 4px !important;
 background-color: transparent;
 border: 0px solid transparent;
 border-bottom: 2px solid #4c8be2;
 border-radius: 0px;
 transition: none;
}
.item_buttons {
 line-height: 1.5em !important;
}
.item_buttons .btn {
 min-width: 11ex;
}
.btn-group > .btn:first-child {
 margin-left: 3px;
}
.btn-group > .btn-mini,
.btn-sm,
.btn-group-sm > .btn,
.btn-xs,
.btn-group-xs > .btn,
.alternate_upload .btn-upload,
.btn-group,
.btn-group-vertical {
 font-size: inherit;
 font-weight: normal;
 height: inherit;
 line-height: inherit;
}
.btn-xs,
.btn-group-xs > .btn {
 font-size: initial !important;
 background-image: none;
 font-weight: normal;
 text-shadow: none;
 display: inline-table;
 padding: 2px 5px;
 line-height: 1.45;
}
.btn-group > .btn:first-child {
 margin-left: 3px;
}
div#new-buttons > button,
#new-buttons > button,
div#refresh_notebook_list,
#refresh_notebook_list {
 background: transparent;
 background-color: transparent;
 border: none;
}
div#new-buttons > button:hover,
#new-buttons > button:hover,
div#refresh_notebook_list,
#refresh_notebook_list,
div.alternate_upload .btn-upload,
.alternate_upload .btn-upload,
div.dynamic-buttons > button,
.dynamic-buttons > button,
.dynamic-buttons > button:focus,
.dynamic-buttons > button:active:focus,
.dynamic-buttons > button.active:focus,
.dynamic-buttons > button.focus,
.dynamic-buttons > button:active.focus,
.dynamic-buttons > button.active.focus,
#new-buttons > button:focus,
#new-buttons > button:active:focus,
#new-buttons > button.active:focus,
#new-buttons > button.focus,
#new-buttons > button:active.focus,
#new-buttons > button.active.focus,
.alternate_upload .btn-upload:focus,
.alternate_upload .btn-upload:active:focus,
.alternate_upload .btn-upload.active:focus,
.alternate_upload .btn-upload.focus,
.alternate_upload .btn-upload:active.focus,
.alternate_upload .btn-upload.active.focus {
 background: transparent !important;
 background-color: transparent !important;
 border: none !important;
}
.alternate_upload input.fileinput {
 text-align: center;
 vertical-align: bottom;
 margin-left: -.5ex;
 display: inline-table;
 border: solid 0px #3a4452;
 margin-bottom: -1ex;
}
.alternate_upload .btn-upload {
 display: inline-table;
 background: transparent;
 border: none;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
 margin-left: -2px;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
 border-bottom-right-radius: 0;
 border-top-right-radius: 0;
 z-index: 2;
}
.dropdown-header {
 font-family: sans-serif !important;
 font-size: 13pt !important;
 color: #4c8be2 !important;
 border-bottom: none !important;
 padding: 0px !important;
 margin: 6px 6px 0px !important;
}
span#last-modified.btn.btn-xs.btn-default.sort-action,
span#sort-name.btn.btn-xs.btn-default.sort-action,
span#file-size.btn.btn-xs.btn-default.sort-action {
 font-family: sans-serif;
 font-size: 16px;
 background-color: transparent;
 background: transparent;
 border: none;
 color: #a2b0c7;
 padding-bottom: 0px;
 margin-bottom: 0px;
 vertical-align: sub;
}
span#last-modified.btn.btn-xs.btn-default.sort-action {
 margin-left: 19px;
}
button.close {
 border: 0px none;
 font-family: sans-serif;
 font-size: 20pt;
 font-weight: normal;
}
.dynamic-buttons {
 padding-top: 0px;
 display: inline-block;
}
.close {
 color: #dc6972;
 opacity: .5;
 text-shadow: none;
 font-weight: normal;
}
.close:hover {
 color: #dc6972;
 opacity: 1;
 font-weight: normal;
}
div.nbext-enable-btns .btn[disabled],
div.nbext-enable-btns .btn[disabled]:hover,
.btn-default.disabled,
.btn-default[disabled],
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
 color: #92a2bd;
 background: #38424f;
 background-color: #38424f;
 border-color: #38424f;
 transition: 200ms ease;
}
.input-group-addon {
 padding: 2px 5px;
 font-size: 13pt;
 font-weight: normal;
 height: auto;
 color: #a2b0c7;
 text-align: center;
 background-color: transparent;
 border: 2px solid transparent !important;
 text-transform: capitalize;
}
a.btn.btn-default.input-group-addon:hover {
 background: transparent !important;
 background-color: transparent !important;
}
.btn-group > .btn + .dropdown-toggle {
 padding-left: 8px;
 padding-right: 8px;
 height: 100%;
}
.btn-group > .btn + .dropdown-toggle:hover {
 background: #363f4c !important;
}
.input-group-btn {
 position: relative;
 font-size: inherit;
 white-space: nowrap;
 background: #2e3642;
 background-color: #2e3642;
 border: none;
}
.input-group-btn:hover {
 background: #2a313c;
 background-color: #2a313c;
 border: none;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
 background: #2e3642;
 background-color: #2e3642;
 border: none;
 margin-left: 2px;
 margin-right: -1px;
 font-size: inherit;
}
.input-group-btn:first-child > .btn:hover,
.input-group-btn:first-child > .btn-group:hover {
 background: #363f4c;
 background-color: #363f4c;
 border: none;
 font-size: inherit;
 transition: 200ms ease;
}
div.modal .btn-group > .btn:first-child {
 background: #2e3642;
 background-color: #2e3642;
 border: 1px solid #2c343f;
 margin-top: 0px !important;
 margin-left: 0px;
 margin-bottom: 2px;
}
div.modal .btn-group > .btn:first-child:hover {
 background: #2a313c;
 background-color: #2a313c;
 border: 1px solid #2a313c;
 transition: 200ms ease;
}
div.modal > button,
div.modal-footer > button {
 background: #2e3642;
 background-color: #2e3642;
 border-color: #2e3642;
}
div.modal > button:hover,
div.modal-footer > button:hover {
 background: #2a313c;
 background-color: #2a313c;
 border-color: #2a313c;
 transition: 200ms ease;
}
.modal-content {
 font-family: sans-serif;
 font-size: 12.0pt;
 position: relative;
 background: #2e3642;
 background-color: #2e3642;
 border: none;
 border-radius: 1px;
 background-clip: padding-box;
 outline: none;
}
.modal-header {
 font-family: sans-serif;
 font-size: 13pt;
 color: #a2b0c7;
 background: #2e3642;
 background-color: #2e3642;
 border-color: rgba(80,92,133,.22);
 padding: 12px;
 min-height: 16.4286px;
}
.modal-content h4 {
 font-family: sans-serif;
 font-size: 16pt;
 color: #a2b0c7;
 padding: 5px;
}
.modal-body {
 background-color: #384152;
 position: relative;
 padding: 15px;
}
.modal-footer {
 padding: 8px;
 text-align: right;
 background-color: #384152;
 border-top: none;
}
.alert-info {
 background-color: #4a5467;
 border-color: rgba(80,92,133,.22);
 color: #a2b0c7;
}
.modal-header .close {
 margin-top: -5px;
 font-size: 25pt;
}
.modal-backdrop,
.modal-backdrop.in {
 opacity: 0.85;
 background-color: notebook-bg;
}
div.panel,
div.panel-default,
.panel,
.panel-default {
 font-family: sans-serif;
 font-size: 13pt;
 background-color: #384152;
 color: #a2b0c7;
 margin-bottom: 14px;
 border: 0;
 box-shadow: none;
}
div.panel > .panel-heading,
div.panel-default > .panel-heading {
 font-size: 14pt;
 color: #a2b0c7;
 background: #2e3642;
 background-color: #2e3642;
 border: 0;
}
.modal .modal-dialog {
 min-width: 950px;
 margin: 50px auto;
}
div.container-fluid {
 margin-right: auto;
 margin-left: auto;
 padding-left: 0px;
 padding-right: 5px;
}
div.form-control,
.form-control {
 font-family: sans-serif;
 font-size: initial;
 color: #a2b0c7;
 background-color: #2a313c;
 border: 1px solid #2a313c !important;
 margin-left: 2px;
 box-shadow: none;
 transition: border-color 0.15s ease-in-out 0s, box-shadow 0.15s ease-in-out 0s;
}
.form-control-static {
 min-height: inherit;
 height: inherit;
}
.form-group.list-group-item {
 color: #a2b0c7;
 background-color: #384152;
 border-color: rgba(80,92,133,.22);
 margin-bottom: 0px;
}
.form-group .input-group {
 float: left;
}
input,
button,
select,
textarea {
 background-color: #2a313c;
 font-weight: normal;
 border: 1px solid rgba(80,92,133,.22);
}
select.form-control.select-xs {
 height: 33px;
 font-size: 13pt;
}
.toolbar select,
.toolbar label {
 width: auto;
 vertical-align: middle;
 margin-right: 0px;
 margin-bottom: 0px;
 display: inline;
 font-size: 92%;
 margin-left: 10px;
 padding: 0px;
 background: #3a4452 !important;
 background-color: #3a4452 !important;
 border: 2px solid #3a4452 !important;
}
.form-control:focus {
 border-color: #4c8be2;
 outline: 2px solid #3572c6;
 -webkit-box-shadow: none;
}
::-webkit-input-placeholder {
 color: #546379;
}
::-moz-placeholder {
 color: #546379;
}
:-ms-input-placeholder {
 color: #546379;
}
:-moz-placeholder {
 color: #546379;
}
[dir="ltr"] #find-and-replace .input-group-btn + .form-control {
 border: 2px solid rgba(80,92,133,.22) !important;
}
[dir="ltr"] #find-and-replace .input-group-btn + .form-control:focus {
 border-color: #4c8be2;
 outline: 2px solid #3572c6;
 -webkit-box-shadow: none;
 box-shadow: none;
}
div.output.output_scroll {
 box-shadow: none;
}
::-webkit-scrollbar {
 width: 11px;
 max-height: 9px;
 background-color: #292d3a;
 border-radius: 3px;
 border: none;
}
::-webkit-scrollbar-track {
 background: #292d3a;
 border: none;
 width: 11px;
 max-height: 9px;
}
::-webkit-scrollbar-thumb {
 border-radius: 2px;
 border: none;
 background: #3f4555;
 background-clip: content-box;
 width: 11px;
}
HTML,
body,
div,
dl,
dt,
dd,
ul,
ol,
li,
h1,
h2,
h3,
h4,
h5,
h6,
pre,
code,
form,
fieldset,
legend,
input,
button,
textarea,
p,
blockquote,
th,
td,
span,
a {
 text-rendering: geometricPrecision;
 -webkit-font-smoothing: subpixel-antialiased;
 font-weight: 400;
}
div.input_area {
 background-color: #303845;
 background: #303845;
 padding-right: 1.2em;
 border: 0px;
 border-radius: 0px;
 border-top-right-radius: 4px;
 border-bottom-right-radius: 4px;
}
div.cell {
 padding: 0px;
 background: #303845;
 background-color: #303845;
 border: medium solid #262931;
 border-radius: 4px;
 top: 0;
}
div.cell.selected {
 background: #303845;
 background-color: #303845;
 border: medium solid #262931;
 padding: 0px;
 border-radius: 5px;
}
.edit_mode div.cell.selected {
 padding: 0px;
 background: #303845;
 background-color: #303845;
 border: medium solid #262931;
 border-radius: 5px;
}
div.cell.edit_mode {
 padding: 0px;
 background: #303845;
 background-color: #303845;
}
div.CodeMirror-sizer {
 margin-left: 0px;
 margin-bottom: -21px;
 border-right-width: 16px;
 min-height: 37px;
 padding-right: 0px;
 padding-bottom: 0px;
 margin-top: 0px;
}
div.cell.selected:before,
.edit_mode div.cell.selected:before,
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
 background: #303845 !important;
 border: none;
 border-radius: 3px;
 position: absolute;
 display: block;
 top: 0px;
 left: 0px;
 width: 0px;
 height: 100%;
}
div.cell.text_cell.selected::before,
.edit_mode div.cell.text_cell.selected:before,
div.cell.text_cell.selected:before,
div.cell.text_cell.selected.jupyter-soft-selected:before {
 background: #303845 !important;
 background-color: #303845 !important;
 border-color: #2769c3 !important;
}
div.cell.code_cell .input {
 border-left: 5px solid #303845 !important;
 border-radius: 3px;
 border-bottom-left-radius: 3px;
 border-top-left-radius: 3px;
}
div.cell.code_cell.selected .input {
 border-left: 5px solid #2769c3 !important;
 border-radius: 3px;
}
.edit_mode div.cell.code_cell.selected .input {
 border-left: 5px solid #33517c !important;
 border-radius: 3px;
}
.edit_mode div.cell.selected:before {
 height: 100%;
 border-left: 5px solid #33517c !important;
 border-radius: 3px;
}
div.cell.jupyter-soft-selected,
div.cell.selected.jupyter-soft-selected {
 border-left-color: #33517c !important;
 border-left-width: 0px !important;
 padding-left: 7px !important;
 border-right-color: #33517c !important;
 border-right-width: 0px !important;
 background: #33517c !important;
 border-radius: 6px !important;
}
div.cell.selected.jupyter-soft-selected .input {
 border-left: 5px solid #303845 !important;
}
div.cell.selected.jupyter-soft-selected {
 border-left-color: #2769c3;
 border-color: #262931;
 padding-left: 7px;
 border-radius: 6px;
}
div.cell.code_cell.selected .input {
 border-left: none;
 border-radius: 3px;
}
div.cell.selected.jupyter-soft-selected .prompt,
div.cell.text_cell.selected.jupyter-soft-selected .prompt {
 top: 0;
 border-left: #303845 !important;
 border-radius: 2px;
}
div.cell.text_cell.selected.jupyter-soft-selected .input_prompt {
 border-left: none !important;
}
div.cell.text_cell.jupyter-soft-selected,
div.cell.text_cell.selected.jupyter-soft-selected {
 border-left-color: #33517c !important;
 border-left-width: 0px !important;
 padding-left: 26px !important;
 border-right-color: #33517c !important;
 border-right-width: 0px !important;
 background: #33517c !important;
 border-radius: 5px !important;
}
div.cell.jupyter-soft-selected .input,
div.cell.selected.jupyter-soft-selected .input {
 border-left-color: #33517c !important;
}
div.prompt,
.prompt {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 9pt !important;
 font-weight: normal;
 color: #446489;
 line-height: 170%;
 padding: 0px;
 padding-top: 4px;
 padding-left: 0px;
 padding-right: 1px;
 text-align: right !important;
 min-width: 11.5ex !important;
 width: 11.5ex !important;
}
div.prompt.input_prompt {
 font-size: 9pt !important;
 background-color: #303845;
 border-top: 0px;
 border-top-right-radius: 0px;
 border-bottom-left-radius: 0px;
 border-bottom-right-radius: 0px;
 padding-right: 3px;
 min-width: 11.5ex;
 width: 11.5ex !important;
}
div.cell.code_cell .input_prompt {
 border-right: 2px solid #3572c6;
}
div.cell.selected .prompt {
 top: 0;
}
.edit_mode div.cell.selected .prompt {
 top: 0;
}
.edit_mode div.cell.selected .prompt {
 top: 0;
}
.run_this_cell {
 visibility: hidden;
 color: transparent;
 padding-top: 0px;
 padding-bottom: 0px;
 padding-left: 3px;
 padding-right: 12px;
 width: 1.5ex;
 width: 0ex;
 background: transparent;
 background-color: transparent;
}
div.code_cell:hover div.input .run_this_cell {
 visibility: visible;
}
div.cell.code_cell.rendered.selected .run_this_cell:hover {
 background-color: #282e39;
 background: #282e39;
 color: #2769c3 !important;
}
div.cell.code_cell.rendered.unselected .run_this_cell:hover {
 background-color: #282e39;
 background: #282e39;
 color: #2769c3 !important;
}
i.fa-step-forward.fa {
 display: inline-block;
 font: normal normal normal 9px "FontAwesome";
}
.fa-step-forward:before {
 content: "\f04b";
}
div.cell.selected.jupyter-soft-selected .run_this_cell,
div.cell.selected.jupyter-soft-selected .run_this_cell:hover,
div.cell.unselected.jupyter-soft-selected .run_this_cell:hover,
div.cell.code_cell.rendered.selected.jupyter-soft-selected .run_this_cell:hover,
div.cell.code_cell.rendered.unselected.jupyter-soft-selected .run_this_cell:hover {
 background-color: #33517c !important;
 background: #33517c !important;
 color: #33517c !important;
}
div.output_wrapper {
 background-color: #384151;
 border: 0px;
 left: 0px;
 margin-bottom: 0em;
 margin-top: 0em;
 border-top-right-radius: 0px;
 border-top-left-radius: 0px;
}
div.output_subarea.output_text.output_stream.output_stdout,
div.output_subarea.output_text {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 11pt !important;
 line-height: 150% !important;
 background-color: #384151;
 color: #cdd2e9;
 border-top-right-radius: 0px;
 border-top-left-radius: 0px;
 margin-left: 11.5px;
}
div.output_area pre {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 11pt !important;
 line-height: 151% !important;
 color: #cdd2e9;
 border-top-right-radius: 0px;
 border-top-left-radius: 0px;
}
div.output_area {
 display: -webkit-box;
}
div.output_html {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 11pt;
 color: #dbdfef;
 background-color: #384151;
 background: #384151;
}
div.output_subarea {
 overflow-x: auto;
 padding: 1.2em !important;
 -webkit-box-flex: 1;
 -moz-box-flex: 1;
 box-flex: 1;
 flex: 1;
}
div.btn.btn-default.output_collapsed {
 background: #242a33;
 background-color: #242a33;
 border-color: #242a33;
}
div.btn.btn-default.output_collapsed:hover {
 background: #1f252d;
 background-color: #1f252d;
 border-color: #1f252d;
}
div.prompt.output_prompt {
 font-family: "Fira Mono", monospace, monospace;
 font-weight: bold !important;
 background-color: #384151;
 color: transparent;
 border-bottom-left-radius: 4px;
 border-top-right-radius: 0px;
 border-top-left-radius: 0px;
 border-bottom-right-radius: 0px;
 min-width: 11.5ex !important;
 width: 11.5ex !important;
 border-right: 2px solid transparent;
}
div.out_prompt_overlay.prompt {
 font-family: "Fira Mono", monospace, monospace;
 font-weight: bold !important;
 background-color: #384151;
 border-bottom-left-radius: 2px;
 border-top-right-radius: 0px;
 border-top-left-radius: 0px;
 border-bottom-right-radius: 0px;
 min-width: 11.5ex !important;
 width: 11.5ex !important;
 border-right: 2px solid transparent;
 color: transparent;
}
div.out_prompt_overlay.prompt:hover {
 background-color: #3e4458;
 box-shadow: none !important;
 border: none;
 border-bottom-left-radius: 2px;
 -webkit-border-: 2px;
 -moz-border-radius: 2px;
 border-top-right-radius: 0px;
 border-top-left-radius: 0px;
 min-width: 11.5ex !important;
 width: 11.5ex !important;
 border-right: 2px solid #3e4458 !important;
}
div.cell.code_cell .output_prompt {
 border-right: 2px solid transparent;
 color: transparent;
}
div.cell.selected .output_prompt,
div.cell.selected .out_prompt_overlay.prompt {
 border-left: 5px solid #33517c;
 border-right: 2px solid #384151;
 border-radius: 0px !important;
}
.edit_mode div.cell.selected .output_prompt,
.edit_mode div.cell.selected .out_prompt_overlay.prompt {
 border-left: 5px solid #33517c;
 border-right: 2px solid #384151;
 border-radius: 0px !important;
}
div.text_cell,
div.text_cell_render pre,
div.text_cell_render {
 font-family: sans-serif;
 font-size: 13pt;
 line-height: 130% !important;
 color: #abc1e2;
 background: #303845;
 background-color: #303845;
 border-radius: 0px;
}
div .text_cell_render {
 padding: 0.4em 0.4em 0.4em 0.4em;
}
div.cell.text_cell .CodeMirror-lines {
 padding-top: .7em !important;
 padding-bottom: .4em !important;
 padding-left: .5em !important;
 padding-right: .5em !important;
 margin-top: .4em;
 margin-bottom: .3em;
}
div.cell.text_cell.unrendered div.input_area,
div.cell.text_cell.rendered div.input_area {
 background-color: #303845;
 background: #303845;
 border: 0px;
 border-radius: 2px;
}
div.cell.text_cell .CodeMirror,
div.cell.text_cell .CodeMirror pre {
 line-height: 170% !important;
}
div.cell.text_cell.rendered.selected {
 font-family: sans-serif;
 line-height: 170% !important;
 background: #303845;
 background-color: #303845;
 border-radius: 0px;
}
div.cell.text_cell.unrendered.selected {
 font-family: sans-serif;
 line-height: 170% !important;
 background: #303845;
 background-color: #303845;
 border-radius: 0px;
}
div.cell.text_cell.selected {
 font-family: sans-serif;
 line-height: 170% !important;
 background: #303845;
 background-color: #303845;
 border-radius: 0px;
}
.edit_mode div.cell.text_cell.selected {
 font-family: sans-serif;
 line-height: 170% !important;
 background: #303845;
 background-color: #303845;
 border-radius: 0px;
}
div.text_cell.unrendered,
div.text_cell.unrendered.selected,
div.edit_mode div.text_cell.unrendered {
 font-family: sans-serif;
 line-height: 170% !important;
 background: #303845;
 background-color: #303845;
 border-radius: 0px;
}
div.cell.text_cell .prompt {
 border-right: 0;
 min-width: 11.5ex !important;
 width: 11.5ex !important;
}
div.cell.text_cell.rendered .prompt {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 9.5pt !important;
 font-weight: normal;
 color: #446489 !important;
 text-align: right !important;
 min-width: 14.5ex !important;
 width: 14.5ex !important;
 background-color: #303845;
 border-right: 2px solid rgba(53,114,198,.5);
 border-left: 4px solid #303845;
}
div.cell.text_cell.unrendered .prompt {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 9.5pt !important;
 font-weight: normal;
 color: #446489 !important;
 text-align: right !important;
 min-width: 14.5ex !important;
 width: 14.5ex !important;
 border-right: 2px solid rgba(53,114,198,.5);
 border-left: 4px solid #303845;
 background-color: #303845;
}
div.cell.text_cell.rendered .prompt {
 border-right: 2px solid rgba(53,114,198,.5);
}
div.cell.text_cell.rendered.selected .prompt {
 top: 0;
 border-left: 4px solid #2769c3;
 border-right: 2px solid rgba(53,114,198,.5);
}
div.text_cell.unrendered.selected .prompt,
div.text_cell.rendered.selected .prompt {
 top: 0;
 background: #303845;
 border-left: 4px solid #33517c;
 border-right: 2px solid rgba(53,114,198,.5);
}
div.rendered_html code {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 13pt;
 padding-top: 3px;
 padding-left: 2px;
 color: #cdd2e9;
 background: #2a313c;
 background-color: #2a313c;
}
pre,
code,
kbd,
samp {
 white-space: pre-wrap;
}
.well code,
code {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 13pt !important;
 line-height: 170% !important;
 color: #abc1e2;
 background: #2a313c;
 background-color: #2a313c;
 border-color: #2a313c;
}
kbd {
 padding: 1px;
 font-size: 13pt;
 font-weight: 800;
 color: #cdd2e9;
 background-color: transparent !important;
 border: 0;
 box-shadow: none;
}
pre {
 display: block;
 padding: 8.5px;
 margin: 0 0 9px;
 font-size: 12.0pt;
 line-height: 1.42857143;
 color: #cdd2e9;
 background-color: #2a313c;
 border: 1px solid #2a313c;
 border-radius: 2px;
}
div.rendered_html {
 color: #abc1e2;
}
.rendered_html * + ul {
 margin-top: .4em;
 margin-bottom: .3em;
}
.rendered_html * + p {
 margin-top: .5em;
 margin-bottom: .5em;
}
div.rendered_html pre {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 13pt !important;
 line-height: 170% !important;
 color: #abc1e2 !important;
 background: #2a313c;
 background-color: #2a313c;
 max-width: 80%;
 border-radius: 0px;
 border-left: 3px solid #2a313c;
 max-width: 80%;
 border-radius: 0px;
 padding-left: 5px;
 margin-left: 6px;
}
div.text_cell_render pre,
div.text_cell_render code {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 13pt !important;
 line-height: 170% !important;
 color: #abc1e2;
 background: #262931;
 background-color: #262931;
 max-width: 80%;
 border-radius: 0px;
 border-left: none;
}
div.text_cell_render pre {
 border-left: 3px solid #3572c6 !important;
 max-width: 80%;
 border-radius: 0px;
 padding-left: 5px;
 margin-left: 6px;
}
div.text_cell_render h1,
div.rendered_html h1,
div.text_cell_render h2,
div.rendered_html h2,
div.text_cell_render h3,
div.rendered_html h3,
div.text_cell_render h4,
div.rendered_html h4,
div.text_cell_render h5,
div.rendered_html h5 {
 font-family: sans-serif;
 margin: 0.4em .2em .3em .2em !important;
}
.rendered_html h1:first-child,
.rendered_html h2:first-child,
.rendered_html h3:first-child,
.rendered_html h4:first-child,
.rendered_html h5:first-child,
.rendered_html h6:first-child {
 margin-top: 0.2em !important;
 margin-bottom: 0.2em !important;
}
.rendered_html h1,
.text_cell_render h1 {
 color: #4c8be2 !important;
 font-size: 200%;
 text-align: left;
 font-style: normal;
 font-weight: normal;
}
.rendered_html h2,
.text_cell_render h2 {
 color: #4c8be2 !important;
 font-size: 170%;
 font-style: normal;
 font-weight: normal;
}
.rendered_html h3,
.text_cell_render h3 {
 color: #4c8be2 !important;
 font-size: 140%;
 font-style: normal;
 font-weight: normal;
}
.rendered_html h4,
.text_cell_render h4 {
 color: #4c8be2 !important;
 font-size: 110%;
 font-style: normal;
 font-weight: normal;
}
.rendered_html h5,
.text_cell_render h5 {
 color: #4c8be2 !important;
 font-size: 100%;
 font-style: normal;
 font-weight: normal;
}
hr {
 margin-top: 8px;
 margin-bottom: 10px;
 border: 0;
 border-top: 1px solid #4c8be2;
}
.rendered_html hr {
 color: #4c8be2;
 background-color: #4c8be2;
 margin-right: 2em;
}
#complete > select > option:hover {
 background: #343d4b;
 background-color: #343d4b;
}
div#_vivaldi-spatnav-focus-indicator._vivaldi-spatnav-focus-indicator {
 position: absolute;
 z-index: 9999999999;
 top: 0px;
 left: 0px;
 box-shadow: none;
 pointer-events: none;
 border-radius: 2px;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
 text-align: left;
 vertical-align: middle;
 padding: 0.42em 0.47em;
 line-height: normal;
 white-space: normal;
 max-width: none;
 border: none;
}
.rendered_html td {
 font-family: sans-serif !important;
 font-size: 11pt;
}
.rendered_html table {
 font-family: sans-serif !important;
 margin-left: 8px;
 margin-right: auto;
 border: none;
 border-collapse: collapse;
 border-spacing: 0;
 color: #dbdfef;
 table-layout: fixed;
}
.rendered_html thead {
 font-family: sans-serif !important;
 font-size: 12.0pt !important;
 background: #2e3642;
 color: #d4d8ec;
 border-bottom: 1px solid #2e3642;
 vertical-align: bottom;
}
.rendered_html tbody tr:nth-child(odd) {
 background: #434d61;
}
.rendered_html tbody tr {
 background: #3d4658;
}
.rendered_html tbody tr:hover:nth-child(odd) {
 background: #414b5e;
}
.rendered_html tbody tr:hover {
 background: #3b4355;
}
.rendered_html * + table {
 margin-top: .05em;
}
div.widget-area {
 background-color: #384151;
 background: #384151;
 color: #cdd2e9;
}
div.widget-area a {
 font-family: sans-serif;
 font-size: 12.0pt;
 font-weight: normal;
 font-style: normal;
 color: #a2b0c7;
 text-shadow: none !important;
}
div.widget-area a:hover,
div.widget-area a:focus {
 font-family: sans-serif;
 font-size: 12.0pt;
 font-weight: normal;
 font-style: normal;
 color: #d8dcee;
 background: rgba(80,92,133,.22);
 background-color: rgba(80,92,133,.22);
 border-color: transparent;
 background-image: none;
 text-shadow: none !important;
}
div.widget_item.btn-group > button.btn.btn-default.widget-combo-btn,
div.widget_item.btn-group > button.btn.btn-default.widget-combo-btn:hover {
 background: #2c343f;
 background-color: #2c343f;
 border: 2px solid #2c343f !important;
 font-size: inherit;
 z-index: 0;
}
div.jupyter-widgets.widget-hprogress.widget-hbox {
 display: inline-table !important;
 width: 38% !important;
 margin-left: 10px;
}
div.jupyter-widgets.widget-hprogress.widget-hbox .widget-label,
div.widget-hbox .widget-label,
.widget-hbox .widget-label,
.widget-inline-hbox .widget-label,
div.widget-label {
 text-align: -webkit-auto !important;
 margin-left: 15px !important;
 max-width: 240px !important;
 min-width: 100px !important;
 vertical-align: text-top !important;
 color: #cdd2e9 !important;
 font-size: 14px !important;
}
.widget-hprogress .progress {
 flex-grow: 1;
 height: 20px;
 margin-top: auto;
 margin-left: 12px;
 margin-bottom: auto;
 width: 300px;
}
.progress {
 overflow: hidden;
 height: 22px;
 margin-bottom: 10px;
 padding-left: 10px;
 background-color: #546379 !important;
 border-radius: 2px;
 -webkit-box-shadow: none;
 box-shadow: none;
 z-index: 10;
}
.progress-bar-danger {
 background-color: #e74c3c !important;
}
.progress-bar-info {
 background-color: #3498db !important;
}
.progress-bar-warning {
 background-color: #ff914d !important;
}
.progress-bar-success {
 background-color: #83a83b !important;
}
.widget-select select {
 margin-left: 12px;
}
.rendered_html :link {
 font-family: sans-serif;
 font-size: 100%;
 color: #4c8be2;
 text-decoration: underline;
}
.rendered_html :visited,
.rendered_html :visited:active,
.rendered_html :visited:focus {
 color: #6297e0;
}
.rendered_html :visited:hover,
.rendered_html :link:hover {
 font-family: sans-serif;
 font-size: 100%;
 color: #1671ef;
}
div.cell.text_cell a.anchor-link:link {
 font-size: inherit;
 text-decoration: none;
 padding: 0px 20px;
 visibility: none;
 color: rgba(0,0,0,.32);
}
div.cell.text_cell a.anchor-link:link:hover {
 font-size: inherit;
 color: #61afef;
}
.navbar-text {
 margin-top: 4px;
 margin-bottom: 0px;
}
#clusters > a {
 color: #61afef;
 text-decoration: underline;
 cursor: auto;
}
#clusters > a:hover {
 color: #4c8be2;
 text-decoration: underline;
 cursor: auto;
}
#nbextensions-configurator-container > div.row.container-fluid.nbext-selector > h3 {
 font-size: 17px;
 margin-top: 5px;
 margin-bottom: 8px;
 height: 24px;
 padding: 4px 0 4px 0;
}
div#nbextensions-configurator-container.container,
#nbextensions-configurator-container.container {
 width: 100%;
 margin-right: auto;
 margin-left: auto;
}
div.nbext-selector > nav > .nav > li > a {
 font-family: sans-serif;
 font-size: 10.5pt;
 padding: 2px 5px;
}
div.nbext-selector > nav > .nav > li > a:hover {
 background: transparent;
}
div.nbext-selector > nav > .nav > li:hover {
 background-color: rgba(80,92,133,.22) !important;
 background: rgba(80,92,133,.22) !important;
}
div.nbext-selector > nav > .nav > li.active:hover {
 background: transparent !important;
 background-color: transparent !important;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:active,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
 color: #e4e8ee;
 background-color: rgba(80,92,133,.22) !important;
 background: rgba(80,92,133,.22) !important;
 -webkit-backface-visibility: hidden;
 -webkit-font-smoothing: subpixel-antialiased !important;
}
div.nbext-readme > .nbext-readme-contents > .rendered_html {
 font-family: sans-serif;
 font-size: 11.5pt;
 line-height: 145%;
 padding: 1em 1em;
 color: #abc1e2;
 background-color: #303845;
 -webkit-box-shadow: none;
 -moz-box-shadow: none;
 box-shadow: none;
}
.nbext-icon,
.nbext-desc,
.nbext-compat-div,
.nbext-enable-btns,
.nbext-params {
 margin-bottom: 8px;
 font-size: 11.5pt;
}
div.nbext-readme > .nbext-readme-contents {
 padding: 0;
 overflow-y: hidden;
}
div.nbext-readme > .nbext-readme-contents:not(:empty) {
 margin-top: 0.5em;
 margin-bottom: 2em;
 border: none;
 border-top-color: rgba(53,114,198,.2);
}
.nbext-showhide-incompat {
 padding-bottom: 0.5em;
 color: #92a2bd;
 font-size: 10.5pt;
}
.nbext-filter-menu.dropdown-menu > li > a:hover,
.nbext-filter-menu.dropdown-menu > li > a:focus,
.nbext-filter-menu.dropdown-menu > li > a.ui-state-focus {
 color: #d8dcee !important;
 background-color: #343d4b !important;
 background: #343d4b !important;
 border-color: #343d4b !important;
}
.nbext-filter-input-wrap > .nbext-filter-input-subwrap,
.nbext-filter-input-wrap > .nbext-filter-input-subwrap > input {
 border: none;
 outline: none;
 background-color: transparent;
 padding: 0;
 vertical-align: middle;
 margin-top: -2px;
}
span.rendered_html code {
 background-color: transparent;
 color: #a2b0c7;
}
#nbextensions-configurator-container > div.row.container-fluid.nbext-selector {
 padding-left: 0px;
 padding-right: 0px;
}
.nbext-filter-menu {
 max-height: 55vh !important;
 overflow-y: auto;
 outline: none;
 border: none;
}
.nbext-filter-menu:hover {
 border: none;
}
.alert-warning {
 background-color: #384152;
 border-color: #384152;
 color: #a2b0c7;
}
.notification_widget.danger {
 color: #ffffff;
 background-color: #e74c3c;
 border-color: #e74c3c;
 padding-right: 5px;
}
#nbextensions-configurator-container > div.nbext-buttons.tree-buttons.no-padding.pull-right > span > button {
 border: none !important;
}
button#refresh_running_list {
 border: none !important;
}
mark,
.mark {
 background-color: #303845;
 color: #abc1e2;
 padding: .15em;
}
a.text-warning,
a.text-warning:hover {
 color: #546379;
}
a.text-warning.bg-warning {
 background-color: #262931;
}
span.bg-success.text-success {
 background-color: transparent;
 color: #48a667;
}
span.bg-danger.text-danger {
 background-color: #262931;
 color: #dc6972;
}
.has-success .input-group-addon {
 color: #48a667;
 border-color: transparent;
 background: inherit;
 background-color: rgba(83,180,115,.10);
}
.has-success .form-control {
 border-color: #48a667;
 -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,0.025);
 box-shadow: inset 0 1px 1px rgba(0,0,0,0.025);
}
.has-error .input-group-addon {
 color: #dc6972;
 border-color: transparent;
 background: inherit;
 background-color: rgba(192,57,67,.10);
}
.has-error .form-control {
 border-color: #dc6972;
 -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,0.025);
 box-shadow: inset 0 1px 1px rgba(0,0,0,0.025);
}
.kse-input-group-pretty > kbd {
 font-family: "Fira Mono", monospace, monospace;
 color: #a2b0c7;
 font-weight: normal;
 background: transparent;
}
.kse-input-group-pretty > kbd {
 font-family: "Fira Mono", monospace, monospace;
 color: #a2b0c7;
 font-weight: normal;
 background: transparent;
}
div.nbext-enable-btns .btn[disabled],
div.nbext-enable-btns .btn[disabled]:hover,
.btn-default.disabled,
.btn-default[disabled] {
 background: #38424f;
 background-color: #38424f;
 color: #98a8c1;
}
label#Keyword-Filter {
 display: none;
}
.input-group .nbext-list-btn-add,
.input-group-btn:last-child > .btn-group > .btn {
 background: #2e3642;
 background-color: #2e3642;
 border-color: #2e3642;
 border: 2px solid #2e3642;
}
.input-group .nbext-list-btn-add:hover,
.input-group-btn:last-child > .btn-group > .btn:hover {
 background: #2a313c;
 background-color: #2a313c;
 border-color: #2a313c;
 border: 2px solid #2a313c;
}
#notebook-container > div.cell.code_cell.rendered.selected > div.widget-area > div.widget-subarea > div > div.widget_item.btn-group > button.btn.btn-default.dropdown-toggle.widget-combo-carrot-btn {
 background: #2e3642;
 background-color: #2e3642;
 border-color: #2e3642;
}
#notebook-container > div.cell.code_cell.rendered.selected > div.widget-area > div.widget-subarea > div > div.widget_item.btn-group > button.btn.btn-default.dropdown-toggle.widget-combo-carrot-btn:hover {
 background: #2a313c;
 background-color: #2a313c;
 border-color: #2a313c;
}
.ui-widget-content {
 background: #3a4452;
 background-color: #3a4452;
 border: 2px solid #3a4452;
 color: #a2b0c7;
}
div.collapsible_headings_toggle {
 color: rgba(80,92,133,.45) !important;
}
div.collapsible_headings_toggle:hover {
 color: #4c8be2 !important;
}
.collapsible_headings_toggle .h1,
.collapsible_headings_toggle .h2,
.collapsible_headings_toggle .h3,
.collapsible_headings_toggle .h4,
.collapsible_headings_toggle .h5,
.collapsible_headings_toggle .h6 {
 margin: 0.3em .4em 0em 0em !important;
 line-height: 1.2 !important;
}
div.collapsible_headings_toggle .fa-caret-down:before,
div.collapsible_headings_toggle .fa-caret-right:before {
 font-size: xx-large;
 transition: transform 1000ms;
 transform: none !important;
}
.collapsible_headings_collapsed.collapsible_headings_ellipsis .rendered_html h1:after,
.collapsible_headings_collapsed.collapsible_headings_ellipsis .rendered_html h2:after,
.collapsible_headings_collapsed.collapsible_headings_ellipsis .rendered_html h3:after,
.collapsible_headings_collapsed.collapsible_headings_ellipsis .rendered_html h4:after,
.collapsible_headings_collapsed.collapsible_headings_ellipsis .rendered_html h5:after,
.collapsible_headings_collapsed.collapsible_headings_ellipsis .rendered_html h6:after {
 position: absolute;
 right: 0;
 bottom: 20% !important;
 content: "[\002026]";
 color: rgba(80,92,133,.45) !important;
 padding: 0.5em 0em 0em 0em !important;
}
.collapsible_headings_ellipsis .rendered_html h1,
.collapsible_headings_ellipsis .rendered_html h2,
.collapsible_headings_ellipsis .rendered_html h3,
.collapsible_headings_ellipsis .rendered_html h4,
.collapsible_headings_ellipsis .rendered_html h5,
.collapsible_headings_ellipsis .rendered_html h6,
.collapsible_headings_toggle .fa {
 transition: transform 1000ms !important;
 -webkit-transform: inherit !important;
 -moz-transform: inherit !important;
 -ms-transform: inherit !important;
 -o-transform: inherit !important;
 transform: inherit !important;
 padding-right: 0px !important;
}
#toc-wrapper {
 z-index: 90;
 position: fixed !important;
 display: flex;
 flex-direction: column;
 overflow: hidden;
 padding: 10px;
 border-style: solid;
 border-width: thin;
 border-right-width: medium !important;
 background-color: #262931 !important;
}
#toc-wrapper.ui-draggable.ui-resizable.sidebar-wrapper {
 border-color: rgba(80,92,133,.22) !important;
}
#toc a,
#navigate_menu a,
.toc {
 color: #a2b0c7 !important;
 font-size: 13pt !important;
}
#toc li > span:hover {
 background-color: #343d4b !important;
}
#toc a:hover,
#navigate_menu a:hover,
.toc {
 color: #e4e8ee !important;
 font-size: 13pt !important;
}
#toc-wrapper .toc-item-num {
 color: #4c8be2 !important;
 font-size: 13pt !important;
}
input.raw_input {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 13pt !important;
 color: #cdd2e9;
 background-color: #2a313c;
 border-color: #282f39;
 background: #282f39;
 width: auto;
 vertical-align: baseline;
 padding: 0em 0.25em;
 margin: 0em 0.25em;
 -webkit-box-shadow: none;
 box-shadow: none;
}
audio,
video {
 display: inline;
 vertical-align: middle;
 align-content: center;
 margin-left: 20%;
}
.cmd-palette .modal-body {
 padding: 0px;
 margin: 0px;
}
.cmd-palette form {
 background: #293547;
 background-color: #293547;
}
.typeahead-field input:last-child,
.typeahead-hint {
 background: #293547;
 background-color: #293547;
 z-index: 1;
}
.typeahead-field input {
 font-family: sans-serif;
 color: #cdd2e9;
 border: none;
 font-size: 28pt;
 display: inline-block;
 line-height: inherit;
 padding: 3px 10px;
 height: 70px;
}
.typeahead-select {
 background-color: #293547;
}
body > div.modal.cmd-palette.typeahead-field {
 display: table;
 border-collapse: separate;
 background-color: #2b3850;
}
.typeahead-container button {
 font-family: sans-serif;
 font-size: 28pt;
 background-color: #2e3642;
 border: none;
 display: inline-block;
 line-height: inherit;
 padding: 3px 10px;
 height: 70px;
}
.typeahead-search-icon {
 min-width: 40px;
 min-height: 55px;
 display: block;
 vertical-align: middle;
 text-align: center;
}
.typeahead-container button:focus,
.typeahead-container button:hover {
 color: #d8dcee;
 background-color: #2a313c;
 border-color: #363f4c;
}
.typeahead-list > li.typeahead-group.active > a,
.typeahead-list > li.typeahead-group > a,
.typeahead-list > li.typeahead-group > a:focus,
.typeahead-list > li.typeahead-group > a:hover {
 display: none;
}
.typeahead-dropdown > li > a,
.typeahead-list > li > a {
 color: #a2b0c7;
 text-decoration: none;
}
.typeahead-dropdown,
.typeahead-list {
 font-family: sans-serif;
 font-size: 13pt;
 color: #a2b0c7;
 background-color: #202937;
 border: none;
 background-clip: padding-box;
 margin-top: 0px;
 padding: 3px 2px 3px 0px;
 line-height: 1.7;
}
.typeahead-dropdown > li.active > a,
.typeahead-dropdown > li > a:focus,
.typeahead-dropdown > li > a:hover,
.typeahead-list > li.active > a,
.typeahead-list > li > a:focus,
.typeahead-list > li > a:hover {
 color: #d8dcee;
 background-color: #2b3850;
 border-color: #2b3850;
}
.command-shortcut:before {
 content: "(command)";
 padding-right: 3px;
 color: #546379;
}
.edit-shortcut:before {
 content: "(edit)";
 padding-right: 3px;
 color: #546379;
}
ul.typeahead-list i {
 margin-left: 1px;
 width: 18px;
 margin-right: 10px;
}
ul.typeahead-list {
 max-height: 50vh;
 overflow: auto;
}
.typeahead-list > li {
 position: relative;
 border: none;
}
div.input.typeahead-hint,
input.typeahead-hint,
body > div.modal.cmd-palette.in > div > div > div > form > div > div.typeahead-field > span.typeahead-query > input.typeahead-hint {
 color: #546379 !important;
 background-color: transparent;
 padding: 3px 10px;
}
.typeahead-dropdown > li > a,
.typeahead-list > li > a {
 display: block;
 padding: 5px;
 clear: both;
 font-weight: 400;
 line-height: 1.7;
 border: 1px solid #202937;
 border-bottom-color: rgba(80,92,133,.45);
}
body > div.modal.cmd-palette.in > div {
 min-width: 750px;
 margin: 150px auto;
}
.typeahead-container strong {
 font-weight: bolder;
 color: #4c8be2;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
 color: #ffffff;
 background-color: #2769c3;
 border-color: #2769c3;
 border-style: solid;
 border-width: 1px;
 border-radius: 0px;
}
#find-and-replace #replace-preview .replace .match {
 background-color: #dc6972;
 border-color: #dc6972;
 border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
 background-color: #48a667;
 border-color: #48a667;
 border-radius: 0px;
}
.jupyter-dashboard-menu-item.selected::before {
 font-family: 'FontAwesome' !important;
 content: '\f00c' !important;
 position: absolute !important;
 color: #4c8be2 !important;
 left: 0px !important;
 top: 13px !important;
 font-size: 12px !important;
}
.shortcut_key,
span.shortcut_key {
 display: inline-block;
 width: 16ex;
 text-align: right;
 font-family: monospace;
}
.jupyter-keybindings {
 padding: 1px;
 line-height: 24px;
 border-bottom: 1px solid rgba(80,92,133,.22);
}
.jupyter-keybindings i {
 background: #2a313c;
 font-size: small;
 padding: 5px;
 margin-left: 7px;
}
div#short-key-bindings-intro.well,
.well {
 background-color: #2e3642;
 border: 1px solid #2e3642;
 color: #a2b0c7;
 border-radius: 2px;
 -webkit-box-shadow: none;
 box-shadow: none;
}
#texteditor-backdrop {
 background: #262931;
 background-color: #262931;
}
#texteditor-backdrop #texteditor-container .CodeMirror-gutter,
#texteditor-backdrop #texteditor-container .CodeMirror-gutters {
 background: #343c4b;
 background-color: #343c4b;
 color: #667fb1;
}
.edit_app #menubar .navbar {
 margin-bottom: 0px;
}
#texteditor-backdrop #texteditor-container {
 padding: 0px;
 background-color: #303845;
 box-shadow: none;
}
.terminal-app {
 background: #262931;
}
.terminal-app > #header {
 background: #262931;
}
.terminal-app .terminal {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 13pt;
 line-height: 170%;
 color: #cdd2e9;
 background: #303845;
 padding: 0.4em;
 border-radius: 2px;
 -webkit-box-shadow: none;
 box-shadow: none;
}
.terminal .xterm-viewport {
 background-color: #303845;
 color: #cdd2e9;
 overflow-y: auto;
}
.terminal .xterm-color-0 {
 color: #4c8be2;
}
.terminal .xterm-color-1 {
 color: #e39194;
}
.terminal .xterm-color-2 {
 color: #caa6ec;
}
.terminal .xterm-color-3 {
 color: #e39194;
}
.terminal .xterm-color-4 {
 color: #efaa8e;
}
.terminal .xterm-color-5 {
 color: #8fca9a;
}
.terminal .xterm-color-6 {
 color: #77abe7;
}
.terminal .xterm-color-7 {
 color: #77abe7;
}
.terminal .xterm-color-8 {
 color: #61afef;
}
.terminal .xterm-color-9 {
 color: #8fca9a;
}
.terminal .xterm-color-10 {
 color: #e39194;
}
.terminal .xterm-color-14 {
 color: #77abe7;
}
.terminal .xterm-bg-color-15 {
 background-color: #303845;
}
.terminal:not(.xterm-cursor-style-underline):not(.xterm-cursor-style-bar) .terminal-cursor {
 background-color: #4c8be2;
 color: #303845;
}
.terminal:not(.focus) .terminal-cursor {
 outline: 1px solid #4c8be2;
 outline-offset: -1px;
}
.celltoolbar {
 font-size: 100%;
 padding-top: 3px;
 border-color: transparent;
 border-bottom: thin solid rgba(53,114,198,.2);
 background: transparent;
}
.cell-tag,
.tags-input input,
.tags-input button {
 color: #a2b0c7;
 background-color: #262931;
 background-image: none;
 border: 1px solid #a2b0c7;
 border-radius: 1px;
 box-shadow: none;
 width: inherit;
 font-size: inherit;
 height: 22px;
 line-height: 22px;
}
#notebook-container > div.cell.code_cell.rendered.selected > div.input > div.inner_cell > div.ctb_hideshow.ctb_show > div > div > button,
#notebook-container > div.input > div.inner_cell > div.ctb_hideshow.ctb_show > div > div > button {
 font-size: 10pt;
 color: #a2b0c7;
 background-color: #262931;
 background-image: none;
 border: 1px solid #a2b0c7;
 border-radius: 1px;
 box-shadow: none;
 width: inherit;
 font-size: inherit;
 height: 22px;
 line-height: 22px;
}
div#pager #pager-contents {
 background: #262931 !important;
 background-color: #262931 !important;
}
div#pager pre {
 color: #cdd2e9 !important;
 background: #303845 !important;
 background-color: #303845 !important;
 padding: 0.4em;
}
div#pager .ui-resizable-handle {
 top: 0px;
 height: 8px;
 background: #4c8be2 !important;
 border-top: 1px solid #4c8be2;
 border-bottom: 1px solid #4c8be2;
}
div.CodeMirror,
div.CodeMirror pre {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 13pt;
 line-height: 170%;
 color: #cdd2e9;
}
div.CodeMirror-lines {
 padding-bottom: .9em;
 padding-left: .5em;
 padding-right: 1.5em;
 padding-top: .7em;
}
span.ansiblack,
.ansi-black-fg {
 color: #2b303b;
}
span.ansiblue,
.ansi-blue-fg,
.ansi-blue-intense-fg {
 color: #61afef;
}
span.ansigray,
.ansi-gray-fg,
.ansi-gray-intense-fg {
 color: #899ab8;
}
span.ansigreen,
.ansi-green-fg {
 color: #8fca9a;
}
.ansi-green-intense-fg {
 color: #899ab8;
}
span.ansipurple,
.ansi-purple-fg,
.ansi-purple-intense-fg {
 color: #b399ef;
}
span.ansicyan,
.ansi-cyan-fg,
.ansi-cyan-intense-fg {
 color: #b399ef;
}
span.ansiyellow,
.ansi-yellow-fg,
.ansi-yellow-intense-fg {
 color: #ddd7a3;
}
span.ansired,
.ansi-red-fg,
.ansi-red-intense-fg {
 color: #e39194;
}
div.output-stderr {
 background-color: #e39194;
}
div.output-stderr pre {
 color: #d0d4e6;
}
div.js-error {
 color: #e39194;
}
.ipython_tooltip {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 13pt;
 line-height: 170%;
 border: 2px solid #2b333f;
 background: #3c4657;
 background-color: #3c4657;
 border-radius: 2px;
 overflow-x: visible;
 overflow-y: visible;
 box-shadow: none;
 position: absolute;
 z-index: 1000;
}
.ipython_tooltip .tooltiptext pre {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 13pt;
 line-height: 170%;
 background: #3c4657;
 background-color: #3c4657;
 color: #cdd2e9;
 overflow-x: visible;
 overflow-y: visible;
 max-width: 900px;
}
div#tooltip.ipython_tooltip {
 overflow-x: wrap;
 overflow-y: visible;
 max-width: 800px;
}
div.tooltiptext.bigtooltip {
 overflow-x: visible;
 overflow-y: scroll;
 height: 400px;
 max-width: 800px;
}
.cm-s-ipython.CodeMirror {
 font-family: "Fira Mono", monospace, monospace;
 font-size: 13pt;
 background: #303845;
 color: #cdd2e9;
 border-radius: 2px;
 font-style: normal;
 font-weight: normal;
}
.cm-s-ipython div.CodeMirror-selected {
 background: #42495e;
}
.CodeMirror-gutters {
 border: none;
 border-right: 1px solid #343c4b !important;
 background-color: #343c4b !important;
 background: #343c4b !important;
 border-radius: 0px;
 white-space: nowrap;
}
.cm-s-ipython .CodeMirror-gutters {
 background: #343c4b;
 border: none;
 border-radius: 0px;
 width: 36px;
}
.cm-s-ipython .CodeMirror-linenumber {
 color: #667fb1;
}
.CodeMirror-sizer {
 margin-left: 40px;
}
.CodeMirror-linenumber,
div.CodeMirror-linenumber,
.CodeMirror-gutter.CodeMirror-linenumberdiv.CodeMirror-gutter.CodeMirror-linenumber {
 padding-right: 1px;
 margin-left: 0px;
 margin: 0px;
 width: 26px !important;
 padding: 0px;
 text-align: right;
}
.CodeMirror-linenumber {
 color: #667fb1;
}
.cm-s-ipython .CodeMirror-cursor {
 border-left: 2px solid #0095ff !important;
}
.cm-s-ipython span.cm-comment {
 color: #667fb1;
 font-style: italic;
}
.cm-s-ipython span.cm-atom {
 color: #caa6ec;
}
.cm-s-ipython span.cm-number {
 color: #efaa8e;
}
.cm-s-ipython span.cm-property {
 color: #cdd2e9;
}
.cm-s-ipython span.cm-attribute {
 color: #cdd2e9;
}
.cm-s-ipython span.cm-keyword {
 color: #caa6ec;
 font-weight: normal;
}
.cm-s-ipython span.cm-string {
 color: #8fca9a;
}
.cm-s-ipython span.cm-meta {
 color: #ddd7a3;
}
.cm-s-ipython span.cm-operator {
 color: #77abe7;
}
.cm-s-ipython span.cm-builtin {
 color: #e39194;
}
.cm-s-ipython span.cm-variable {
 color: #cdd2e9;
}
.cm-s-ipython span.cm-variable-2 {
 color: #e39194;
}
.cm-s-ipython span.cm-variable-3 {
 color: #ddd7a3;
}
.cm-s-ipython span.cm-def {
 color: #77abe7;
 font-weight: normal;
}
.cm-s-ipython span.cm-error {
 background: rgba(191,97,106,.4);
}
.cm-s-ipython span.cm-tag {
 color: #caa6ec;
}
.cm-s-ipython span.cm-link {
 color: #61afef;
}
.cm-s-ipython span.cm-storage {
 color: #caa6ec;
}
.cm-s-ipython span.cm-entity {
 color: #e39194;
}
.cm-s-ipython span.cm-quote {
 color: #8fca9a;
}
div.CodeMirror span.CodeMirror-matchingbracket {
 color: #ffffff;
 font-weight: bold;
 background-color: #4c8be2;
}
div.CodeMirror span.CodeMirror-nonmatchingbracket {
 color: #ffffff;
 font-weight: bold;
 background: rgba(191,97,106,.4) !important;
}
.cm-header-1 {
 font-size: 215%;
}
.cm-header-2 {
 font-size: 180%;
}
.cm-header-3 {
 font-size: 150%;
}
.cm-header-4 {
 font-size: 120%;
}
.cm-header-5 {
 font-size: 100%;
}
.cm-s-default .cm-hr {
 color: #77abe7;
}
div.cell.text_cell .cm-s-default .cm-header {
 font-family: sans-serif;
 font-weight: normal;
 color: #4c8be2 !important;
 margin-top: 0.3em !important;
 margin-bottom: 0.3em !important;
}
div.cell.text_cell .cm-s-default span.cm-variable-2 {
 color: #abc1e2 !important;
}
div.cell.text_cell .cm-s-default span.cm-variable-3 {
 color: #ddd7a3 !important;
}
.cm-s-default span.cm-comment {
 color: #667fb1 !important;
}
.cm-s-default .cm-tag {
 color: #8fb36a;
}
.cm-s-default .cm-builtin {
 color: #e39194;
}
.cm-s-default .cm-string {
 color: #8fca9a;
}
.cm-s-default .cm-keyword {
 color: #caa6ec;
}
.cm-s-default .cm-number {
 color: #efaa8e;
}
.cm-s-default .cm-error {
 color: #caa6ec;
}
.cm-s-default .cm-link {
 color: #61afef;
}
.cm-s-default .cm-atom {
 color: #efaa8e;
}
.cm-s-default .cm-def {
 color: #77abe7;
}
.CodeMirror-cursor {
 border-left: 2px solid #0095ff !important;
 border-right: none;
 width: 0;
}
.cm-s-default div.CodeMirror-selected {
 background: #42495e;
}
.cm-s-default .cm-selected {
 background: #42495e;
}
.MathJax_Display,
.MathJax {
 border: 0 !important;
 font-size: 100% !important;
 text-align: center !important;
 margin: 0em !important;
 line-height: 2.25 !important;
}
.MathJax:focus,
body :focus .MathJax {
 display: inline-block !important;
}
.MathJax:focus,
body :focus .MathJax {
 display: inline-block !important;
}
.completions {
 position: absolute;
 z-index: 110;
 overflow: hidden;
 border: medium solid #3572c6;
 box-shadow: none;
 line-height: 1;
}
.completions select {
 background: #303845;
 background-color: #303845;
 outline: none;
 border: none;
 padding: 0px;
 margin: 0px;
 margin-left: 2px;
 overflow: auto;
 font-family: "Fira Mono", monospace, monospace;
 font-size: 13pt;
 color: #cdd2e9;
 width: auto;
}
div#maintoolbar {
 margin-left: 8px !important;
}
.toolbar.container {
 width: 100% !important;
}
#header-container {
 display: none !important;
}

<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            /*preferredFont: "TeX",*/
            /*availableFonts: ["TeX", "STIX"],*/
            styles: {
                scale: 100,
                ".MathJax_Display": {
                    "font-size": "100%",
                }
            }
        }
    });
</script>
    
  </style>



<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/*
 * Webkit scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar,
[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-corner {
  background: var(--jp-scrollbar-background-color);
}

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-thumb {
  background: rgb(var(--jp-scrollbar-thumb-color));
  border: var(--jp-scrollbar-thumb-margin) solid transparent;
  background-clip: content-box;
  border-radius: var(--jp-scrollbar-thumb-radius);
}

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-track:horizontal {
  border-left: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
  border-right: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
}

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-track:vertical {
  border-top: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
  border-bottom: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar */

[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar::-webkit-scrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar::-webkit-scrollbar,
[data-jp-theme-scrollbars='true']
  .CodeMirror-hscrollbar::-webkit-scrollbar-corner,
[data-jp-theme-scrollbars='true']
  .CodeMirror-vscrollbar::-webkit-scrollbar-corner {
  background-color: transparent;
}

[data-jp-theme-scrollbars='true']
  .CodeMirror-hscrollbar::-webkit-scrollbar-thumb,
[data-jp-theme-scrollbars='true']
  .CodeMirror-vscrollbar::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
  border: var(--jp-scrollbar-thumb-margin) solid transparent;
  background-clip: content-box;
  border-radius: var(--jp-scrollbar-thumb-radius);
}

[data-jp-theme-scrollbars='true']
  .CodeMirror-hscrollbar::-webkit-scrollbar-track:horizontal {
  border-left: var(--jp-scrollbar-endpad) solid transparent;
  border-right: var(--jp-scrollbar-endpad) solid transparent;
}

[data-jp-theme-scrollbars='true']
  .CodeMirror-vscrollbar::-webkit-scrollbar-track:vertical {
  border-top: var(--jp-scrollbar-endpad) solid transparent;
  border-bottom: var(--jp-scrollbar-endpad) solid transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0px solid transparent;
  border-right: 0px solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0px solid transparent;
  border-bottom: 0px solid transparent;
}

/*
 * Phosphor
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-Widget, /* </DEPRECATED> */
.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  cursor: default;
}


/* <DEPRECATED> */ .p-Widget.p-mod-hidden, /* </DEPRECATED> */
.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-CommandPalette, /* </DEPRECATED> */
.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-CommandPalette-search, /* </DEPRECATED> */
.lm-CommandPalette-search {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-CommandPalette-content, /* </DEPRECATED> */
.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}


/* <DEPRECATED> */ .p-CommandPalette-header, /* </DEPRECATED> */
.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}


/* <DEPRECATED> */ .p-CommandPalette-item, /* </DEPRECATED> */
.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}


/* <DEPRECATED> */ .p-CommandPalette-itemIcon, /* </DEPRECATED> */
.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-CommandPalette-itemContent, /* </DEPRECATED> */
.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}


/* <DEPRECATED> */ .p-CommandPalette-itemShortcut, /* </DEPRECATED> */
.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-CommandPalette-itemLabel, /* </DEPRECATED> */
.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
	border:1px solid transparent;
  background-color: transparent;
  position: absolute;
	z-index:1;
	right:3%;
	top: 0;
	bottom: 0;
	margin: auto;
	padding: 7px 0;
	display: none;
	vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
	content: "X";
	display: block;
	width: 15px;
	height: 15px;
	text-align: center;
	color:#000;
	font-weight: normal;
	font-size: 12px;
	cursor: pointer;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-DockPanel, /* </DEPRECATED> */
.lm-DockPanel {
  z-index: 0;
}


/* <DEPRECATED> */ .p-DockPanel-widget, /* </DEPRECATED> */
.lm-DockPanel-widget {
  z-index: 0;
}


/* <DEPRECATED> */ .p-DockPanel-tabBar, /* </DEPRECATED> */
.lm-DockPanel-tabBar {
  z-index: 1;
}


/* <DEPRECATED> */ .p-DockPanel-handle, /* </DEPRECATED> */
.lm-DockPanel-handle {
  z-index: 2;
}


/* <DEPRECATED> */ .p-DockPanel-handle.p-mod-hidden, /* </DEPRECATED> */
.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}


/* <DEPRECATED> */ .p-DockPanel-handle:after, /* </DEPRECATED> */
.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='horizontal'],
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='vertical'],
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='horizontal']:after,
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='vertical']:after,
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}


/* <DEPRECATED> */ .p-DockPanel-overlay, /* </DEPRECATED> */
.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}


/* <DEPRECATED> */ .p-DockPanel-overlay.p-mod-hidden, /* </DEPRECATED> */
.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-Menu, /* </DEPRECATED> */
.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-Menu-content, /* </DEPRECATED> */
.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}


/* <DEPRECATED> */ .p-Menu-item, /* </DEPRECATED> */
.lm-Menu-item {
  display: table-row;
}


/* <DEPRECATED> */
.p-Menu-item.p-mod-hidden,
.p-Menu-item.p-mod-collapsed,
/* </DEPRECATED> */
.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}


/* <DEPRECATED> */
.p-Menu-itemIcon,
.p-Menu-itemSubmenuIcon,
/* </DEPRECATED> */
.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}


/* <DEPRECATED> */ .p-Menu-itemLabel, /* </DEPRECATED> */
.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}


/* <DEPRECATED> */ .p-Menu-itemShortcut, /* </DEPRECATED> */
.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-MenuBar, /* </DEPRECATED> */
.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-MenuBar-content, /* </DEPRECATED> */
.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}


/* <DEPRECATED> */ .p--MenuBar-item, /* </DEPRECATED> */
.lm-MenuBar-item {
  box-sizing: border-box;
}


/* <DEPRECATED> */
.p-MenuBar-itemIcon,
.p-MenuBar-itemLabel,
/* </DEPRECATED> */
.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-ScrollBar, /* </DEPRECATED> */
.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */
.p-ScrollBar[data-orientation='horizontal'],
/* </DEPRECATED> */
.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}


/* <DEPRECATED> */
.p-ScrollBar[data-orientation='vertical'],
/* </DEPRECATED> */
.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}


/* <DEPRECATED> */ .p-ScrollBar-button, /* </DEPRECATED> */
.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-ScrollBar-track, /* </DEPRECATED> */
.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}


/* <DEPRECATED> */ .p-ScrollBar-thumb, /* </DEPRECATED> */
.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-SplitPanel-child, /* </DEPRECATED> */
.lm-SplitPanel-child {
  z-index: 0;
}


/* <DEPRECATED> */ .p-SplitPanel-handle, /* </DEPRECATED> */
.lm-SplitPanel-handle {
  z-index: 1;
}


/* <DEPRECATED> */ .p-SplitPanel-handle.p-mod-hidden, /* </DEPRECATED> */
.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}


/* <DEPRECATED> */ .p-SplitPanel-handle:after, /* </DEPRECATED> */
.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='horizontal'] > .p-SplitPanel-handle,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='vertical'] > .p-SplitPanel-handle,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='horizontal'] > .p-SplitPanel-handle:after,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='vertical'] > .p-SplitPanel-handle:after,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-TabBar, /* </DEPRECATED> */
.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-TabBar[data-orientation='horizontal'], /* </DEPRECATED> */
.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}


/* <DEPRECATED> */ .p-TabBar[data-orientation='vertical'], /* </DEPRECATED> */
.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}


/* <DEPRECATED> */ .p-TabBar-content, /* </DEPRECATED> */
.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}


/* <DEPRECATED> */
.p-TabBar[data-orientation='horizontal'] > .p-TabBar-content,
/* </DEPRECATED> */
.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}


/* <DEPRECATED> */
.p-TabBar[data-orientation='vertical'] > .p-TabBar-content,
/* </DEPRECATED> */
.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}


/* <DEPRECATED> */ .p-TabBar-tab, /* </DEPRECATED> */
.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
}


/* <DEPRECATED> */
.p-TabBar-tabIcon,
.p-TabBar-tabCloseIcon,
/* </DEPRECATED> */
.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-TabBar-tabLabel, /* </DEPRECATED> */
.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}


.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing : border-box;
}


/* <DEPRECATED> */ .p-TabBar-tab.p-mod-hidden, /* </DEPRECATED> */
.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}


.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}


/* <DEPRECATED> */ .p-TabBar.p-mod-dragging .p-TabBar-tab, /* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}


/* <DEPRECATED> */
.p-TabBar.p-mod-dragging[data-orientation='horizontal'] .p-TabBar-tab,
/* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}


/* <DEPRECATED> */
.p-TabBar.p-mod-dragging[data-orientation='vertical'] .p-TabBar-tab,
/* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}


/* <DEPRECATED> */
.p-TabBar.p-mod-dragging .p-TabBar-tab.p-mod-dragging,
/* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing : border-box;
  background: inherit;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-TabPanel-tabBar, /* </DEPRECATED> */
.lm-TabPanel-tabBar {
  z-index: 1;
}


/* <DEPRECATED> */ .p-TabPanel-stackedPanel, /* </DEPRECATED> */
.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

@charset "UTF-8";
html{
  -webkit-box-sizing:border-box;
          box-sizing:border-box; }

*,
*::before,
*::after{
  -webkit-box-sizing:inherit;
          box-sizing:inherit; }

body{
  font-size:14px;
  font-weight:400;
  letter-spacing:0;
  line-height:1.28581;
  text-transform:none;
  color:#182026;
  font-family:-apple-system, "BlinkMacSystemFont", "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Open Sans", "Helvetica Neue", "Icons16", sans-serif; }

p{
  margin-bottom:10px;
  margin-top:0; }

small{
  font-size:12px; }

strong{
  font-weight:600; }

::-moz-selection{
  background:rgba(125, 188, 255, 0.6); }

::selection{
  background:rgba(125, 188, 255, 0.6); }
.bp3-heading{
  color:#182026;
  font-weight:600;
  margin:0 0 10px;
  padding:0; }
  .bp3-dark .bp3-heading{
    color:#f5f8fa; }

h1.bp3-heading, .bp3-running-text h1{
  font-size:36px;
  line-height:40px; }

h2.bp3-heading, .bp3-running-text h2{
  font-size:28px;
  line-height:32px; }

h3.bp3-heading, .bp3-running-text h3{
  font-size:22px;
  line-height:25px; }

h4.bp3-heading, .bp3-running-text h4{
  font-size:18px;
  line-height:21px; }

h5.bp3-heading, .bp3-running-text h5{
  font-size:16px;
  line-height:19px; }

h6.bp3-heading, .bp3-running-text h6{
  font-size:14px;
  line-height:16px; }
.bp3-ui-text{
  font-size:14px;
  font-weight:400;
  letter-spacing:0;
  line-height:1.28581;
  text-transform:none; }

.bp3-monospace-text{
  font-family:monospace;
  text-transform:none; }

.bp3-text-muted{
  color:#5c7080; }
  .bp3-dark .bp3-text-muted{
    color:#a7b6c2; }

.bp3-text-disabled{
  color:rgba(92, 112, 128, 0.6); }
  .bp3-dark .bp3-text-disabled{
    color:rgba(167, 182, 194, 0.6); }

.bp3-text-overflow-ellipsis{
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal; }
.bp3-running-text{
  font-size:14px;
  line-height:1.5; }
  .bp3-running-text h1{
    color:#182026;
    font-weight:600;
    margin-bottom:20px;
    margin-top:40px; }
    .bp3-dark .bp3-running-text h1{
      color:#f5f8fa; }
  .bp3-running-text h2{
    color:#182026;
    font-weight:600;
    margin-bottom:20px;
    margin-top:40px; }
    .bp3-dark .bp3-running-text h2{
      color:#f5f8fa; }
  .bp3-running-text h3{
    color:#182026;
    font-weight:600;
    margin-bottom:20px;
    margin-top:40px; }
    .bp3-dark .bp3-running-text h3{
      color:#f5f8fa; }
  .bp3-running-text h4{
    color:#182026;
    font-weight:600;
    margin-bottom:20px;
    margin-top:40px; }
    .bp3-dark .bp3-running-text h4{
      color:#f5f8fa; }
  .bp3-running-text h5{
    color:#182026;
    font-weight:600;
    margin-bottom:20px;
    margin-top:40px; }
    .bp3-dark .bp3-running-text h5{
      color:#f5f8fa; }
  .bp3-running-text h6{
    color:#182026;
    font-weight:600;
    margin-bottom:20px;
    margin-top:40px; }
    .bp3-dark .bp3-running-text h6{
      color:#f5f8fa; }
  .bp3-running-text hr{
    border:none;
    border-bottom:1px solid rgba(16, 22, 26, 0.15);
    margin:20px 0; }
    .bp3-dark .bp3-running-text hr{
      border-color:rgba(255, 255, 255, 0.15); }
  .bp3-running-text p{
    margin:0 0 10px;
    padding:0; }

.bp3-text-large{
  font-size:16px; }

.bp3-text-small{
  font-size:12px; }
a{
  color:#106ba3;
  text-decoration:none; }
  a:hover{
    color:#106ba3;
    cursor:pointer;
    text-decoration:underline; }
  a .bp3-icon, a .bp3-icon-standard, a .bp3-icon-large{
    color:inherit; }
  a code,
  .bp3-dark a code{
    color:inherit; }
  .bp3-dark a,
  .bp3-dark a:hover{
    color:#48aff0; }
    .bp3-dark a .bp3-icon, .bp3-dark a .bp3-icon-standard, .bp3-dark a .bp3-icon-large,
    .bp3-dark a:hover .bp3-icon,
    .bp3-dark a:hover .bp3-icon-standard,
    .bp3-dark a:hover .bp3-icon-large{
      color:inherit; }
.bp3-running-text code, .bp3-code{
  font-family:monospace;
  text-transform:none;
  background:rgba(255, 255, 255, 0.7);
  border-radius:3px;
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2);
  color:#5c7080;
  font-size:smaller;
  padding:2px 5px; }
  .bp3-dark .bp3-running-text code, .bp3-running-text .bp3-dark code, .bp3-dark .bp3-code{
    background:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
    color:#a7b6c2; }
  .bp3-running-text a > code, a > .bp3-code{
    color:#137cbd; }
    .bp3-dark .bp3-running-text a > code, .bp3-running-text .bp3-dark a > code, .bp3-dark a > .bp3-code{
      color:inherit; }

.bp3-running-text pre, .bp3-code-block{
  font-family:monospace;
  text-transform:none;
  background:rgba(255, 255, 255, 0.7);
  border-radius:3px;
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
  color:#182026;
  display:block;
  font-size:13px;
  line-height:1.4;
  margin:10px 0;
  padding:13px 15px 12px;
  word-break:break-all;
  word-wrap:break-word; }
  .bp3-dark .bp3-running-text pre, .bp3-running-text .bp3-dark pre, .bp3-dark .bp3-code-block{
    background:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }
  .bp3-running-text pre > code, .bp3-code-block > code{
    background:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    color:inherit;
    font-size:inherit;
    padding:0; }

.bp3-running-text kbd, .bp3-key{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  background:#ffffff;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
  color:#5c7080;
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  font-family:inherit;
  font-size:12px;
  height:24px;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  line-height:24px;
  min-width:24px;
  padding:3px 6px;
  vertical-align:middle; }
  .bp3-running-text kbd .bp3-icon, .bp3-key .bp3-icon, .bp3-running-text kbd .bp3-icon-standard, .bp3-key .bp3-icon-standard, .bp3-running-text kbd .bp3-icon-large, .bp3-key .bp3-icon-large{
    margin-right:5px; }
  .bp3-dark .bp3-running-text kbd, .bp3-running-text .bp3-dark kbd, .bp3-dark .bp3-key{
    background:#394b59;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
    color:#a7b6c2; }
.bp3-running-text blockquote, .bp3-blockquote{
  border-left:solid 4px rgba(167, 182, 194, 0.5);
  margin:0 0 10px;
  padding:0 20px; }
  .bp3-dark .bp3-running-text blockquote, .bp3-running-text .bp3-dark blockquote, .bp3-dark .bp3-blockquote{
    border-color:rgba(115, 134, 148, 0.5); }
.bp3-running-text ul,
.bp3-running-text ol, .bp3-list{
  margin:10px 0;
  padding-left:30px; }
  .bp3-running-text ul li:not(:last-child), .bp3-running-text ol li:not(:last-child), .bp3-list li:not(:last-child){
    margin-bottom:5px; }
  .bp3-running-text ul ol, .bp3-running-text ol ol, .bp3-list ol,
  .bp3-running-text ul ul,
  .bp3-running-text ol ul,
  .bp3-list ul{
    margin-top:5px; }

.bp3-list-unstyled{
  list-style:none;
  margin:0;
  padding:0; }
  .bp3-list-unstyled li{
    padding:0; }
.bp3-rtl{
  text-align:right; }

.bp3-dark{
  color:#f5f8fa; }

:focus{
  outline:rgba(19, 124, 189, 0.6) auto 2px;
  outline-offset:2px;
  -moz-outline-radius:6px; }

.bp3-focus-disabled :focus{
  outline:none !important; }
  .bp3-focus-disabled :focus ~ .bp3-control-indicator{
    outline:none !important; }

.bp3-alert{
  max-width:400px;
  padding:20px; }

.bp3-alert-body{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex; }
  .bp3-alert-body .bp3-icon{
    font-size:40px;
    margin-right:20px;
    margin-top:0; }

.bp3-alert-contents{
  word-break:break-word; }

.bp3-alert-footer{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:reverse;
      -ms-flex-direction:row-reverse;
          flex-direction:row-reverse;
  margin-top:10px; }
  .bp3-alert-footer .bp3-button{
    margin-left:10px; }
.bp3-breadcrumbs{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  cursor:default;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-wrap:wrap;
      flex-wrap:wrap;
  height:30px;
  list-style:none;
  margin:0;
  padding:0; }
  .bp3-breadcrumbs > li{
    -webkit-box-align:center;
        -ms-flex-align:center;
            align-items:center;
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex; }
    .bp3-breadcrumbs > li::after{
      background:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M10.71 7.29l-4-4a1.003 1.003 0 00-1.42 1.42L8.59 8 5.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 001.71.71l4-4c.18-.18.29-.43.29-.71 0-.28-.11-.53-.29-.71z' fill='%235C7080'/%3e%3c/svg%3e");
      content:"";
      display:block;
      height:16px;
      margin:0 5px;
      width:16px; }
    .bp3-breadcrumbs > li:last-of-type::after{
      display:none; }

.bp3-breadcrumb,
.bp3-breadcrumb-current,
.bp3-breadcrumbs-collapsed{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  font-size:16px; }

.bp3-breadcrumb,
.bp3-breadcrumbs-collapsed{
  color:#5c7080; }

.bp3-breadcrumb:hover{
  text-decoration:none; }

.bp3-breadcrumb.bp3-disabled{
  color:rgba(92, 112, 128, 0.6);
  cursor:not-allowed; }

.bp3-breadcrumb .bp3-icon{
  margin-right:5px; }

.bp3-breadcrumb-current{
  color:inherit;
  font-weight:600; }
  .bp3-breadcrumb-current .bp3-input{
    font-size:inherit;
    font-weight:inherit;
    vertical-align:baseline; }

.bp3-breadcrumbs-collapsed{
  background:#ced9e0;
  border:none;
  border-radius:3px;
  cursor:pointer;
  margin-right:2px;
  padding:1px 5px;
  vertical-align:text-bottom; }
  .bp3-breadcrumbs-collapsed::before{
    background:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cg fill='%235C7080'%3e%3ccircle cx='2' cy='8.03' r='2'/%3e%3ccircle cx='14' cy='8.03' r='2'/%3e%3ccircle cx='8' cy='8.03' r='2'/%3e%3c/g%3e%3c/svg%3e") center no-repeat;
    content:"";
    display:block;
    height:16px;
    width:16px; }
  .bp3-breadcrumbs-collapsed:hover{
    background:#bfccd6;
    color:#182026;
    text-decoration:none; }

.bp3-dark .bp3-breadcrumb,
.bp3-dark .bp3-breadcrumbs-collapsed{
  color:#a7b6c2; }

.bp3-dark .bp3-breadcrumbs > li::after{
  color:#a7b6c2; }

.bp3-dark .bp3-breadcrumb.bp3-disabled{
  color:rgba(167, 182, 194, 0.6); }

.bp3-dark .bp3-breadcrumb-current{
  color:#f5f8fa; }

.bp3-dark .bp3-breadcrumbs-collapsed{
  background:rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-breadcrumbs-collapsed:hover{
    background:rgba(16, 22, 26, 0.6);
    color:#f5f8fa; }
.bp3-button{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  border:none;
  border-radius:3px;
  cursor:pointer;
  font-size:14px;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  padding:5px 10px;
  text-align:left;
  vertical-align:middle;
  min-height:30px;
  min-width:30px; }
  .bp3-button > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-button > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-button::before,
  .bp3-button > *{
    margin-right:7px; }
  .bp3-button:empty::before,
  .bp3-button > :last-child{
    margin-right:0; }
  .bp3-button:empty{
    padding:0 !important; }
  .bp3-button:disabled, .bp3-button.bp3-disabled{
    cursor:not-allowed; }
  .bp3-button.bp3-fill{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    width:100%; }
  .bp3-button.bp3-align-right,
  .bp3-align-right .bp3-button{
    text-align:right; }
  .bp3-button.bp3-align-left,
  .bp3-align-left .bp3-button{
    text-align:left; }
  .bp3-button:not([class*="bp3-intent-"]){
    background-color:#f5f8fa;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    color:#182026; }
    .bp3-button:not([class*="bp3-intent-"]):hover{
      background-clip:padding-box;
      background-color:#ebf1f5;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
    .bp3-button:not([class*="bp3-intent-"]):active, .bp3-button:not([class*="bp3-intent-"]).bp3-active{
      background-color:#d8e1e8;
      background-image:none;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-button:not([class*="bp3-intent-"]):disabled, .bp3-button:not([class*="bp3-intent-"]).bp3-disabled{
      background-color:rgba(206, 217, 224, 0.5);
      background-image:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(92, 112, 128, 0.6);
      cursor:not-allowed;
      outline:none; }
      .bp3-button:not([class*="bp3-intent-"]):disabled.bp3-active, .bp3-button:not([class*="bp3-intent-"]):disabled.bp3-active:hover, .bp3-button:not([class*="bp3-intent-"]).bp3-disabled.bp3-active, .bp3-button:not([class*="bp3-intent-"]).bp3-disabled.bp3-active:hover{
        background:rgba(206, 217, 224, 0.7); }
  .bp3-button.bp3-intent-primary{
    background-color:#137cbd;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    color:#ffffff; }
    .bp3-button.bp3-intent-primary:hover, .bp3-button.bp3-intent-primary:active, .bp3-button.bp3-intent-primary.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-primary:hover{
      background-color:#106ba3;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-primary:active, .bp3-button.bp3-intent-primary.bp3-active{
      background-color:#0e5a8a;
      background-image:none;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-primary:disabled, .bp3-button.bp3-intent-primary.bp3-disabled{
      background-color:rgba(19, 124, 189, 0.5);
      background-image:none;
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button.bp3-intent-success{
    background-color:#0f9960;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    color:#ffffff; }
    .bp3-button.bp3-intent-success:hover, .bp3-button.bp3-intent-success:active, .bp3-button.bp3-intent-success.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-success:hover{
      background-color:#0d8050;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-success:active, .bp3-button.bp3-intent-success.bp3-active{
      background-color:#0a6640;
      background-image:none;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-success:disabled, .bp3-button.bp3-intent-success.bp3-disabled{
      background-color:rgba(15, 153, 96, 0.5);
      background-image:none;
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button.bp3-intent-warning{
    background-color:#d9822b;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    color:#ffffff; }
    .bp3-button.bp3-intent-warning:hover, .bp3-button.bp3-intent-warning:active, .bp3-button.bp3-intent-warning.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-warning:hover{
      background-color:#bf7326;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-warning:active, .bp3-button.bp3-intent-warning.bp3-active{
      background-color:#a66321;
      background-image:none;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-warning:disabled, .bp3-button.bp3-intent-warning.bp3-disabled{
      background-color:rgba(217, 130, 43, 0.5);
      background-image:none;
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button.bp3-intent-danger{
    background-color:#db3737;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    color:#ffffff; }
    .bp3-button.bp3-intent-danger:hover, .bp3-button.bp3-intent-danger:active, .bp3-button.bp3-intent-danger.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-danger:hover{
      background-color:#c23030;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-danger:active, .bp3-button.bp3-intent-danger.bp3-active{
      background-color:#a82a2a;
      background-image:none;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-danger:disabled, .bp3-button.bp3-intent-danger.bp3-disabled{
      background-color:rgba(219, 55, 55, 0.5);
      background-image:none;
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button[class*="bp3-intent-"] .bp3-button-spinner .bp3-spinner-head{
    stroke:#ffffff; }
  .bp3-button.bp3-large,
  .bp3-large .bp3-button{
    min-height:40px;
    min-width:40px;
    font-size:16px;
    padding:5px 15px; }
    .bp3-button.bp3-large::before,
    .bp3-button.bp3-large > *,
    .bp3-large .bp3-button::before,
    .bp3-large .bp3-button > *{
      margin-right:10px; }
    .bp3-button.bp3-large:empty::before,
    .bp3-button.bp3-large > :last-child,
    .bp3-large .bp3-button:empty::before,
    .bp3-large .bp3-button > :last-child{
      margin-right:0; }
  .bp3-button.bp3-small,
  .bp3-small .bp3-button{
    min-height:24px;
    min-width:24px;
    padding:0 7px; }
  .bp3-button.bp3-loading{
    position:relative; }
    .bp3-button.bp3-loading[class*="bp3-icon-"]::before{
      visibility:hidden; }
    .bp3-button.bp3-loading .bp3-button-spinner{
      margin:0;
      position:absolute; }
    .bp3-button.bp3-loading > :not(.bp3-button-spinner){
      visibility:hidden; }
  .bp3-button[class*="bp3-icon-"]::before{
    font-family:"Icons16", sans-serif;
    font-size:16px;
    font-style:normal;
    font-weight:400;
    line-height:1;
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased;
    color:#5c7080; }
  .bp3-button .bp3-icon, .bp3-button .bp3-icon-standard, .bp3-button .bp3-icon-large{
    color:#5c7080; }
    .bp3-button .bp3-icon.bp3-align-right, .bp3-button .bp3-icon-standard.bp3-align-right, .bp3-button .bp3-icon-large.bp3-align-right{
      margin-left:7px; }
  .bp3-button .bp3-icon:first-child:last-child,
  .bp3-button .bp3-spinner + .bp3-icon:last-child{
    margin:0 -7px; }
  .bp3-dark .bp3-button:not([class*="bp3-intent-"]){
    background-color:#394b59;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):hover, .bp3-dark .bp3-button:not([class*="bp3-intent-"]):active, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-active{
      color:#f5f8fa; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):hover{
      background-color:#30404d;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):active, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-active{
      background-color:#202b33;
      background-image:none;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):disabled, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-disabled{
      background-color:rgba(57, 75, 89, 0.5);
      background-image:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(167, 182, 194, 0.6); }
      .bp3-dark .bp3-button:not([class*="bp3-intent-"]):disabled.bp3-active, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-disabled.bp3-active{
        background:rgba(57, 75, 89, 0.7); }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-button-spinner .bp3-spinner-head{
      background:rgba(16, 22, 26, 0.5);
      stroke:#8a9ba8; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"])[class*="bp3-icon-"]::before{
      color:#a7b6c2; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-icon, .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-icon-standard, .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-icon-large{
      color:#a7b6c2; }
  .bp3-dark .bp3-button[class*="bp3-intent-"]{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-button[class*="bp3-intent-"]:hover{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-button[class*="bp3-intent-"]:active, .bp3-dark .bp3-button[class*="bp3-intent-"].bp3-active{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-dark .bp3-button[class*="bp3-intent-"]:disabled, .bp3-dark .bp3-button[class*="bp3-intent-"].bp3-disabled{
      background-image:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(255, 255, 255, 0.3); }
    .bp3-dark .bp3-button[class*="bp3-intent-"] .bp3-button-spinner .bp3-spinner-head{
      stroke:#8a9ba8; }
  .bp3-button:disabled::before,
  .bp3-button:disabled .bp3-icon, .bp3-button:disabled .bp3-icon-standard, .bp3-button:disabled .bp3-icon-large, .bp3-button.bp3-disabled::before,
  .bp3-button.bp3-disabled .bp3-icon, .bp3-button.bp3-disabled .bp3-icon-standard, .bp3-button.bp3-disabled .bp3-icon-large, .bp3-button[class*="bp3-intent-"]::before,
  .bp3-button[class*="bp3-intent-"] .bp3-icon, .bp3-button[class*="bp3-intent-"] .bp3-icon-standard, .bp3-button[class*="bp3-intent-"] .bp3-icon-large{
    color:inherit !important; }
  .bp3-button.bp3-minimal{
    background:none;
    -webkit-box-shadow:none;
            box-shadow:none; }
    .bp3-button.bp3-minimal:hover{
      background:rgba(167, 182, 194, 0.3);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#182026;
      text-decoration:none; }
    .bp3-button.bp3-minimal:active, .bp3-button.bp3-minimal.bp3-active{
      background:rgba(115, 134, 148, 0.3);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#182026; }
    .bp3-button.bp3-minimal:disabled, .bp3-button.bp3-minimal:disabled:hover, .bp3-button.bp3-minimal.bp3-disabled, .bp3-button.bp3-minimal.bp3-disabled:hover{
      background:none;
      color:rgba(92, 112, 128, 0.6);
      cursor:not-allowed; }
      .bp3-button.bp3-minimal:disabled.bp3-active, .bp3-button.bp3-minimal:disabled:hover.bp3-active, .bp3-button.bp3-minimal.bp3-disabled.bp3-active, .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{
        background:rgba(115, 134, 148, 0.3); }
    .bp3-dark .bp3-button.bp3-minimal{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:inherit; }
      .bp3-dark .bp3-button.bp3-minimal:hover, .bp3-dark .bp3-button.bp3-minimal:active, .bp3-dark .bp3-button.bp3-minimal.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none; }
      .bp3-dark .bp3-button.bp3-minimal:hover{
        background:rgba(138, 155, 168, 0.15); }
      .bp3-dark .bp3-button.bp3-minimal:active, .bp3-dark .bp3-button.bp3-minimal.bp3-active{
        background:rgba(138, 155, 168, 0.3);
        color:#f5f8fa; }
      .bp3-dark .bp3-button.bp3-minimal:disabled, .bp3-dark .bp3-button.bp3-minimal:disabled:hover, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover{
        background:none;
        color:rgba(167, 182, 194, 0.6);
        cursor:not-allowed; }
        .bp3-dark .bp3-button.bp3-minimal:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal:disabled:hover.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{
          background:rgba(138, 155, 168, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-primary{
      color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:hover, .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.15);
        color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:disabled, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(16, 107, 163, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
        stroke:#106ba3; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary{
        color:#48aff0; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:hover{
          background:rgba(19, 124, 189, 0.2);
          color:#48aff0; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{
          background:rgba(19, 124, 189, 0.3);
          color:#48aff0; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{
          background:none;
          color:rgba(72, 175, 240, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{
            background:rgba(19, 124, 189, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-success{
      color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:hover, .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.15);
        color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:disabled, .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(13, 128, 80, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
        stroke:#0d8050; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success{
        color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:hover{
          background:rgba(15, 153, 96, 0.2);
          color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{
          background:rgba(15, 153, 96, 0.3);
          color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{
          background:none;
          color:rgba(61, 204, 145, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{
            background:rgba(15, 153, 96, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-warning{
      color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:hover, .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.15);
        color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:disabled, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(191, 115, 38, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
        stroke:#bf7326; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning{
        color:#ffb366; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:hover{
          background:rgba(217, 130, 43, 0.2);
          color:#ffb366; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{
          background:rgba(217, 130, 43, 0.3);
          color:#ffb366; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{
          background:none;
          color:rgba(255, 179, 102, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{
            background:rgba(217, 130, 43, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-danger{
      color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:hover, .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.15);
        color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:disabled, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(194, 48, 48, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
        stroke:#c23030; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger{
        color:#ff7373; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:hover{
          background:rgba(219, 55, 55, 0.2);
          color:#ff7373; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{
          background:rgba(219, 55, 55, 0.3);
          color:#ff7373; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{
          background:none;
          color:rgba(255, 115, 115, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{
            background:rgba(219, 55, 55, 0.3); }
  .bp3-button.bp3-outlined{
    background:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    border:1px solid rgba(24, 32, 38, 0.2);
    -webkit-box-sizing:border-box;
            box-sizing:border-box; }
    .bp3-button.bp3-outlined:hover{
      background:rgba(167, 182, 194, 0.3);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#182026;
      text-decoration:none; }
    .bp3-button.bp3-outlined:active, .bp3-button.bp3-outlined.bp3-active{
      background:rgba(115, 134, 148, 0.3);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#182026; }
    .bp3-button.bp3-outlined:disabled, .bp3-button.bp3-outlined:disabled:hover, .bp3-button.bp3-outlined.bp3-disabled, .bp3-button.bp3-outlined.bp3-disabled:hover{
      background:none;
      color:rgba(92, 112, 128, 0.6);
      cursor:not-allowed; }
      .bp3-button.bp3-outlined:disabled.bp3-active, .bp3-button.bp3-outlined:disabled:hover.bp3-active, .bp3-button.bp3-outlined.bp3-disabled.bp3-active, .bp3-button.bp3-outlined.bp3-disabled:hover.bp3-active{
        background:rgba(115, 134, 148, 0.3); }
    .bp3-dark .bp3-button.bp3-outlined{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:inherit; }
      .bp3-dark .bp3-button.bp3-outlined:hover, .bp3-dark .bp3-button.bp3-outlined:active, .bp3-dark .bp3-button.bp3-outlined.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none; }
      .bp3-dark .bp3-button.bp3-outlined:hover{
        background:rgba(138, 155, 168, 0.15); }
      .bp3-dark .bp3-button.bp3-outlined:active, .bp3-dark .bp3-button.bp3-outlined.bp3-active{
        background:rgba(138, 155, 168, 0.3);
        color:#f5f8fa; }
      .bp3-dark .bp3-button.bp3-outlined:disabled, .bp3-dark .bp3-button.bp3-outlined:disabled:hover, .bp3-dark .bp3-button.bp3-outlined.bp3-disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-disabled:hover{
        background:none;
        color:rgba(167, 182, 194, 0.6);
        cursor:not-allowed; }
        .bp3-dark .bp3-button.bp3-outlined:disabled.bp3-active, .bp3-dark .bp3-button.bp3-outlined:disabled:hover.bp3-active, .bp3-dark .bp3-button.bp3-outlined.bp3-disabled.bp3-active, .bp3-dark .bp3-button.bp3-outlined.bp3-disabled:hover.bp3-active{
          background:rgba(138, 155, 168, 0.3); }
    .bp3-button.bp3-outlined.bp3-intent-primary{
      color:#106ba3; }
      .bp3-button.bp3-outlined.bp3-intent-primary:hover, .bp3-button.bp3-outlined.bp3-intent-primary:active, .bp3-button.bp3-outlined.bp3-intent-primary.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#106ba3; }
      .bp3-button.bp3-outlined.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.15);
        color:#106ba3; }
      .bp3-button.bp3-outlined.bp3-intent-primary:active, .bp3-button.bp3-outlined.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#106ba3; }
      .bp3-button.bp3-outlined.bp3-intent-primary:disabled, .bp3-button.bp3-outlined.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(16, 107, 163, 0.5); }
        .bp3-button.bp3-outlined.bp3-intent-primary:disabled.bp3-active, .bp3-button.bp3-outlined.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
      .bp3-button.bp3-outlined.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
        stroke:#106ba3; }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary{
        color:#48aff0; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary:hover{
          background:rgba(19, 124, 189, 0.2);
          color:#48aff0; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary:active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary.bp3-active{
          background:rgba(19, 124, 189, 0.3);
          color:#48aff0; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary.bp3-disabled{
          background:none;
          color:rgba(72, 175, 240, 0.5); }
          .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary.bp3-disabled.bp3-active{
            background:rgba(19, 124, 189, 0.3); }
    .bp3-button.bp3-outlined.bp3-intent-success{
      color:#0d8050; }
      .bp3-button.bp3-outlined.bp3-intent-success:hover, .bp3-button.bp3-outlined.bp3-intent-success:active, .bp3-button.bp3-outlined.bp3-intent-success.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#0d8050; }
      .bp3-button.bp3-outlined.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.15);
        color:#0d8050; }
      .bp3-button.bp3-outlined.bp3-intent-success:active, .bp3-button.bp3-outlined.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#0d8050; }
      .bp3-button.bp3-outlined.bp3-intent-success:disabled, .bp3-button.bp3-outlined.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(13, 128, 80, 0.5); }
        .bp3-button.bp3-outlined.bp3-intent-success:disabled.bp3-active, .bp3-button.bp3-outlined.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
      .bp3-button.bp3-outlined.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
        stroke:#0d8050; }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success{
        color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success:hover{
          background:rgba(15, 153, 96, 0.2);
          color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success:active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success.bp3-active{
          background:rgba(15, 153, 96, 0.3);
          color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success.bp3-disabled{
          background:none;
          color:rgba(61, 204, 145, 0.5); }
          .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success.bp3-disabled.bp3-active{
            background:rgba(15, 153, 96, 0.3); }
    .bp3-button.bp3-outlined.bp3-intent-warning{
      color:#bf7326; }
      .bp3-button.bp3-outlined.bp3-intent-warning:hover, .bp3-button.bp3-outlined.bp3-intent-warning:active, .bp3-button.bp3-outlined.bp3-intent-warning.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#bf7326; }
      .bp3-button.bp3-outlined.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.15);
        color:#bf7326; }
      .bp3-button.bp3-outlined.bp3-intent-warning:active, .bp3-button.bp3-outlined.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#bf7326; }
      .bp3-button.bp3-outlined.bp3-intent-warning:disabled, .bp3-button.bp3-outlined.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(191, 115, 38, 0.5); }
        .bp3-button.bp3-outlined.bp3-intent-warning:disabled.bp3-active, .bp3-button.bp3-outlined.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
      .bp3-button.bp3-outlined.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
        stroke:#bf7326; }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning{
        color:#ffb366; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning:hover{
          background:rgba(217, 130, 43, 0.2);
          color:#ffb366; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning:active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning.bp3-active{
          background:rgba(217, 130, 43, 0.3);
          color:#ffb366; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning.bp3-disabled{
          background:none;
          color:rgba(255, 179, 102, 0.5); }
          .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning.bp3-disabled.bp3-active{
            background:rgba(217, 130, 43, 0.3); }
    .bp3-button.bp3-outlined.bp3-intent-danger{
      color:#c23030; }
      .bp3-button.bp3-outlined.bp3-intent-danger:hover, .bp3-button.bp3-outlined.bp3-intent-danger:active, .bp3-button.bp3-outlined.bp3-intent-danger.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#c23030; }
      .bp3-button.bp3-outlined.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.15);
        color:#c23030; }
      .bp3-button.bp3-outlined.bp3-intent-danger:active, .bp3-button.bp3-outlined.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#c23030; }
      .bp3-button.bp3-outlined.bp3-intent-danger:disabled, .bp3-button.bp3-outlined.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(194, 48, 48, 0.5); }
        .bp3-button.bp3-outlined.bp3-intent-danger:disabled.bp3-active, .bp3-button.bp3-outlined.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }
      .bp3-button.bp3-outlined.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
        stroke:#c23030; }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger{
        color:#ff7373; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger:hover{
          background:rgba(219, 55, 55, 0.2);
          color:#ff7373; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger:active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger.bp3-active{
          background:rgba(219, 55, 55, 0.3);
          color:#ff7373; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger.bp3-disabled{
          background:none;
          color:rgba(255, 115, 115, 0.5); }
          .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger.bp3-disabled.bp3-active{
            background:rgba(219, 55, 55, 0.3); }
    .bp3-button.bp3-outlined:disabled, .bp3-button.bp3-outlined.bp3-disabled, .bp3-button.bp3-outlined:disabled:hover, .bp3-button.bp3-outlined.bp3-disabled:hover{
      border-color:rgba(92, 112, 128, 0.1); }
    .bp3-dark .bp3-button.bp3-outlined{
      border-color:rgba(255, 255, 255, 0.4); }
      .bp3-dark .bp3-button.bp3-outlined:disabled, .bp3-dark .bp3-button.bp3-outlined:disabled:hover, .bp3-dark .bp3-button.bp3-outlined.bp3-disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-disabled:hover{
        border-color:rgba(255, 255, 255, 0.2); }
    .bp3-button.bp3-outlined.bp3-intent-primary{
      border-color:rgba(16, 107, 163, 0.6); }
      .bp3-button.bp3-outlined.bp3-intent-primary:disabled, .bp3-button.bp3-outlined.bp3-intent-primary.bp3-disabled{
        border-color:rgba(16, 107, 163, 0.2); }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary{
        border-color:rgba(72, 175, 240, 0.6); }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary.bp3-disabled{
          border-color:rgba(72, 175, 240, 0.2); }
    .bp3-button.bp3-outlined.bp3-intent-success{
      border-color:rgba(13, 128, 80, 0.6); }
      .bp3-button.bp3-outlined.bp3-intent-success:disabled, .bp3-button.bp3-outlined.bp3-intent-success.bp3-disabled{
        border-color:rgba(13, 128, 80, 0.2); }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success{
        border-color:rgba(61, 204, 145, 0.6); }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success.bp3-disabled{
          border-color:rgba(61, 204, 145, 0.2); }
    .bp3-button.bp3-outlined.bp3-intent-warning{
      border-color:rgba(191, 115, 38, 0.6); }
      .bp3-button.bp3-outlined.bp3-intent-warning:disabled, .bp3-button.bp3-outlined.bp3-intent-warning.bp3-disabled{
        border-color:rgba(191, 115, 38, 0.2); }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning{
        border-color:rgba(255, 179, 102, 0.6); }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning.bp3-disabled{
          border-color:rgba(255, 179, 102, 0.2); }
    .bp3-button.bp3-outlined.bp3-intent-danger{
      border-color:rgba(194, 48, 48, 0.6); }
      .bp3-button.bp3-outlined.bp3-intent-danger:disabled, .bp3-button.bp3-outlined.bp3-intent-danger.bp3-disabled{
        border-color:rgba(194, 48, 48, 0.2); }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger{
        border-color:rgba(255, 115, 115, 0.6); }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger.bp3-disabled{
          border-color:rgba(255, 115, 115, 0.2); }

a.bp3-button{
  text-align:center;
  text-decoration:none;
  -webkit-transition:none;
  transition:none; }
  a.bp3-button, a.bp3-button:hover, a.bp3-button:active{
    color:#182026; }
  a.bp3-button.bp3-disabled{
    color:rgba(92, 112, 128, 0.6); }

.bp3-button-text{
  -webkit-box-flex:0;
      -ms-flex:0 1 auto;
          flex:0 1 auto; }

.bp3-button.bp3-align-left .bp3-button-text, .bp3-button.bp3-align-right .bp3-button-text,
.bp3-button-group.bp3-align-left .bp3-button-text,
.bp3-button-group.bp3-align-right .bp3-button-text{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto; }
.bp3-button-group{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex; }
  .bp3-button-group .bp3-button{
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    position:relative;
    z-index:4; }
    .bp3-button-group .bp3-button:focus{
      z-index:5; }
    .bp3-button-group .bp3-button:hover{
      z-index:6; }
    .bp3-button-group .bp3-button:active, .bp3-button-group .bp3-button.bp3-active{
      z-index:7; }
    .bp3-button-group .bp3-button:disabled, .bp3-button-group .bp3-button.bp3-disabled{
      z-index:3; }
    .bp3-button-group .bp3-button[class*="bp3-intent-"]{
      z-index:9; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:focus{
        z-index:10; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:hover{
        z-index:11; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:active, .bp3-button-group .bp3-button[class*="bp3-intent-"].bp3-active{
        z-index:12; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:disabled, .bp3-button-group .bp3-button[class*="bp3-intent-"].bp3-disabled{
        z-index:8; }
  .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:first-child) .bp3-button,
  .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:first-child){
    border-bottom-left-radius:0;
    border-top-left-radius:0; }
  .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button,
  .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:last-child){
    border-bottom-right-radius:0;
    border-top-right-radius:0;
    margin-right:-1px; }
  .bp3-button-group.bp3-minimal .bp3-button{
    background:none;
    -webkit-box-shadow:none;
            box-shadow:none; }
    .bp3-button-group.bp3-minimal .bp3-button:hover{
      background:rgba(167, 182, 194, 0.3);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#182026;
      text-decoration:none; }
    .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-active{
      background:rgba(115, 134, 148, 0.3);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#182026; }
    .bp3-button-group.bp3-minimal .bp3-button:disabled, .bp3-button-group.bp3-minimal .bp3-button:disabled:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{
      background:none;
      color:rgba(92, 112, 128, 0.6);
      cursor:not-allowed; }
      .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{
        background:rgba(115, 134, 148, 0.3); }
    .bp3-dark .bp3-button-group.bp3-minimal .bp3-button{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:inherit; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover{
        background:rgba(138, 155, 168, 0.15); }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{
        background:rgba(138, 155, 168, 0.3);
        color:#f5f8fa; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{
        background:none;
        color:rgba(167, 182, 194, 0.6);
        cursor:not-allowed; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{
          background:rgba(138, 155, 168, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{
      color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.15);
        color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(16, 107, 163, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
        stroke:#106ba3; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{
        color:#48aff0; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{
          background:rgba(19, 124, 189, 0.2);
          color:#48aff0; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{
          background:rgba(19, 124, 189, 0.3);
          color:#48aff0; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{
          background:none;
          color:rgba(72, 175, 240, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{
            background:rgba(19, 124, 189, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{
      color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.15);
        color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(13, 128, 80, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
        stroke:#0d8050; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{
        color:#3dcc91; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{
          background:rgba(15, 153, 96, 0.2);
          color:#3dcc91; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{
          background:rgba(15, 153, 96, 0.3);
          color:#3dcc91; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{
          background:none;
          color:rgba(61, 204, 145, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{
            background:rgba(15, 153, 96, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{
      color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.15);
        color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(191, 115, 38, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
        stroke:#bf7326; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{
        color:#ffb366; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{
          background:rgba(217, 130, 43, 0.2);
          color:#ffb366; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{
          background:rgba(217, 130, 43, 0.3);
          color:#ffb366; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{
          background:none;
          color:rgba(255, 179, 102, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{
            background:rgba(217, 130, 43, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{
      color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.15);
        color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(194, 48, 48, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
        stroke:#c23030; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{
        color:#ff7373; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{
          background:rgba(219, 55, 55, 0.2);
          color:#ff7373; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{
          background:rgba(219, 55, 55, 0.3);
          color:#ff7373; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{
          background:none;
          color:rgba(255, 115, 115, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{
            background:rgba(219, 55, 55, 0.3); }
  .bp3-button-group .bp3-popover-wrapper,
  .bp3-button-group .bp3-popover-target{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-button-group.bp3-fill{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    width:100%; }
  .bp3-button-group .bp3-button.bp3-fill,
  .bp3-button-group.bp3-fill .bp3-button:not(.bp3-fixed){
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-button-group.bp3-vertical{
    -webkit-box-align:stretch;
        -ms-flex-align:stretch;
            align-items:stretch;
    -webkit-box-orient:vertical;
    -webkit-box-direction:normal;
        -ms-flex-direction:column;
            flex-direction:column;
    vertical-align:top; }
    .bp3-button-group.bp3-vertical.bp3-fill{
      height:100%;
      width:unset; }
    .bp3-button-group.bp3-vertical .bp3-button{
      margin-right:0 !important;
      width:100%; }
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:first-child .bp3-button,
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:first-child{
      border-radius:3px 3px 0 0; }
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:last-child .bp3-button,
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:last-child{
      border-radius:0 0 3px 3px; }
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button,
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:not(:last-child){
      margin-bottom:-1px; }
  .bp3-button-group.bp3-align-left .bp3-button{
    text-align:left; }
  .bp3-dark .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button,
  .bp3-dark .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:last-child){
    margin-right:1px; }
  .bp3-dark .bp3-button-group.bp3-vertical > .bp3-popover-wrapper:not(:last-child) .bp3-button,
  .bp3-dark .bp3-button-group.bp3-vertical > .bp3-button:not(:last-child){
    margin-bottom:1px; }
.bp3-callout{
  font-size:14px;
  line-height:1.5;
  background-color:rgba(138, 155, 168, 0.15);
  border-radius:3px;
  padding:10px 12px 9px;
  position:relative;
  width:100%; }
  .bp3-callout[class*="bp3-icon-"]{
    padding-left:40px; }
    .bp3-callout[class*="bp3-icon-"]::before{
      font-family:"Icons20", sans-serif;
      font-size:20px;
      font-style:normal;
      font-weight:400;
      line-height:1;
      -moz-osx-font-smoothing:grayscale;
      -webkit-font-smoothing:antialiased;
      color:#5c7080;
      left:10px;
      position:absolute;
      top:10px; }
  .bp3-callout.bp3-callout-icon{
    padding-left:40px; }
    .bp3-callout.bp3-callout-icon > .bp3-icon:first-child{
      color:#5c7080;
      left:10px;
      position:absolute;
      top:10px; }
  .bp3-callout .bp3-heading{
    line-height:20px;
    margin-bottom:5px;
    margin-top:0; }
    .bp3-callout .bp3-heading:last-child{
      margin-bottom:0; }
  .bp3-dark .bp3-callout{
    background-color:rgba(138, 155, 168, 0.2); }
    .bp3-dark .bp3-callout[class*="bp3-icon-"]::before{
      color:#a7b6c2; }
  .bp3-callout.bp3-intent-primary{
    background-color:rgba(19, 124, 189, 0.15); }
    .bp3-callout.bp3-intent-primary[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-primary > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-primary .bp3-heading{
      color:#106ba3; }
    .bp3-dark .bp3-callout.bp3-intent-primary{
      background-color:rgba(19, 124, 189, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-primary[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-primary > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-primary .bp3-heading{
        color:#48aff0; }
  .bp3-callout.bp3-intent-success{
    background-color:rgba(15, 153, 96, 0.15); }
    .bp3-callout.bp3-intent-success[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-success > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-success .bp3-heading{
      color:#0d8050; }
    .bp3-dark .bp3-callout.bp3-intent-success{
      background-color:rgba(15, 153, 96, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-success[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-success > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-success .bp3-heading{
        color:#3dcc91; }
  .bp3-callout.bp3-intent-warning{
    background-color:rgba(217, 130, 43, 0.15); }
    .bp3-callout.bp3-intent-warning[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-warning > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-warning .bp3-heading{
      color:#bf7326; }
    .bp3-dark .bp3-callout.bp3-intent-warning{
      background-color:rgba(217, 130, 43, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-warning[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-warning > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-warning .bp3-heading{
        color:#ffb366; }
  .bp3-callout.bp3-intent-danger{
    background-color:rgba(219, 55, 55, 0.15); }
    .bp3-callout.bp3-intent-danger[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-danger > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-danger .bp3-heading{
      color:#c23030; }
    .bp3-dark .bp3-callout.bp3-intent-danger{
      background-color:rgba(219, 55, 55, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-danger[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-danger > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-danger .bp3-heading{
        color:#ff7373; }
  .bp3-running-text .bp3-callout{
    margin:20px 0; }
.bp3-card{
  background-color:#ffffff;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
  padding:20px;
  -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-card.bp3-dark,
  .bp3-dark .bp3-card{
    background-color:#30404d;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); }

.bp3-elevation-0{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); }
  .bp3-elevation-0.bp3-dark,
  .bp3-dark .bp3-elevation-0{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); }

.bp3-elevation-1{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-1.bp3-dark,
  .bp3-dark .bp3-elevation-1{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-elevation-2{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 1px 1px rgba(16, 22, 26, 0.2), 0 2px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 1px 1px rgba(16, 22, 26, 0.2), 0 2px 6px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-2.bp3-dark,
  .bp3-dark .bp3-elevation-2{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.4), 0 2px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.4), 0 2px 6px rgba(16, 22, 26, 0.4); }

.bp3-elevation-3{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-3.bp3-dark,
  .bp3-dark .bp3-elevation-3{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }

.bp3-elevation-4{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-4.bp3-dark,
  .bp3-dark .bp3-elevation-4{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); }

.bp3-card.bp3-interactive:hover{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  cursor:pointer; }
  .bp3-card.bp3-interactive:hover.bp3-dark,
  .bp3-dark .bp3-card.bp3-interactive:hover{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }

.bp3-card.bp3-interactive:active{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
  opacity:0.9;
  -webkit-transition-duration:0;
          transition-duration:0; }
  .bp3-card.bp3-interactive:active.bp3-dark,
  .bp3-dark .bp3-card.bp3-interactive:active{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-collapse{
  height:0;
  overflow-y:hidden;
  -webkit-transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-collapse .bp3-collapse-body{
    -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-collapse .bp3-collapse-body[aria-hidden="true"]{
      display:none; }

.bp3-context-menu .bp3-popover-target{
  display:block; }

.bp3-context-menu-popover-target{
  position:fixed; }

.bp3-divider{
  border-bottom:1px solid rgba(16, 22, 26, 0.15);
  border-right:1px solid rgba(16, 22, 26, 0.15);
  margin:5px; }
  .bp3-dark .bp3-divider{
    border-color:rgba(16, 22, 26, 0.4); }
.bp3-dialog-container{
  opacity:1;
  -webkit-transform:scale(1);
          transform:scale(1);
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  min-height:100%;
  pointer-events:none;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none;
  width:100%; }
  .bp3-dialog-container.bp3-overlay-enter > .bp3-dialog, .bp3-dialog-container.bp3-overlay-appear > .bp3-dialog{
    opacity:0;
    -webkit-transform:scale(0.5);
            transform:scale(0.5); }
  .bp3-dialog-container.bp3-overlay-enter-active > .bp3-dialog, .bp3-dialog-container.bp3-overlay-appear-active > .bp3-dialog{
    opacity:1;
    -webkit-transform:scale(1);
            transform:scale(1);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:opacity, -webkit-transform;
    transition-property:opacity, -webkit-transform;
    transition-property:opacity, transform;
    transition-property:opacity, transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); }
  .bp3-dialog-container.bp3-overlay-exit > .bp3-dialog{
    opacity:1;
    -webkit-transform:scale(1);
            transform:scale(1); }
  .bp3-dialog-container.bp3-overlay-exit-active > .bp3-dialog{
    opacity:0;
    -webkit-transform:scale(0.5);
            transform:scale(0.5);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:opacity, -webkit-transform;
    transition-property:opacity, -webkit-transform;
    transition-property:opacity, transform;
    transition-property:opacity, transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); }

.bp3-dialog{
  background:#ebf1f5;
  border-radius:6px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin:30px 0;
  padding-bottom:20px;
  pointer-events:all;
  -webkit-user-select:text;
     -moz-user-select:text;
      -ms-user-select:text;
          user-select:text;
  width:500px; }
  .bp3-dialog:focus{
    outline:0; }
  .bp3-dialog.bp3-dark,
  .bp3-dark .bp3-dialog{
    background:#293742;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }

.bp3-dialog-header{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  background:#ffffff;
  border-radius:6px 6px 0 0;
  -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
          box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  min-height:40px;
  padding-left:20px;
  padding-right:5px;
  z-index:30; }
  .bp3-dialog-header .bp3-icon-large,
  .bp3-dialog-header .bp3-icon{
    color:#5c7080;
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    margin-right:10px; }
  .bp3-dialog-header .bp3-heading{
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    line-height:inherit;
    margin:0; }
    .bp3-dialog-header .bp3-heading:last-child{
      margin-right:20px; }
  .bp3-dark .bp3-dialog-header{
    background:#30404d;
    -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.4);
            box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-dialog-header .bp3-icon-large,
    .bp3-dark .bp3-dialog-header .bp3-icon{
      color:#a7b6c2; }

.bp3-dialog-body{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  line-height:18px;
  margin:20px; }

.bp3-dialog-footer{
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  margin:0 20px; }

.bp3-dialog-footer-actions{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-pack:end;
      -ms-flex-pack:end;
          justify-content:flex-end; }
  .bp3-dialog-footer-actions .bp3-button{
    margin-left:10px; }
.bp3-multistep-dialog-panels{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex; }

.bp3-multistep-dialog-left-panel{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:1;
      -ms-flex:1;
          flex:1;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column; }
  .bp3-dark .bp3-multistep-dialog-left-panel{
    background:#202b33; }

.bp3-multistep-dialog-right-panel{
  background-color:#f5f8fa;
  border-left:1px solid rgba(16, 22, 26, 0.15);
  border-radius:0 0 6px 0;
  -webkit-box-flex:3;
      -ms-flex:3;
          flex:3;
  min-width:0; }
  .bp3-dark .bp3-multistep-dialog-right-panel{
    background-color:#293742;
    border-left:1px solid rgba(16, 22, 26, 0.4); }

.bp3-multistep-dialog-footer{
  background-color:#ffffff;
  border-radius:0 0 6px 0;
  border-top:1px solid rgba(16, 22, 26, 0.15);
  padding:10px; }
  .bp3-dark .bp3-multistep-dialog-footer{
    background:#30404d;
    border-top:1px solid rgba(16, 22, 26, 0.4); }

.bp3-dialog-step-container{
  background-color:#f5f8fa;
  border-bottom:1px solid rgba(16, 22, 26, 0.15); }
  .bp3-dark .bp3-dialog-step-container{
    background:#293742;
    border-bottom:1px solid rgba(16, 22, 26, 0.4); }
  .bp3-dialog-step-container.bp3-dialog-step-viewed{
    background-color:#ffffff; }
    .bp3-dark .bp3-dialog-step-container.bp3-dialog-step-viewed{
      background:#30404d; }

.bp3-dialog-step{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  background-color:#f5f8fa;
  border-radius:6px;
  cursor:not-allowed;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  margin:4px;
  padding:6px 14px; }
  .bp3-dark .bp3-dialog-step{
    background:#293742; }
  .bp3-dialog-step-viewed .bp3-dialog-step{
    background-color:#ffffff;
    cursor:pointer; }
    .bp3-dark .bp3-dialog-step-viewed .bp3-dialog-step{
      background:#30404d; }
  .bp3-dialog-step:hover{
    background-color:#f5f8fa; }
    .bp3-dark .bp3-dialog-step:hover{
      background:#293742; }

.bp3-dialog-step-icon{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  background-color:rgba(92, 112, 128, 0.6);
  border-radius:50%;
  color:#ffffff;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  height:25px;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  width:25px; }
  .bp3-dark .bp3-dialog-step-icon{
    background-color:rgba(167, 182, 194, 0.6); }
  .bp3-active.bp3-dialog-step-viewed .bp3-dialog-step-icon{
    background-color:#2b95d6; }
  .bp3-dialog-step-viewed .bp3-dialog-step-icon{
    background-color:#8a9ba8; }

.bp3-dialog-step-title{
  color:rgba(92, 112, 128, 0.6);
  -webkit-box-flex:1;
      -ms-flex:1;
          flex:1;
  padding-left:10px; }
  .bp3-dark .bp3-dialog-step-title{
    color:rgba(167, 182, 194, 0.6); }
  .bp3-active.bp3-dialog-step-viewed .bp3-dialog-step-title{
    color:#2b95d6; }
  .bp3-dialog-step-viewed:not(.bp3-active) .bp3-dialog-step-title{
    color:#182026; }
    .bp3-dark .bp3-dialog-step-viewed:not(.bp3-active) .bp3-dialog-step-title{
      color:#f5f8fa; }
.bp3-drawer{
  background:#ffffff;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin:0;
  padding:0; }
  .bp3-drawer:focus{
    outline:0; }
  .bp3-drawer.bp3-position-top{
    height:50%;
    left:0;
    right:0;
    top:0; }
    .bp3-drawer.bp3-position-top.bp3-overlay-enter, .bp3-drawer.bp3-position-top.bp3-overlay-appear{
      -webkit-transform:translateY(-100%);
              transform:translateY(-100%); }
    .bp3-drawer.bp3-position-top.bp3-overlay-enter-active, .bp3-drawer.bp3-position-top.bp3-overlay-appear-active{
      -webkit-transform:translateY(0);
              transform:translateY(0);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-drawer.bp3-position-top.bp3-overlay-exit{
      -webkit-transform:translateY(0);
              transform:translateY(0); }
    .bp3-drawer.bp3-position-top.bp3-overlay-exit-active{
      -webkit-transform:translateY(-100%);
              transform:translateY(-100%);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-drawer.bp3-position-bottom{
    bottom:0;
    height:50%;
    left:0;
    right:0; }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-enter, .bp3-drawer.bp3-position-bottom.bp3-overlay-appear{
      -webkit-transform:translateY(100%);
              transform:translateY(100%); }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-enter-active, .bp3-drawer.bp3-position-bottom.bp3-overlay-appear-active{
      -webkit-transform:translateY(0);
              transform:translateY(0);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-exit{
      -webkit-transform:translateY(0);
              transform:translateY(0); }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-exit-active{
      -webkit-transform:translateY(100%);
              transform:translateY(100%);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-drawer.bp3-position-left{
    bottom:0;
    left:0;
    top:0;
    width:50%; }
    .bp3-drawer.bp3-position-left.bp3-overlay-enter, .bp3-drawer.bp3-position-left.bp3-overlay-appear{
      -webkit-transform:translateX(-100%);
              transform:translateX(-100%); }
    .bp3-drawer.bp3-position-left.bp3-overlay-enter-active, .bp3-drawer.bp3-position-left.bp3-overlay-appear-active{
      -webkit-transform:translateX(0);
              transform:translateX(0);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-drawer.bp3-position-left.bp3-overlay-exit{
      -webkit-transform:translateX(0);
              transform:translateX(0); }
    .bp3-drawer.bp3-position-left.bp3-overlay-exit-active{
      -webkit-transform:translateX(-100%);
              transform:translateX(-100%);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-drawer.bp3-position-right{
    bottom:0;
    right:0;
    top:0;
    width:50%; }
    .bp3-drawer.bp3-position-right.bp3-overlay-enter, .bp3-drawer.bp3-position-right.bp3-overlay-appear{
      -webkit-transform:translateX(100%);
              transform:translateX(100%); }
    .bp3-drawer.bp3-position-right.bp3-overlay-enter-active, .bp3-drawer.bp3-position-right.bp3-overlay-appear-active{
      -webkit-transform:translateX(0);
              transform:translateX(0);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-drawer.bp3-position-right.bp3-overlay-exit{
      -webkit-transform:translateX(0);
              transform:translateX(0); }
    .bp3-drawer.bp3-position-right.bp3-overlay-exit-active{
      -webkit-transform:translateX(100%);
              transform:translateX(100%);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
  .bp3-position-right):not(.bp3-vertical){
    bottom:0;
    right:0;
    top:0;
    width:50%; }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-enter, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-appear{
      -webkit-transform:translateX(100%);
              transform:translateX(100%); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-enter-active, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-appear-active{
      -webkit-transform:translateX(0);
              transform:translateX(0);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-exit{
      -webkit-transform:translateX(0);
              transform:translateX(0); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-exit-active{
      -webkit-transform:translateX(100%);
              transform:translateX(100%);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
  .bp3-position-right).bp3-vertical{
    bottom:0;
    height:50%;
    left:0;
    right:0; }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-enter, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-appear{
      -webkit-transform:translateY(100%);
              transform:translateY(100%); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-enter-active, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-appear-active{
      -webkit-transform:translateY(0);
              transform:translateY(0);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-exit{
      -webkit-transform:translateY(0);
              transform:translateY(0); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-exit-active{
      -webkit-transform:translateY(100%);
              transform:translateY(100%);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-drawer.bp3-dark,
  .bp3-dark .bp3-drawer{
    background:#30404d;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }

.bp3-drawer-header{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  border-radius:0;
  -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
          box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  min-height:40px;
  padding:5px;
  padding-left:20px;
  position:relative; }
  .bp3-drawer-header .bp3-icon-large,
  .bp3-drawer-header .bp3-icon{
    color:#5c7080;
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    margin-right:10px; }
  .bp3-drawer-header .bp3-heading{
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    line-height:inherit;
    margin:0; }
    .bp3-drawer-header .bp3-heading:last-child{
      margin-right:20px; }
  .bp3-dark .bp3-drawer-header{
    -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.4);
            box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-drawer-header .bp3-icon-large,
    .bp3-dark .bp3-drawer-header .bp3-icon{
      color:#a7b6c2; }

.bp3-drawer-body{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  line-height:18px;
  overflow:auto; }

.bp3-drawer-footer{
  -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
          box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  padding:10px 20px;
  position:relative; }
  .bp3-dark .bp3-drawer-footer{
    -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.4); }
.bp3-editable-text{
  cursor:text;
  display:inline-block;
  max-width:100%;
  position:relative;
  vertical-align:top;
  white-space:nowrap; }
  .bp3-editable-text::before{
    bottom:-3px;
    left:-3px;
    position:absolute;
    right:-3px;
    top:-3px;
    border-radius:3px;
    content:"";
    -webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-editable-text:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15); }
  .bp3-editable-text.bp3-editable-text-editing::before{
    background-color:#ffffff;
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-disabled::before{
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-editable-text.bp3-intent-primary .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{
    color:#137cbd; }
  .bp3-editable-text.bp3-intent-primary:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(19, 124, 189, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(19, 124, 189, 0.4); }
  .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-intent-success .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{
    color:#0f9960; }
  .bp3-editable-text.bp3-intent-success:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px rgba(15, 153, 96, 0.4);
            box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px rgba(15, 153, 96, 0.4); }
  .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-intent-warning .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{
    color:#d9822b; }
  .bp3-editable-text.bp3-intent-warning:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px rgba(217, 130, 43, 0.4);
            box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px rgba(217, 130, 43, 0.4); }
  .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-intent-danger .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{
    color:#db3737; }
  .bp3-editable-text.bp3-intent-danger:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px rgba(219, 55, 55, 0.4);
            box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px rgba(219, 55, 55, 0.4); }
  .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-editable-text:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(255, 255, 255, 0.15);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(255, 255, 255, 0.15); }
  .bp3-dark .bp3-editable-text.bp3-editable-text-editing::before{
    background-color:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-disabled::before{
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-dark .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{
    color:#48aff0; }
  .bp3-dark .bp3-editable-text.bp3-intent-primary:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(72, 175, 240, 0), 0 0 0 0 rgba(72, 175, 240, 0), inset 0 0 0 1px rgba(72, 175, 240, 0.4);
            box-shadow:0 0 0 0 rgba(72, 175, 240, 0), 0 0 0 0 rgba(72, 175, 240, 0), inset 0 0 0 1px rgba(72, 175, 240, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #48aff0, 0 0 0 3px rgba(72, 175, 240, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #48aff0, 0 0 0 3px rgba(72, 175, 240, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{
    color:#3dcc91; }
  .bp3-dark .bp3-editable-text.bp3-intent-success:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(61, 204, 145, 0), 0 0 0 0 rgba(61, 204, 145, 0), inset 0 0 0 1px rgba(61, 204, 145, 0.4);
            box-shadow:0 0 0 0 rgba(61, 204, 145, 0), 0 0 0 0 rgba(61, 204, 145, 0), inset 0 0 0 1px rgba(61, 204, 145, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #3dcc91, 0 0 0 3px rgba(61, 204, 145, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #3dcc91, 0 0 0 3px rgba(61, 204, 145, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{
    color:#ffb366; }
  .bp3-dark .bp3-editable-text.bp3-intent-warning:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(255, 179, 102, 0), 0 0 0 0 rgba(255, 179, 102, 0), inset 0 0 0 1px rgba(255, 179, 102, 0.4);
            box-shadow:0 0 0 0 rgba(255, 179, 102, 0), 0 0 0 0 rgba(255, 179, 102, 0), inset 0 0 0 1px rgba(255, 179, 102, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #ffb366, 0 0 0 3px rgba(255, 179, 102, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #ffb366, 0 0 0 3px rgba(255, 179, 102, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{
    color:#ff7373; }
  .bp3-dark .bp3-editable-text.bp3-intent-danger:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(255, 115, 115, 0), 0 0 0 0 rgba(255, 115, 115, 0), inset 0 0 0 1px rgba(255, 115, 115, 0.4);
            box-shadow:0 0 0 0 rgba(255, 115, 115, 0), 0 0 0 0 rgba(255, 115, 115, 0), inset 0 0 0 1px rgba(255, 115, 115, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #ff7373, 0 0 0 3px rgba(255, 115, 115, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #ff7373, 0 0 0 3px rgba(255, 115, 115, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-editable-text-input,
.bp3-editable-text-content{
  color:inherit;
  display:inherit;
  font:inherit;
  letter-spacing:inherit;
  max-width:inherit;
  min-width:inherit;
  position:relative;
  resize:none;
  text-transform:inherit;
  vertical-align:top; }

.bp3-editable-text-input{
  background:none;
  border:none;
  -webkit-box-shadow:none;
          box-shadow:none;
  padding:0;
  white-space:pre-wrap;
  width:100%; }
  .bp3-editable-text-input::-webkit-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-editable-text-input::-moz-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-editable-text-input:-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-editable-text-input::-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-editable-text-input::placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-editable-text-input:focus{
    outline:none; }
  .bp3-editable-text-input::-ms-clear{
    display:none; }

.bp3-editable-text-content{
  overflow:hidden;
  padding-right:2px;
  text-overflow:ellipsis;
  white-space:pre; }
  .bp3-editable-text-editing > .bp3-editable-text-content{
    left:0;
    position:absolute;
    visibility:hidden; }
  .bp3-editable-text-placeholder > .bp3-editable-text-content{
    color:rgba(92, 112, 128, 0.6); }
    .bp3-dark .bp3-editable-text-placeholder > .bp3-editable-text-content{
      color:rgba(167, 182, 194, 0.6); }

.bp3-editable-text.bp3-multiline{
  display:block; }
  .bp3-editable-text.bp3-multiline .bp3-editable-text-content{
    overflow:auto;
    white-space:pre-wrap;
    word-wrap:break-word; }
.bp3-divider{
  border-bottom:1px solid rgba(16, 22, 26, 0.15);
  border-right:1px solid rgba(16, 22, 26, 0.15);
  margin:5px; }
  .bp3-dark .bp3-divider{
    border-color:rgba(16, 22, 26, 0.4); }
.bp3-control-group{
  -webkit-transform:translateZ(0);
          transform:translateZ(0);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:stretch;
      -ms-flex-align:stretch;
          align-items:stretch; }
  .bp3-control-group > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-control-group > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-control-group .bp3-button,
  .bp3-control-group .bp3-html-select,
  .bp3-control-group .bp3-input,
  .bp3-control-group .bp3-select{
    position:relative; }
  .bp3-control-group .bp3-input{
    border-radius:inherit;
    z-index:2; }
    .bp3-control-group .bp3-input:focus{
      border-radius:3px;
      z-index:14; }
    .bp3-control-group .bp3-input[class*="bp3-intent"]{
      z-index:13; }
      .bp3-control-group .bp3-input[class*="bp3-intent"]:focus{
        z-index:15; }
    .bp3-control-group .bp3-input[readonly], .bp3-control-group .bp3-input:disabled, .bp3-control-group .bp3-input.bp3-disabled{
      z-index:1; }
  .bp3-control-group .bp3-input-group[class*="bp3-intent"] .bp3-input{
    z-index:13; }
    .bp3-control-group .bp3-input-group[class*="bp3-intent"] .bp3-input:focus{
      z-index:15; }
  .bp3-control-group .bp3-button,
  .bp3-control-group .bp3-html-select select,
  .bp3-control-group .bp3-select select{
    -webkit-transform:translateZ(0);
            transform:translateZ(0);
    border-radius:inherit;
    z-index:4; }
    .bp3-control-group .bp3-button:focus,
    .bp3-control-group .bp3-html-select select:focus,
    .bp3-control-group .bp3-select select:focus{
      z-index:5; }
    .bp3-control-group .bp3-button:hover,
    .bp3-control-group .bp3-html-select select:hover,
    .bp3-control-group .bp3-select select:hover{
      z-index:6; }
    .bp3-control-group .bp3-button:active,
    .bp3-control-group .bp3-html-select select:active,
    .bp3-control-group .bp3-select select:active{
      z-index:7; }
    .bp3-control-group .bp3-button[readonly], .bp3-control-group .bp3-button:disabled, .bp3-control-group .bp3-button.bp3-disabled,
    .bp3-control-group .bp3-html-select select[readonly],
    .bp3-control-group .bp3-html-select select:disabled,
    .bp3-control-group .bp3-html-select select.bp3-disabled,
    .bp3-control-group .bp3-select select[readonly],
    .bp3-control-group .bp3-select select:disabled,
    .bp3-control-group .bp3-select select.bp3-disabled{
      z-index:3; }
    .bp3-control-group .bp3-button[class*="bp3-intent"],
    .bp3-control-group .bp3-html-select select[class*="bp3-intent"],
    .bp3-control-group .bp3-select select[class*="bp3-intent"]{
      z-index:9; }
      .bp3-control-group .bp3-button[class*="bp3-intent"]:focus,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:focus,
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:focus{
        z-index:10; }
      .bp3-control-group .bp3-button[class*="bp3-intent"]:hover,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:hover,
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:hover{
        z-index:11; }
      .bp3-control-group .bp3-button[class*="bp3-intent"]:active,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:active,
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:active{
        z-index:12; }
      .bp3-control-group .bp3-button[class*="bp3-intent"][readonly], .bp3-control-group .bp3-button[class*="bp3-intent"]:disabled, .bp3-control-group .bp3-button[class*="bp3-intent"].bp3-disabled,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"][readonly],
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:disabled,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"].bp3-disabled,
      .bp3-control-group .bp3-select select[class*="bp3-intent"][readonly],
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:disabled,
      .bp3-control-group .bp3-select select[class*="bp3-intent"].bp3-disabled{
        z-index:8; }
  .bp3-control-group .bp3-input-group > .bp3-icon,
  .bp3-control-group .bp3-input-group > .bp3-button,
  .bp3-control-group .bp3-input-group > .bp3-input-left-container,
  .bp3-control-group .bp3-input-group > .bp3-input-action{
    z-index:16; }
  .bp3-control-group .bp3-select::after,
  .bp3-control-group .bp3-html-select::after,
  .bp3-control-group .bp3-select > .bp3-icon,
  .bp3-control-group .bp3-html-select > .bp3-icon{
    z-index:17; }
  .bp3-control-group .bp3-select:focus-within{
    z-index:5; }
  .bp3-control-group:not(.bp3-vertical) > *:not(.bp3-divider){
    margin-right:-1px; }
  .bp3-control-group:not(.bp3-vertical) > .bp3-divider:not(:first-child){
    margin-left:6px; }
  .bp3-dark .bp3-control-group:not(.bp3-vertical) > *:not(.bp3-divider){
    margin-right:0; }
  .bp3-dark .bp3-control-group:not(.bp3-vertical) > .bp3-button + .bp3-button{
    margin-left:1px; }
  .bp3-control-group .bp3-popover-wrapper,
  .bp3-control-group .bp3-popover-target{
    border-radius:inherit; }
  .bp3-control-group > :first-child{
    border-radius:3px 0 0 3px; }
  .bp3-control-group > :last-child{
    border-radius:0 3px 3px 0;
    margin-right:0; }
  .bp3-control-group > :only-child{
    border-radius:3px;
    margin-right:0; }
  .bp3-control-group .bp3-input-group .bp3-button{
    border-radius:3px; }
  .bp3-control-group .bp3-numeric-input:not(:first-child) .bp3-input-group{
    border-bottom-left-radius:0;
    border-top-left-radius:0; }
  .bp3-control-group.bp3-fill{
    width:100%; }
  .bp3-control-group > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-control-group.bp3-fill > *:not(.bp3-fixed){
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-control-group.bp3-vertical{
    -webkit-box-orient:vertical;
    -webkit-box-direction:normal;
        -ms-flex-direction:column;
            flex-direction:column; }
    .bp3-control-group.bp3-vertical > *{
      margin-top:-1px; }
    .bp3-control-group.bp3-vertical > :first-child{
      border-radius:3px 3px 0 0;
      margin-top:0; }
    .bp3-control-group.bp3-vertical > :last-child{
      border-radius:0 0 3px 3px; }
.bp3-control{
  cursor:pointer;
  display:block;
  margin-bottom:10px;
  position:relative;
  text-transform:none; }
  .bp3-control input:checked ~ .bp3-control-indicator{
    background-color:#137cbd;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    color:#ffffff; }
  .bp3-control:hover input:checked ~ .bp3-control-indicator{
    background-color:#106ba3;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); }
  .bp3-control input:not(:disabled):active:checked ~ .bp3-control-indicator{
    background:#0e5a8a;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-control input:disabled:checked ~ .bp3-control-indicator{
    background:rgba(19, 124, 189, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-dark .bp3-control input:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control:hover input:checked ~ .bp3-control-indicator{
    background-color:#106ba3;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control input:not(:disabled):active:checked ~ .bp3-control-indicator{
    background-color:#0e5a8a;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-control input:disabled:checked ~ .bp3-control-indicator{
    background:rgba(14, 90, 138, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-control:not(.bp3-align-right){
    padding-left:26px; }
    .bp3-control:not(.bp3-align-right) .bp3-control-indicator{
      margin-left:-26px; }
  .bp3-control.bp3-align-right{
    padding-right:26px; }
    .bp3-control.bp3-align-right .bp3-control-indicator{
      margin-right:-26px; }
  .bp3-control.bp3-disabled{
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed; }
  .bp3-control.bp3-inline{
    display:inline-block;
    margin-right:20px; }
  .bp3-control input{
    left:0;
    opacity:0;
    position:absolute;
    top:0;
    z-index:-1; }
  .bp3-control .bp3-control-indicator{
    background-clip:padding-box;
    background-color:#f5f8fa;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
    border:none;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    cursor:pointer;
    display:inline-block;
    font-size:16px;
    height:1em;
    margin-right:10px;
    margin-top:-3px;
    position:relative;
    -webkit-user-select:none;
       -moz-user-select:none;
        -ms-user-select:none;
            user-select:none;
    vertical-align:middle;
    width:1em; }
    .bp3-control .bp3-control-indicator::before{
      content:"";
      display:block;
      height:1em;
      width:1em; }
  .bp3-control:hover .bp3-control-indicator{
    background-color:#ebf1f5; }
  .bp3-control input:not(:disabled):active ~ .bp3-control-indicator{
    background:#d8e1e8;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-control input:disabled ~ .bp3-control-indicator{
    background:rgba(206, 217, 224, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none;
    cursor:not-allowed; }
  .bp3-control input:focus ~ .bp3-control-indicator{
    outline:rgba(19, 124, 189, 0.6) auto 2px;
    outline-offset:2px;
    -moz-outline-radius:6px; }
  .bp3-control.bp3-align-right .bp3-control-indicator{
    float:right;
    margin-left:10px;
    margin-top:1px; }
  .bp3-control.bp3-large{
    font-size:16px; }
    .bp3-control.bp3-large:not(.bp3-align-right){
      padding-left:30px; }
      .bp3-control.bp3-large:not(.bp3-align-right) .bp3-control-indicator{
        margin-left:-30px; }
    .bp3-control.bp3-large.bp3-align-right{
      padding-right:30px; }
      .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{
        margin-right:-30px; }
    .bp3-control.bp3-large .bp3-control-indicator{
      font-size:20px; }
    .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{
      margin-top:0; }
  .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator{
    background-color:#137cbd;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    color:#ffffff; }
  .bp3-control.bp3-checkbox:hover input:indeterminate ~ .bp3-control-indicator{
    background-color:#106ba3;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); }
  .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate ~ .bp3-control-indicator{
    background:#0e5a8a;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{
    background:rgba(19, 124, 189, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-dark .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-checkbox:hover input:indeterminate ~ .bp3-control-indicator{
    background-color:#106ba3;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate ~ .bp3-control-indicator{
    background-color:#0e5a8a;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{
    background:rgba(14, 90, 138, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-control.bp3-checkbox .bp3-control-indicator{
    border-radius:3px; }
  .bp3-control.bp3-checkbox input:checked ~ .bp3-control-indicator::before{
    background-image:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M12 5c-.28 0-.53.11-.71.29L7 9.59l-2.29-2.3a1.003 1.003 0 00-1.42 1.42l3 3c.18.18.43.29.71.29s.53-.11.71-.29l5-5A1.003 1.003 0 0012 5z' fill='white'/%3e%3c/svg%3e"); }
  .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator::before{
    background-image:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M11 7H5c-.55 0-1 .45-1 1s.45 1 1 1h6c.55 0 1-.45 1-1s-.45-1-1-1z' fill='white'/%3e%3c/svg%3e"); }
  .bp3-control.bp3-radio .bp3-control-indicator{
    border-radius:50%; }
  .bp3-control.bp3-radio input:checked ~ .bp3-control-indicator::before{
    background-image:radial-gradient(#ffffff, #ffffff 28%, transparent 32%); }
  .bp3-control.bp3-radio input:checked:disabled ~ .bp3-control-indicator::before{
    opacity:0.5; }
  .bp3-control.bp3-radio input:focus ~ .bp3-control-indicator{
    -moz-outline-radius:16px; }
  .bp3-control.bp3-switch input ~ .bp3-control-indicator{
    background:rgba(167, 182, 194, 0.5); }
  .bp3-control.bp3-switch:hover input ~ .bp3-control-indicator{
    background:rgba(115, 134, 148, 0.5); }
  .bp3-control.bp3-switch input:not(:disabled):active ~ .bp3-control-indicator{
    background:rgba(92, 112, 128, 0.5); }
  .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator{
    background:rgba(206, 217, 224, 0.5); }
    .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator::before{
      background:rgba(255, 255, 255, 0.8); }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator{
    background:#137cbd; }
  .bp3-control.bp3-switch:hover input:checked ~ .bp3-control-indicator{
    background:#106ba3; }
  .bp3-control.bp3-switch input:checked:not(:disabled):active ~ .bp3-control-indicator{
    background:#0e5a8a; }
  .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator{
    background:rgba(19, 124, 189, 0.5); }
    .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator::before{
      background:rgba(255, 255, 255, 0.8); }
  .bp3-control.bp3-switch:not(.bp3-align-right){
    padding-left:38px; }
    .bp3-control.bp3-switch:not(.bp3-align-right) .bp3-control-indicator{
      margin-left:-38px; }
  .bp3-control.bp3-switch.bp3-align-right{
    padding-right:38px; }
    .bp3-control.bp3-switch.bp3-align-right .bp3-control-indicator{
      margin-right:-38px; }
  .bp3-control.bp3-switch .bp3-control-indicator{
    border:none;
    border-radius:1.75em;
    -webkit-box-shadow:none !important;
            box-shadow:none !important;
    min-width:1.75em;
    -webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    width:auto; }
    .bp3-control.bp3-switch .bp3-control-indicator::before{
      background:#ffffff;
      border-radius:50%;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
      height:calc(1em - 4px);
      left:0;
      margin:2px;
      position:absolute;
      -webkit-transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
      transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
      width:calc(1em - 4px); }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator::before{
    left:calc(100% - 1em); }
  .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right){
    padding-left:45px; }
    .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right) .bp3-control-indicator{
      margin-left:-45px; }
  .bp3-control.bp3-switch.bp3-large.bp3-align-right{
    padding-right:45px; }
    .bp3-control.bp3-switch.bp3-large.bp3-align-right .bp3-control-indicator{
      margin-right:-45px; }
  .bp3-dark .bp3-control.bp3-switch input ~ .bp3-control-indicator{
    background:rgba(16, 22, 26, 0.5); }
  .bp3-dark .bp3-control.bp3-switch:hover input ~ .bp3-control-indicator{
    background:rgba(16, 22, 26, 0.7); }
  .bp3-dark .bp3-control.bp3-switch input:not(:disabled):active ~ .bp3-control-indicator{
    background:rgba(16, 22, 26, 0.9); }
  .bp3-dark .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator{
    background:rgba(57, 75, 89, 0.5); }
    .bp3-dark .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator::before{
      background:rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator{
    background:#137cbd; }
  .bp3-dark .bp3-control.bp3-switch:hover input:checked ~ .bp3-control-indicator{
    background:#106ba3; }
  .bp3-dark .bp3-control.bp3-switch input:checked:not(:disabled):active ~ .bp3-control-indicator{
    background:#0e5a8a; }
  .bp3-dark .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator{
    background:rgba(14, 90, 138, 0.5); }
    .bp3-dark .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator::before{
      background:rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-switch .bp3-control-indicator::before{
    background:#394b59;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator::before{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-control.bp3-switch .bp3-switch-inner-text{
    font-size:0.7em;
    text-align:center; }
  .bp3-control.bp3-switch .bp3-control-indicator-child:first-child{
    line-height:0;
    margin-left:0.5em;
    margin-right:1.2em;
    visibility:hidden; }
  .bp3-control.bp3-switch .bp3-control-indicator-child:last-child{
    line-height:1em;
    margin-left:1.2em;
    margin-right:0.5em;
    visibility:visible; }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator .bp3-control-indicator-child:first-child{
    line-height:1em;
    visibility:visible; }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator .bp3-control-indicator-child:last-child{
    line-height:0;
    visibility:hidden; }
  .bp3-dark .bp3-control{
    color:#f5f8fa; }
    .bp3-dark .bp3-control.bp3-disabled{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-control .bp3-control-indicator{
      background-color:#394b59;
      background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
      background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-control:hover .bp3-control-indicator{
      background-color:#30404d; }
    .bp3-dark .bp3-control input:not(:disabled):active ~ .bp3-control-indicator{
      background:#202b33;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-dark .bp3-control input:disabled ~ .bp3-control-indicator{
      background:rgba(57, 75, 89, 0.5);
      -webkit-box-shadow:none;
              box-shadow:none;
      cursor:not-allowed; }
    .bp3-dark .bp3-control.bp3-checkbox input:disabled:checked ~ .bp3-control-indicator, .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{
      color:rgba(167, 182, 194, 0.6); }
.bp3-file-input{
  cursor:pointer;
  display:inline-block;
  height:30px;
  position:relative; }
  .bp3-file-input input{
    margin:0;
    min-width:200px;
    opacity:0; }
    .bp3-file-input input:disabled + .bp3-file-upload-input,
    .bp3-file-input input.bp3-disabled + .bp3-file-upload-input{
      background:rgba(206, 217, 224, 0.5);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(92, 112, 128, 0.6);
      cursor:not-allowed;
      resize:none; }
      .bp3-file-input input:disabled + .bp3-file-upload-input::after,
      .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after{
        background-color:rgba(206, 217, 224, 0.5);
        background-image:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:rgba(92, 112, 128, 0.6);
        cursor:not-allowed;
        outline:none; }
        .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active, .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active:hover,
        .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active,
        .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active:hover{
          background:rgba(206, 217, 224, 0.7); }
      .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input, .bp3-dark
      .bp3-file-input input.bp3-disabled + .bp3-file-upload-input{
        background:rgba(57, 75, 89, 0.5);
        -webkit-box-shadow:none;
                box-shadow:none;
        color:rgba(167, 182, 194, 0.6); }
        .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input::after, .bp3-dark
        .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after{
          background-color:rgba(57, 75, 89, 0.5);
          background-image:none;
          -webkit-box-shadow:none;
                  box-shadow:none;
          color:rgba(167, 182, 194, 0.6); }
          .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active, .bp3-dark
          .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active{
            background:rgba(57, 75, 89, 0.7); }
  .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{
    color:#182026; }
  .bp3-dark .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{
    color:#f5f8fa; }
  .bp3-file-input.bp3-fill{
    width:100%; }
  .bp3-file-input.bp3-large,
  .bp3-large .bp3-file-input{
    height:40px; }
  .bp3-file-input .bp3-file-upload-input-custom-text::after{
    content:attr(bp3-button-text); }

.bp3-file-upload-input{
  -webkit-appearance:none;
     -moz-appearance:none;
          appearance:none;
  background:#ffffff;
  border:none;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
  color:#182026;
  font-size:14px;
  font-weight:400;
  height:30px;
  line-height:30px;
  outline:none;
  padding:0 10px;
  -webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  vertical-align:middle;
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal;
  color:rgba(92, 112, 128, 0.6);
  left:0;
  padding-right:80px;
  position:absolute;
  right:0;
  top:0;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-file-upload-input::-webkit-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-file-upload-input::-moz-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-file-upload-input:-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-file-upload-input::-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-file-upload-input::placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-file-upload-input:focus, .bp3-file-upload-input.bp3-active{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-file-upload-input[type="search"], .bp3-file-upload-input.bp3-round{
    border-radius:30px;
    -webkit-box-sizing:border-box;
            box-sizing:border-box;
    padding-left:10px; }
  .bp3-file-upload-input[readonly]{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); }
  .bp3-file-upload-input:disabled, .bp3-file-upload-input.bp3-disabled{
    background:rgba(206, 217, 224, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none;
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed;
    resize:none; }
  .bp3-file-upload-input::after{
    background-color:#f5f8fa;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    color:#182026;
    min-height:24px;
    min-width:24px;
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    border-radius:3px;
    content:"Browse";
    line-height:24px;
    margin:3px;
    position:absolute;
    right:0;
    text-align:center;
    top:0;
    width:70px; }
    .bp3-file-upload-input::after:hover{
      background-clip:padding-box;
      background-color:#ebf1f5;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
    .bp3-file-upload-input::after:active, .bp3-file-upload-input::after.bp3-active{
      background-color:#d8e1e8;
      background-image:none;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-file-upload-input::after:disabled, .bp3-file-upload-input::after.bp3-disabled{
      background-color:rgba(206, 217, 224, 0.5);
      background-image:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(92, 112, 128, 0.6);
      cursor:not-allowed;
      outline:none; }
      .bp3-file-upload-input::after:disabled.bp3-active, .bp3-file-upload-input::after:disabled.bp3-active:hover, .bp3-file-upload-input::after.bp3-disabled.bp3-active, .bp3-file-upload-input::after.bp3-disabled.bp3-active:hover{
        background:rgba(206, 217, 224, 0.7); }
  .bp3-file-upload-input:hover::after{
    background-clip:padding-box;
    background-color:#ebf1f5;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
  .bp3-file-upload-input:active::after{
    background-color:#d8e1e8;
    background-image:none;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-large .bp3-file-upload-input{
    font-size:16px;
    height:40px;
    line-height:40px;
    padding-right:95px; }
    .bp3-large .bp3-file-upload-input[type="search"], .bp3-large .bp3-file-upload-input.bp3-round{
      padding:0 15px; }
    .bp3-large .bp3-file-upload-input::after{
      min-height:30px;
      min-width:30px;
      line-height:30px;
      margin:5px;
      width:85px; }
  .bp3-dark .bp3-file-upload-input{
    background:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    color:#f5f8fa;
    color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-file-upload-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-file-upload-input:disabled, .bp3-dark .bp3-file-upload-input.bp3-disabled{
      background:rgba(57, 75, 89, 0.5);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::after{
      background-color:#394b59;
      background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
      background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
      color:#f5f8fa; }
      .bp3-dark .bp3-file-upload-input::after:hover, .bp3-dark .bp3-file-upload-input::after:active, .bp3-dark .bp3-file-upload-input::after.bp3-active{
        color:#f5f8fa; }
      .bp3-dark .bp3-file-upload-input::after:hover{
        background-color:#30404d;
        -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-file-upload-input::after:active, .bp3-dark .bp3-file-upload-input::after.bp3-active{
        background-color:#202b33;
        background-image:none;
        -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
                box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
      .bp3-dark .bp3-file-upload-input::after:disabled, .bp3-dark .bp3-file-upload-input::after.bp3-disabled{
        background-color:rgba(57, 75, 89, 0.5);
        background-image:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:rgba(167, 182, 194, 0.6); }
        .bp3-dark .bp3-file-upload-input::after:disabled.bp3-active, .bp3-dark .bp3-file-upload-input::after.bp3-disabled.bp3-active{
          background:rgba(57, 75, 89, 0.7); }
      .bp3-dark .bp3-file-upload-input::after .bp3-button-spinner .bp3-spinner-head{
        background:rgba(16, 22, 26, 0.5);
        stroke:#8a9ba8; }
    .bp3-dark .bp3-file-upload-input:hover::after{
      background-color:#30404d;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-file-upload-input:active::after{
      background-color:#202b33;
      background-image:none;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
.bp3-file-upload-input::after{
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
.bp3-form-group{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin:0 0 15px; }
  .bp3-form-group label.bp3-label{
    margin-bottom:5px; }
  .bp3-form-group .bp3-control{
    margin-top:7px; }
  .bp3-form-group .bp3-form-helper-text{
    color:#5c7080;
    font-size:12px;
    margin-top:5px; }
  .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{
    color:#106ba3; }
  .bp3-form-group.bp3-intent-success .bp3-form-helper-text{
    color:#0d8050; }
  .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{
    color:#bf7326; }
  .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{
    color:#c23030; }
  .bp3-form-group.bp3-inline{
    -webkit-box-align:start;
        -ms-flex-align:start;
            align-items:flex-start;
    -webkit-box-orient:horizontal;
    -webkit-box-direction:normal;
        -ms-flex-direction:row;
            flex-direction:row; }
    .bp3-form-group.bp3-inline.bp3-large label.bp3-label{
      line-height:40px;
      margin:0 10px 0 0; }
    .bp3-form-group.bp3-inline label.bp3-label{
      line-height:30px;
      margin:0 10px 0 0; }
  .bp3-form-group.bp3-disabled .bp3-label,
  .bp3-form-group.bp3-disabled .bp3-text-muted,
  .bp3-form-group.bp3-disabled .bp3-form-helper-text{
    color:rgba(92, 112, 128, 0.6) !important; }
  .bp3-dark .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{
    color:#48aff0; }
  .bp3-dark .bp3-form-group.bp3-intent-success .bp3-form-helper-text{
    color:#3dcc91; }
  .bp3-dark .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{
    color:#ffb366; }
  .bp3-dark .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{
    color:#ff7373; }
  .bp3-dark .bp3-form-group .bp3-form-helper-text{
    color:#a7b6c2; }
  .bp3-dark .bp3-form-group.bp3-disabled .bp3-label,
  .bp3-dark .bp3-form-group.bp3-disabled .bp3-text-muted,
  .bp3-dark .bp3-form-group.bp3-disabled .bp3-form-helper-text{
    color:rgba(167, 182, 194, 0.6) !important; }
.bp3-input-group{
  display:block;
  position:relative; }
  .bp3-input-group .bp3-input{
    position:relative;
    width:100%; }
    .bp3-input-group .bp3-input:not(:first-child){
      padding-left:30px; }
    .bp3-input-group .bp3-input:not(:last-child){
      padding-right:30px; }
  .bp3-input-group .bp3-input-action,
  .bp3-input-group > .bp3-input-left-container,
  .bp3-input-group > .bp3-button,
  .bp3-input-group > .bp3-icon{
    position:absolute;
    top:0; }
    .bp3-input-group .bp3-input-action:first-child,
    .bp3-input-group > .bp3-input-left-container:first-child,
    .bp3-input-group > .bp3-button:first-child,
    .bp3-input-group > .bp3-icon:first-child{
      left:0; }
    .bp3-input-group .bp3-input-action:last-child,
    .bp3-input-group > .bp3-input-left-container:last-child,
    .bp3-input-group > .bp3-button:last-child,
    .bp3-input-group > .bp3-icon:last-child{
      right:0; }
  .bp3-input-group .bp3-button{
    min-height:24px;
    min-width:24px;
    margin:3px;
    padding:0 7px; }
    .bp3-input-group .bp3-button:empty{
      padding:0; }
  .bp3-input-group > .bp3-input-left-container,
  .bp3-input-group > .bp3-icon{
    z-index:1; }
  .bp3-input-group > .bp3-input-left-container > .bp3-icon,
  .bp3-input-group > .bp3-icon{
    color:#5c7080; }
    .bp3-input-group > .bp3-input-left-container > .bp3-icon:empty,
    .bp3-input-group > .bp3-icon:empty{
      font-family:"Icons16", sans-serif;
      font-size:16px;
      font-style:normal;
      font-weight:400;
      line-height:1;
      -moz-osx-font-smoothing:grayscale;
      -webkit-font-smoothing:antialiased; }
  .bp3-input-group > .bp3-input-left-container > .bp3-icon,
  .bp3-input-group > .bp3-icon,
  .bp3-input-group .bp3-input-action > .bp3-spinner{
    margin:7px; }
  .bp3-input-group .bp3-tag{
    margin:5px; }
  .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus),
  .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){
    color:#5c7080; }
    .bp3-dark .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus), .bp3-dark
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){
      color:#a7b6c2; }
    .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large{
      color:#5c7080; }
  .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled,
  .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled{
    color:rgba(92, 112, 128, 0.6) !important; }
    .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon-large,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-standard,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-large{
      color:rgba(92, 112, 128, 0.6) !important; }
  .bp3-input-group.bp3-disabled{
    cursor:not-allowed; }
    .bp3-input-group.bp3-disabled .bp3-icon{
      color:rgba(92, 112, 128, 0.6); }
  .bp3-input-group.bp3-large .bp3-button{
    min-height:30px;
    min-width:30px;
    margin:5px; }
  .bp3-input-group.bp3-large > .bp3-input-left-container > .bp3-icon,
  .bp3-input-group.bp3-large > .bp3-icon,
  .bp3-input-group.bp3-large .bp3-input-action > .bp3-spinner{
    margin:12px; }
  .bp3-input-group.bp3-large .bp3-input{
    font-size:16px;
    height:40px;
    line-height:40px; }
    .bp3-input-group.bp3-large .bp3-input[type="search"], .bp3-input-group.bp3-large .bp3-input.bp3-round{
      padding:0 15px; }
    .bp3-input-group.bp3-large .bp3-input:not(:first-child){
      padding-left:40px; }
    .bp3-input-group.bp3-large .bp3-input:not(:last-child){
      padding-right:40px; }
  .bp3-input-group.bp3-small .bp3-button{
    min-height:20px;
    min-width:20px;
    margin:2px; }
  .bp3-input-group.bp3-small .bp3-tag{
    min-height:20px;
    min-width:20px;
    margin:2px; }
  .bp3-input-group.bp3-small > .bp3-input-left-container > .bp3-icon,
  .bp3-input-group.bp3-small > .bp3-icon,
  .bp3-input-group.bp3-small .bp3-input-action > .bp3-spinner{
    margin:4px; }
  .bp3-input-group.bp3-small .bp3-input{
    font-size:12px;
    height:24px;
    line-height:24px;
    padding-left:8px;
    padding-right:8px; }
    .bp3-input-group.bp3-small .bp3-input[type="search"], .bp3-input-group.bp3-small .bp3-input.bp3-round{
      padding:0 12px; }
    .bp3-input-group.bp3-small .bp3-input:not(:first-child){
      padding-left:24px; }
    .bp3-input-group.bp3-small .bp3-input:not(:last-child){
      padding-right:24px; }
  .bp3-input-group.bp3-fill{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    width:100%; }
  .bp3-input-group.bp3-round .bp3-button,
  .bp3-input-group.bp3-round .bp3-input,
  .bp3-input-group.bp3-round .bp3-tag{
    border-radius:30px; }
  .bp3-dark .bp3-input-group .bp3-icon{
    color:#a7b6c2; }
  .bp3-dark .bp3-input-group.bp3-disabled .bp3-icon{
    color:rgba(167, 182, 194, 0.6); }
  .bp3-input-group.bp3-intent-primary .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-primary .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-primary .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #137cbd;
              box-shadow:inset 0 0 0 1px #137cbd; }
    .bp3-input-group.bp3-intent-primary .bp3-input:disabled, .bp3-input-group.bp3-intent-primary .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-primary > .bp3-icon{
    color:#106ba3; }
    .bp3-dark .bp3-input-group.bp3-intent-primary > .bp3-icon{
      color:#48aff0; }
  .bp3-input-group.bp3-intent-success .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-success .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-success .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #0f9960;
              box-shadow:inset 0 0 0 1px #0f9960; }
    .bp3-input-group.bp3-intent-success .bp3-input:disabled, .bp3-input-group.bp3-intent-success .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-success > .bp3-icon{
    color:#0d8050; }
    .bp3-dark .bp3-input-group.bp3-intent-success > .bp3-icon{
      color:#3dcc91; }
  .bp3-input-group.bp3-intent-warning .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-warning .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-warning .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #d9822b;
              box-shadow:inset 0 0 0 1px #d9822b; }
    .bp3-input-group.bp3-intent-warning .bp3-input:disabled, .bp3-input-group.bp3-intent-warning .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-warning > .bp3-icon{
    color:#bf7326; }
    .bp3-dark .bp3-input-group.bp3-intent-warning > .bp3-icon{
      color:#ffb366; }
  .bp3-input-group.bp3-intent-danger .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-danger .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-danger .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #db3737;
              box-shadow:inset 0 0 0 1px #db3737; }
    .bp3-input-group.bp3-intent-danger .bp3-input:disabled, .bp3-input-group.bp3-intent-danger .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-danger > .bp3-icon{
    color:#c23030; }
    .bp3-dark .bp3-input-group.bp3-intent-danger > .bp3-icon{
      color:#ff7373; }
.bp3-input{
  -webkit-appearance:none;
     -moz-appearance:none;
          appearance:none;
  background:#ffffff;
  border:none;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
  color:#182026;
  font-size:14px;
  font-weight:400;
  height:30px;
  line-height:30px;
  outline:none;
  padding:0 10px;
  -webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  vertical-align:middle; }
  .bp3-input::-webkit-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input::-moz-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input:-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input::-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input::placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input:focus, .bp3-input.bp3-active{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-input[type="search"], .bp3-input.bp3-round{
    border-radius:30px;
    -webkit-box-sizing:border-box;
            box-sizing:border-box;
    padding-left:10px; }
  .bp3-input[readonly]{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); }
  .bp3-input:disabled, .bp3-input.bp3-disabled{
    background:rgba(206, 217, 224, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none;
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed;
    resize:none; }
  .bp3-input.bp3-large{
    font-size:16px;
    height:40px;
    line-height:40px; }
    .bp3-input.bp3-large[type="search"], .bp3-input.bp3-large.bp3-round{
      padding:0 15px; }
  .bp3-input.bp3-small{
    font-size:12px;
    height:24px;
    line-height:24px;
    padding-left:8px;
    padding-right:8px; }
    .bp3-input.bp3-small[type="search"], .bp3-input.bp3-small.bp3-round{
      padding:0 12px; }
  .bp3-input.bp3-fill{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    width:100%; }
  .bp3-dark .bp3-input{
    background:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }
    .bp3-dark .bp3-input::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input::placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-input:disabled, .bp3-dark .bp3-input.bp3-disabled{
      background:rgba(57, 75, 89, 0.5);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(167, 182, 194, 0.6); }
  .bp3-input.bp3-intent-primary{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-primary:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-primary[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #137cbd;
              box-shadow:inset 0 0 0 1px #137cbd; }
    .bp3-input.bp3-intent-primary:disabled, .bp3-input.bp3-intent-primary.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-primary{
      -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-primary:focus{
        -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-primary[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #137cbd;
                box-shadow:inset 0 0 0 1px #137cbd; }
      .bp3-dark .bp3-input.bp3-intent-primary:disabled, .bp3-dark .bp3-input.bp3-intent-primary.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input.bp3-intent-success{
    -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-success:focus{
      -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-success[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #0f9960;
              box-shadow:inset 0 0 0 1px #0f9960; }
    .bp3-input.bp3-intent-success:disabled, .bp3-input.bp3-intent-success.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-success{
      -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-success:focus{
        -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #0f9960, 0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-success[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #0f9960;
                box-shadow:inset 0 0 0 1px #0f9960; }
      .bp3-dark .bp3-input.bp3-intent-success:disabled, .bp3-dark .bp3-input.bp3-intent-success.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input.bp3-intent-warning{
    -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-warning:focus{
      -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-warning[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #d9822b;
              box-shadow:inset 0 0 0 1px #d9822b; }
    .bp3-input.bp3-intent-warning:disabled, .bp3-input.bp3-intent-warning.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-warning{
      -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-warning:focus{
        -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #d9822b, 0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-warning[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #d9822b;
                box-shadow:inset 0 0 0 1px #d9822b; }
      .bp3-dark .bp3-input.bp3-intent-warning:disabled, .bp3-dark .bp3-input.bp3-intent-warning.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input.bp3-intent-danger{
    -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-danger:focus{
      -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-danger[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #db3737;
              box-shadow:inset 0 0 0 1px #db3737; }
    .bp3-input.bp3-intent-danger:disabled, .bp3-input.bp3-intent-danger.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-danger{
      -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-danger:focus{
        -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #db3737, 0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-danger[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #db3737;
                box-shadow:inset 0 0 0 1px #db3737; }
      .bp3-dark .bp3-input.bp3-intent-danger:disabled, .bp3-dark .bp3-input.bp3-intent-danger.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input::-ms-clear{
    display:none; }
textarea.bp3-input{
  max-width:100%;
  padding:10px; }
  textarea.bp3-input, textarea.bp3-input.bp3-large, textarea.bp3-input.bp3-small{
    height:auto;
    line-height:inherit; }
  textarea.bp3-input.bp3-small{
    padding:8px; }
  .bp3-dark textarea.bp3-input{
    background:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }
    .bp3-dark textarea.bp3-input::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input::placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark textarea.bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark textarea.bp3-input:disabled, .bp3-dark textarea.bp3-input.bp3-disabled{
      background:rgba(57, 75, 89, 0.5);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(167, 182, 194, 0.6); }
label.bp3-label{
  display:block;
  margin-bottom:15px;
  margin-top:0; }
  label.bp3-label .bp3-html-select,
  label.bp3-label .bp3-input,
  label.bp3-label .bp3-select,
  label.bp3-label .bp3-slider,
  label.bp3-label .bp3-popover-wrapper{
    display:block;
    margin-top:5px;
    text-transform:none; }
  label.bp3-label .bp3-button-group{
    margin-top:5px; }
  label.bp3-label .bp3-select select,
  label.bp3-label .bp3-html-select select{
    font-weight:400;
    vertical-align:top;
    width:100%; }
  label.bp3-label.bp3-disabled,
  label.bp3-label.bp3-disabled .bp3-text-muted{
    color:rgba(92, 112, 128, 0.6); }
  label.bp3-label.bp3-inline{
    line-height:30px; }
    label.bp3-label.bp3-inline .bp3-html-select,
    label.bp3-label.bp3-inline .bp3-input,
    label.bp3-label.bp3-inline .bp3-input-group,
    label.bp3-label.bp3-inline .bp3-select,
    label.bp3-label.bp3-inline .bp3-popover-wrapper{
      display:inline-block;
      margin:0 0 0 5px;
      vertical-align:top; }
    label.bp3-label.bp3-inline .bp3-button-group{
      margin:0 0 0 5px; }
    label.bp3-label.bp3-inline .bp3-input-group .bp3-input{
      margin-left:0; }
    label.bp3-label.bp3-inline.bp3-large{
      line-height:40px; }
  label.bp3-label:not(.bp3-inline) .bp3-popover-target{
    display:block; }
  .bp3-dark label.bp3-label{
    color:#f5f8fa; }
    .bp3-dark label.bp3-label.bp3-disabled,
    .bp3-dark label.bp3-label.bp3-disabled .bp3-text-muted{
      color:rgba(167, 182, 194, 0.6); }
.bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button{
  -webkit-box-flex:1;
      -ms-flex:1 1 14px;
          flex:1 1 14px;
  min-height:0;
  padding:0;
  width:30px; }
  .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button:first-child{
    border-radius:0 3px 0 0; }
  .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button:last-child{
    border-radius:0 0 3px 0; }

.bp3-numeric-input .bp3-button-group.bp3-vertical:first-child > .bp3-button:first-child{
  border-radius:3px 0 0 0; }

.bp3-numeric-input .bp3-button-group.bp3-vertical:first-child > .bp3-button:last-child{
  border-radius:0 0 0 3px; }

.bp3-numeric-input.bp3-large .bp3-button-group.bp3-vertical > .bp3-button{
  width:40px; }

form{
  display:block; }
.bp3-html-select select,
.bp3-select select{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  border:none;
  border-radius:3px;
  cursor:pointer;
  font-size:14px;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  padding:5px 10px;
  text-align:left;
  vertical-align:middle;
  background-color:#f5f8fa;
  background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
  background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
  color:#182026;
  -moz-appearance:none;
  -webkit-appearance:none;
  border-radius:3px;
  height:30px;
  padding:0 25px 0 10px;
  width:100%; }
  .bp3-html-select select > *, .bp3-select select > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-html-select select > .bp3-fill, .bp3-select select > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-html-select select::before,
  .bp3-select select::before, .bp3-html-select select > *, .bp3-select select > *{
    margin-right:7px; }
  .bp3-html-select select:empty::before,
  .bp3-select select:empty::before,
  .bp3-html-select select > :last-child,
  .bp3-select select > :last-child{
    margin-right:0; }
  .bp3-html-select select:hover,
  .bp3-select select:hover{
    background-clip:padding-box;
    background-color:#ebf1f5;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
  .bp3-html-select select:active,
  .bp3-select select:active, .bp3-html-select select.bp3-active,
  .bp3-select select.bp3-active{
    background-color:#d8e1e8;
    background-image:none;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-html-select select:disabled,
  .bp3-select select:disabled, .bp3-html-select select.bp3-disabled,
  .bp3-select select.bp3-disabled{
    background-color:rgba(206, 217, 224, 0.5);
    background-image:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed;
    outline:none; }
    .bp3-html-select select:disabled.bp3-active,
    .bp3-select select:disabled.bp3-active, .bp3-html-select select:disabled.bp3-active:hover,
    .bp3-select select:disabled.bp3-active:hover, .bp3-html-select select.bp3-disabled.bp3-active,
    .bp3-select select.bp3-disabled.bp3-active, .bp3-html-select select.bp3-disabled.bp3-active:hover,
    .bp3-select select.bp3-disabled.bp3-active:hover{
      background:rgba(206, 217, 224, 0.7); }

.bp3-html-select.bp3-minimal select,
.bp3-select.bp3-minimal select{
  background:none;
  -webkit-box-shadow:none;
          box-shadow:none; }
  .bp3-html-select.bp3-minimal select:hover,
  .bp3-select.bp3-minimal select:hover{
    background:rgba(167, 182, 194, 0.3);
    -webkit-box-shadow:none;
            box-shadow:none;
    color:#182026;
    text-decoration:none; }
  .bp3-html-select.bp3-minimal select:active,
  .bp3-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal select.bp3-active,
  .bp3-select.bp3-minimal select.bp3-active{
    background:rgba(115, 134, 148, 0.3);
    -webkit-box-shadow:none;
            box-shadow:none;
    color:#182026; }
  .bp3-html-select.bp3-minimal select:disabled,
  .bp3-select.bp3-minimal select:disabled, .bp3-html-select.bp3-minimal select:disabled:hover,
  .bp3-select.bp3-minimal select:disabled:hover, .bp3-html-select.bp3-minimal select.bp3-disabled,
  .bp3-select.bp3-minimal select.bp3-disabled, .bp3-html-select.bp3-minimal select.bp3-disabled:hover,
  .bp3-select.bp3-minimal select.bp3-disabled:hover{
    background:none;
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed; }
    .bp3-html-select.bp3-minimal select:disabled.bp3-active,
    .bp3-select.bp3-minimal select:disabled.bp3-active, .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,
    .bp3-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,
    .bp3-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,
    .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active{
      background:rgba(115, 134, 148, 0.3); }
  .bp3-dark .bp3-html-select.bp3-minimal select, .bp3-html-select.bp3-minimal .bp3-dark select,
  .bp3-dark .bp3-select.bp3-minimal select, .bp3-select.bp3-minimal .bp3-dark select{
    background:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    color:inherit; }
    .bp3-dark .bp3-html-select.bp3-minimal select:hover, .bp3-html-select.bp3-minimal .bp3-dark select:hover,
    .bp3-dark .bp3-select.bp3-minimal select:hover, .bp3-select.bp3-minimal .bp3-dark select:hover, .bp3-dark .bp3-html-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal .bp3-dark select:active,
    .bp3-dark .bp3-select.bp3-minimal select:active, .bp3-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-active{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-html-select.bp3-minimal select:hover, .bp3-html-select.bp3-minimal .bp3-dark select:hover,
    .bp3-dark .bp3-select.bp3-minimal select:hover, .bp3-select.bp3-minimal .bp3-dark select:hover{
      background:rgba(138, 155, 168, 0.15); }
    .bp3-dark .bp3-html-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal .bp3-dark select:active,
    .bp3-dark .bp3-select.bp3-minimal select:active, .bp3-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-active{
      background:rgba(138, 155, 168, 0.3);
      color:#f5f8fa; }
    .bp3-dark .bp3-html-select.bp3-minimal select:disabled, .bp3-html-select.bp3-minimal .bp3-dark select:disabled,
    .bp3-dark .bp3-select.bp3-minimal select:disabled, .bp3-select.bp3-minimal .bp3-dark select:disabled, .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover, .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover,
    .bp3-dark .bp3-select.bp3-minimal select:disabled:hover, .bp3-select.bp3-minimal .bp3-dark select:disabled:hover, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover{
      background:none;
      color:rgba(167, 182, 194, 0.6);
      cursor:not-allowed; }
      .bp3-dark .bp3-html-select.bp3-minimal select:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select:disabled.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active{
        background:rgba(138, 155, 168, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-primary,
  .bp3-select.bp3-minimal select.bp3-intent-primary{
    color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,
    .bp3-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,
    .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,
    .bp3-select.bp3-minimal select.bp3-intent-primary:hover{
      background:rgba(19, 124, 189, 0.15);
      color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,
    .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{
      background:rgba(19, 124, 189, 0.3);
      color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled{
      background:none;
      color:rgba(16, 107, 163, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active{
        background:rgba(19, 124, 189, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
      stroke:#106ba3; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary{
      color:#48aff0; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.2);
        color:#48aff0; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#48aff0; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(72, 175, 240, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-success,
  .bp3-select.bp3-minimal select.bp3-intent-success{
    color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,
    .bp3-select.bp3-minimal select.bp3-intent-success:hover, .bp3-html-select.bp3-minimal select.bp3-intent-success:active,
    .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,
    .bp3-select.bp3-minimal select.bp3-intent-success:hover{
      background:rgba(15, 153, 96, 0.15);
      color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:active,
    .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{
      background:rgba(15, 153, 96, 0.3);
      color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled{
      background:none;
      color:rgba(13, 128, 80, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active{
        background:rgba(15, 153, 96, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
      stroke:#0d8050; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success{
      color:#3dcc91; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.2);
        color:#3dcc91; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#3dcc91; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(61, 204, 145, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-warning,
  .bp3-select.bp3-minimal select.bp3-intent-warning{
    color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,
    .bp3-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,
    .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,
    .bp3-select.bp3-minimal select.bp3-intent-warning:hover{
      background:rgba(217, 130, 43, 0.15);
      color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,
    .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{
      background:rgba(217, 130, 43, 0.3);
      color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled{
      background:none;
      color:rgba(191, 115, 38, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active{
        background:rgba(217, 130, 43, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
      stroke:#bf7326; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning{
      color:#ffb366; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.2);
        color:#ffb366; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#ffb366; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(255, 179, 102, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-danger,
  .bp3-select.bp3-minimal select.bp3-intent-danger{
    color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,
    .bp3-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,
    .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,
    .bp3-select.bp3-minimal select.bp3-intent-danger:hover{
      background:rgba(219, 55, 55, 0.15);
      color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,
    .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{
      background:rgba(219, 55, 55, 0.3);
      color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled{
      background:none;
      color:rgba(194, 48, 48, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active{
        background:rgba(219, 55, 55, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
      stroke:#c23030; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger{
      color:#ff7373; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.2);
        color:#ff7373; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#ff7373; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(255, 115, 115, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }

.bp3-html-select.bp3-large select,
.bp3-select.bp3-large select{
  font-size:16px;
  height:40px;
  padding-right:35px; }

.bp3-dark .bp3-html-select select, .bp3-dark .bp3-select select{
  background-color:#394b59;
  background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
  background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
  color:#f5f8fa; }
  .bp3-dark .bp3-html-select select:hover, .bp3-dark .bp3-select select:hover, .bp3-dark .bp3-html-select select:active, .bp3-dark .bp3-select select:active, .bp3-dark .bp3-html-select select.bp3-active, .bp3-dark .bp3-select select.bp3-active{
    color:#f5f8fa; }
  .bp3-dark .bp3-html-select select:hover, .bp3-dark .bp3-select select:hover{
    background-color:#30404d;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-html-select select:active, .bp3-dark .bp3-select select:active, .bp3-dark .bp3-html-select select.bp3-active, .bp3-dark .bp3-select select.bp3-active{
    background-color:#202b33;
    background-image:none;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-html-select select:disabled, .bp3-dark .bp3-select select:disabled, .bp3-dark .bp3-html-select select.bp3-disabled, .bp3-dark .bp3-select select.bp3-disabled{
    background-color:rgba(57, 75, 89, 0.5);
    background-image:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-html-select select:disabled.bp3-active, .bp3-dark .bp3-select select:disabled.bp3-active, .bp3-dark .bp3-html-select select.bp3-disabled.bp3-active, .bp3-dark .bp3-select select.bp3-disabled.bp3-active{
      background:rgba(57, 75, 89, 0.7); }
  .bp3-dark .bp3-html-select select .bp3-button-spinner .bp3-spinner-head, .bp3-dark .bp3-select select .bp3-button-spinner .bp3-spinner-head{
    background:rgba(16, 22, 26, 0.5);
    stroke:#8a9ba8; }

.bp3-html-select select:disabled,
.bp3-select select:disabled{
  background-color:rgba(206, 217, 224, 0.5);
  -webkit-box-shadow:none;
          box-shadow:none;
  color:rgba(92, 112, 128, 0.6);
  cursor:not-allowed; }

.bp3-html-select .bp3-icon,
.bp3-select .bp3-icon, .bp3-select::after{
  color:#5c7080;
  pointer-events:none;
  position:absolute;
  right:7px;
  top:7px; }
  .bp3-html-select .bp3-disabled.bp3-icon,
  .bp3-select .bp3-disabled.bp3-icon, .bp3-disabled.bp3-select::after{
    color:rgba(92, 112, 128, 0.6); }
.bp3-html-select,
.bp3-select{
  display:inline-block;
  letter-spacing:normal;
  position:relative;
  vertical-align:middle; }
  .bp3-html-select select::-ms-expand,
  .bp3-select select::-ms-expand{
    display:none; }
  .bp3-html-select .bp3-icon,
  .bp3-select .bp3-icon{
    color:#5c7080; }
    .bp3-html-select .bp3-icon:hover,
    .bp3-select .bp3-icon:hover{
      color:#182026; }
    .bp3-dark .bp3-html-select .bp3-icon, .bp3-dark
    .bp3-select .bp3-icon{
      color:#a7b6c2; }
      .bp3-dark .bp3-html-select .bp3-icon:hover, .bp3-dark
      .bp3-select .bp3-icon:hover{
        color:#f5f8fa; }
  .bp3-html-select.bp3-large::after,
  .bp3-html-select.bp3-large .bp3-icon,
  .bp3-select.bp3-large::after,
  .bp3-select.bp3-large .bp3-icon{
    right:12px;
    top:12px; }
  .bp3-html-select.bp3-fill,
  .bp3-html-select.bp3-fill select,
  .bp3-select.bp3-fill,
  .bp3-select.bp3-fill select{
    width:100%; }
  .bp3-dark .bp3-html-select option, .bp3-dark
  .bp3-select option{
    background-color:#30404d;
    color:#f5f8fa; }
  .bp3-dark .bp3-html-select option:disabled, .bp3-dark
  .bp3-select option:disabled{
    color:rgba(167, 182, 194, 0.6); }
  .bp3-dark .bp3-html-select::after, .bp3-dark
  .bp3-select::after{
    color:#a7b6c2; }

.bp3-select::after{
  font-family:"Icons16", sans-serif;
  font-size:16px;
  font-style:normal;
  font-weight:400;
  line-height:1;
  -moz-osx-font-smoothing:grayscale;
  -webkit-font-smoothing:antialiased;
  content:""; }
.bp3-running-text table, table.bp3-html-table{
  border-spacing:0;
  font-size:14px; }
  .bp3-running-text table th, table.bp3-html-table th,
  .bp3-running-text table td,
  table.bp3-html-table td{
    padding:11px;
    text-align:left;
    vertical-align:top; }
  .bp3-running-text table th, table.bp3-html-table th{
    color:#182026;
    font-weight:600; }
  
  .bp3-running-text table td,
  table.bp3-html-table td{
    color:#182026; }
  .bp3-running-text table tbody tr:first-child th, table.bp3-html-table tbody tr:first-child th,
  .bp3-running-text table tbody tr:first-child td,
  table.bp3-html-table tbody tr:first-child td,
  .bp3-running-text table tfoot tr:first-child th,
  table.bp3-html-table tfoot tr:first-child th,
  .bp3-running-text table tfoot tr:first-child td,
  table.bp3-html-table tfoot tr:first-child td{
    -webkit-box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); }
  .bp3-dark .bp3-running-text table th, .bp3-running-text .bp3-dark table th, .bp3-dark table.bp3-html-table th{
    color:#f5f8fa; }
  .bp3-dark .bp3-running-text table td, .bp3-running-text .bp3-dark table td, .bp3-dark table.bp3-html-table td{
    color:#f5f8fa; }
  .bp3-dark .bp3-running-text table tbody tr:first-child th, .bp3-running-text .bp3-dark table tbody tr:first-child th, .bp3-dark table.bp3-html-table tbody tr:first-child th,
  .bp3-dark .bp3-running-text table tbody tr:first-child td,
  .bp3-running-text .bp3-dark table tbody tr:first-child td,
  .bp3-dark table.bp3-html-table tbody tr:first-child td,
  .bp3-dark .bp3-running-text table tfoot tr:first-child th,
  .bp3-running-text .bp3-dark table tfoot tr:first-child th,
  .bp3-dark table.bp3-html-table tfoot tr:first-child th,
  .bp3-dark .bp3-running-text table tfoot tr:first-child td,
  .bp3-running-text .bp3-dark table tfoot tr:first-child td,
  .bp3-dark table.bp3-html-table tfoot tr:first-child td{
    -webkit-box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15);
            box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); }

table.bp3-html-table.bp3-html-table-condensed th,
table.bp3-html-table.bp3-html-table-condensed td, table.bp3-html-table.bp3-small th,
table.bp3-html-table.bp3-small td{
  padding-bottom:6px;
  padding-top:6px; }

table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{
  background:rgba(191, 204, 214, 0.15); }

table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){
  -webkit-box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15);
          box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); }

table.bp3-html-table.bp3-html-table-bordered tbody tr td,
table.bp3-html-table.bp3-html-table-bordered tfoot tr td{
  -webkit-box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15);
          box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); }
  table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child),
  table.bp3-html-table.bp3-html-table-bordered tfoot tr td:not(:first-child){
    -webkit-box-shadow:inset 1px 1px 0 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 1px 1px 0 0 rgba(16, 22, 26, 0.15); }

table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{
  -webkit-box-shadow:none;
          box-shadow:none; }
  table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:not(:first-child){
    -webkit-box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); }

table.bp3-html-table.bp3-interactive tbody tr:hover td{
  background-color:rgba(191, 204, 214, 0.3);
  cursor:pointer; }

table.bp3-html-table.bp3-interactive tbody tr:active td{
  background-color:rgba(191, 204, 214, 0.4); }

.bp3-dark table.bp3-html-table{ }
  .bp3-dark table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{
    background:rgba(92, 112, 128, 0.15); }
  .bp3-dark table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){
    -webkit-box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15);
            box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); }
  .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td,
  .bp3-dark table.bp3-html-table.bp3-html-table-bordered tfoot tr td{
    -webkit-box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15);
            box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); }
    .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child),
    .bp3-dark table.bp3-html-table.bp3-html-table-bordered tfoot tr td:not(:first-child){
      -webkit-box-shadow:inset 1px 1px 0 0 rgba(255, 255, 255, 0.15);
              box-shadow:inset 1px 1px 0 0 rgba(255, 255, 255, 0.15); }
  .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{
    -webkit-box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15);
            box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); }
    .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:first-child{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:hover td{
    background-color:rgba(92, 112, 128, 0.3);
    cursor:pointer; }
  .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:active td{
    background-color:rgba(92, 112, 128, 0.4); }

.bp3-key-combo{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center; }
  .bp3-key-combo > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-key-combo > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-key-combo::before,
  .bp3-key-combo > *{
    margin-right:5px; }
  .bp3-key-combo:empty::before,
  .bp3-key-combo > :last-child{
    margin-right:0; }

.bp3-hotkey-dialog{
  padding-bottom:0;
  top:40px; }
  .bp3-hotkey-dialog .bp3-dialog-body{
    margin:0;
    padding:0; }
  .bp3-hotkey-dialog .bp3-hotkey-label{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1; }

.bp3-hotkey-column{
  margin:auto;
  max-height:80vh;
  overflow-y:auto;
  padding:30px; }
  .bp3-hotkey-column .bp3-heading{
    margin-bottom:20px; }
    .bp3-hotkey-column .bp3-heading:not(:first-child){
      margin-top:40px; }

.bp3-hotkey{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-pack:justify;
      -ms-flex-pack:justify;
          justify-content:space-between;
  margin-left:0;
  margin-right:0; }
  .bp3-hotkey:not(:last-child){
    margin-bottom:10px; }
.bp3-icon{
  display:inline-block;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  vertical-align:text-bottom; }
  .bp3-icon:not(:empty)::before{
    content:"" !important;
    content:unset !important; }
  .bp3-icon > svg{
    display:block; }
    .bp3-icon > svg:not([fill]){
      fill:currentColor; }

.bp3-icon.bp3-intent-primary, .bp3-icon-standard.bp3-intent-primary, .bp3-icon-large.bp3-intent-primary{
  color:#106ba3; }
  .bp3-dark .bp3-icon.bp3-intent-primary, .bp3-dark .bp3-icon-standard.bp3-intent-primary, .bp3-dark .bp3-icon-large.bp3-intent-primary{
    color:#48aff0; }

.bp3-icon.bp3-intent-success, .bp3-icon-standard.bp3-intent-success, .bp3-icon-large.bp3-intent-success{
  color:#0d8050; }
  .bp3-dark .bp3-icon.bp3-intent-success, .bp3-dark .bp3-icon-standard.bp3-intent-success, .bp3-dark .bp3-icon-large.bp3-intent-success{
    color:#3dcc91; }

.bp3-icon.bp3-intent-warning, .bp3-icon-standard.bp3-intent-warning, .bp3-icon-large.bp3-intent-warning{
  color:#bf7326; }
  .bp3-dark .bp3-icon.bp3-intent-warning, .bp3-dark .bp3-icon-standard.bp3-intent-warning, .bp3-dark .bp3-icon-large.bp3-intent-warning{
    color:#ffb366; }

.bp3-icon.bp3-intent-danger, .bp3-icon-standard.bp3-intent-danger, .bp3-icon-large.bp3-intent-danger{
  color:#c23030; }
  .bp3-dark .bp3-icon.bp3-intent-danger, .bp3-dark .bp3-icon-standard.bp3-intent-danger, .bp3-dark .bp3-icon-large.bp3-intent-danger{
    color:#ff7373; }

span.bp3-icon-standard{
  font-family:"Icons16", sans-serif;
  font-size:16px;
  font-style:normal;
  font-weight:400;
  line-height:1;
  -moz-osx-font-smoothing:grayscale;
  -webkit-font-smoothing:antialiased;
  display:inline-block; }

span.bp3-icon-large{
  font-family:"Icons20", sans-serif;
  font-size:20px;
  font-style:normal;
  font-weight:400;
  line-height:1;
  -moz-osx-font-smoothing:grayscale;
  -webkit-font-smoothing:antialiased;
  display:inline-block; }

span.bp3-icon:empty{
  font-family:"Icons20";
  font-size:inherit;
  font-style:normal;
  font-weight:400;
  line-height:1; }
  span.bp3-icon:empty::before{
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased; }

.bp3-icon-add::before{
  content:""; }

.bp3-icon-add-column-left::before{
  content:""; }

.bp3-icon-add-column-right::before{
  content:""; }

.bp3-icon-add-row-bottom::before{
  content:""; }

.bp3-icon-add-row-top::before{
  content:""; }

.bp3-icon-add-to-artifact::before{
  content:""; }

.bp3-icon-add-to-folder::before{
  content:""; }

.bp3-icon-airplane::before{
  content:""; }

.bp3-icon-align-center::before{
  content:""; }

.bp3-icon-align-justify::before{
  content:""; }

.bp3-icon-align-left::before{
  content:""; }

.bp3-icon-align-right::before{
  content:""; }

.bp3-icon-alignment-bottom::before{
  content:""; }

.bp3-icon-alignment-horizontal-center::before{
  content:""; }

.bp3-icon-alignment-left::before{
  content:""; }

.bp3-icon-alignment-right::before{
  content:""; }

.bp3-icon-alignment-top::before{
  content:""; }

.bp3-icon-alignment-vertical-center::before{
  content:""; }

.bp3-icon-annotation::before{
  content:""; }

.bp3-icon-application::before{
  content:""; }

.bp3-icon-applications::before{
  content:""; }

.bp3-icon-archive::before{
  content:""; }

.bp3-icon-arrow-bottom-left::before{
  content:"↙"; }

.bp3-icon-arrow-bottom-right::before{
  content:"↘"; }

.bp3-icon-arrow-down::before{
  content:"↓"; }

.bp3-icon-arrow-left::before{
  content:"←"; }

.bp3-icon-arrow-right::before{
  content:"→"; }

.bp3-icon-arrow-top-left::before{
  content:"↖"; }

.bp3-icon-arrow-top-right::before{
  content:"↗"; }

.bp3-icon-arrow-up::before{
  content:"↑"; }

.bp3-icon-arrows-horizontal::before{
  content:"↔"; }

.bp3-icon-arrows-vertical::before{
  content:"↕"; }

.bp3-icon-asterisk::before{
  content:"*"; }

.bp3-icon-automatic-updates::before{
  content:""; }

.bp3-icon-badge::before{
  content:""; }

.bp3-icon-ban-circle::before{
  content:""; }

.bp3-icon-bank-account::before{
  content:""; }

.bp3-icon-barcode::before{
  content:""; }

.bp3-icon-blank::before{
  content:""; }

.bp3-icon-blocked-person::before{
  content:""; }

.bp3-icon-bold::before{
  content:""; }

.bp3-icon-book::before{
  content:""; }

.bp3-icon-bookmark::before{
  content:""; }

.bp3-icon-box::before{
  content:""; }

.bp3-icon-briefcase::before{
  content:""; }

.bp3-icon-bring-data::before{
  content:""; }

.bp3-icon-build::before{
  content:""; }

.bp3-icon-calculator::before{
  content:""; }

.bp3-icon-calendar::before{
  content:""; }

.bp3-icon-camera::before{
  content:""; }

.bp3-icon-caret-down::before{
  content:"⌄"; }

.bp3-icon-caret-left::before{
  content:"〈"; }

.bp3-icon-caret-right::before{
  content:"〉"; }

.bp3-icon-caret-up::before{
  content:"⌃"; }

.bp3-icon-cell-tower::before{
  content:""; }

.bp3-icon-changes::before{
  content:""; }

.bp3-icon-chart::before{
  content:""; }

.bp3-icon-chat::before{
  content:""; }

.bp3-icon-chevron-backward::before{
  content:""; }

.bp3-icon-chevron-down::before{
  content:""; }

.bp3-icon-chevron-forward::before{
  content:""; }

.bp3-icon-chevron-left::before{
  content:""; }

.bp3-icon-chevron-right::before{
  content:""; }

.bp3-icon-chevron-up::before{
  content:""; }

.bp3-icon-circle::before{
  content:""; }

.bp3-icon-circle-arrow-down::before{
  content:""; }

.bp3-icon-circle-arrow-left::before{
  content:""; }

.bp3-icon-circle-arrow-right::before{
  content:""; }

.bp3-icon-circle-arrow-up::before{
  content:""; }

.bp3-icon-citation::before{
  content:""; }

.bp3-icon-clean::before{
  content:""; }

.bp3-icon-clipboard::before{
  content:""; }

.bp3-icon-cloud::before{
  content:"☁"; }

.bp3-icon-cloud-download::before{
  content:""; }

.bp3-icon-cloud-upload::before{
  content:""; }

.bp3-icon-code::before{
  content:""; }

.bp3-icon-code-block::before{
  content:""; }

.bp3-icon-cog::before{
  content:""; }

.bp3-icon-collapse-all::before{
  content:""; }

.bp3-icon-column-layout::before{
  content:""; }

.bp3-icon-comment::before{
  content:""; }

.bp3-icon-comparison::before{
  content:""; }

.bp3-icon-compass::before{
  content:""; }

.bp3-icon-compressed::before{
  content:""; }

.bp3-icon-confirm::before{
  content:""; }

.bp3-icon-console::before{
  content:""; }

.bp3-icon-contrast::before{
  content:""; }

.bp3-icon-control::before{
  content:""; }

.bp3-icon-credit-card::before{
  content:""; }

.bp3-icon-cross::before{
  content:"✗"; }

.bp3-icon-crown::before{
  content:""; }

.bp3-icon-cube::before{
  content:""; }

.bp3-icon-cube-add::before{
  content:""; }

.bp3-icon-cube-remove::before{
  content:""; }

.bp3-icon-curved-range-chart::before{
  content:""; }

.bp3-icon-cut::before{
  content:""; }

.bp3-icon-dashboard::before{
  content:""; }

.bp3-icon-data-lineage::before{
  content:""; }

.bp3-icon-database::before{
  content:""; }

.bp3-icon-delete::before{
  content:""; }

.bp3-icon-delta::before{
  content:"Δ"; }

.bp3-icon-derive-column::before{
  content:""; }

.bp3-icon-desktop::before{
  content:""; }

.bp3-icon-diagnosis::before{
  content:""; }

.bp3-icon-diagram-tree::before{
  content:""; }

.bp3-icon-direction-left::before{
  content:""; }

.bp3-icon-direction-right::before{
  content:""; }

.bp3-icon-disable::before{
  content:""; }

.bp3-icon-document::before{
  content:""; }

.bp3-icon-document-open::before{
  content:""; }

.bp3-icon-document-share::before{
  content:""; }

.bp3-icon-dollar::before{
  content:"$"; }

.bp3-icon-dot::before{
  content:"•"; }

.bp3-icon-double-caret-horizontal::before{
  content:""; }

.bp3-icon-double-caret-vertical::before{
  content:""; }

.bp3-icon-double-chevron-down::before{
  content:""; }

.bp3-icon-double-chevron-left::before{
  content:""; }

.bp3-icon-double-chevron-right::before{
  content:""; }

.bp3-icon-double-chevron-up::before{
  content:""; }

.bp3-icon-doughnut-chart::before{
  content:""; }

.bp3-icon-download::before{
  content:""; }

.bp3-icon-drag-handle-horizontal::before{
  content:""; }

.bp3-icon-drag-handle-vertical::before{
  content:""; }

.bp3-icon-draw::before{
  content:""; }

.bp3-icon-drive-time::before{
  content:""; }

.bp3-icon-duplicate::before{
  content:""; }

.bp3-icon-edit::before{
  content:"✎"; }

.bp3-icon-eject::before{
  content:"⏏"; }

.bp3-icon-endorsed::before{
  content:""; }

.bp3-icon-envelope::before{
  content:"✉"; }

.bp3-icon-equals::before{
  content:""; }

.bp3-icon-eraser::before{
  content:""; }

.bp3-icon-error::before{
  content:""; }

.bp3-icon-euro::before{
  content:"€"; }

.bp3-icon-exchange::before{
  content:""; }

.bp3-icon-exclude-row::before{
  content:""; }

.bp3-icon-expand-all::before{
  content:""; }

.bp3-icon-export::before{
  content:""; }

.bp3-icon-eye-off::before{
  content:""; }

.bp3-icon-eye-on::before{
  content:""; }

.bp3-icon-eye-open::before{
  content:""; }

.bp3-icon-fast-backward::before{
  content:""; }

.bp3-icon-fast-forward::before{
  content:""; }

.bp3-icon-feed::before{
  content:""; }

.bp3-icon-feed-subscribed::before{
  content:""; }

.bp3-icon-film::before{
  content:""; }

.bp3-icon-filter::before{
  content:""; }

.bp3-icon-filter-keep::before{
  content:""; }

.bp3-icon-filter-list::before{
  content:""; }

.bp3-icon-filter-open::before{
  content:""; }

.bp3-icon-filter-remove::before{
  content:""; }

.bp3-icon-flag::before{
  content:"⚑"; }

.bp3-icon-flame::before{
  content:""; }

.bp3-icon-flash::before{
  content:""; }

.bp3-icon-floppy-disk::before{
  content:""; }

.bp3-icon-flow-branch::before{
  content:""; }

.bp3-icon-flow-end::before{
  content:""; }

.bp3-icon-flow-linear::before{
  content:""; }

.bp3-icon-flow-review::before{
  content:""; }

.bp3-icon-flow-review-branch::before{
  content:""; }

.bp3-icon-flows::before{
  content:""; }

.bp3-icon-folder-close::before{
  content:""; }

.bp3-icon-folder-new::before{
  content:""; }

.bp3-icon-folder-open::before{
  content:""; }

.bp3-icon-folder-shared::before{
  content:""; }

.bp3-icon-folder-shared-open::before{
  content:""; }

.bp3-icon-follower::before{
  content:""; }

.bp3-icon-following::before{
  content:""; }

.bp3-icon-font::before{
  content:""; }

.bp3-icon-fork::before{
  content:""; }

.bp3-icon-form::before{
  content:""; }

.bp3-icon-full-circle::before{
  content:""; }

.bp3-icon-full-stacked-chart::before{
  content:""; }

.bp3-icon-fullscreen::before{
  content:""; }

.bp3-icon-function::before{
  content:""; }

.bp3-icon-gantt-chart::before{
  content:""; }

.bp3-icon-geolocation::before{
  content:""; }

.bp3-icon-geosearch::before{
  content:""; }

.bp3-icon-git-branch::before{
  content:""; }

.bp3-icon-git-commit::before{
  content:""; }

.bp3-icon-git-merge::before{
  content:""; }

.bp3-icon-git-new-branch::before{
  content:""; }

.bp3-icon-git-pull::before{
  content:""; }

.bp3-icon-git-push::before{
  content:""; }

.bp3-icon-git-repo::before{
  content:""; }

.bp3-icon-glass::before{
  content:""; }

.bp3-icon-globe::before{
  content:""; }

.bp3-icon-globe-network::before{
  content:""; }

.bp3-icon-graph::before{
  content:""; }

.bp3-icon-graph-remove::before{
  content:""; }

.bp3-icon-greater-than::before{
  content:""; }

.bp3-icon-greater-than-or-equal-to::before{
  content:""; }

.bp3-icon-grid::before{
  content:""; }

.bp3-icon-grid-view::before{
  content:""; }

.bp3-icon-group-objects::before{
  content:""; }

.bp3-icon-grouped-bar-chart::before{
  content:""; }

.bp3-icon-hand::before{
  content:""; }

.bp3-icon-hand-down::before{
  content:""; }

.bp3-icon-hand-left::before{
  content:""; }

.bp3-icon-hand-right::before{
  content:""; }

.bp3-icon-hand-up::before{
  content:""; }

.bp3-icon-header::before{
  content:""; }

.bp3-icon-header-one::before{
  content:""; }

.bp3-icon-header-two::before{
  content:""; }

.bp3-icon-headset::before{
  content:""; }

.bp3-icon-heart::before{
  content:"♥"; }

.bp3-icon-heart-broken::before{
  content:""; }

.bp3-icon-heat-grid::before{
  content:""; }

.bp3-icon-heatmap::before{
  content:""; }

.bp3-icon-help::before{
  content:"?"; }

.bp3-icon-helper-management::before{
  content:""; }

.bp3-icon-highlight::before{
  content:""; }

.bp3-icon-history::before{
  content:""; }

.bp3-icon-home::before{
  content:"⌂"; }

.bp3-icon-horizontal-bar-chart::before{
  content:""; }

.bp3-icon-horizontal-bar-chart-asc::before{
  content:""; }

.bp3-icon-horizontal-bar-chart-desc::before{
  content:""; }

.bp3-icon-horizontal-distribution::before{
  content:""; }

.bp3-icon-id-number::before{
  content:""; }

.bp3-icon-image-rotate-left::before{
  content:""; }

.bp3-icon-image-rotate-right::before{
  content:""; }

.bp3-icon-import::before{
  content:""; }

.bp3-icon-inbox::before{
  content:""; }

.bp3-icon-inbox-filtered::before{
  content:""; }

.bp3-icon-inbox-geo::before{
  content:""; }

.bp3-icon-inbox-search::before{
  content:""; }

.bp3-icon-inbox-update::before{
  content:""; }

.bp3-icon-info-sign::before{
  content:"ℹ"; }

.bp3-icon-inheritance::before{
  content:""; }

.bp3-icon-inner-join::before{
  content:""; }

.bp3-icon-insert::before{
  content:""; }

.bp3-icon-intersection::before{
  content:""; }

.bp3-icon-ip-address::before{
  content:""; }

.bp3-icon-issue::before{
  content:""; }

.bp3-icon-issue-closed::before{
  content:""; }

.bp3-icon-issue-new::before{
  content:""; }

.bp3-icon-italic::before{
  content:""; }

.bp3-icon-join-table::before{
  content:""; }

.bp3-icon-key::before{
  content:""; }

.bp3-icon-key-backspace::before{
  content:""; }

.bp3-icon-key-command::before{
  content:""; }

.bp3-icon-key-control::before{
  content:""; }

.bp3-icon-key-delete::before{
  content:""; }

.bp3-icon-key-enter::before{
  content:""; }

.bp3-icon-key-escape::before{
  content:""; }

.bp3-icon-key-option::before{
  content:""; }

.bp3-icon-key-shift::before{
  content:""; }

.bp3-icon-key-tab::before{
  content:""; }

.bp3-icon-known-vehicle::before{
  content:""; }

.bp3-icon-lab-test::before{
  content:""; }

.bp3-icon-label::before{
  content:""; }

.bp3-icon-layer::before{
  content:""; }

.bp3-icon-layers::before{
  content:""; }

.bp3-icon-layout::before{
  content:""; }

.bp3-icon-layout-auto::before{
  content:""; }

.bp3-icon-layout-balloon::before{
  content:""; }

.bp3-icon-layout-circle::before{
  content:""; }

.bp3-icon-layout-grid::before{
  content:""; }

.bp3-icon-layout-group-by::before{
  content:""; }

.bp3-icon-layout-hierarchy::before{
  content:""; }

.bp3-icon-layout-linear::before{
  content:""; }

.bp3-icon-layout-skew-grid::before{
  content:""; }

.bp3-icon-layout-sorted-clusters::before{
  content:""; }

.bp3-icon-learning::before{
  content:""; }

.bp3-icon-left-join::before{
  content:""; }

.bp3-icon-less-than::before{
  content:""; }

.bp3-icon-less-than-or-equal-to::before{
  content:""; }

.bp3-icon-lifesaver::before{
  content:""; }

.bp3-icon-lightbulb::before{
  content:""; }

.bp3-icon-link::before{
  content:""; }

.bp3-icon-list::before{
  content:"☰"; }

.bp3-icon-list-columns::before{
  content:""; }

.bp3-icon-list-detail-view::before{
  content:""; }

.bp3-icon-locate::before{
  content:""; }

.bp3-icon-lock::before{
  content:""; }

.bp3-icon-log-in::before{
  content:""; }

.bp3-icon-log-out::before{
  content:""; }

.bp3-icon-manual::before{
  content:""; }

.bp3-icon-manually-entered-data::before{
  content:""; }

.bp3-icon-map::before{
  content:""; }

.bp3-icon-map-create::before{
  content:""; }

.bp3-icon-map-marker::before{
  content:""; }

.bp3-icon-maximize::before{
  content:""; }

.bp3-icon-media::before{
  content:""; }

.bp3-icon-menu::before{
  content:""; }

.bp3-icon-menu-closed::before{
  content:""; }

.bp3-icon-menu-open::before{
  content:""; }

.bp3-icon-merge-columns::before{
  content:""; }

.bp3-icon-merge-links::before{
  content:""; }

.bp3-icon-minimize::before{
  content:""; }

.bp3-icon-minus::before{
  content:"−"; }

.bp3-icon-mobile-phone::before{
  content:""; }

.bp3-icon-mobile-video::before{
  content:""; }

.bp3-icon-moon::before{
  content:""; }

.bp3-icon-more::before{
  content:""; }

.bp3-icon-mountain::before{
  content:""; }

.bp3-icon-move::before{
  content:""; }

.bp3-icon-mugshot::before{
  content:""; }

.bp3-icon-multi-select::before{
  content:""; }

.bp3-icon-music::before{
  content:""; }

.bp3-icon-new-drawing::before{
  content:""; }

.bp3-icon-new-grid-item::before{
  content:""; }

.bp3-icon-new-layer::before{
  content:""; }

.bp3-icon-new-layers::before{
  content:""; }

.bp3-icon-new-link::before{
  content:""; }

.bp3-icon-new-object::before{
  content:""; }

.bp3-icon-new-person::before{
  content:""; }

.bp3-icon-new-prescription::before{
  content:""; }

.bp3-icon-new-text-box::before{
  content:""; }

.bp3-icon-ninja::before{
  content:""; }

.bp3-icon-not-equal-to::before{
  content:""; }

.bp3-icon-notifications::before{
  content:""; }

.bp3-icon-notifications-updated::before{
  content:""; }

.bp3-icon-numbered-list::before{
  content:""; }

.bp3-icon-numerical::before{
  content:""; }

.bp3-icon-office::before{
  content:""; }

.bp3-icon-offline::before{
  content:""; }

.bp3-icon-oil-field::before{
  content:""; }

.bp3-icon-one-column::before{
  content:""; }

.bp3-icon-outdated::before{
  content:""; }

.bp3-icon-page-layout::before{
  content:""; }

.bp3-icon-panel-stats::before{
  content:""; }

.bp3-icon-panel-table::before{
  content:""; }

.bp3-icon-paperclip::before{
  content:""; }

.bp3-icon-paragraph::before{
  content:""; }

.bp3-icon-path::before{
  content:""; }

.bp3-icon-path-search::before{
  content:""; }

.bp3-icon-pause::before{
  content:""; }

.bp3-icon-people::before{
  content:""; }

.bp3-icon-percentage::before{
  content:""; }

.bp3-icon-person::before{
  content:""; }

.bp3-icon-phone::before{
  content:"☎"; }

.bp3-icon-pie-chart::before{
  content:""; }

.bp3-icon-pin::before{
  content:""; }

.bp3-icon-pivot::before{
  content:""; }

.bp3-icon-pivot-table::before{
  content:""; }

.bp3-icon-play::before{
  content:""; }

.bp3-icon-plus::before{
  content:"+"; }

.bp3-icon-polygon-filter::before{
  content:""; }

.bp3-icon-power::before{
  content:""; }

.bp3-icon-predictive-analysis::before{
  content:""; }

.bp3-icon-prescription::before{
  content:""; }

.bp3-icon-presentation::before{
  content:""; }

.bp3-icon-print::before{
  content:"⎙"; }

.bp3-icon-projects::before{
  content:""; }

.bp3-icon-properties::before{
  content:""; }

.bp3-icon-property::before{
  content:""; }

.bp3-icon-publish-function::before{
  content:""; }

.bp3-icon-pulse::before{
  content:""; }

.bp3-icon-random::before{
  content:""; }

.bp3-icon-record::before{
  content:""; }

.bp3-icon-redo::before{
  content:""; }

.bp3-icon-refresh::before{
  content:""; }

.bp3-icon-regression-chart::before{
  content:""; }

.bp3-icon-remove::before{
  content:""; }

.bp3-icon-remove-column::before{
  content:""; }

.bp3-icon-remove-column-left::before{
  content:""; }

.bp3-icon-remove-column-right::before{
  content:""; }

.bp3-icon-remove-row-bottom::before{
  content:""; }

.bp3-icon-remove-row-top::before{
  content:""; }

.bp3-icon-repeat::before{
  content:""; }

.bp3-icon-reset::before{
  content:""; }

.bp3-icon-resolve::before{
  content:""; }

.bp3-icon-rig::before{
  content:""; }

.bp3-icon-right-join::before{
  content:""; }

.bp3-icon-ring::before{
  content:""; }

.bp3-icon-rotate-document::before{
  content:""; }

.bp3-icon-rotate-page::before{
  content:""; }

.bp3-icon-satellite::before{
  content:""; }

.bp3-icon-saved::before{
  content:""; }

.bp3-icon-scatter-plot::before{
  content:""; }

.bp3-icon-search::before{
  content:""; }

.bp3-icon-search-around::before{
  content:""; }

.bp3-icon-search-template::before{
  content:""; }

.bp3-icon-search-text::before{
  content:""; }

.bp3-icon-segmented-control::before{
  content:""; }

.bp3-icon-select::before{
  content:""; }

.bp3-icon-selection::before{
  content:"⦿"; }

.bp3-icon-send-to::before{
  content:""; }

.bp3-icon-send-to-graph::before{
  content:""; }

.bp3-icon-send-to-map::before{
  content:""; }

.bp3-icon-series-add::before{
  content:""; }

.bp3-icon-series-configuration::before{
  content:""; }

.bp3-icon-series-derived::before{
  content:""; }

.bp3-icon-series-filtered::before{
  content:""; }

.bp3-icon-series-search::before{
  content:""; }

.bp3-icon-settings::before{
  content:""; }

.bp3-icon-share::before{
  content:""; }

.bp3-icon-shield::before{
  content:""; }

.bp3-icon-shop::before{
  content:""; }

.bp3-icon-shopping-cart::before{
  content:""; }

.bp3-icon-signal-search::before{
  content:""; }

.bp3-icon-sim-card::before{
  content:""; }

.bp3-icon-slash::before{
  content:""; }

.bp3-icon-small-cross::before{
  content:""; }

.bp3-icon-small-minus::before{
  content:""; }

.bp3-icon-small-plus::before{
  content:""; }

.bp3-icon-small-tick::before{
  content:""; }

.bp3-icon-snowflake::before{
  content:""; }

.bp3-icon-social-media::before{
  content:""; }

.bp3-icon-sort::before{
  content:""; }

.bp3-icon-sort-alphabetical::before{
  content:""; }

.bp3-icon-sort-alphabetical-desc::before{
  content:""; }

.bp3-icon-sort-asc::before{
  content:""; }

.bp3-icon-sort-desc::before{
  content:""; }

.bp3-icon-sort-numerical::before{
  content:""; }

.bp3-icon-sort-numerical-desc::before{
  content:""; }

.bp3-icon-split-columns::before{
  content:""; }

.bp3-icon-square::before{
  content:""; }

.bp3-icon-stacked-chart::before{
  content:""; }

.bp3-icon-star::before{
  content:"★"; }

.bp3-icon-star-empty::before{
  content:"☆"; }

.bp3-icon-step-backward::before{
  content:""; }

.bp3-icon-step-chart::before{
  content:""; }

.bp3-icon-step-forward::before{
  content:""; }

.bp3-icon-stop::before{
  content:""; }

.bp3-icon-stopwatch::before{
  content:""; }

.bp3-icon-strikethrough::before{
  content:""; }

.bp3-icon-style::before{
  content:""; }

.bp3-icon-swap-horizontal::before{
  content:""; }

.bp3-icon-swap-vertical::before{
  content:""; }

.bp3-icon-symbol-circle::before{
  content:""; }

.bp3-icon-symbol-cross::before{
  content:""; }

.bp3-icon-symbol-diamond::before{
  content:""; }

.bp3-icon-symbol-square::before{
  content:""; }

.bp3-icon-symbol-triangle-down::before{
  content:""; }

.bp3-icon-symbol-triangle-up::before{
  content:""; }

.bp3-icon-tag::before{
  content:""; }

.bp3-icon-take-action::before{
  content:""; }

.bp3-icon-taxi::before{
  content:""; }

.bp3-icon-text-highlight::before{
  content:""; }

.bp3-icon-th::before{
  content:""; }

.bp3-icon-th-derived::before{
  content:""; }

.bp3-icon-th-disconnect::before{
  content:""; }

.bp3-icon-th-filtered::before{
  content:""; }

.bp3-icon-th-list::before{
  content:""; }

.bp3-icon-thumbs-down::before{
  content:""; }

.bp3-icon-thumbs-up::before{
  content:""; }

.bp3-icon-tick::before{
  content:"✓"; }

.bp3-icon-tick-circle::before{
  content:""; }

.bp3-icon-time::before{
  content:"⏲"; }

.bp3-icon-timeline-area-chart::before{
  content:""; }

.bp3-icon-timeline-bar-chart::before{
  content:""; }

.bp3-icon-timeline-events::before{
  content:""; }

.bp3-icon-timeline-line-chart::before{
  content:""; }

.bp3-icon-tint::before{
  content:""; }

.bp3-icon-torch::before{
  content:""; }

.bp3-icon-tractor::before{
  content:""; }

.bp3-icon-train::before{
  content:""; }

.bp3-icon-translate::before{
  content:""; }

.bp3-icon-trash::before{
  content:""; }

.bp3-icon-tree::before{
  content:""; }

.bp3-icon-trending-down::before{
  content:""; }

.bp3-icon-trending-up::before{
  content:""; }

.bp3-icon-truck::before{
  content:""; }

.bp3-icon-two-columns::before{
  content:""; }

.bp3-icon-unarchive::before{
  content:""; }

.bp3-icon-underline::before{
  content:"⎁"; }

.bp3-icon-undo::before{
  content:"⎌"; }

.bp3-icon-ungroup-objects::before{
  content:""; }

.bp3-icon-unknown-vehicle::before{
  content:""; }

.bp3-icon-unlock::before{
  content:""; }

.bp3-icon-unpin::before{
  content:""; }

.bp3-icon-unresolve::before{
  content:""; }

.bp3-icon-updated::before{
  content:""; }

.bp3-icon-upload::before{
  content:""; }

.bp3-icon-user::before{
  content:""; }

.bp3-icon-variable::before{
  content:""; }

.bp3-icon-vertical-bar-chart-asc::before{
  content:""; }

.bp3-icon-vertical-bar-chart-desc::before{
  content:""; }

.bp3-icon-vertical-distribution::before{
  content:""; }

.bp3-icon-video::before{
  content:""; }

.bp3-icon-volume-down::before{
  content:""; }

.bp3-icon-volume-off::before{
  content:""; }

.bp3-icon-volume-up::before{
  content:""; }

.bp3-icon-walk::before{
  content:""; }

.bp3-icon-warning-sign::before{
  content:""; }

.bp3-icon-waterfall-chart::before{
  content:""; }

.bp3-icon-widget::before{
  content:""; }

.bp3-icon-widget-button::before{
  content:""; }

.bp3-icon-widget-footer::before{
  content:""; }

.bp3-icon-widget-header::before{
  content:""; }

.bp3-icon-wrench::before{
  content:""; }

.bp3-icon-zoom-in::before{
  content:""; }

.bp3-icon-zoom-out::before{
  content:""; }

.bp3-icon-zoom-to-fit::before{
  content:""; }
.bp3-submenu > .bp3-popover-wrapper{
  display:block; }

.bp3-submenu .bp3-popover-target{
  display:block; }
  .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{ }

.bp3-submenu.bp3-popover{
  -webkit-box-shadow:none;
          box-shadow:none;
  padding:0 5px; }
  .bp3-submenu.bp3-popover > .bp3-popover-content{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-submenu.bp3-popover, .bp3-submenu.bp3-popover.bp3-dark{
    -webkit-box-shadow:none;
            box-shadow:none; }
    .bp3-dark .bp3-submenu.bp3-popover > .bp3-popover-content, .bp3-submenu.bp3-popover.bp3-dark > .bp3-popover-content{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
.bp3-menu{
  background:#ffffff;
  border-radius:3px;
  color:#182026;
  list-style:none;
  margin:0;
  min-width:180px;
  padding:5px;
  text-align:left; }

.bp3-menu-divider{
  border-top:1px solid rgba(16, 22, 26, 0.15);
  display:block;
  margin:5px; }
  .bp3-dark .bp3-menu-divider{
    border-top-color:rgba(255, 255, 255, 0.15); }

.bp3-menu-item{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:start;
      -ms-flex-align:start;
          align-items:flex-start;
  border-radius:2px;
  color:inherit;
  line-height:20px;
  padding:5px 7px;
  text-decoration:none;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-menu-item > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-menu-item > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-menu-item::before,
  .bp3-menu-item > *{
    margin-right:7px; }
  .bp3-menu-item:empty::before,
  .bp3-menu-item > :last-child{
    margin-right:0; }
  .bp3-menu-item > .bp3-fill{
    word-break:break-word; }
  .bp3-menu-item:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{
    background-color:rgba(167, 182, 194, 0.3);
    cursor:pointer;
    text-decoration:none; }
  .bp3-menu-item.bp3-disabled{
    background-color:inherit;
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed; }
  .bp3-dark .bp3-menu-item{
    color:inherit; }
    .bp3-dark .bp3-menu-item:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-menu-item{
      background-color:rgba(138, 155, 168, 0.15);
      color:inherit; }
    .bp3-dark .bp3-menu-item.bp3-disabled{
      background-color:inherit;
      color:rgba(167, 182, 194, 0.6); }
  .bp3-menu-item.bp3-intent-primary{
    color:#106ba3; }
    .bp3-menu-item.bp3-intent-primary .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-primary::before, .bp3-menu-item.bp3-intent-primary::after,
    .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{
      color:#106ba3; }
    .bp3-menu-item.bp3-intent-primary:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-menu-item.bp3-intent-primary.bp3-active{
      background-color:#137cbd; }
    .bp3-menu-item.bp3-intent-primary:active{
      background-color:#106ba3; }
    .bp3-menu-item.bp3-intent-primary:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-menu-item.bp3-intent-primary:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-menu-item.bp3-intent-primary:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-primary:active, .bp3-menu-item.bp3-intent-primary:active::before, .bp3-menu-item.bp3-intent-primary:active::after,
    .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-primary.bp3-active, .bp3-menu-item.bp3-intent-primary.bp3-active::before, .bp3-menu-item.bp3-intent-primary.bp3-active::after,
    .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item.bp3-intent-success{
    color:#0d8050; }
    .bp3-menu-item.bp3-intent-success .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-success::before, .bp3-menu-item.bp3-intent-success::after,
    .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{
      color:#0d8050; }
    .bp3-menu-item.bp3-intent-success:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-menu-item.bp3-intent-success.bp3-active{
      background-color:#0f9960; }
    .bp3-menu-item.bp3-intent-success:active{
      background-color:#0d8050; }
    .bp3-menu-item.bp3-intent-success:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-menu-item.bp3-intent-success:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-menu-item.bp3-intent-success:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-success:active, .bp3-menu-item.bp3-intent-success:active::before, .bp3-menu-item.bp3-intent-success:active::after,
    .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-success.bp3-active, .bp3-menu-item.bp3-intent-success.bp3-active::before, .bp3-menu-item.bp3-intent-success.bp3-active::after,
    .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item.bp3-intent-warning{
    color:#bf7326; }
    .bp3-menu-item.bp3-intent-warning .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-warning::before, .bp3-menu-item.bp3-intent-warning::after,
    .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{
      color:#bf7326; }
    .bp3-menu-item.bp3-intent-warning:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-menu-item.bp3-intent-warning.bp3-active{
      background-color:#d9822b; }
    .bp3-menu-item.bp3-intent-warning:active{
      background-color:#bf7326; }
    .bp3-menu-item.bp3-intent-warning:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-menu-item.bp3-intent-warning:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-menu-item.bp3-intent-warning:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-warning:active, .bp3-menu-item.bp3-intent-warning:active::before, .bp3-menu-item.bp3-intent-warning:active::after,
    .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-warning.bp3-active, .bp3-menu-item.bp3-intent-warning.bp3-active::before, .bp3-menu-item.bp3-intent-warning.bp3-active::after,
    .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item.bp3-intent-danger{
    color:#c23030; }
    .bp3-menu-item.bp3-intent-danger .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-danger::before, .bp3-menu-item.bp3-intent-danger::after,
    .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{
      color:#c23030; }
    .bp3-menu-item.bp3-intent-danger:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-menu-item.bp3-intent-danger.bp3-active{
      background-color:#db3737; }
    .bp3-menu-item.bp3-intent-danger:active{
      background-color:#c23030; }
    .bp3-menu-item.bp3-intent-danger:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-menu-item.bp3-intent-danger:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-menu-item.bp3-intent-danger:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-danger:active, .bp3-menu-item.bp3-intent-danger:active::before, .bp3-menu-item.bp3-intent-danger:active::after,
    .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-danger.bp3-active, .bp3-menu-item.bp3-intent-danger.bp3-active::before, .bp3-menu-item.bp3-intent-danger.bp3-active::after,
    .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item::before{
    font-family:"Icons16", sans-serif;
    font-size:16px;
    font-style:normal;
    font-weight:400;
    line-height:1;
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased;
    margin-right:7px; }
  .bp3-menu-item::before,
  .bp3-menu-item > .bp3-icon{
    color:#5c7080;
    margin-top:2px; }
  .bp3-menu-item .bp3-menu-item-label{
    color:#5c7080; }
  .bp3-menu-item:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{
    color:inherit; }
  .bp3-menu-item.bp3-active, .bp3-menu-item:active{
    background-color:rgba(115, 134, 148, 0.3); }
  .bp3-menu-item.bp3-disabled{
    background-color:inherit !important;
    color:rgba(92, 112, 128, 0.6) !important;
    cursor:not-allowed !important;
    outline:none !important; }
    .bp3-menu-item.bp3-disabled::before,
    .bp3-menu-item.bp3-disabled > .bp3-icon,
    .bp3-menu-item.bp3-disabled .bp3-menu-item-label{
      color:rgba(92, 112, 128, 0.6) !important; }
  .bp3-large .bp3-menu-item{
    font-size:16px;
    line-height:22px;
    padding:9px 7px; }
    .bp3-large .bp3-menu-item .bp3-icon{
      margin-top:3px; }
    .bp3-large .bp3-menu-item::before{
      font-family:"Icons20", sans-serif;
      font-size:20px;
      font-style:normal;
      font-weight:400;
      line-height:1;
      -moz-osx-font-smoothing:grayscale;
      -webkit-font-smoothing:antialiased;
      margin-right:10px;
      margin-top:1px; }

button.bp3-menu-item{
  background:none;
  border:none;
  text-align:left;
  width:100%; }
.bp3-menu-header{
  border-top:1px solid rgba(16, 22, 26, 0.15);
  display:block;
  margin:5px;
  cursor:default;
  padding-left:2px; }
  .bp3-dark .bp3-menu-header{
    border-top-color:rgba(255, 255, 255, 0.15); }
  .bp3-menu-header:first-of-type{
    border-top:none; }
  .bp3-menu-header > h6{
    color:#182026;
    font-weight:600;
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    line-height:17px;
    margin:0;
    padding:10px 7px 0 1px; }
    .bp3-dark .bp3-menu-header > h6{
      color:#f5f8fa; }
  .bp3-menu-header:first-of-type > h6{
    padding-top:0; }
  .bp3-large .bp3-menu-header > h6{
    font-size:18px;
    padding-bottom:5px;
    padding-top:15px; }
  .bp3-large .bp3-menu-header:first-of-type > h6{
    padding-top:0; }

.bp3-dark .bp3-menu{
  background:#30404d;
  color:#f5f8fa; }

.bp3-dark .bp3-menu-item{ }
  .bp3-dark .bp3-menu-item.bp3-intent-primary{
    color:#48aff0; }
    .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-icon{
      color:inherit; }
    .bp3-dark .bp3-menu-item.bp3-intent-primary::before, .bp3-dark .bp3-menu-item.bp3-intent-primary::after,
    .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{
      color:#48aff0; }
    .bp3-dark .bp3-menu-item.bp3-intent-primary:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active{
      background-color:#137cbd; }
    .bp3-dark .bp3-menu-item.bp3-intent-primary:active{
      background-color:#106ba3; }
    .bp3-dark .bp3-menu-item.bp3-intent-primary:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after,
    .bp3-dark .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,
    .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,
    .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-primary:active, .bp3-dark .bp3-menu-item.bp3-intent-primary:active::before, .bp3-dark .bp3-menu-item.bp3-intent-primary:active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-dark .bp3-menu-item.bp3-intent-success{
    color:#3dcc91; }
    .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-icon{
      color:inherit; }
    .bp3-dark .bp3-menu-item.bp3-intent-success::before, .bp3-dark .bp3-menu-item.bp3-intent-success::after,
    .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{
      color:#3dcc91; }
    .bp3-dark .bp3-menu-item.bp3-intent-success:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active{
      background-color:#0f9960; }
    .bp3-dark .bp3-menu-item.bp3-intent-success:active{
      background-color:#0d8050; }
    .bp3-dark .bp3-menu-item.bp3-intent-success:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-success:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-success:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after,
    .bp3-dark .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,
    .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label,
    .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-success:active, .bp3-dark .bp3-menu-item.bp3-intent-success:active::before, .bp3-dark .bp3-menu-item.bp3-intent-success:active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-dark .bp3-menu-item.bp3-intent-warning{
    color:#ffb366; }
    .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-icon{
      color:inherit; }
    .bp3-dark .bp3-menu-item.bp3-intent-warning::before, .bp3-dark .bp3-menu-item.bp3-intent-warning::after,
    .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{
      color:#ffb366; }
    .bp3-dark .bp3-menu-item.bp3-intent-warning:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active{
      background-color:#d9822b; }
    .bp3-dark .bp3-menu-item.bp3-intent-warning:active{
      background-color:#bf7326; }
    .bp3-dark .bp3-menu-item.bp3-intent-warning:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after,
    .bp3-dark .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,
    .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,
    .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-warning:active, .bp3-dark .bp3-menu-item.bp3-intent-warning:active::before, .bp3-dark .bp3-menu-item.bp3-intent-warning:active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-dark .bp3-menu-item.bp3-intent-danger{
    color:#ff7373; }
    .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-icon{
      color:inherit; }
    .bp3-dark .bp3-menu-item.bp3-intent-danger::before, .bp3-dark .bp3-menu-item.bp3-intent-danger::after,
    .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{
      color:#ff7373; }
    .bp3-dark .bp3-menu-item.bp3-intent-danger:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active{
      background-color:#db3737; }
    .bp3-dark .bp3-menu-item.bp3-intent-danger:active{
      background-color:#c23030; }
    .bp3-dark .bp3-menu-item.bp3-intent-danger:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after,
    .bp3-dark .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,
    .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,
    .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-danger:active, .bp3-dark .bp3-menu-item.bp3-intent-danger:active::before, .bp3-dark .bp3-menu-item.bp3-intent-danger:active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-dark .bp3-menu-item::before,
  .bp3-dark .bp3-menu-item > .bp3-icon{
    color:#a7b6c2; }
  .bp3-dark .bp3-menu-item .bp3-menu-item-label{
    color:#a7b6c2; }
  .bp3-dark .bp3-menu-item.bp3-active, .bp3-dark .bp3-menu-item:active{
    background-color:rgba(138, 155, 168, 0.3); }
  .bp3-dark .bp3-menu-item.bp3-disabled{
    color:rgba(167, 182, 194, 0.6) !important; }
    .bp3-dark .bp3-menu-item.bp3-disabled::before,
    .bp3-dark .bp3-menu-item.bp3-disabled > .bp3-icon,
    .bp3-dark .bp3-menu-item.bp3-disabled .bp3-menu-item-label{
      color:rgba(167, 182, 194, 0.6) !important; }

.bp3-dark .bp3-menu-divider,
.bp3-dark .bp3-menu-header{
  border-color:rgba(255, 255, 255, 0.15); }

.bp3-dark .bp3-menu-header > h6{
  color:#f5f8fa; }

.bp3-label .bp3-menu{
  margin-top:5px; }
.bp3-navbar{
  background-color:#ffffff;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
  height:50px;
  padding:0 15px;
  position:relative;
  width:100%;
  z-index:10; }
  .bp3-navbar.bp3-dark,
  .bp3-dark .bp3-navbar{
    background-color:#394b59; }
  .bp3-navbar.bp3-dark{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-navbar{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-navbar.bp3-fixed-top{
    left:0;
    position:fixed;
    right:0;
    top:0; }

.bp3-navbar-heading{
  font-size:16px;
  margin-right:15px; }

.bp3-navbar-group{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  height:50px; }
  .bp3-navbar-group.bp3-align-left{
    float:left; }
  .bp3-navbar-group.bp3-align-right{
    float:right; }

.bp3-navbar-divider{
  border-left:1px solid rgba(16, 22, 26, 0.15);
  height:20px;
  margin:0 10px; }
  .bp3-dark .bp3-navbar-divider{
    border-left-color:rgba(255, 255, 255, 0.15); }
.bp3-non-ideal-state{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  height:100%;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  text-align:center;
  width:100%; }
  .bp3-non-ideal-state > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-non-ideal-state > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-non-ideal-state::before,
  .bp3-non-ideal-state > *{
    margin-bottom:20px; }
  .bp3-non-ideal-state:empty::before,
  .bp3-non-ideal-state > :last-child{
    margin-bottom:0; }
  .bp3-non-ideal-state > *{
    max-width:400px; }

.bp3-non-ideal-state-visual{
  color:rgba(92, 112, 128, 0.6);
  font-size:60px; }
  .bp3-dark .bp3-non-ideal-state-visual{
    color:rgba(167, 182, 194, 0.6); }

.bp3-overflow-list{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-wrap:nowrap;
      flex-wrap:nowrap;
  min-width:0; }

.bp3-overflow-list-spacer{
  -ms-flex-negative:1;
      flex-shrink:1;
  width:1px; }

body.bp3-overlay-open{
  overflow:hidden; }

.bp3-overlay{
  bottom:0;
  left:0;
  position:static;
  right:0;
  top:0;
  z-index:20; }
  .bp3-overlay:not(.bp3-overlay-open){
    pointer-events:none; }
  .bp3-overlay.bp3-overlay-container{
    overflow:hidden;
    position:fixed; }
    .bp3-overlay.bp3-overlay-container.bp3-overlay-inline{
      position:absolute; }
  .bp3-overlay.bp3-overlay-scroll-container{
    overflow:auto;
    position:fixed; }
    .bp3-overlay.bp3-overlay-scroll-container.bp3-overlay-inline{
      position:absolute; }
  .bp3-overlay.bp3-overlay-inline{
    display:inline;
    overflow:visible; }

.bp3-overlay-content{
  position:fixed;
  z-index:20; }
  .bp3-overlay-inline .bp3-overlay-content,
  .bp3-overlay-scroll-container .bp3-overlay-content{
    position:absolute; }

.bp3-overlay-backdrop{
  bottom:0;
  left:0;
  position:fixed;
  right:0;
  top:0;
  opacity:1;
  background-color:rgba(16, 22, 26, 0.7);
  overflow:auto;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none;
  z-index:20; }
  .bp3-overlay-backdrop.bp3-overlay-enter, .bp3-overlay-backdrop.bp3-overlay-appear{
    opacity:0; }
  .bp3-overlay-backdrop.bp3-overlay-enter-active, .bp3-overlay-backdrop.bp3-overlay-appear-active{
    opacity:1;
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-overlay-backdrop.bp3-overlay-exit{
    opacity:1; }
  .bp3-overlay-backdrop.bp3-overlay-exit-active{
    opacity:0;
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-overlay-backdrop:focus{
    outline:none; }
  .bp3-overlay-inline .bp3-overlay-backdrop{
    position:absolute; }
.bp3-panel-stack{
  overflow:hidden;
  position:relative; }

.bp3-panel-stack-header{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-shadow:0 1px rgba(16, 22, 26, 0.15);
          box-shadow:0 1px rgba(16, 22, 26, 0.15);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-negative:0;
      flex-shrink:0;
  height:30px;
  z-index:1; }
  .bp3-dark .bp3-panel-stack-header{
    -webkit-box-shadow:0 1px rgba(255, 255, 255, 0.15);
            box-shadow:0 1px rgba(255, 255, 255, 0.15); }
  .bp3-panel-stack-header > span{
    -webkit-box-align:stretch;
        -ms-flex-align:stretch;
            align-items:stretch;
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-flex:1;
        -ms-flex:1;
            flex:1; }
  .bp3-panel-stack-header .bp3-heading{
    margin:0 5px; }

.bp3-button.bp3-panel-stack-header-back{
  margin-left:5px;
  padding-left:0;
  white-space:nowrap; }
  .bp3-button.bp3-panel-stack-header-back .bp3-icon{
    margin:0 2px; }

.bp3-panel-stack-view{
  bottom:0;
  left:0;
  position:absolute;
  right:0;
  top:0;
  background-color:#ffffff;
  border-right:1px solid rgba(16, 22, 26, 0.15);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin-right:-1px;
  overflow-y:auto;
  z-index:1; }
  .bp3-dark .bp3-panel-stack-view{
    background-color:#30404d; }
  .bp3-panel-stack-view:nth-last-child(n + 4){
    display:none; }

.bp3-panel-stack-push .bp3-panel-stack-enter, .bp3-panel-stack-push .bp3-panel-stack-appear{
  -webkit-transform:translateX(100%);
          transform:translateX(100%);
  opacity:0; }

.bp3-panel-stack-push .bp3-panel-stack-enter-active, .bp3-panel-stack-push .bp3-panel-stack-appear-active{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1;
  -webkit-transition-delay:0;
          transition-delay:0;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease; }

.bp3-panel-stack-push .bp3-panel-stack-exit{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1; }

.bp3-panel-stack-push .bp3-panel-stack-exit-active{
  -webkit-transform:translateX(-50%);
          transform:translateX(-50%);
  opacity:0;
  -webkit-transition-delay:0;
          transition-delay:0;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease; }

.bp3-panel-stack-pop .bp3-panel-stack-enter, .bp3-panel-stack-pop .bp3-panel-stack-appear{
  -webkit-transform:translateX(-50%);
          transform:translateX(-50%);
  opacity:0; }

.bp3-panel-stack-pop .bp3-panel-stack-enter-active, .bp3-panel-stack-pop .bp3-panel-stack-appear-active{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1;
  -webkit-transition-delay:0;
          transition-delay:0;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease; }

.bp3-panel-stack-pop .bp3-panel-stack-exit{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1; }

.bp3-panel-stack-pop .bp3-panel-stack-exit-active{
  -webkit-transform:translateX(100%);
          transform:translateX(100%);
  opacity:0;
  -webkit-transition-delay:0;
          transition-delay:0;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease; }
.bp3-panel-stack2{
  overflow:hidden;
  position:relative; }

.bp3-panel-stack2-header{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-shadow:0 1px rgba(16, 22, 26, 0.15);
          box-shadow:0 1px rgba(16, 22, 26, 0.15);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-negative:0;
      flex-shrink:0;
  height:30px;
  z-index:1; }
  .bp3-dark .bp3-panel-stack2-header{
    -webkit-box-shadow:0 1px rgba(255, 255, 255, 0.15);
            box-shadow:0 1px rgba(255, 255, 255, 0.15); }
  .bp3-panel-stack2-header > span{
    -webkit-box-align:stretch;
        -ms-flex-align:stretch;
            align-items:stretch;
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-flex:1;
        -ms-flex:1;
            flex:1; }
  .bp3-panel-stack2-header .bp3-heading{
    margin:0 5px; }

.bp3-button.bp3-panel-stack2-header-back{
  margin-left:5px;
  padding-left:0;
  white-space:nowrap; }
  .bp3-button.bp3-panel-stack2-header-back .bp3-icon{
    margin:0 2px; }

.bp3-panel-stack2-view{
  bottom:0;
  left:0;
  position:absolute;
  right:0;
  top:0;
  background-color:#ffffff;
  border-right:1px solid rgba(16, 22, 26, 0.15);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin-right:-1px;
  overflow-y:auto;
  z-index:1; }
  .bp3-dark .bp3-panel-stack2-view{
    background-color:#30404d; }
  .bp3-panel-stack2-view:nth-last-child(n + 4){
    display:none; }

.bp3-panel-stack2-push .bp3-panel-stack2-enter, .bp3-panel-stack2-push .bp3-panel-stack2-appear{
  -webkit-transform:translateX(100%);
          transform:translateX(100%);
  opacity:0; }

.bp3-panel-stack2-push .bp3-panel-stack2-enter-active, .bp3-panel-stack2-push .bp3-panel-stack2-appear-active{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1;
  -webkit-transition-delay:0;
          transition-delay:0;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease; }

.bp3-panel-stack2-push .bp3-panel-stack2-exit{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1; }

.bp3-panel-stack2-push .bp3-panel-stack2-exit-active{
  -webkit-transform:translateX(-50%);
          transform:translateX(-50%);
  opacity:0;
  -webkit-transition-delay:0;
          transition-delay:0;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease; }

.bp3-panel-stack2-pop .bp3-panel-stack2-enter, .bp3-panel-stack2-pop .bp3-panel-stack2-appear{
  -webkit-transform:translateX(-50%);
          transform:translateX(-50%);
  opacity:0; }

.bp3-panel-stack2-pop .bp3-panel-stack2-enter-active, .bp3-panel-stack2-pop .bp3-panel-stack2-appear-active{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1;
  -webkit-transition-delay:0;
          transition-delay:0;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease; }

.bp3-panel-stack2-pop .bp3-panel-stack2-exit{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1; }

.bp3-panel-stack2-pop .bp3-panel-stack2-exit-active{
  -webkit-transform:translateX(100%);
          transform:translateX(100%);
  opacity:0;
  -webkit-transition-delay:0;
          transition-delay:0;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease; }
.bp3-popover{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  -webkit-transform:scale(1);
          transform:scale(1);
  border-radius:3px;
  display:inline-block;
  z-index:20; }
  .bp3-popover .bp3-popover-arrow{
    height:30px;
    position:absolute;
    width:30px; }
    .bp3-popover .bp3-popover-arrow::before{
      height:20px;
      margin:5px;
      width:20px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover{
    margin-bottom:17px;
    margin-top:-17px; }
    .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow{
      bottom:-11px; }
      .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(-90deg);
                transform:rotate(-90deg); }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover{
    margin-left:17px; }
    .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow{
      left:-11px; }
      .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(0);
                transform:rotate(0); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover{
    margin-top:17px; }
    .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow{
      top:-11px; }
      .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(90deg);
                transform:rotate(90deg); }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover{
    margin-left:-17px;
    margin-right:17px; }
    .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow{
      right:-11px; }
      .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(180deg);
                transform:rotate(180deg); }
  .bp3-tether-element-attached-middle > .bp3-popover > .bp3-popover-arrow{
    top:50%;
    -webkit-transform:translateY(-50%);
            transform:translateY(-50%); }
  .bp3-tether-element-attached-center > .bp3-popover > .bp3-popover-arrow{
    right:50%;
    -webkit-transform:translateX(50%);
            transform:translateX(50%); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow{
    top:-0.3934px; }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow{
    right:-0.3934px; }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow{
    left:-0.3934px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow{
    bottom:-0.3934px; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-left > .bp3-popover{
    -webkit-transform-origin:top left;
            transform-origin:top left; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-center > .bp3-popover{
    -webkit-transform-origin:top center;
            transform-origin:top center; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-right > .bp3-popover{
    -webkit-transform-origin:top right;
            transform-origin:top right; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-left > .bp3-popover{
    -webkit-transform-origin:center left;
            transform-origin:center left; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-center > .bp3-popover{
    -webkit-transform-origin:center center;
            transform-origin:center center; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-right > .bp3-popover{
    -webkit-transform-origin:center right;
            transform-origin:center right; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left > .bp3-popover{
    -webkit-transform-origin:bottom left;
            transform-origin:bottom left; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center > .bp3-popover{
    -webkit-transform-origin:bottom center;
            transform-origin:bottom center; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right > .bp3-popover{
    -webkit-transform-origin:bottom right;
            transform-origin:bottom right; }
  .bp3-popover .bp3-popover-content{
    background:#ffffff;
    color:inherit; }
  .bp3-popover .bp3-popover-arrow::before{
    -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2);
            box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); }
  .bp3-popover .bp3-popover-arrow-border{
    fill:#10161a;
    fill-opacity:0.1; }
  .bp3-popover .bp3-popover-arrow-fill{
    fill:#ffffff; }
  .bp3-popover-enter > .bp3-popover, .bp3-popover-appear > .bp3-popover{
    -webkit-transform:scale(0.3);
            transform:scale(0.3); }
  .bp3-popover-enter-active > .bp3-popover, .bp3-popover-appear-active > .bp3-popover{
    -webkit-transform:scale(1);
            transform:scale(1);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); }
  .bp3-popover-exit > .bp3-popover{
    -webkit-transform:scale(1);
            transform:scale(1); }
  .bp3-popover-exit-active > .bp3-popover{
    -webkit-transform:scale(0.3);
            transform:scale(0.3);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); }
  .bp3-popover .bp3-popover-content{
    border-radius:3px;
    position:relative; }
  .bp3-popover.bp3-popover-content-sizing .bp3-popover-content{
    max-width:350px;
    padding:20px; }
  .bp3-popover-target + .bp3-overlay .bp3-popover.bp3-popover-content-sizing{
    width:350px; }
  .bp3-popover.bp3-minimal{
    margin:0 !important; }
    .bp3-popover.bp3-minimal .bp3-popover-arrow{
      display:none; }
    .bp3-popover.bp3-minimal.bp3-popover{
      -webkit-transform:scale(1);
              transform:scale(1); }
      .bp3-popover-enter > .bp3-popover.bp3-minimal.bp3-popover, .bp3-popover-appear > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1); }
      .bp3-popover-enter-active > .bp3-popover.bp3-minimal.bp3-popover, .bp3-popover-appear-active > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1);
        -webkit-transition-delay:0;
                transition-delay:0;
        -webkit-transition-duration:100ms;
                transition-duration:100ms;
        -webkit-transition-property:-webkit-transform;
        transition-property:-webkit-transform;
        transition-property:transform;
        transition-property:transform, -webkit-transform;
        -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
                transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
      .bp3-popover-exit > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1); }
      .bp3-popover-exit-active > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1);
        -webkit-transition-delay:0;
                transition-delay:0;
        -webkit-transition-duration:100ms;
                transition-duration:100ms;
        -webkit-transition-property:-webkit-transform;
        transition-property:-webkit-transform;
        transition-property:transform;
        transition-property:transform, -webkit-transform;
        -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
                transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-popover.bp3-dark,
  .bp3-dark .bp3-popover{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
    .bp3-popover.bp3-dark .bp3-popover-content,
    .bp3-dark .bp3-popover .bp3-popover-content{
      background:#30404d;
      color:inherit; }
    .bp3-popover.bp3-dark .bp3-popover-arrow::before,
    .bp3-dark .bp3-popover .bp3-popover-arrow::before{
      -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4);
              box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); }
    .bp3-popover.bp3-dark .bp3-popover-arrow-border,
    .bp3-dark .bp3-popover .bp3-popover-arrow-border{
      fill:#10161a;
      fill-opacity:0.2; }
    .bp3-popover.bp3-dark .bp3-popover-arrow-fill,
    .bp3-dark .bp3-popover .bp3-popover-arrow-fill{
      fill:#30404d; }

.bp3-popover-arrow::before{
  border-radius:2px;
  content:"";
  display:block;
  position:absolute;
  -webkit-transform:rotate(45deg);
          transform:rotate(45deg); }

.bp3-tether-pinned .bp3-popover-arrow{
  display:none; }

.bp3-popover-backdrop{
  background:rgba(255, 255, 255, 0); }

.bp3-transition-container{
  opacity:1;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  z-index:20; }
  .bp3-transition-container.bp3-popover-enter, .bp3-transition-container.bp3-popover-appear{
    opacity:0; }
  .bp3-transition-container.bp3-popover-enter-active, .bp3-transition-container.bp3-popover-appear-active{
    opacity:1;
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-transition-container.bp3-popover-exit{
    opacity:1; }
  .bp3-transition-container.bp3-popover-exit-active{
    opacity:0;
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-transition-container:focus{
    outline:none; }
  .bp3-transition-container.bp3-popover-leave .bp3-popover-content{
    pointer-events:none; }
  .bp3-transition-container[data-x-out-of-boundaries]{
    display:none; }

span.bp3-popover-target{
  display:inline-block; }

.bp3-popover-wrapper.bp3-fill{
  width:100%; }

.bp3-portal{
  left:0;
  position:absolute;
  right:0;
  top:0; }
@-webkit-keyframes linear-progress-bar-stripes{
  from{
    background-position:0 0; }
  to{
    background-position:30px 0; } }
@keyframes linear-progress-bar-stripes{
  from{
    background-position:0 0; }
  to{
    background-position:30px 0; } }

.bp3-progress-bar{
  background:rgba(92, 112, 128, 0.2);
  border-radius:40px;
  display:block;
  height:8px;
  overflow:hidden;
  position:relative;
  width:100%; }
  .bp3-progress-bar .bp3-progress-meter{
    background:linear-gradient(-45deg, rgba(255, 255, 255, 0.2) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.2) 50%, rgba(255, 255, 255, 0.2) 75%, transparent 75%);
    background-color:rgba(92, 112, 128, 0.8);
    background-size:30px 30px;
    border-radius:40px;
    height:100%;
    position:absolute;
    -webkit-transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    width:100%; }
  .bp3-progress-bar:not(.bp3-no-animation):not(.bp3-no-stripes) .bp3-progress-meter{
    animation:linear-progress-bar-stripes 300ms linear infinite reverse; }
  .bp3-progress-bar.bp3-no-stripes .bp3-progress-meter{
    background-image:none; }

.bp3-dark .bp3-progress-bar{
  background:rgba(16, 22, 26, 0.5); }
  .bp3-dark .bp3-progress-bar .bp3-progress-meter{
    background-color:#8a9ba8; }

.bp3-progress-bar.bp3-intent-primary .bp3-progress-meter{
  background-color:#137cbd; }

.bp3-progress-bar.bp3-intent-success .bp3-progress-meter{
  background-color:#0f9960; }

.bp3-progress-bar.bp3-intent-warning .bp3-progress-meter{
  background-color:#d9822b; }

.bp3-progress-bar.bp3-intent-danger .bp3-progress-meter{
  background-color:#db3737; }
@-webkit-keyframes skeleton-glow{
  from{
    background:rgba(206, 217, 224, 0.2);
    border-color:rgba(206, 217, 224, 0.2); }
  to{
    background:rgba(92, 112, 128, 0.2);
    border-color:rgba(92, 112, 128, 0.2); } }
@keyframes skeleton-glow{
  from{
    background:rgba(206, 217, 224, 0.2);
    border-color:rgba(206, 217, 224, 0.2); }
  to{
    background:rgba(92, 112, 128, 0.2);
    border-color:rgba(92, 112, 128, 0.2); } }
.bp3-skeleton{
  -webkit-animation:1000ms linear infinite alternate skeleton-glow;
          animation:1000ms linear infinite alternate skeleton-glow;
  background:rgba(206, 217, 224, 0.2);
  background-clip:padding-box !important;
  border-color:rgba(206, 217, 224, 0.2) !important;
  border-radius:2px;
  -webkit-box-shadow:none !important;
          box-shadow:none !important;
  color:transparent !important;
  cursor:default;
  pointer-events:none;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-skeleton::before, .bp3-skeleton::after,
  .bp3-skeleton *{
    visibility:hidden !important; }
.bp3-slider{
  height:40px;
  min-width:150px;
  width:100%;
  cursor:default;
  outline:none;
  position:relative;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-slider:hover{
    cursor:pointer; }
  .bp3-slider:active{
    cursor:-webkit-grabbing;
    cursor:grabbing; }
  .bp3-slider.bp3-disabled{
    cursor:not-allowed;
    opacity:0.5; }
  .bp3-slider.bp3-slider-unlabeled{
    height:16px; }

.bp3-slider-track,
.bp3-slider-progress{
  height:6px;
  left:0;
  right:0;
  top:5px;
  position:absolute; }

.bp3-slider-track{
  border-radius:3px;
  overflow:hidden; }

.bp3-slider-progress{
  background:rgba(92, 112, 128, 0.2); }
  .bp3-dark .bp3-slider-progress{
    background:rgba(16, 22, 26, 0.5); }
  .bp3-slider-progress.bp3-intent-primary{
    background-color:#137cbd; }
  .bp3-slider-progress.bp3-intent-success{
    background-color:#0f9960; }
  .bp3-slider-progress.bp3-intent-warning{
    background-color:#d9822b; }
  .bp3-slider-progress.bp3-intent-danger{
    background-color:#db3737; }

.bp3-slider-handle{
  background-color:#f5f8fa;
  background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
  background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
  color:#182026;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
  cursor:pointer;
  height:16px;
  left:0;
  position:absolute;
  top:0;
  width:16px; }
  .bp3-slider-handle:hover{
    background-clip:padding-box;
    background-color:#ebf1f5;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
  .bp3-slider-handle:active, .bp3-slider-handle.bp3-active{
    background-color:#d8e1e8;
    background-image:none;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-slider-handle:disabled, .bp3-slider-handle.bp3-disabled{
    background-color:rgba(206, 217, 224, 0.5);
    background-image:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed;
    outline:none; }
    .bp3-slider-handle:disabled.bp3-active, .bp3-slider-handle:disabled.bp3-active:hover, .bp3-slider-handle.bp3-disabled.bp3-active, .bp3-slider-handle.bp3-disabled.bp3-active:hover{
      background:rgba(206, 217, 224, 0.7); }
  .bp3-slider-handle:focus{
    z-index:1; }
  .bp3-slider-handle:hover{
    background-clip:padding-box;
    background-color:#ebf1f5;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
    cursor:-webkit-grab;
    cursor:grab;
    z-index:2; }
  .bp3-slider-handle.bp3-active{
    background-color:#d8e1e8;
    background-image:none;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 1px rgba(16, 22, 26, 0.1);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 1px rgba(16, 22, 26, 0.1);
    cursor:-webkit-grabbing;
    cursor:grabbing; }
  .bp3-disabled .bp3-slider-handle{
    background:#bfccd6;
    -webkit-box-shadow:none;
            box-shadow:none;
    pointer-events:none; }
  .bp3-dark .bp3-slider-handle{
    background-color:#394b59;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }
    .bp3-dark .bp3-slider-handle:hover, .bp3-dark .bp3-slider-handle:active, .bp3-dark .bp3-slider-handle.bp3-active{
      color:#f5f8fa; }
    .bp3-dark .bp3-slider-handle:hover{
      background-color:#30404d;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-slider-handle:active, .bp3-dark .bp3-slider-handle.bp3-active{
      background-color:#202b33;
      background-image:none;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-dark .bp3-slider-handle:disabled, .bp3-dark .bp3-slider-handle.bp3-disabled{
      background-color:rgba(57, 75, 89, 0.5);
      background-image:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(167, 182, 194, 0.6); }
      .bp3-dark .bp3-slider-handle:disabled.bp3-active, .bp3-dark .bp3-slider-handle.bp3-disabled.bp3-active{
        background:rgba(57, 75, 89, 0.7); }
    .bp3-dark .bp3-slider-handle .bp3-button-spinner .bp3-spinner-head{
      background:rgba(16, 22, 26, 0.5);
      stroke:#8a9ba8; }
    .bp3-dark .bp3-slider-handle, .bp3-dark .bp3-slider-handle:hover{
      background-color:#394b59; }
    .bp3-dark .bp3-slider-handle.bp3-active{
      background-color:#293742; }
  .bp3-dark .bp3-disabled .bp3-slider-handle{
    background:#5c7080;
    border-color:#5c7080;
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-slider-handle .bp3-slider-label{
    background:#394b59;
    border-radius:3px;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
    color:#f5f8fa;
    margin-left:8px; }
    .bp3-dark .bp3-slider-handle .bp3-slider-label{
      background:#e1e8ed;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
      color:#394b59; }
    .bp3-disabled .bp3-slider-handle .bp3-slider-label{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-slider-handle.bp3-start, .bp3-slider-handle.bp3-end{
    width:8px; }
  .bp3-slider-handle.bp3-start{
    border-bottom-right-radius:0;
    border-top-right-radius:0; }
  .bp3-slider-handle.bp3-end{
    border-bottom-left-radius:0;
    border-top-left-radius:0;
    margin-left:8px; }
    .bp3-slider-handle.bp3-end .bp3-slider-label{
      margin-left:0; }

.bp3-slider-label{
  -webkit-transform:translate(-50%, 20px);
          transform:translate(-50%, 20px);
  display:inline-block;
  font-size:12px;
  line-height:1;
  padding:2px 5px;
  position:absolute;
  vertical-align:top; }

.bp3-slider.bp3-vertical{
  height:150px;
  min-width:40px;
  width:40px; }
  .bp3-slider.bp3-vertical .bp3-slider-track,
  .bp3-slider.bp3-vertical .bp3-slider-progress{
    bottom:0;
    height:auto;
    left:5px;
    top:0;
    width:6px; }
  .bp3-slider.bp3-vertical .bp3-slider-progress{
    top:auto; }
  .bp3-slider.bp3-vertical .bp3-slider-label{
    -webkit-transform:translate(20px, 50%);
            transform:translate(20px, 50%); }
  .bp3-slider.bp3-vertical .bp3-slider-handle{
    top:auto; }
    .bp3-slider.bp3-vertical .bp3-slider-handle .bp3-slider-label{
      margin-left:0;
      margin-top:-8px; }
    .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end, .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{
      height:8px;
      margin-left:0;
      width:16px; }
    .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{
      border-bottom-right-radius:3px;
      border-top-left-radius:0; }
      .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start .bp3-slider-label{
        -webkit-transform:translate(20px);
                transform:translate(20px); }
    .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end{
      border-bottom-left-radius:0;
      border-bottom-right-radius:0;
      border-top-left-radius:3px;
      margin-bottom:8px; }

@-webkit-keyframes pt-spinner-animation{
  from{
    -webkit-transform:rotate(0deg);
            transform:rotate(0deg); }
  to{
    -webkit-transform:rotate(360deg);
            transform:rotate(360deg); } }

@keyframes pt-spinner-animation{
  from{
    -webkit-transform:rotate(0deg);
            transform:rotate(0deg); }
  to{
    -webkit-transform:rotate(360deg);
            transform:rotate(360deg); } }

.bp3-spinner{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  overflow:visible;
  vertical-align:middle; }
  .bp3-spinner svg{
    display:block; }
  .bp3-spinner path{
    fill-opacity:0; }
  .bp3-spinner .bp3-spinner-head{
    stroke:rgba(92, 112, 128, 0.8);
    stroke-linecap:round;
    -webkit-transform-origin:center;
            transform-origin:center;
    -webkit-transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-spinner .bp3-spinner-track{
    stroke:rgba(92, 112, 128, 0.2); }

.bp3-spinner-animation{
  -webkit-animation:pt-spinner-animation 500ms linear infinite;
          animation:pt-spinner-animation 500ms linear infinite; }
  .bp3-no-spin > .bp3-spinner-animation{
    -webkit-animation:none;
            animation:none; }

.bp3-dark .bp3-spinner .bp3-spinner-head{
  stroke:#8a9ba8; }

.bp3-dark .bp3-spinner .bp3-spinner-track{
  stroke:rgba(16, 22, 26, 0.5); }

.bp3-spinner.bp3-intent-primary .bp3-spinner-head{
  stroke:#137cbd; }

.bp3-spinner.bp3-intent-success .bp3-spinner-head{
  stroke:#0f9960; }

.bp3-spinner.bp3-intent-warning .bp3-spinner-head{
  stroke:#d9822b; }

.bp3-spinner.bp3-intent-danger .bp3-spinner-head{
  stroke:#db3737; }
.bp3-tabs.bp3-vertical{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex; }
  .bp3-tabs.bp3-vertical > .bp3-tab-list{
    -webkit-box-align:start;
        -ms-flex-align:start;
            align-items:flex-start;
    -webkit-box-orient:vertical;
    -webkit-box-direction:normal;
        -ms-flex-direction:column;
            flex-direction:column; }
    .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab{
      border-radius:3px;
      padding:0 10px;
      width:100%; }
      .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab[aria-selected="true"]{
        background-color:rgba(19, 124, 189, 0.2);
        -webkit-box-shadow:none;
                box-shadow:none; }
    .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab-indicator-wrapper .bp3-tab-indicator{
      background-color:rgba(19, 124, 189, 0.2);
      border-radius:3px;
      bottom:0;
      height:auto;
      left:0;
      right:0;
      top:0; }
  .bp3-tabs.bp3-vertical > .bp3-tab-panel{
    margin-top:0;
    padding-left:20px; }

.bp3-tab-list{
  -webkit-box-align:end;
      -ms-flex-align:end;
          align-items:flex-end;
  border:none;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  list-style:none;
  margin:0;
  padding:0;
  position:relative; }
  .bp3-tab-list > *:not(:last-child){
    margin-right:20px; }

.bp3-tab{
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal;
  color:#182026;
  cursor:pointer;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  font-size:14px;
  line-height:30px;
  max-width:100%;
  position:relative;
  vertical-align:top; }
  .bp3-tab a{
    color:inherit;
    display:block;
    text-decoration:none; }
  .bp3-tab-indicator-wrapper ~ .bp3-tab{
    background-color:transparent !important;
    -webkit-box-shadow:none !important;
            box-shadow:none !important; }
  .bp3-tab[aria-disabled="true"]{
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed; }
  .bp3-tab[aria-selected="true"]{
    border-radius:0;
    -webkit-box-shadow:inset 0 -3px 0 #106ba3;
            box-shadow:inset 0 -3px 0 #106ba3; }
  .bp3-tab[aria-selected="true"], .bp3-tab:not([aria-disabled="true"]):hover{
    color:#106ba3; }
  .bp3-tab:focus{
    -moz-outline-radius:0; }
  .bp3-large > .bp3-tab{
    font-size:16px;
    line-height:40px; }

.bp3-tab-panel{
  margin-top:20px; }
  .bp3-tab-panel[aria-hidden="true"]{
    display:none; }

.bp3-tab-indicator-wrapper{
  left:0;
  pointer-events:none;
  position:absolute;
  top:0;
  -webkit-transform:translateX(0), translateY(0);
          transform:translateX(0), translateY(0);
  -webkit-transition:height, width, -webkit-transform;
  transition:height, width, -webkit-transform;
  transition:height, transform, width;
  transition:height, transform, width, -webkit-transform;
  -webkit-transition-duration:200ms;
          transition-duration:200ms;
  -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
          transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-tab-indicator-wrapper .bp3-tab-indicator{
    background-color:#106ba3;
    bottom:0;
    height:3px;
    left:0;
    position:absolute;
    right:0; }
  .bp3-tab-indicator-wrapper.bp3-no-animation{
    -webkit-transition:none;
    transition:none; }

.bp3-dark .bp3-tab{
  color:#f5f8fa; }
  .bp3-dark .bp3-tab[aria-disabled="true"]{
    color:rgba(167, 182, 194, 0.6); }
  .bp3-dark .bp3-tab[aria-selected="true"]{
    -webkit-box-shadow:inset 0 -3px 0 #48aff0;
            box-shadow:inset 0 -3px 0 #48aff0; }
  .bp3-dark .bp3-tab[aria-selected="true"], .bp3-dark .bp3-tab:not([aria-disabled="true"]):hover{
    color:#48aff0; }

.bp3-dark .bp3-tab-indicator{
  background-color:#48aff0; }

.bp3-flex-expander{
  -webkit-box-flex:1;
      -ms-flex:1 1;
          flex:1 1; }
.bp3-tag{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  background-color:#5c7080;
  border:none;
  border-radius:3px;
  -webkit-box-shadow:none;
          box-shadow:none;
  color:#f5f8fa;
  font-size:12px;
  line-height:16px;
  max-width:100%;
  min-height:20px;
  min-width:20px;
  padding:2px 6px;
  position:relative; }
  .bp3-tag.bp3-interactive{
    cursor:pointer; }
    .bp3-tag.bp3-interactive:hover{
      background-color:rgba(92, 112, 128, 0.85); }
    .bp3-tag.bp3-interactive.bp3-active, .bp3-tag.bp3-interactive:active{
      background-color:rgba(92, 112, 128, 0.7); }
  .bp3-tag > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-tag > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-tag::before,
  .bp3-tag > *{
    margin-right:4px; }
  .bp3-tag:empty::before,
  .bp3-tag > :last-child{
    margin-right:0; }
  .bp3-tag:focus{
    outline:rgba(19, 124, 189, 0.6) auto 2px;
    outline-offset:0;
    -moz-outline-radius:6px; }
  .bp3-tag.bp3-round{
    border-radius:30px;
    padding-left:8px;
    padding-right:8px; }
  .bp3-dark .bp3-tag{
    background-color:#bfccd6;
    color:#182026; }
    .bp3-dark .bp3-tag.bp3-interactive{
      cursor:pointer; }
      .bp3-dark .bp3-tag.bp3-interactive:hover{
        background-color:rgba(191, 204, 214, 0.85); }
      .bp3-dark .bp3-tag.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-interactive:active{
        background-color:rgba(191, 204, 214, 0.7); }
    .bp3-dark .bp3-tag > .bp3-icon, .bp3-dark .bp3-tag .bp3-icon-standard, .bp3-dark .bp3-tag .bp3-icon-large{
      fill:currentColor; }
  .bp3-tag > .bp3-icon, .bp3-tag .bp3-icon-standard, .bp3-tag .bp3-icon-large{
    fill:#ffffff; }
  .bp3-tag.bp3-large,
  .bp3-large .bp3-tag{
    font-size:14px;
    line-height:20px;
    min-height:30px;
    min-width:30px;
    padding:5px 10px; }
    .bp3-tag.bp3-large::before,
    .bp3-tag.bp3-large > *,
    .bp3-large .bp3-tag::before,
    .bp3-large .bp3-tag > *{
      margin-right:7px; }
    .bp3-tag.bp3-large:empty::before,
    .bp3-tag.bp3-large > :last-child,
    .bp3-large .bp3-tag:empty::before,
    .bp3-large .bp3-tag > :last-child{
      margin-right:0; }
    .bp3-tag.bp3-large.bp3-round,
    .bp3-large .bp3-tag.bp3-round{
      padding-left:12px;
      padding-right:12px; }
  .bp3-tag.bp3-intent-primary{
    background:#137cbd;
    color:#ffffff; }
    .bp3-tag.bp3-intent-primary.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-primary.bp3-interactive:hover{
        background-color:rgba(19, 124, 189, 0.85); }
      .bp3-tag.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-primary.bp3-interactive:active{
        background-color:rgba(19, 124, 189, 0.7); }
  .bp3-tag.bp3-intent-success{
    background:#0f9960;
    color:#ffffff; }
    .bp3-tag.bp3-intent-success.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-success.bp3-interactive:hover{
        background-color:rgba(15, 153, 96, 0.85); }
      .bp3-tag.bp3-intent-success.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-success.bp3-interactive:active{
        background-color:rgba(15, 153, 96, 0.7); }
  .bp3-tag.bp3-intent-warning{
    background:#d9822b;
    color:#ffffff; }
    .bp3-tag.bp3-intent-warning.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-warning.bp3-interactive:hover{
        background-color:rgba(217, 130, 43, 0.85); }
      .bp3-tag.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-warning.bp3-interactive:active{
        background-color:rgba(217, 130, 43, 0.7); }
  .bp3-tag.bp3-intent-danger{
    background:#db3737;
    color:#ffffff; }
    .bp3-tag.bp3-intent-danger.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-danger.bp3-interactive:hover{
        background-color:rgba(219, 55, 55, 0.85); }
      .bp3-tag.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-danger.bp3-interactive:active{
        background-color:rgba(219, 55, 55, 0.7); }
  .bp3-tag.bp3-fill{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    width:100%; }
  .bp3-tag.bp3-minimal > .bp3-icon, .bp3-tag.bp3-minimal .bp3-icon-standard, .bp3-tag.bp3-minimal .bp3-icon-large{
    fill:#5c7080; }
  .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]){
    background-color:rgba(138, 155, 168, 0.2);
    color:#182026; }
    .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:hover{
        background-color:rgba(92, 112, 128, 0.3); }
      .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive.bp3-active, .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:active{
        background-color:rgba(92, 112, 128, 0.4); }
    .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]){
      color:#f5f8fa; }
      .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:hover{
          background-color:rgba(191, 204, 214, 0.3); }
        .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:active{
          background-color:rgba(191, 204, 214, 0.4); }
      .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]) > .bp3-icon, .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]) .bp3-icon-standard, .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]) .bp3-icon-large{
        fill:#a7b6c2; }
  .bp3-tag.bp3-minimal.bp3-intent-primary{
    background-color:rgba(19, 124, 189, 0.15);
    color:#106ba3; }
    .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{
        background-color:rgba(19, 124, 189, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{
        background-color:rgba(19, 124, 189, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-primary > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-large{
      fill:#137cbd; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary{
      background-color:rgba(19, 124, 189, 0.25);
      color:#48aff0; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{
          background-color:rgba(19, 124, 189, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{
          background-color:rgba(19, 124, 189, 0.45); }
  .bp3-tag.bp3-minimal.bp3-intent-success{
    background-color:rgba(15, 153, 96, 0.15);
    color:#0d8050; }
    .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{
        background-color:rgba(15, 153, 96, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{
        background-color:rgba(15, 153, 96, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-success > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-large{
      fill:#0f9960; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success{
      background-color:rgba(15, 153, 96, 0.25);
      color:#3dcc91; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{
          background-color:rgba(15, 153, 96, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{
          background-color:rgba(15, 153, 96, 0.45); }
  .bp3-tag.bp3-minimal.bp3-intent-warning{
    background-color:rgba(217, 130, 43, 0.15);
    color:#bf7326; }
    .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{
        background-color:rgba(217, 130, 43, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{
        background-color:rgba(217, 130, 43, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-warning > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-large{
      fill:#d9822b; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning{
      background-color:rgba(217, 130, 43, 0.25);
      color:#ffb366; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{
          background-color:rgba(217, 130, 43, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{
          background-color:rgba(217, 130, 43, 0.45); }
  .bp3-tag.bp3-minimal.bp3-intent-danger{
    background-color:rgba(219, 55, 55, 0.15);
    color:#c23030; }
    .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{
        background-color:rgba(219, 55, 55, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{
        background-color:rgba(219, 55, 55, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-danger > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-large{
      fill:#db3737; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger{
      background-color:rgba(219, 55, 55, 0.25);
      color:#ff7373; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{
          background-color:rgba(219, 55, 55, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{
          background-color:rgba(219, 55, 55, 0.45); }

.bp3-tag-remove{
  background:none;
  border:none;
  color:inherit;
  cursor:pointer;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  margin-bottom:-2px;
  margin-right:-6px !important;
  margin-top:-2px;
  opacity:0.5;
  padding:2px;
  padding-left:0; }
  .bp3-tag-remove:hover{
    background:none;
    opacity:0.8;
    text-decoration:none; }
  .bp3-tag-remove:active{
    opacity:1; }
  .bp3-tag-remove:empty::before{
    font-family:"Icons16", sans-serif;
    font-size:16px;
    font-style:normal;
    font-weight:400;
    line-height:1;
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased;
    content:""; }
  .bp3-large .bp3-tag-remove{
    margin-right:-10px !important;
    padding:0 5px 0 0; }
    .bp3-large .bp3-tag-remove:empty::before{
      font-family:"Icons20", sans-serif;
      font-size:20px;
      font-style:normal;
      font-weight:400;
      line-height:1; }
.bp3-tag-input{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:start;
      -ms-flex-align:start;
          align-items:flex-start;
  cursor:text;
  height:auto;
  line-height:inherit;
  min-height:30px;
  padding-left:5px;
  padding-right:0; }
  .bp3-tag-input > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-tag-input > .bp3-tag-input-values{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-tag-input .bp3-tag-input-icon{
    color:#5c7080;
    margin-left:2px;
    margin-right:7px;
    margin-top:7px; }
  .bp3-tag-input .bp3-tag-input-values{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-orient:horizontal;
    -webkit-box-direction:normal;
        -ms-flex-direction:row;
            flex-direction:row;
    -webkit-box-align:center;
        -ms-flex-align:center;
            align-items:center;
    -ms-flex-item-align:stretch;
        align-self:stretch;
    -ms-flex-wrap:wrap;
        flex-wrap:wrap;
    margin-right:7px;
    margin-top:5px;
    min-width:0; }
    .bp3-tag-input .bp3-tag-input-values > *{
      -webkit-box-flex:0;
          -ms-flex-positive:0;
              flex-grow:0;
      -ms-flex-negative:0;
          flex-shrink:0; }
    .bp3-tag-input .bp3-tag-input-values > .bp3-fill{
      -webkit-box-flex:1;
          -ms-flex-positive:1;
              flex-grow:1;
      -ms-flex-negative:1;
          flex-shrink:1; }
    .bp3-tag-input .bp3-tag-input-values::before,
    .bp3-tag-input .bp3-tag-input-values > *{
      margin-right:5px; }
    .bp3-tag-input .bp3-tag-input-values:empty::before,
    .bp3-tag-input .bp3-tag-input-values > :last-child{
      margin-right:0; }
    .bp3-tag-input .bp3-tag-input-values:first-child .bp3-input-ghost:first-child{
      padding-left:5px; }
    .bp3-tag-input .bp3-tag-input-values > *{
      margin-bottom:5px; }
  .bp3-tag-input .bp3-tag{
    overflow-wrap:break-word; }
    .bp3-tag-input .bp3-tag.bp3-active{
      outline:rgba(19, 124, 189, 0.6) auto 2px;
      outline-offset:0;
      -moz-outline-radius:6px; }
  .bp3-tag-input .bp3-input-ghost{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    line-height:20px;
    width:80px; }
    .bp3-tag-input .bp3-input-ghost:disabled, .bp3-tag-input .bp3-input-ghost.bp3-disabled{
      cursor:not-allowed; }
  .bp3-tag-input .bp3-button,
  .bp3-tag-input .bp3-spinner{
    margin:3px;
    margin-left:0; }
  .bp3-tag-input .bp3-button{
    min-height:24px;
    min-width:24px;
    padding:0 7px; }
  .bp3-tag-input.bp3-large{
    height:auto;
    min-height:40px; }
    .bp3-tag-input.bp3-large::before,
    .bp3-tag-input.bp3-large > *{
      margin-right:10px; }
    .bp3-tag-input.bp3-large:empty::before,
    .bp3-tag-input.bp3-large > :last-child{
      margin-right:0; }
    .bp3-tag-input.bp3-large .bp3-tag-input-icon{
      margin-left:5px;
      margin-top:10px; }
    .bp3-tag-input.bp3-large .bp3-input-ghost{
      line-height:30px; }
    .bp3-tag-input.bp3-large .bp3-button{
      min-height:30px;
      min-width:30px;
      padding:5px 10px;
      margin:5px;
      margin-left:0; }
    .bp3-tag-input.bp3-large .bp3-spinner{
      margin:8px;
      margin-left:0; }
  .bp3-tag-input.bp3-active{
    background-color:#ffffff;
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-primary{
      -webkit-box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-success{
      -webkit-box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-warning{
      -webkit-box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-danger{
      -webkit-box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-tag-input .bp3-tag-input-icon, .bp3-tag-input.bp3-dark .bp3-tag-input-icon{
    color:#a7b6c2; }
  .bp3-dark .bp3-tag-input .bp3-input-ghost, .bp3-tag-input.bp3-dark .bp3-input-ghost{
    color:#f5f8fa; }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::-webkit-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::-moz-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost:-ms-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::-ms-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::placeholder{
      color:rgba(167, 182, 194, 0.6); }
  .bp3-dark .bp3-tag-input.bp3-active, .bp3-tag-input.bp3-dark.bp3-active{
    background-color:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-primary, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-primary{
      -webkit-box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-success, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-success{
      -webkit-box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-warning, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-warning{
      -webkit-box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-danger, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-danger{
      -webkit-box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-input-ghost{
  background:none;
  border:none;
  -webkit-box-shadow:none;
          box-shadow:none;
  padding:0; }
  .bp3-input-ghost::-webkit-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input-ghost::-moz-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input-ghost:-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input-ghost::-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input-ghost::placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input-ghost:focus{
    outline:none !important; }
.bp3-toast{
  -webkit-box-align:start;
      -ms-flex-align:start;
          align-items:flex-start;
  background-color:#ffffff;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  margin:20px 0 0;
  max-width:500px;
  min-width:300px;
  pointer-events:all;
  position:relative !important; }
  .bp3-toast.bp3-toast-enter, .bp3-toast.bp3-toast-appear{
    -webkit-transform:translateY(-40px);
            transform:translateY(-40px); }
  .bp3-toast.bp3-toast-enter-active, .bp3-toast.bp3-toast-appear-active{
    -webkit-transform:translateY(0);
            transform:translateY(0);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); }
  .bp3-toast.bp3-toast-enter ~ .bp3-toast, .bp3-toast.bp3-toast-appear ~ .bp3-toast{
    -webkit-transform:translateY(-40px);
            transform:translateY(-40px); }
  .bp3-toast.bp3-toast-enter-active ~ .bp3-toast, .bp3-toast.bp3-toast-appear-active ~ .bp3-toast{
    -webkit-transform:translateY(0);
            transform:translateY(0);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); }
  .bp3-toast.bp3-toast-exit{
    opacity:1;
    -webkit-filter:blur(0);
            filter:blur(0); }
  .bp3-toast.bp3-toast-exit-active{
    opacity:0;
    -webkit-filter:blur(10px);
            filter:blur(10px);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:opacity, -webkit-filter;
    transition-property:opacity, -webkit-filter;
    transition-property:opacity, filter;
    transition-property:opacity, filter, -webkit-filter;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-toast.bp3-toast-exit ~ .bp3-toast{
    -webkit-transform:translateY(0);
            transform:translateY(0); }
  .bp3-toast.bp3-toast-exit-active ~ .bp3-toast{
    -webkit-transform:translateY(-40px);
            transform:translateY(-40px);
    -webkit-transition-delay:50ms;
            transition-delay:50ms;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-toast .bp3-button-group{
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    padding:5px;
    padding-left:0; }
  .bp3-toast > .bp3-icon{
    color:#5c7080;
    margin:12px;
    margin-right:0; }
  .bp3-toast.bp3-dark,
  .bp3-dark .bp3-toast{
    background-color:#394b59;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
    .bp3-toast.bp3-dark > .bp3-icon,
    .bp3-dark .bp3-toast > .bp3-icon{
      color:#a7b6c2; }
  .bp3-toast[class*="bp3-intent-"] a{
    color:rgba(255, 255, 255, 0.7); }
    .bp3-toast[class*="bp3-intent-"] a:hover{
      color:#ffffff; }
  .bp3-toast[class*="bp3-intent-"] > .bp3-icon{
    color:#ffffff; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button, .bp3-toast[class*="bp3-intent-"] .bp3-button::before,
  .bp3-toast[class*="bp3-intent-"] .bp3-button .bp3-icon, .bp3-toast[class*="bp3-intent-"] .bp3-button:active{
    color:rgba(255, 255, 255, 0.7) !important; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button:focus{
    outline-color:rgba(255, 255, 255, 0.5); }
  .bp3-toast[class*="bp3-intent-"] .bp3-button:hover{
    background-color:rgba(255, 255, 255, 0.15) !important;
    color:#ffffff !important; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button:active{
    background-color:rgba(255, 255, 255, 0.3) !important;
    color:#ffffff !important; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button::after{
    background:rgba(255, 255, 255, 0.3) !important; }
  .bp3-toast.bp3-intent-primary{
    background-color:#137cbd;
    color:#ffffff; }
  .bp3-toast.bp3-intent-success{
    background-color:#0f9960;
    color:#ffffff; }
  .bp3-toast.bp3-intent-warning{
    background-color:#d9822b;
    color:#ffffff; }
  .bp3-toast.bp3-intent-danger{
    background-color:#db3737;
    color:#ffffff; }

.bp3-toast-message{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  padding:11px;
  word-break:break-word; }

.bp3-toast-container{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-box !important;
  display:-ms-flexbox !important;
  display:flex !important;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  left:0;
  overflow:hidden;
  padding:0 20px 20px;
  pointer-events:none;
  right:0;
  z-index:40; }
  .bp3-toast-container.bp3-toast-container-in-portal{
    position:fixed; }
  .bp3-toast-container.bp3-toast-container-inline{
    position:absolute; }
  .bp3-toast-container.bp3-toast-container-top{
    top:0; }
  .bp3-toast-container.bp3-toast-container-bottom{
    bottom:0;
    -webkit-box-orient:vertical;
    -webkit-box-direction:reverse;
        -ms-flex-direction:column-reverse;
            flex-direction:column-reverse;
    top:auto; }
  .bp3-toast-container.bp3-toast-container-left{
    -webkit-box-align:start;
        -ms-flex-align:start;
            align-items:flex-start; }
  .bp3-toast-container.bp3-toast-container-right{
    -webkit-box-align:end;
        -ms-flex-align:end;
            align-items:flex-end; }

.bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active),
.bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active) ~ .bp3-toast, .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active),
.bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active) ~ .bp3-toast,
.bp3-toast-container-bottom .bp3-toast.bp3-toast-exit-active ~ .bp3-toast,
.bp3-toast-container-bottom .bp3-toast.bp3-toast-leave-active ~ .bp3-toast{
  -webkit-transform:translateY(60px);
          transform:translateY(60px); }
.bp3-tooltip{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  -webkit-transform:scale(1);
          transform:scale(1); }
  .bp3-tooltip .bp3-popover-arrow{
    height:22px;
    position:absolute;
    width:22px; }
    .bp3-tooltip .bp3-popover-arrow::before{
      height:14px;
      margin:4px;
      width:14px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip{
    margin-bottom:11px;
    margin-top:-11px; }
    .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow{
      bottom:-8px; }
      .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(-90deg);
                transform:rotate(-90deg); }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip{
    margin-left:11px; }
    .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow{
      left:-8px; }
      .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(0);
                transform:rotate(0); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip{
    margin-top:11px; }
    .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow{
      top:-8px; }
      .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(90deg);
                transform:rotate(90deg); }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip{
    margin-left:-11px;
    margin-right:11px; }
    .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow{
      right:-8px; }
      .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(180deg);
                transform:rotate(180deg); }
  .bp3-tether-element-attached-middle > .bp3-tooltip > .bp3-popover-arrow{
    top:50%;
    -webkit-transform:translateY(-50%);
            transform:translateY(-50%); }
  .bp3-tether-element-attached-center > .bp3-tooltip > .bp3-popover-arrow{
    right:50%;
    -webkit-transform:translateX(50%);
            transform:translateX(50%); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow{
    top:-0.22183px; }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow{
    right:-0.22183px; }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow{
    left:-0.22183px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow{
    bottom:-0.22183px; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-left > .bp3-tooltip{
    -webkit-transform-origin:top left;
            transform-origin:top left; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-center > .bp3-tooltip{
    -webkit-transform-origin:top center;
            transform-origin:top center; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-right > .bp3-tooltip{
    -webkit-transform-origin:top right;
            transform-origin:top right; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-left > .bp3-tooltip{
    -webkit-transform-origin:center left;
            transform-origin:center left; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-center > .bp3-tooltip{
    -webkit-transform-origin:center center;
            transform-origin:center center; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-right > .bp3-tooltip{
    -webkit-transform-origin:center right;
            transform-origin:center right; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left > .bp3-tooltip{
    -webkit-transform-origin:bottom left;
            transform-origin:bottom left; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center > .bp3-tooltip{
    -webkit-transform-origin:bottom center;
            transform-origin:bottom center; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right > .bp3-tooltip{
    -webkit-transform-origin:bottom right;
            transform-origin:bottom right; }
  .bp3-tooltip .bp3-popover-content{
    background:#394b59;
    color:#f5f8fa; }
  .bp3-tooltip .bp3-popover-arrow::before{
    -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2);
            box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); }
  .bp3-tooltip .bp3-popover-arrow-border{
    fill:#10161a;
    fill-opacity:0.1; }
  .bp3-tooltip .bp3-popover-arrow-fill{
    fill:#394b59; }
  .bp3-popover-enter > .bp3-tooltip, .bp3-popover-appear > .bp3-tooltip{
    -webkit-transform:scale(0.8);
            transform:scale(0.8); }
  .bp3-popover-enter-active > .bp3-tooltip, .bp3-popover-appear-active > .bp3-tooltip{
    -webkit-transform:scale(1);
            transform:scale(1);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-popover-exit > .bp3-tooltip{
    -webkit-transform:scale(1);
            transform:scale(1); }
  .bp3-popover-exit-active > .bp3-tooltip{
    -webkit-transform:scale(0.8);
            transform:scale(0.8);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-tooltip .bp3-popover-content{
    padding:10px 12px; }
  .bp3-tooltip.bp3-dark,
  .bp3-dark .bp3-tooltip{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
    .bp3-tooltip.bp3-dark .bp3-popover-content,
    .bp3-dark .bp3-tooltip .bp3-popover-content{
      background:#e1e8ed;
      color:#394b59; }
    .bp3-tooltip.bp3-dark .bp3-popover-arrow::before,
    .bp3-dark .bp3-tooltip .bp3-popover-arrow::before{
      -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4);
              box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); }
    .bp3-tooltip.bp3-dark .bp3-popover-arrow-border,
    .bp3-dark .bp3-tooltip .bp3-popover-arrow-border{
      fill:#10161a;
      fill-opacity:0.2; }
    .bp3-tooltip.bp3-dark .bp3-popover-arrow-fill,
    .bp3-dark .bp3-tooltip .bp3-popover-arrow-fill{
      fill:#e1e8ed; }
  .bp3-tooltip.bp3-intent-primary .bp3-popover-content{
    background:#137cbd;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-primary .bp3-popover-arrow-fill{
    fill:#137cbd; }
  .bp3-tooltip.bp3-intent-success .bp3-popover-content{
    background:#0f9960;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-success .bp3-popover-arrow-fill{
    fill:#0f9960; }
  .bp3-tooltip.bp3-intent-warning .bp3-popover-content{
    background:#d9822b;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-warning .bp3-popover-arrow-fill{
    fill:#d9822b; }
  .bp3-tooltip.bp3-intent-danger .bp3-popover-content{
    background:#db3737;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-danger .bp3-popover-arrow-fill{
    fill:#db3737; }

.bp3-tooltip-indicator{
  border-bottom:dotted 1px;
  cursor:help; }
.bp3-tree .bp3-icon, .bp3-tree .bp3-icon-standard, .bp3-tree .bp3-icon-large{
  color:#5c7080; }
  .bp3-tree .bp3-icon.bp3-intent-primary, .bp3-tree .bp3-icon-standard.bp3-intent-primary, .bp3-tree .bp3-icon-large.bp3-intent-primary{
    color:#137cbd; }
  .bp3-tree .bp3-icon.bp3-intent-success, .bp3-tree .bp3-icon-standard.bp3-intent-success, .bp3-tree .bp3-icon-large.bp3-intent-success{
    color:#0f9960; }
  .bp3-tree .bp3-icon.bp3-intent-warning, .bp3-tree .bp3-icon-standard.bp3-intent-warning, .bp3-tree .bp3-icon-large.bp3-intent-warning{
    color:#d9822b; }
  .bp3-tree .bp3-icon.bp3-intent-danger, .bp3-tree .bp3-icon-standard.bp3-intent-danger, .bp3-tree .bp3-icon-large.bp3-intent-danger{
    color:#db3737; }

.bp3-tree-node-list{
  list-style:none;
  margin:0;
  padding-left:0; }

.bp3-tree-root{
  background-color:transparent;
  cursor:default;
  padding-left:0;
  position:relative; }

.bp3-tree-node-content-0{
  padding-left:0px; }

.bp3-tree-node-content-1{
  padding-left:23px; }

.bp3-tree-node-content-2{
  padding-left:46px; }

.bp3-tree-node-content-3{
  padding-left:69px; }

.bp3-tree-node-content-4{
  padding-left:92px; }

.bp3-tree-node-content-5{
  padding-left:115px; }

.bp3-tree-node-content-6{
  padding-left:138px; }

.bp3-tree-node-content-7{
  padding-left:161px; }

.bp3-tree-node-content-8{
  padding-left:184px; }

.bp3-tree-node-content-9{
  padding-left:207px; }

.bp3-tree-node-content-10{
  padding-left:230px; }

.bp3-tree-node-content-11{
  padding-left:253px; }

.bp3-tree-node-content-12{
  padding-left:276px; }

.bp3-tree-node-content-13{
  padding-left:299px; }

.bp3-tree-node-content-14{
  padding-left:322px; }

.bp3-tree-node-content-15{
  padding-left:345px; }

.bp3-tree-node-content-16{
  padding-left:368px; }

.bp3-tree-node-content-17{
  padding-left:391px; }

.bp3-tree-node-content-18{
  padding-left:414px; }

.bp3-tree-node-content-19{
  padding-left:437px; }

.bp3-tree-node-content-20{
  padding-left:460px; }

.bp3-tree-node-content{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  height:30px;
  padding-right:5px;
  width:100%; }
  .bp3-tree-node-content:hover{
    background-color:rgba(191, 204, 214, 0.4); }

.bp3-tree-node-caret,
.bp3-tree-node-caret-none{
  min-width:30px; }

.bp3-tree-node-caret{
  color:#5c7080;
  cursor:pointer;
  padding:7px;
  -webkit-transform:rotate(0deg);
          transform:rotate(0deg);
  -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-tree-node-caret:hover{
    color:#182026; }
  .bp3-dark .bp3-tree-node-caret{
    color:#a7b6c2; }
    .bp3-dark .bp3-tree-node-caret:hover{
      color:#f5f8fa; }
  .bp3-tree-node-caret.bp3-tree-node-caret-open{
    -webkit-transform:rotate(90deg);
            transform:rotate(90deg); }
  .bp3-tree-node-caret.bp3-icon-standard::before{
    content:""; }

.bp3-tree-node-icon{
  margin-right:7px;
  position:relative; }

.bp3-tree-node-label{
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal;
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  position:relative;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-tree-node-label span{
    display:inline; }

.bp3-tree-node-secondary-label{
  padding:0 5px;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-tree-node-secondary-label .bp3-popover-wrapper,
  .bp3-tree-node-secondary-label .bp3-popover-target{
    -webkit-box-align:center;
        -ms-flex-align:center;
            align-items:center;
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex; }

.bp3-tree-node.bp3-disabled .bp3-tree-node-content{
  background-color:inherit;
  color:rgba(92, 112, 128, 0.6);
  cursor:not-allowed; }

.bp3-tree-node.bp3-disabled .bp3-tree-node-caret,
.bp3-tree-node.bp3-disabled .bp3-tree-node-icon{
  color:rgba(92, 112, 128, 0.6);
  cursor:not-allowed; }

.bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content{
  background-color:#137cbd; }
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content,
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon-standard, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon-large{
    color:#ffffff; }
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-tree-node-caret::before{
    color:rgba(255, 255, 255, 0.7); }
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-tree-node-caret:hover::before{
    color:#ffffff; }

.bp3-dark .bp3-tree-node-content:hover{
  background-color:rgba(92, 112, 128, 0.3); }

.bp3-dark .bp3-tree .bp3-icon, .bp3-dark .bp3-tree .bp3-icon-standard, .bp3-dark .bp3-tree .bp3-icon-large{
  color:#a7b6c2; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-primary, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-primary, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-primary{
    color:#137cbd; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-success, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-success, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-success{
    color:#0f9960; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-warning, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-warning, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-warning{
    color:#d9822b; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-danger, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-danger, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-danger{
    color:#db3737; }

.bp3-dark .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content{
  background-color:#137cbd; }
.bp3-omnibar{
  -webkit-filter:blur(0);
          filter:blur(0);
  opacity:1;
  background-color:#ffffff;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
  left:calc(50% - 250px);
  top:20vh;
  width:500px;
  z-index:21; }
  .bp3-omnibar.bp3-overlay-enter, .bp3-omnibar.bp3-overlay-appear{
    -webkit-filter:blur(20px);
            filter:blur(20px);
    opacity:0.2; }
  .bp3-omnibar.bp3-overlay-enter-active, .bp3-omnibar.bp3-overlay-appear-active{
    -webkit-filter:blur(0);
            filter:blur(0);
    opacity:1;
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-property:opacity, -webkit-filter;
    transition-property:opacity, -webkit-filter;
    transition-property:filter, opacity;
    transition-property:filter, opacity, -webkit-filter;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-omnibar.bp3-overlay-exit{
    -webkit-filter:blur(0);
            filter:blur(0);
    opacity:1; }
  .bp3-omnibar.bp3-overlay-exit-active{
    -webkit-filter:blur(20px);
            filter:blur(20px);
    opacity:0.2;
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-property:opacity, -webkit-filter;
    transition-property:opacity, -webkit-filter;
    transition-property:filter, opacity;
    transition-property:filter, opacity, -webkit-filter;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-omnibar .bp3-input{
    background-color:transparent;
    border-radius:0; }
    .bp3-omnibar .bp3-input, .bp3-omnibar .bp3-input:focus{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-omnibar .bp3-menu{
    background-color:transparent;
    border-radius:0;
    -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
    max-height:calc(60vh - 40px);
    overflow:auto; }
    .bp3-omnibar .bp3-menu:empty{
      display:none; }
  .bp3-dark .bp3-omnibar, .bp3-omnibar.bp3-dark{
    background-color:#30404d;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); }

.bp3-omnibar-overlay .bp3-overlay-backdrop{
  background-color:rgba(16, 22, 26, 0.2); }

.bp3-select-popover .bp3-popover-content{
  padding:5px; }

.bp3-select-popover .bp3-input-group{
  margin-bottom:0; }

.bp3-select-popover .bp3-menu{
  max-height:300px;
  max-width:400px;
  overflow:auto;
  padding:0; }
  .bp3-select-popover .bp3-menu:not(:first-child){
    padding-top:5px; }

.bp3-multi-select{
  min-width:150px; }

.bp3-multi-select-popover .bp3-menu{
  max-height:300px;
  max-width:400px;
  overflow:auto; }

.bp3-select-popover .bp3-popover-content{
  padding:5px; }

.bp3-select-popover .bp3-input-group{
  margin-bottom:0; }

.bp3-select-popover .bp3-menu{
  max-height:300px;
  max-width:400px;
  overflow:auto;
  padding:0; }
  .bp3-select-popover .bp3-menu:not(:first-child){
    padding-top:5px; }
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE0LjkgMTcuNDVDMTYuMjUgMTcuNDUgMTcuMzUgMTYuMzUgMTcuMzUgMTVDMTcuMzUgMTMuNjUgMTYuMjUgMTIuNTUgMTQuOSAxMi41NUMxMy41NCAxMi41NSAxMi40NSAxMy42NSAxMi40NSAxNUMxMi40NSAxNi4zNSAxMy41NCAxNy40NSAxNC45IDE3LjQ1Wk0yMC4xIDE1LjY4TDIxLjU4IDE2Ljg0QzIxLjcxIDE2Ljk1IDIxLjc1IDE3LjEzIDIxLjY2IDE3LjI5TDIwLjI2IDE5LjcxQzIwLjE3IDE5Ljg2IDIwIDE5LjkyIDE5LjgzIDE5Ljg2TDE4LjA5IDE5LjE2QzE3LjczIDE5LjQ0IDE3LjMzIDE5LjY3IDE2LjkxIDE5Ljg1TDE2LjY0IDIxLjdDMTYuNjIgMjEuODcgMTYuNDcgMjIgMTYuMyAyMkgxMy41QzEzLjMyIDIyIDEzLjE4IDIxLjg3IDEzLjE1IDIxLjdMMTIuODkgMTkuODVDMTIuNDYgMTkuNjcgMTIuMDcgMTkuNDQgMTEuNzEgMTkuMTZMOS45NjAwMiAxOS44NkM5LjgxMDAyIDE5LjkyIDkuNjIwMDIgMTkuODYgOS41NDAwMiAxOS43MUw4LjE0MDAyIDE3LjI5QzguMDUwMDIgMTcuMTMgOC4wOTAwMiAxNi45NSA4LjIyMDAyIDE2Ljg0TDkuNzAwMDIgMTUuNjhMOS42NTAwMSAxNUw5LjcwMDAyIDE0LjMxTDguMjIwMDIgMTMuMTZDOC4wOTAwMiAxMy4wNSA4LjA1MDAyIDEyLjg2IDguMTQwMDIgMTIuNzFMOS41NDAwMiAxMC4yOUM5LjYyMDAyIDEwLjEzIDkuODEwMDIgMTAuMDcgOS45NjAwMiAxMC4xM0wxMS43MSAxMC44NEMxMi4wNyAxMC41NiAxMi40NiAxMC4zMiAxMi44OSAxMC4xNUwxMy4xNSA4LjI4OTk4QzEzLjE4IDguMTI5OTggMTMuMzIgNy45OTk5OCAxMy41IDcuOTk5OThIMTYuM0MxNi40NyA3Ljk5OTk4IDE2LjYyIDguMTI5OTggMTYuNjQgOC4yODk5OEwxNi45MSAxMC4xNUMxNy4zMyAxMC4zMiAxNy43MyAxMC41NiAxOC4wOSAxMC44NEwxOS44MyAxMC4xM0MyMCAxMC4wNyAyMC4xNyAxMC4xMyAyMC4yNiAxMC4yOUwyMS42NiAxMi43MUMyMS43NSAxMi44NiAyMS43MSAxMy4wNSAyMS41OCAxMy4xNkwyMC4xIDE0LjMxTDIwLjE1IDE1TDIwLjEgMTUuNjhaIi8+CiAgICA8cGF0aCBkPSJNNy4zMjk2NiA3LjQ0NDU0QzguMDgzMSA3LjAwOTU0IDguMzM5MzIgNi4wNTMzMiA3LjkwNDMyIDUuMjk5ODhDNy40NjkzMiA0LjU0NjQzIDYuNTA4MSA0LjI4MTU2IDUuNzU0NjYgNC43MTY1NkM1LjM5MTc2IDQuOTI2MDggNS4xMjY5NSA1LjI3MTE4IDUuMDE4NDkgNS42NzU5NEM0LjkxMDA0IDYuMDgwNzEgNC45NjY4MiA2LjUxMTk4IDUuMTc2MzQgNi44NzQ4OEM1LjYxMTM0IDcuNjI4MzIgNi41NzYyMiA3Ljg3OTU0IDcuMzI5NjYgNy40NDQ1NFpNOS42NTcxOCA0Ljc5NTkzTDEwLjg2NzIgNC45NTE3OUMxMC45NjI4IDQuOTc3NDEgMTEuMDQwMiA1LjA3MTMzIDExLjAzODIgNS4xODc5M0wxMS4wMzg4IDYuOTg4OTNDMTEuMDQ1NSA3LjEwMDU0IDEwLjk2MTYgNy4xOTUxOCAxMC44NTUgNy4yMTA1NEw5LjY2MDAxIDcuMzgwODNMOS4yMzkxNSA4LjEzMTg4TDkuNjY5NjEgOS4yNTc0NUM5LjcwNzI5IDkuMzYyNzEgOS42NjkzNCA5LjQ3Njk5IDkuNTc0MDggOS41MzE5OUw4LjAxNTIzIDEwLjQzMkM3LjkxMTMxIDEwLjQ5MiA3Ljc5MzM3IDEwLjQ2NzcgNy43MjEwNSAxMC4zODI0TDYuOTg3NDggOS40MzE4OEw2LjEwOTMxIDkuNDMwODNMNS4zNDcwNCAxMC4zOTA1QzUuMjg5MDkgMTAuNDcwMiA1LjE3MzgzIDEwLjQ5MDUgNS4wNzE4NyAxMC40MzM5TDMuNTEyNDUgOS41MzI5M0MzLjQxMDQ5IDkuNDc2MzMgMy4zNzY0NyA5LjM1NzQxIDMuNDEwNzUgOS4yNTY3OUwzLjg2MzQ3IDguMTQwOTNMMy42MTc0OSA3Ljc3NDg4TDMuNDIzNDcgNy4zNzg4M0wyLjIzMDc1IDcuMjEyOTdDMi4xMjY0NyA3LjE5MjM1IDIuMDQwNDkgNy4xMDM0MiAyLjA0MjQ1IDYuOTg2ODJMMi4wNDE4NyA1LjE4NTgyQzIuMDQzODMgNS4wNjkyMiAyLjExOTA5IDQuOTc5NTggMi4yMTcwNCA0Ljk2OTIyTDMuNDIwNjUgNC43OTM5M0wzLjg2NzQ5IDQuMDI3ODhMMy40MTEwNSAyLjkxNzMxQzMuMzczMzcgMi44MTIwNCAzLjQxMTMxIDIuNjk3NzYgMy41MTUyMyAyLjYzNzc2TDUuMDc0MDggMS43Mzc3NkM1LjE2OTM0IDEuNjgyNzYgNS4yODcyOSAxLjcwNzA0IDUuMzU5NjEgMS43OTIzMUw2LjExOTE1IDIuNzI3ODhMNi45ODAwMSAyLjczODkzTDcuNzI0OTYgMS43ODkyMkM3Ljc5MTU2IDEuNzA0NTggNy45MTU0OCAxLjY3OTIyIDguMDA4NzkgMS43NDA4Mkw5LjU2ODIxIDIuNjQxODJDOS42NzAxNyAyLjY5ODQyIDkuNzEyODUgMi44MTIzNCA5LjY4NzIzIDIuOTA3OTdMOS4yMTcxOCA0LjAzMzgzTDkuNDYzMTYgNC4zOTk4OEw5LjY1NzE4IDQuNzk1OTNaIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1pY29uLWJyYW5kMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNmZmYiPgogICAgPHBhdGggZD0iTTEwNSAxMjcuM2g0MHYxMi44aC00MHpNNTEuMSA3N0w3NCA5OS45bC0yMy4zIDIzLjMgMTAuNSAxMC41IDIzLjMtMjMuM0w5NSA5OS45IDg0LjUgODkuNCA2MS42IDY2LjV6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNGOUE4MjUiPgogICAgPHBhdGggZD0iTTIwLjIgMTEuOGMtMS42IDAtMS43LjUtMS43IDEgMCAuNC4xLjkuMSAxLjMuMS41LjEuOS4xIDEuMyAwIDEuNy0xLjQgMi4zLTMuNSAyLjNoLS45di0xLjloLjVjMS4xIDAgMS40IDAgMS40LS44IDAtLjMgMC0uNi0uMS0xIDAtLjQtLjEtLjgtLjEtMS4yIDAtMS4zIDAtMS44IDEuMy0yLTEuMy0uMi0xLjMtLjctMS4zLTIgMC0uNC4xLS44LjEtMS4yLjEtLjQuMS0uNy4xLTEgMC0uOC0uNC0uNy0xLjQtLjhoLS41VjQuMWguOWMyLjIgMCAzLjUuNyAzLjUgMi4zIDAgLjQtLjEuOS0uMSAxLjMtLjEuNS0uMS45LS4xIDEuMyAwIC41LjIgMSAxLjcgMXYxLjh6TTEuOCAxMC4xYzEuNiAwIDEuNy0uNSAxLjctMSAwLS40LS4xLS45LS4xLTEuMy0uMS0uNS0uMS0uOS0uMS0xLjMgMC0xLjYgMS40LTIuMyAzLjUtMi4zaC45djEuOWgtLjVjLTEgMC0xLjQgMC0xLjQuOCAwIC4zIDAgLjYuMSAxIDAgLjIuMS42LjEgMSAwIDEuMyAwIDEuOC0xLjMgMkM2IDExLjIgNiAxMS43IDYgMTNjMCAuNC0uMS44LS4xIDEuMi0uMS4zLS4xLjctLjEgMSAwIC44LjMuOCAxLjQuOGguNXYxLjloLS45Yy0yLjEgMC0zLjUtLjYtMy41LTIuMyAwLS40LjEtLjkuMS0xLjMuMS0uNS4xLS45LjEtMS4zIDAtLjUtLjItMS0xLjctMXYtMS45eiIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSIxMy44IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY3g9IjExIiBjeT0iOC4yIiByPSIyLjEiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4=);
  --jp-icon-listings-info: url(data:image/svg+xml;base64,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);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMEQ0N0ExIj4KICAgIDxwYXRoIGQ9Ik0xMS4xIDYuOVY1LjhINi45YzAtLjUgMC0xLjMuMi0xLjYuNC0uNy44LTEuMSAxLjctMS40IDEuNy0uMyAyLjUtLjMgMy45LS4xIDEgLjEgMS45LjkgMS45IDEuOXY0LjJjMCAuNS0uOSAxLjYtMiAxLjZIOC44Yy0xLjUgMC0yLjQgMS40LTIuNCAyLjh2Mi4ySDQuN0MzLjUgMTUuMSAzIDE0IDMgMTMuMVY5Yy0uMS0xIC42LTIgMS44LTIgMS41LS4xIDYuMy0uMSA2LjMtLjF6Ii8+CiAgICA8cGF0aCBkPSJNMTAuOSAxNS4xdjEuMWg0LjJjMCAuNSAwIDEuMy0uMiAxLjYtLjQuNy0uOCAxLjEtMS43IDEuNC0xLjcuMy0yLjUuMy0zLjkuMS0xLS4xLTEuOS0uOS0xLjktMS45di00LjJjMC0uNS45LTEuNiAyLTEuNmgzLjhjMS41IDAgMi40LTEuNCAyLjQtMi44VjYuNmgxLjdDMTguNSA2LjkgMTkgOCAxOSA4LjlWMTNjMCAxLS43IDIuMS0xLjkgMi4xaC02LjJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4=);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTUgMTVIM3YyaDEydi0yem0wLThIM3YyaDEyVjd6TTMgMTNoMTh2LTJIM3Yyem0wIDhoMTh2LTJIM3Yyek0zIDN2MmgxOFYzSDN6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4=);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}
.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}
.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}
.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}
.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}
.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}
.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}
.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}
.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}
.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}
.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}
.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}
.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}
.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}
.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}
.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}
.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}
.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}
.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}
.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}
.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}
.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}
.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}
.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}
.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}
.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}
.jp-FileIcon {
  background-image: var(--jp-icon-file);
}
.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}
.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}
.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}
.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}
.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}
.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}
.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}
.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}
.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}
.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}
.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}
.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}
.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}
.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}
.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}
.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}
.jp-ListIcon {
  background-image: var(--jp-icon-list);
}
.jp-ListingsInfoIcon {
  background-image: var(--jp-icon-listings-info);
}
.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}
.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}
.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}
.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}
.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}
.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}
.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}
.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}
.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}
.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}
.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}
.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}
.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}
.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}
.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}
.jp-RunIcon {
  background-image: var(--jp-icon-run);
}
.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}
.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}
.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}
.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}
.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}
.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}
.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}
.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}
.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}
.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}
.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}
.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}
.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}
.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}
.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}
.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}
.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}
/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}
/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}
/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}
.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}
.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}
.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}
.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}
.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}
.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}
.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}
.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}
/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}
.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}
.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}
.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}
.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}
.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}
.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}
/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}
.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}
.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}
.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}
.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}
.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}
.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

/* CSS for icons in selected items in the settings editor */
#setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}
#setting-editor
  .jp-PluginList
  .jp-mod-selected
  .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* CSS for icons in selected tabs in the sidebar tab manager */
#tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable[fill] {
  fill: #fff;
}

#tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}
#tab-manager
  .lm-TabBar-tab.jp-mod-active
  .jp-icon-hover
  :hover
  .jp-icon-selectable[fill] {
  fill: var(--jp-brand-color1);
}

#tab-manager
  .lm-TabBar-tab.jp-mod-active
  .jp-icon-hover
  :hover
  .jp-icon-selectable-inverse[fill] {
  fill: #fff;
}

/**
 * TODO: come up with non css-hack solution for showing the busy icon on top
 *  of the close icon
 * CSS for complex behavior of close icon of tabs in the sidebar tab manager
 */
#tab-manager
  .lm-TabBar-tab.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}
#tab-manager
  .lm-TabBar-tab.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

#tab-manager
  .lm-TabBar-tab.jp-mod-dirty.jp-mod-active
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: #fff;
}

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}
/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) svg {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-border-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0px;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-warn-color0);
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

/* Override Blueprint's _reset.scss styles */
html {
  box-sizing: unset;
}

*,
*::before,
*::after {
  box-sizing: unset;
}

body {
  color: unset;
  font-family: var(--jp-ui-font-family);
}

p {
  margin-top: unset;
  margin-bottom: unset;
}

small {
  font-size: unset;
}

strong {
  font-weight: unset;
}

/* Override Blueprint's _typography.scss styles */
a {
  text-decoration: unset;
  color: unset;
}
a:hover {
  text-decoration: unset;
  color: unset;
}

/* Override Blueprint's _accessibility.scss styles */
:focus {
  outline: unset;
  outline-offset: unset;
  -moz-outline-radius: unset;
}

/* Styles for ui-components */
.jp-Button {
  border-radius: var(--jp-border-radius);
  padding: 0px 12px;
  font-size: var(--jp-ui-font-size1);
}

/* Use our own theme for hover styles */
button.jp-Button.bp3-button.bp3-minimal:hover {
  background-color: var(--jp-layout-color2);
}
.jp-Button.minimal {
  color: unset !important;
}

.jp-Button.jp-ToolbarButtonComponent {
  text-transform: none;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color3);
}

.jp-BPIcon {
  display: inline-block;
  vertical-align: middle;
  margin: auto;
}

/* Stop blueprint futzing with our icon fills */
.bp3-icon.jp-BPIcon > svg:not([fill]) {
  fill: var(--jp-inverse-layout-color3);
}

.jp-InputGroupAction {
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

/* Use our own theme for hover and option styles */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}
select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-top: 1px solid var(--jp-border-color2);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-Collapse-header {
  padding: 1px 12px;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size2);
}

.jp-Collapse-header:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Collapse-contents {
  padding: 0px 12px 0px 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0px;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0px 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px 5px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0px;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty:after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0px 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0px;
  left: 0px;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px;
  padding-bottom: 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);
  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0px;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0px 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

.jp-HoverBox.jp-mod-outofview {
  display: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `p-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame:before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;

  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;

  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #aa00ff;

  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;

  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;

  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;

  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;

  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;

  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;

  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;

  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;

  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;

  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ffff00;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;

  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;

  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;

  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;

  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;

  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eeeeee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;

  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent:before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent:after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }
  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }
  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0px 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FileDialog-Checkbox {
  margin-top: 35px;
  display: flex;
  flex-direction: row;
  align-items: end;
  width: 100%;
}

.jp-FileDialog-Checkbox > label {
  flex: 1 1 auto;
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  height: 28px;
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  background-color: var(--jp-layout-color1);
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0px 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  height: 32px;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 1;
  overflow-x: auto;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0px;
  margin: 0px;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0px 6px;
  margin: 0px;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent span {
  padding: 0px;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ body.p-mod-override-cursor *, /* </DEPRECATED> */
body.lm-mod-override-cursor * {
  cursor: inherit !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0px;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/* BASICS */

.CodeMirror {
  /* Set height, width, borders, and global font properties here */
  font-family: monospace;
  height: 300px;
  color: black;
  direction: ltr;
}

/* PADDING */

.CodeMirror-lines {
  padding: 4px 0; /* Vertical padding around content */
}
.CodeMirror pre.CodeMirror-line,
.CodeMirror pre.CodeMirror-line-like {
  padding: 0 4px; /* Horizontal padding of content */
}

.CodeMirror-scrollbar-filler, .CodeMirror-gutter-filler {
  background-color: white; /* The little square between H and V scrollbars */
}

/* GUTTER */

.CodeMirror-gutters {
  border-right: 1px solid #ddd;
  background-color: #f7f7f7;
  white-space: nowrap;
}
.CodeMirror-linenumbers {}
.CodeMirror-linenumber {
  padding: 0 3px 0 5px;
  min-width: 20px;
  text-align: right;
  color: #999;
  white-space: nowrap;
}

.CodeMirror-guttermarker { color: black; }
.CodeMirror-guttermarker-subtle { color: #999; }

/* CURSOR */

.CodeMirror-cursor {
  border-left: 1px solid black;
  border-right: none;
  width: 0;
}
/* Shown when moving in bi-directional text */
.CodeMirror div.CodeMirror-secondarycursor {
  border-left: 1px solid silver;
}
.cm-fat-cursor .CodeMirror-cursor {
  width: auto;
  border: 0 !important;
  background: #7e7;
}
.cm-fat-cursor div.CodeMirror-cursors {
  z-index: 1;
}
.cm-fat-cursor-mark {
  background-color: rgba(20, 255, 20, 0.5);
  -webkit-animation: blink 1.06s steps(1) infinite;
  -moz-animation: blink 1.06s steps(1) infinite;
  animation: blink 1.06s steps(1) infinite;
}
.cm-animate-fat-cursor {
  width: auto;
  border: 0;
  -webkit-animation: blink 1.06s steps(1) infinite;
  -moz-animation: blink 1.06s steps(1) infinite;
  animation: blink 1.06s steps(1) infinite;
  background-color: #7e7;
}
@-moz-keyframes blink {
  0% {}
  50% { background-color: transparent; }
  100% {}
}
@-webkit-keyframes blink {
  0% {}
  50% { background-color: transparent; }
  100% {}
}
@keyframes blink {
  0% {}
  50% { background-color: transparent; }
  100% {}
}

/* Can style cursor different in overwrite (non-insert) mode */
.CodeMirror-overwrite .CodeMirror-cursor {}

.cm-tab { display: inline-block; text-decoration: inherit; }

.CodeMirror-rulers {
  position: absolute;
  left: 0; right: 0; top: -50px; bottom: 0;
  overflow: hidden;
}
.CodeMirror-ruler {
  border-left: 1px solid #ccc;
  top: 0; bottom: 0;
  position: absolute;
}

/* DEFAULT THEME */

.cm-s-default .cm-header {color: blue;}
.cm-s-default .cm-quote {color: #090;}
.cm-negative {color: #d44;}
.cm-positive {color: #292;}
.cm-header, .cm-strong {font-weight: bold;}
.cm-em {font-style: italic;}
.cm-link {text-decoration: underline;}
.cm-strikethrough {text-decoration: line-through;}

.cm-s-default .cm-keyword {color: #708;}
.cm-s-default .cm-atom {color: #219;}
.cm-s-default .cm-number {color: #164;}
.cm-s-default .cm-def {color: #00f;}
.cm-s-default .cm-variable,
.cm-s-default .cm-punctuation,
.cm-s-default .cm-property,
.cm-s-default .cm-operator {}
.cm-s-default .cm-variable-2 {color: #05a;}
.cm-s-default .cm-variable-3, .cm-s-default .cm-type {color: #085;}
.cm-s-default .cm-comment {color: #a50;}
.cm-s-default .cm-string {color: #a11;}
.cm-s-default .cm-string-2 {color: #f50;}
.cm-s-default .cm-meta {color: #555;}
.cm-s-default .cm-qualifier {color: #555;}
.cm-s-default .cm-builtin {color: #30a;}
.cm-s-default .cm-bracket {color: #997;}
.cm-s-default .cm-tag {color: #170;}
.cm-s-default .cm-attribute {color: #00c;}
.cm-s-default .cm-hr {color: #999;}
.cm-s-default .cm-link {color: #00c;}

.cm-s-default .cm-error {color: #f00;}
.cm-invalidchar {color: #f00;}

.CodeMirror-composing { border-bottom: 2px solid; }

/* Default styles for common addons */

div.CodeMirror span.CodeMirror-matchingbracket {color: #0b0;}
div.CodeMirror span.CodeMirror-nonmatchingbracket {color: #a22;}
.CodeMirror-matchingtag { background: rgba(255, 150, 0, .3); }
.CodeMirror-activeline-background {background: #e8f2ff;}

/* STOP */

/* The rest of this file contains styles related to the mechanics of
   the editor. You probably shouldn't touch them. */

.CodeMirror {
  position: relative;
  overflow: hidden;
  background: white;
}

.CodeMirror-scroll {
  overflow: scroll !important; /* Things will break if this is overridden */
  /* 50px is the magic margin used to hide the element's real scrollbars */
  /* See overflow: hidden in .CodeMirror */
  margin-bottom: -50px; margin-right: -50px;
  padding-bottom: 50px;
  height: 100%;
  outline: none; /* Prevent dragging from highlighting the element */
  position: relative;
}
.CodeMirror-sizer {
  position: relative;
  border-right: 50px solid transparent;
}

/* The fake, visible scrollbars. Used to force redraw during scrolling
   before actual scrolling happens, thus preventing shaking and
   flickering artifacts. */
.CodeMirror-vscrollbar, .CodeMirror-hscrollbar, .CodeMirror-scrollbar-filler, .CodeMirror-gutter-filler {
  position: absolute;
  z-index: 6;
  display: none;
  outline: none;
}
.CodeMirror-vscrollbar {
  right: 0; top: 0;
  overflow-x: hidden;
  overflow-y: scroll;
}
.CodeMirror-hscrollbar {
  bottom: 0; left: 0;
  overflow-y: hidden;
  overflow-x: scroll;
}
.CodeMirror-scrollbar-filler {
  right: 0; bottom: 0;
}
.CodeMirror-gutter-filler {
  left: 0; bottom: 0;
}

.CodeMirror-gutters {
  position: absolute; left: 0; top: 0;
  min-height: 100%;
  z-index: 3;
}
.CodeMirror-gutter {
  white-space: normal;
  height: 100%;
  display: inline-block;
  vertical-align: top;
  margin-bottom: -50px;
}
.CodeMirror-gutter-wrapper {
  position: absolute;
  z-index: 4;
  background: none !important;
  border: none !important;
}
.CodeMirror-gutter-background {
  position: absolute;
  top: 0; bottom: 0;
  z-index: 4;
}
.CodeMirror-gutter-elt {
  position: absolute;
  cursor: default;
  z-index: 4;
}
.CodeMirror-gutter-wrapper ::selection { background-color: transparent }
.CodeMirror-gutter-wrapper ::-moz-selection { background-color: transparent }

.CodeMirror-lines {
  cursor: text;
  min-height: 1px; /* prevents collapsing before first draw */
}
.CodeMirror pre.CodeMirror-line,
.CodeMirror pre.CodeMirror-line-like {
  /* Reset some styles that the rest of the page might have set */
  -moz-border-radius: 0; -webkit-border-radius: 0; border-radius: 0;
  border-width: 0;
  background: transparent;
  font-family: inherit;
  font-size: inherit;
  margin: 0;
  white-space: pre;
  word-wrap: normal;
  line-height: inherit;
  color: inherit;
  z-index: 2;
  position: relative;
  overflow: visible;
  -webkit-tap-highlight-color: transparent;
  -webkit-font-variant-ligatures: contextual;
  font-variant-ligatures: contextual;
}
.CodeMirror-wrap pre.CodeMirror-line,
.CodeMirror-wrap pre.CodeMirror-line-like {
  word-wrap: break-word;
  white-space: pre-wrap;
  word-break: normal;
}

.CodeMirror-linebackground {
  position: absolute;
  left: 0; right: 0; top: 0; bottom: 0;
  z-index: 0;
}

.CodeMirror-linewidget {
  position: relative;
  z-index: 2;
  padding: 0.1px; /* Force widget margins to stay inside of the container */
}

.CodeMirror-widget {}

.CodeMirror-rtl pre { direction: rtl; }

.CodeMirror-code {
  outline: none;
}

/* Force content-box sizing for the elements where we expect it */
.CodeMirror-scroll,
.CodeMirror-sizer,
.CodeMirror-gutter,
.CodeMirror-gutters,
.CodeMirror-linenumber {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
}

.CodeMirror-measure {
  position: absolute;
  width: 100%;
  height: 0;
  overflow: hidden;
  visibility: hidden;
}

.CodeMirror-cursor {
  position: absolute;
  pointer-events: none;
}
.CodeMirror-measure pre { position: static; }

div.CodeMirror-cursors {
  visibility: hidden;
  position: relative;
  z-index: 3;
}
div.CodeMirror-dragcursors {
  visibility: visible;
}

.CodeMirror-focused div.CodeMirror-cursors {
  visibility: visible;
}

.CodeMirror-selected { background: #d9d9d9; }
.CodeMirror-focused .CodeMirror-selected { background: #d7d4f0; }
.CodeMirror-crosshair { cursor: crosshair; }
.CodeMirror-line::selection, .CodeMirror-line > span::selection, .CodeMirror-line > span > span::selection { background: #d7d4f0; }
.CodeMirror-line::-moz-selection, .CodeMirror-line > span::-moz-selection, .CodeMirror-line > span > span::-moz-selection { background: #d7d4f0; }

.cm-searching {
  background-color: #ffa;
  background-color: rgba(255, 255, 0, .4);
}

/* Used to force a border model for a node */
.cm-force-border { padding-right: .1px; }

@media print {
  /* Hide the cursor when printing */
  .CodeMirror div.CodeMirror-cursors {
    visibility: hidden;
  }
}

/* See issue #2901 */
.cm-tab-wrap-hack:after { content: ''; }

/* Help users use markselection to safely style text background */
span.CodeMirror-selectedtext { background: none; }

.CodeMirror-dialog {
  position: absolute;
  left: 0; right: 0;
  background: inherit;
  z-index: 15;
  padding: .1em .8em;
  overflow: hidden;
  color: inherit;
}

.CodeMirror-dialog-top {
  border-bottom: 1px solid #eee;
  top: 0;
}

.CodeMirror-dialog-bottom {
  border-top: 1px solid #eee;
  bottom: 0;
}

.CodeMirror-dialog input {
  border: none;
  outline: none;
  background: transparent;
  width: 20em;
  color: inherit;
  font-family: monospace;
}

.CodeMirror-dialog button {
  font-size: 70%;
}

.CodeMirror-foldmarker {
  color: blue;
  text-shadow: #b9f 1px 1px 2px, #b9f -1px -1px 2px, #b9f 1px -1px 2px, #b9f -1px 1px 2px;
  font-family: arial;
  line-height: .3;
  cursor: pointer;
}
.CodeMirror-foldgutter {
  width: .7em;
}
.CodeMirror-foldgutter-open,
.CodeMirror-foldgutter-folded {
  cursor: pointer;
}
.CodeMirror-foldgutter-open:after {
  content: "\25BE";
}
.CodeMirror-foldgutter-folded:after {
  content: "\25B8";
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.CodeMirror {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;
  /* Changed to auto to autogrow */
}

.CodeMirror pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

/* This causes https://github.com/jupyter/jupyterlab/issues/522 */
/* May not cause it not because we changed it! */
.CodeMirror-lines {
  padding: var(--jp-code-padding) 0;
}

.CodeMirror-linenumber {
  padding: 0 8px;
}

.jp-CodeMirrorEditor {
  cursor: text;
}

.jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor {
  border-left: var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color);
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.CodeMirror.jp-mod-readOnly .CodeMirror-cursor {
  display: none;
}

.CodeMirror-gutters {
  border-right: 1px solid var(--jp-border-color2);
  background-color: var(--jp-layout-color0);
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.CodeMirror-selectedtext.cm-searching {
  background-color: var(--jp-search-selected-match-background-color) !important;
  color: var(--jp-search-selected-match-color) !important;
}

.cm-searching {
  background-color: var(
    --jp-search-unselected-match-background-color
  ) !important;
  color: var(--jp-search-unselected-match-color) !important;
}

.CodeMirror-focused .CodeMirror-selected {
  background-color: var(--jp-editor-selected-focused-background);
}

.CodeMirror-selected {
  background-color: var(--jp-editor-selected-background);
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/**
 * Here is our jupyter theme for CodeMirror syntax highlighting
 * This is used in our marked.js syntax highlighting and CodeMirror itself
 * The string "jupyter" is set in ../codemirror/widget.DEFAULT_CODEMIRROR_THEME
 * This came from the classic notebook, which came form highlight.js/GitHub
 */

/**
 * CodeMirror themes are handling the background/color in this way. This works
 * fine for CodeMirror editors outside the notebook, but the notebook styles
 * these things differently.
 */
.CodeMirror.cm-s-jupyter {
  background: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

/* In the notebook, we want this styling to be handled by its container */
.jp-CodeConsole .CodeMirror.cm-s-jupyter,
.jp-Notebook .CodeMirror.cm-s-jupyter {
  background: transparent;
}

.cm-s-jupyter .CodeMirror-cursor {
  border-left: var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color);
}
.cm-s-jupyter span.cm-keyword {
  color: var(--jp-mirror-editor-keyword-color);
  font-weight: bold;
}
.cm-s-jupyter span.cm-atom {
  color: var(--jp-mirror-editor-atom-color);
}
.cm-s-jupyter span.cm-number {
  color: var(--jp-mirror-editor-number-color);
}
.cm-s-jupyter span.cm-def {
  color: var(--jp-mirror-editor-def-color);
}
.cm-s-jupyter span.cm-variable {
  color: var(--jp-mirror-editor-variable-color);
}
.cm-s-jupyter span.cm-variable-2 {
  color: var(--jp-mirror-editor-variable-2-color);
}
.cm-s-jupyter span.cm-variable-3 {
  color: var(--jp-mirror-editor-variable-3-color);
}
.cm-s-jupyter span.cm-punctuation {
  color: var(--jp-mirror-editor-punctuation-color);
}
.cm-s-jupyter span.cm-property {
  color: var(--jp-mirror-editor-property-color);
}
.cm-s-jupyter span.cm-operator {
  color: var(--jp-mirror-editor-operator-color);
  font-weight: bold;
}
.cm-s-jupyter span.cm-comment {
  color: var(--jp-mirror-editor-comment-color);
  font-style: italic;
}
.cm-s-jupyter span.cm-string {
  color: var(--jp-mirror-editor-string-color);
}
.cm-s-jupyter span.cm-string-2 {
  color: var(--jp-mirror-editor-string-2-color);
}
.cm-s-jupyter span.cm-meta {
  color: var(--jp-mirror-editor-meta-color);
}
.cm-s-jupyter span.cm-qualifier {
  color: var(--jp-mirror-editor-qualifier-color);
}
.cm-s-jupyter span.cm-builtin {
  color: var(--jp-mirror-editor-builtin-color);
}
.cm-s-jupyter span.cm-bracket {
  color: var(--jp-mirror-editor-bracket-color);
}
.cm-s-jupyter span.cm-tag {
  color: var(--jp-mirror-editor-tag-color);
}
.cm-s-jupyter span.cm-attribute {
  color: var(--jp-mirror-editor-attribute-color);
}
.cm-s-jupyter span.cm-header {
  color: var(--jp-mirror-editor-header-color);
}
.cm-s-jupyter span.cm-quote {
  color: var(--jp-mirror-editor-quote-color);
}
.cm-s-jupyter span.cm-link {
  color: var(--jp-mirror-editor-link-color);
}
.cm-s-jupyter span.cm-error {
  color: var(--jp-mirror-editor-error-color);
}
.cm-s-jupyter span.cm-hr {
  color: #999;
}

.cm-s-jupyter span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}

.cm-s-jupyter .CodeMirror-activeline-background,
.cm-s-jupyter .CodeMirror-gutter {
  background-color: var(--jp-layout-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .remote-caret {
  position: relative;
  border-left: 2px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .remote-caret > div {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -2px;
  font-size: 0.95em;
  background-color: rgb(250, 129, 0);
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 3;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .remote-caret.hide-name > div {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .remote-caret:hover > div {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0px;
  padding: 0px;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}
.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}
.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}
.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}
.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}
.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}
.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}
.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}
.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}
.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}
.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}
.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}
.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}
.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}
.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}
.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}
.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}
.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}
.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0em;
}

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: 12px;
  table-layout: fixed;
  margin-left: auto;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon table {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0px;
}

.jp-RenderedHTMLCommon p {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}
[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}
/* ...or leave it untouched if they don't */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-dark-background {
}
[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-light-background {
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}
.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}
.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}
.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}
.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}
.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}
.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}
.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}
.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: 0.8em;
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser {
  display: flex;
  flex-direction: column;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  border-bottom: none;
  height: auto;
  margin: var(--jp-toolbar-header-margin);
  box-shadow: none;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0px 2px;
  padding: 0px 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0px;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar.jp-Toolbar {
  padding: 0px;
  margin: 8px 12px 0px 12px;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  justify-content: flex-start;
}

.jp-FileBrowser-toolbar.jp-Toolbar .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0px;
  padding-right: 2px;
}

.jp-FileBrowser-toolbar.jp-Toolbar .jp-ToolbarButtonComponent {
  width: 40px;
}

.jp-FileBrowser-toolbar.jp-Toolbar
  .jp-Toolbar-item:first-child
  .jp-ToolbarButtonComponent {
  width: 72px;
  background: var(--jp-brand-color1);
}

.jp-FileBrowser-toolbar.jp-Toolbar
  .jp-Toolbar-item:first-child
  .jp-ToolbarButtonComponent:focus-visible {
  background-color: var(--jp-brand-color0);
}

.jp-FileBrowser-toolbar.jp-Toolbar
  .jp-Toolbar-item:first-child
  .jp-ToolbarButtonComponent
  .jp-icon3 {
  fill: white;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileBrowser-filterBox {
  padding: 0px;
  flex: 0 0 auto;
  margin: 8px 12px 0px 12px;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing:focus-visible {
  border: 1px solid var(--jp-brand-color1);
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px 12px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon:before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon:before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0px;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-DirListing-deadSpace {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
}

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: flex;
  flex-direction: row;
}

body[data-format='mobile'] .jp-OutputArea-child {
  flex-direction: column;
}

.jp-OutputPrompt {
  flex: 0 0 var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);
  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

body[data-format='mobile'] .jp-OutputPrompt {
  flex: 0 0 auto;
  text-align: left;
}

.jp-OutputArea-output {
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea-child .jp-OutputArea-output {
  flex-grow: 1;
  flex-shrink: 1;
}

body[data-format='mobile'] .jp-OutputArea-child .jp-OutputArea-output {
  margin-left: var(--jp-notebook-padding);
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `p-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated:before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0px;
  padding: 0px;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0px;
  flex: 1 1 auto;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-OutputArea-stdin {
  line-height: var(--jp-code-line-height);
  padding-top: var(--jp-code-padding);
  display: flex;
}

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0px;
  margin: 0px;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;
  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0px;
  bottom: 0px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0px;
  width: 100%;
  padding: 0px;
  margin: 0px;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: flex;
  flex-direction: row;
  overflow: hidden;
}

body[data-format='mobile'] .jp-InputArea {
  flex-direction: column;
}

.jp-InputArea-editor {
  flex: 1 1 auto;
  overflow: hidden;
}

.jp-InputArea-editor {
  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0px;
  background: var(--jp-cell-editor-background);
}

body[data-format='mobile'] .jp-InputArea-editor {
  margin-left: var(--jp-notebook-padding);
}

.jp-InputPrompt {
  flex: 0 0 var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);
  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

body[data-format='mobile'] .jp-InputPrompt {
  flex: 0 0 auto;
  text-align: left;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: flex;
  flex-direction: row;
  flex: 1 1 auto;
}

.jp-Placeholder-prompt {
  box-sizing: border-box;
}

.jp-Placeholder-content {
  flex: 1 1 auto;
  border: none;
  background: transparent;
  height: 20px;
  box-sizing: border-box;
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0px;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0px;
  margin: 0px;
  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 200px;
  box-shadow: inset 0 0 6px 2px rgba(0, 0, 0, 0.3);
  margin-left: var(--jp-private-cell-scrolling-output-offset);
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  flex: 0 0
    calc(
      var(--jp-cell-prompt-width) -
        var(--jp-private-cell-scrolling-output-offset)
    );
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  flex: 1 1 auto;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

.jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
}

.jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-collapseHeadingButton {
  display: none;
}

.jp-MarkdownCell:hover .jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  position: absolute;
  right: 0;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: 2px;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook-render * {
  contain: none !important;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
  float: left;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt:before {
  color: var(--jp-warn-color1);
  content: '•';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0px rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0px;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0px rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-NotebookTools-tool {
  padding: 0px 12px 0 12px;
}

.jp-ActiveCellTool {
  padding: 12px;
  background-color: var(--jp-layout-color1);
  border-top: none !important;
}

.jp-ActiveCellTool .jp-InputArea-prompt {
  flex: 0 0 auto;
  padding-left: 0px;
}

.jp-ActiveCellTool .jp-InputArea-editor {
  flex: 1 1 auto;
  background: var(--jp-cell-editor-background);
  border-color: var(--jp-cell-editor-border-color);
}

.jp-ActiveCellTool .jp-InputArea-editor .CodeMirror {
  background: transparent;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0px 12px 0px;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body div {
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>

    <style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color),
    0px 1px 1px 0px var(--jp-shadow-penumbra-color),
    0px 1px 3px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color),
    0px 2px 2px 0px var(--jp-shadow-penumbra-color),
    0px 1px 5px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color),
    0px 4px 5px 0px var(--jp-shadow-penumbra-color),
    0px 1px 10px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color),
    0px 6px 10px 0px var(--jp-shadow-penumbra-color),
    0px 1px 18px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color),
    0px 8px 10px 1px var(--jp-shadow-penumbra-color),
    0px 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color),
    0px 12px 17px 2px var(--jp-shadow-penumbra-color),
    0px 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color),
    0px 16px 24px 2px var(--jp-shadow-penumbra-color),
    0px 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color),
    0px 20px 31px 3px var(--jp-shadow-penumbra-color),
    0px 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color),
    0px 24px 38px 3px var(--jp-shadow-penumbra-color),
    0px 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;

  --jp-ui-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica,
    Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;

  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);

  --jp-content-link-color: var(--md-blue-700);

  --jp-content-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI',
    Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: Menlo, Consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);

  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);

  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);

  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);

  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;

  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;

  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);

  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0px;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);
  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;
  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0px 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-border-color1);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: #05a;
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #aa22ff;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #aa22ff;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);
}
</style>

<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

.highlight  {
  margin: 0.4em;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.CodeMirror pre {
  margin: 0;
  padding: 0;
}

/* Using table instead of flexbox so that we can use break-inside property */
/* CSS rules under this comment should not be required anymore after we move to the JupyterLab 4.0 CSS */


.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  min-width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-OutputArea-child {
  display: table;
  width: 100%;
}

.jp-OutputPrompt {
  display: table-cell;
  vertical-align: top;
  min-width: var(--jp-cell-prompt-width);
}

body[data-format='mobile'] .jp-OutputPrompt {
  display: table-row;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
}

body[data-format='mobile'] .jp-OutputArea-child .jp-OutputArea-output {
  display: table-row;
}

.jp-OutputArea-output.jp-OutputArea-executeResult {
  width: 100%;
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }

  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}
</style>

<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: { 
                    automatic: true 
                    }
                }
            });
        
            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
    <!-- End of mathjax configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">

<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="AT&amp;T_face_data">AT&amp;T_face_data<a class="anchor-link" href="#AT&amp;T_face_data">&#182;</a></h1>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="import-package">import package<a class="anchor-link" href="#import-package">&#182;</a></h1>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import package</span>
<span class="c1"># warning</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="c1"># data</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="c1"># visulaize</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.ticker</span> <span class="kn">import</span> <span class="n">PercentFormatter</span>
<span class="c1"># adjust data</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="c1"># ML</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span><span class="p">,</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="c1"># PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="c1"># evaluate</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="import-function">import function<a class="anchor-link" href="#import-function">&#182;</a></h1>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import ML_func.py</span>
<span class="kn">from</span> <span class="nn">ML_func</span> <span class="kn">import</span> <span class="n">ml_func</span>
<span class="n">func</span> <span class="o">=</span> <span class="n">ml_func</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="import-data">import data<a class="anchor-link" href="#import-data">&#182;</a></h1>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">att_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;face_data.csv&quot;</span><span class="p">)</span>
<span class="n">att_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[3]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>4087</th>
      <th>4088</th>
      <th>4089</th>
      <th>4090</th>
      <th>4091</th>
      <th>4092</th>
      <th>4093</th>
      <th>4094</th>
      <th>4095</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.309917</td>
      <td>0.367769</td>
      <td>0.417355</td>
      <td>0.442149</td>
      <td>0.528926</td>
      <td>0.607438</td>
      <td>0.657025</td>
      <td>0.677686</td>
      <td>0.690083</td>
      <td>0.685950</td>
      <td>...</td>
      <td>0.669422</td>
      <td>0.652893</td>
      <td>0.661157</td>
      <td>0.475207</td>
      <td>0.132231</td>
      <td>0.148760</td>
      <td>0.152893</td>
      <td>0.161157</td>
      <td>0.157025</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.454545</td>
      <td>0.471074</td>
      <td>0.512397</td>
      <td>0.557851</td>
      <td>0.595041</td>
      <td>0.640496</td>
      <td>0.681818</td>
      <td>0.702479</td>
      <td>0.710744</td>
      <td>0.702479</td>
      <td>...</td>
      <td>0.157025</td>
      <td>0.136364</td>
      <td>0.148760</td>
      <td>0.152893</td>
      <td>0.152893</td>
      <td>0.152893</td>
      <td>0.152893</td>
      <td>0.152893</td>
      <td>0.152893</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.318182</td>
      <td>0.400826</td>
      <td>0.491736</td>
      <td>0.528926</td>
      <td>0.586777</td>
      <td>0.657025</td>
      <td>0.681818</td>
      <td>0.685950</td>
      <td>0.702479</td>
      <td>0.698347</td>
      <td>...</td>
      <td>0.132231</td>
      <td>0.181818</td>
      <td>0.136364</td>
      <td>0.128099</td>
      <td>0.148760</td>
      <td>0.144628</td>
      <td>0.140496</td>
      <td>0.148760</td>
      <td>0.152893</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.198347</td>
      <td>0.194215</td>
      <td>0.194215</td>
      <td>0.194215</td>
      <td>0.190083</td>
      <td>0.190083</td>
      <td>0.243802</td>
      <td>0.404959</td>
      <td>0.483471</td>
      <td>0.516529</td>
      <td>...</td>
      <td>0.636364</td>
      <td>0.657025</td>
      <td>0.685950</td>
      <td>0.727273</td>
      <td>0.743802</td>
      <td>0.764463</td>
      <td>0.752066</td>
      <td>0.752066</td>
      <td>0.739669</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.500000</td>
      <td>0.545455</td>
      <td>0.582645</td>
      <td>0.623967</td>
      <td>0.648760</td>
      <td>0.690083</td>
      <td>0.694215</td>
      <td>0.714876</td>
      <td>0.723140</td>
      <td>0.731405</td>
      <td>...</td>
      <td>0.161157</td>
      <td>0.177686</td>
      <td>0.173554</td>
      <td>0.177686</td>
      <td>0.177686</td>
      <td>0.177686</td>
      <td>0.177686</td>
      <td>0.173554</td>
      <td>0.173554</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 4097 columns</p>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="process-data">process data<a class="anchor-link" href="#process-data">&#182;</a></h1>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># take out X, y</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">att_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">att_data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="original-data">original data<a class="anchor-link" href="#original-data">&#182;</a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train_original</span><span class="p">,</span> <span class="n">X_test_original</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">preprocess_data</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">65536</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>training data : 320
testing data : 80
</pre>
</div>
</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="standardize-data">standardize data<a class="anchor-link" href="#standardize-data">&#182;</a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">preprocess_data</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">65536</span><span class="p">,</span> <span class="n">standard</span><span class="o">=</span><span class="s2">&quot;True&quot;</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>standardlize training data : 320
standardlize testing data : 80
</pre>
</div>
</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="pca-data">pca data<a class="anchor-link" href="#pca-data">&#182;</a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train_pca</span><span class="p">,</span> <span class="n">X_test_pca</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">preprocess_data</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">65536</span><span class="p">,</span> <span class="n">standard</span><span class="o">=</span><span class="s2">&quot;True&quot;</span><span class="p">,</span> <span class="n">pca</span><span class="o">=</span><span class="s2">&quot;True&quot;</span><span class="p">,</span> <span class="n">n_pc</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">show_plot</span><span class="o">=</span><span class="s2">&quot;True&quot;</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>standardlize training data : 320
standardlize testing data : 80
finish doing pca
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>




<div class="jp-RenderedImage jp-OutputArea-output ">
<img src="data:image/png;base64,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
"
class="
jp-needs-light-background
"
>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>上述得到的檔案共有 3 個，分別是以下 :  <br></p>
<ul>
<li>原始的資料 (original) <br></li>
<li>標準化後的資料 (scaled) <br></li>
<li>先標準化過後，再進行 PCA 的資料，取累積解釋力到 0.8 的 (pca) <br></li>
</ul>
<p>之後藉由這些資料各自拆分成不同的 train &amp; test <br>
共得到 3 種類型的資料 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="modeling-standardize-data">modeling standardize data<a class="anchor-link" href="#modeling-standardize-data">&#182;</a></h1><p>把標準化後的資料建模 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="MLR">MLR<a class="anchor-link" href="#MLR">&#182;</a></h2><p>多元羅吉斯回歸 (Multinomial Logistic Regression) <br>
建模之後以 測試集 進行評估 <br>
並且列出 4 種指標以供觀察 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="default">default<a class="anchor-link" href="#default">&#182;</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># MLR model &amp; hyperparametersmodel</span>
<span class="n">mlr_opts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mlr_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="o">**</span><span class="n">mlr_opts</span><span class="p">)</span>
<span class="n">mlr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># predict</span>
<span class="n">y_pred_mlr</span> <span class="o">=</span> <span class="n">mlr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[75]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">mlr_eva</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlr</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">mlr_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">mlr_eva</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                      <span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;standard&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;MLR&quot;</span><span class="p">,</span> <span class="s2">&quot;MLR&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">mlr_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[75]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">standard</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">MLR</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.99</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="grid-search">grid search<a class="anchor-link" href="#grid-search">&#182;</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># search best parameter</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="s1">&#39;solver&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;newton-cg&#39;</span><span class="p">,</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">],</span>
    <span class="s1">&#39;penalty&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># model</span>
<span class="n">mlr_opts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mlr_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="o">**</span><span class="n">mlr_opts</span><span class="p">)</span>

<span class="c1"># grid search</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">mlr_model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># print</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters found: &quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="n">y_pred_mlr_best</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Fitting 5 folds for each of 12 candidates, totalling 60 fits
Best parameters found:  {&#39;C&#39;: 1, &#39;penalty&#39;: &#39;l2&#39;, &#39;solver&#39;: &#39;newton-cg&#39;}
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[76]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">mlr_eva_best</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlr_best</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">mlr_df_best</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">mlr_eva_best</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                      <span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;standard&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;MLR&quot;</span><span class="p">,</span> <span class="s2">&quot;MLR&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">mlr_df_best</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[76]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">standard</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">MLR</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.99</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="MLR-conclusion">MLR conclusion<a class="anchor-link" href="#MLR-conclusion">&#182;</a></h3><p>可以發現 2個的結果相同 <br>
為了避免麻煩 <br>
因此使用 default 為最佳模型 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="SVM">SVM<a class="anchor-link" href="#SVM">&#182;</a></h2><p>支援向量機 (Support Vector Machine) <br>
建模之後以 測試集 進行評估 <br>
並且列出 4 種指標以供觀察 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="default">default<a class="anchor-link" href="#default">&#182;</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[17]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># SVM model &amp; hyperparameter</span>
<span class="n">svm_opts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">))</span>
<span class="n">svm_model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">svm_opts</span><span class="p">)</span>
<span class="n">svm_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># predict</span>
<span class="n">y_pred_svm</span> <span class="o">=</span> <span class="n">svm_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[78]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">svm_eva</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">svm_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">svm_eva</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                      <span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;standard&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;SVM&quot;</span><span class="p">,</span> <span class="s2">&quot;SVM&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">svm_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[78]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">standard</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">SVM</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.99</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="grid-search">grid search<a class="anchor-link" href="#grid-search">&#182;</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[19]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># search best parameter</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">],</span>
    <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;auto&#39;</span><span class="p">],</span>
<span class="p">}</span>

<span class="c1"># model</span>
<span class="n">svm_opts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">))</span>
<span class="n">svm_model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="o">**</span><span class="n">svm_opts</span><span class="p">)</span>

<span class="c1"># grid search</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">svm_model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># print</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters found: &quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="n">y_pred_svm_best</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Fitting 5 folds for each of 32 candidates, totalling 160 fits
Best parameters found:  {&#39;C&#39;: 0.08, &#39;gamma&#39;: &#39;scale&#39;, &#39;kernel&#39;: &#39;linear&#39;}
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[20]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">svm_eva_best</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm_best</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">svm_df_best</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">svm_eva_best</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                      <span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;standard&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;SVM&quot;</span><span class="p">,</span> <span class="s2">&quot;SVM&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">svm_df_best</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[20]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">standard</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">SVM</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.96</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.98</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.96</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.97</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="SVM-conclusion">SVM conclusion<a class="anchor-link" href="#SVM-conclusion">&#182;</a></h3><p>可以發現 2個的結果相同 <br>
為了避免麻煩 <br>
因此使用 default 為最佳模型 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="NN--MLP">NN--MLP<a class="anchor-link" href="#NN--MLP">&#182;</a></h2><p>神經網路 (Neural Network) -- 多層感知機 (Multilayer perceptron) <br>
建模之後以 測試集 進行評估 <br>
並且列出 4 種指標以供觀察 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="default">default<a class="anchor-link" href="#default">&#182;</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[21]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># NN-MLP model &amp; hyperparameter</span>
<span class="n">hidden_layers</span> <span class="o">=</span> <span class="p">(</span><span class="mi">30</span><span class="p">,)</span>
<span class="n">opts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">))</span>
<span class="n">nn_mlp_model</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">opts</span><span class="p">)</span>
<span class="n">nn_mlp_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># predict</span>
<span class="n">y_pred_nn_mlp</span> <span class="o">=</span> <span class="n">nn_mlp_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[32]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">nn_mlp_eva</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nn_mlp</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">nn_mlp_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">nn_mlp_eva</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                         <span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;standard&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;NN_MLP&quot;</span><span class="p">,</span> <span class="s2">&quot;NN_MLP&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">nn_mlp_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[32]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">standard</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">NN_MLP</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.89</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.94</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.89</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.89</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="grid-search">grid search<a class="anchor-link" href="#grid-search">&#182;</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[24]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># search best parameter</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;hidden_layer_sizes&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mi">30</span><span class="p">,)],</span>
    <span class="s1">&#39;activation&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;logistic&#39;</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">],</span>
    <span class="s1">&#39;solver&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="s1">&#39;sgd&#39;</span><span class="p">,</span> <span class="s1">&#39;adam&#39;</span><span class="p">],</span>
<span class="p">}</span>

<span class="c1"># model</span>
<span class="n">opts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">))</span>
<span class="n">nn_mlp_model</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">opts</span><span class="p">)</span>

<span class="c1"># grid search</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">nn_mlp_model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># print</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters found: &quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="n">y_pred_nn_mlp_best</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="c1"># running time : 37m 43s</span>
<span class="c1"># Iteration 23580, loss = 0.02097050</span>
<span class="c1"># Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.</span>
<span class="c1"># Best parameters found:  {&#39;activation&#39;: &#39;logistic&#39;, &#39;hidden_layer_sizes&#39;: (30,), &#39;solver&#39;: &#39;sgd&#39;}</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Fitting 5 folds for each of 6 candidates, totalling 30 fits
Iteration 1, loss = 3.74697672
Iteration 2, loss = 3.74202818
Iteration 3, loss = 3.73408647
Iteration 4, loss = 3.72429764
Iteration 5, loss = 3.71252714
Iteration 6, loss = 3.69983980
Iteration 7, loss = 3.68623623
Iteration 8, loss = 3.67252057
Iteration 9, loss = 3.65841552
Iteration 10, loss = 3.64447281
Iteration 11, loss = 3.63094157
Iteration 12, loss = 3.61783022
Iteration 13, loss = 3.60511916
Iteration 14, loss = 3.59271689
Iteration 15, loss = 3.58076370
Iteration 16, loss = 3.56936486
Iteration 17, loss = 3.55823431
Iteration 18, loss = 3.54765610
Iteration 19, loss = 3.53738687
Iteration 20, loss = 3.52743680
Iteration 21, loss = 3.51790555
Iteration 22, loss = 3.50870775
Iteration 23, loss = 3.49973912
Iteration 24, loss = 3.49100037
Iteration 25, loss = 3.48268464
Iteration 26, loss = 3.47450938
Iteration 27, loss = 3.46666637
Iteration 28, loss = 3.45892082
Iteration 29, loss = 3.45144703
Iteration 30, loss = 3.44416208
Iteration 31, loss = 3.43710296
Iteration 32, loss = 3.43013712
Iteration 33, loss = 3.42338096
Iteration 34, loss = 3.41679905
Iteration 35, loss = 3.41040235
Iteration 36, loss = 3.40401083
Iteration 37, loss = 3.39790445
Iteration 38, loss = 3.39179723
Iteration 39, loss = 3.38579636
Iteration 40, loss = 3.37992199
Iteration 41, loss = 3.37419671
Iteration 42, loss = 3.36856697
Iteration 43, loss = 3.36296793
Iteration 44, loss = 3.35755146
Iteration 45, loss = 3.35212180
Iteration 46, loss = 3.34677286
Iteration 47, loss = 3.34154075
Iteration 48, loss = 3.33647176
Iteration 49, loss = 3.33135471
Iteration 50, loss = 3.32633870
Iteration 51, loss = 3.32135353
Iteration 52, loss = 3.31646785
Iteration 53, loss = 3.31160245
Iteration 54, loss = 3.30683829
Iteration 55, loss = 3.30213072
Iteration 56, loss = 3.29745145
Iteration 57, loss = 3.29282666
Iteration 58, loss = 3.28825390
Iteration 59, loss = 3.28365878
Iteration 60, loss = 3.27918521
Iteration 61, loss = 3.27476499
Iteration 62, loss = 3.27030734
Iteration 63, loss = 3.26600127
Iteration 64, loss = 3.26161161
Iteration 65, loss = 3.25736597
Iteration 66, loss = 3.25308695
Iteration 67, loss = 3.24885958
Iteration 68, loss = 3.24469412
Iteration 69, loss = 3.24060442
Iteration 70, loss = 3.23643266
Iteration 71, loss = 3.23238947
Iteration 72, loss = 3.22833460
Iteration 73, loss = 3.22430763
Iteration 74, loss = 3.22035003
Iteration 75, loss = 3.21642257
Iteration 76, loss = 3.21250161
Iteration 77, loss = 3.20858761
Iteration 78, loss = 3.20472181
Iteration 79, loss = 3.20090130
Iteration 80, loss = 3.19709997
Iteration 81, loss = 3.19328342
Iteration 82, loss = 3.18952816
Iteration 83, loss = 3.18585333
Iteration 84, loss = 3.18206187
Iteration 85, loss = 3.17838564
Iteration 86, loss = 3.17472345
Iteration 87, loss = 3.17109695
Iteration 88, loss = 3.16746441
Iteration 89, loss = 3.16389226
Iteration 90, loss = 3.16031215
Iteration 91, loss = 3.15675729
Iteration 92, loss = 3.15322831
Iteration 93, loss = 3.14972620
Iteration 94, loss = 3.14623803
Iteration 95, loss = 3.14274836
Iteration 96, loss = 3.13932788
Iteration 97, loss = 3.13583642
Iteration 98, loss = 3.13243916
Iteration 99, loss = 3.12909301
Iteration 100, loss = 3.12567156
Iteration 101, loss = 3.12226614
Iteration 102, loss = 3.11896675
Iteration 103, loss = 3.11564343
Iteration 104, loss = 3.11231958
Iteration 105, loss = 3.10902619
Iteration 106, loss = 3.10575171
Iteration 107, loss = 3.10249751
Iteration 108, loss = 3.09924204
Iteration 109, loss = 3.09598430
Iteration 110, loss = 3.09276771
Iteration 111, loss = 3.08955161
Iteration 112, loss = 3.08640665
Iteration 113, loss = 3.08324408
Iteration 114, loss = 3.08005859
Iteration 115, loss = 3.07692231
Iteration 116, loss = 3.07379240
Iteration 117, loss = 3.07066068
Iteration 118, loss = 3.06755058
Iteration 119, loss = 3.06446797
Iteration 120, loss = 3.06140555
Iteration 121, loss = 3.05831603
Iteration 122, loss = 3.05528239
Iteration 123, loss = 3.05222406
Iteration 124, loss = 3.04918592
Iteration 125, loss = 3.04618264
Iteration 126, loss = 3.04318663
Iteration 127, loss = 3.04017937
Iteration 128, loss = 3.03720994
Iteration 129, loss = 3.03426220
Iteration 130, loss = 3.03128998
Iteration 131, loss = 3.02834098
Iteration 132, loss = 3.02538299
Iteration 133, loss = 3.02250560
Iteration 134, loss = 3.01957376
Iteration 135, loss = 3.01666256
Iteration 136, loss = 3.01375652
Iteration 137, loss = 3.01086941
Iteration 138, loss = 3.00798416
Iteration 139, loss = 3.00511987
Iteration 140, loss = 3.00226303
Iteration 141, loss = 2.99936079
Iteration 142, loss = 2.99653463
Iteration 143, loss = 2.99370852
Iteration 144, loss = 2.99087240
Iteration 145, loss = 2.98804003
Iteration 146, loss = 2.98521763
Iteration 147, loss = 2.98242912
Iteration 148, loss = 2.97962749
Iteration 149, loss = 2.97683557
Iteration 150, loss = 2.97403434
Iteration 151, loss = 2.97134086
Iteration 152, loss = 2.96853842
Iteration 153, loss = 2.96579950
Iteration 154, loss = 2.96304623
Iteration 155, loss = 2.96031157
Iteration 156, loss = 2.95759755
Iteration 157, loss = 2.95487707
Iteration 158, loss = 2.95217482
Iteration 159, loss = 2.94947243
Iteration 160, loss = 2.94674453
Iteration 161, loss = 2.94407270
Iteration 162, loss = 2.94137701
Iteration 163, loss = 2.93870387
Iteration 164, loss = 2.93603673
Iteration 165, loss = 2.93334584
Iteration 166, loss = 2.93069377
Iteration 167, loss = 2.92807672
Iteration 168, loss = 2.92538553
Iteration 169, loss = 2.92272501
Iteration 170, loss = 2.92009206
Iteration 171, loss = 2.91749621
Iteration 172, loss = 2.91484472
Iteration 173, loss = 2.91223224
Iteration 174, loss = 2.90959320
Iteration 175, loss = 2.90700071
Iteration 176, loss = 2.90442101
Iteration 177, loss = 2.90178484
Iteration 178, loss = 2.89921499
Iteration 179, loss = 2.89667257
Iteration 180, loss = 2.89404772
Iteration 181, loss = 2.89148056
Iteration 182, loss = 2.88889888
Iteration 183, loss = 2.88635992
Iteration 184, loss = 2.88377643
Iteration 185, loss = 2.88121660
Iteration 186, loss = 2.87865994
Iteration 187, loss = 2.87613655
Iteration 188, loss = 2.87359744
Iteration 189, loss = 2.87104827
Iteration 190, loss = 2.86853114
Iteration 191, loss = 2.86600530
Iteration 192, loss = 2.86350432
Iteration 193, loss = 2.86096994
Iteration 194, loss = 2.85848066
Iteration 195, loss = 2.85601541
Iteration 196, loss = 2.85347978
Iteration 197, loss = 2.85100757
Iteration 198, loss = 2.84855840
Iteration 199, loss = 2.84604404
Iteration 200, loss = 2.84360272
Iteration 201, loss = 2.84107457
Iteration 202, loss = 2.83863074
Iteration 203, loss = 2.83617706
Iteration 204, loss = 2.83372925
Iteration 205, loss = 2.83125071
Iteration 206, loss = 2.82881896
Iteration 207, loss = 2.82639372
Iteration 208, loss = 2.82393389
Iteration 209, loss = 2.82149191
Iteration 210, loss = 2.81906092
Iteration 211, loss = 2.81662566
Iteration 212, loss = 2.81419936
Iteration 213, loss = 2.81178079
Iteration 214, loss = 2.80938320
Iteration 215, loss = 2.80695800
Iteration 216, loss = 2.80456713
Iteration 217, loss = 2.80214241
Iteration 218, loss = 2.79975976
Iteration 219, loss = 2.79735098
Iteration 220, loss = 2.79495757
Iteration 221, loss = 2.79256453
Iteration 222, loss = 2.79020263
Iteration 223, loss = 2.78781146
Iteration 224, loss = 2.78543672
Iteration 225, loss = 2.78303992
Iteration 226, loss = 2.78065589
Iteration 227, loss = 2.77829342
Iteration 228, loss = 2.77593138
Iteration 229, loss = 2.77356219
Iteration 230, loss = 2.77120585
Iteration 231, loss = 2.76885466
Iteration 232, loss = 2.76647766
Iteration 233, loss = 2.76411202
Iteration 234, loss = 2.76179178
Iteration 235, loss = 2.75943980
Iteration 236, loss = 2.75708377
Iteration 237, loss = 2.75472355
Iteration 238, loss = 2.75238882
Iteration 239, loss = 2.75003767
Iteration 240, loss = 2.74772916
Iteration 241, loss = 2.74537044
Iteration 242, loss = 2.74304655
Iteration 243, loss = 2.74073147
Iteration 244, loss = 2.73840163
Iteration 245, loss = 2.73608434
Iteration 246, loss = 2.73376675
Iteration 247, loss = 2.73145670
Iteration 248, loss = 2.72912591
Iteration 249, loss = 2.72680321
Iteration 250, loss = 2.72452507
Iteration 251, loss = 2.72222550
Iteration 252, loss = 2.71989465
Iteration 253, loss = 2.71760500
Iteration 254, loss = 2.71532434
Iteration 255, loss = 2.71300876
Iteration 256, loss = 2.71073341
Iteration 257, loss = 2.70844251
Iteration 258, loss = 2.70615017
Iteration 259, loss = 2.70386832
Iteration 260, loss = 2.70157651
Iteration 261, loss = 2.69931231
Iteration 262, loss = 2.69701053
Iteration 263, loss = 2.69475545
Iteration 264, loss = 2.69247821
Iteration 265, loss = 2.69017773
Iteration 266, loss = 2.68793112
Iteration 267, loss = 2.68567450
Iteration 268, loss = 2.68339823
Iteration 269, loss = 2.68113014
Iteration 270, loss = 2.67885372
Iteration 271, loss = 2.67661297
Iteration 272, loss = 2.67433781
Iteration 273, loss = 2.67210459
Iteration 274, loss = 2.66982854
Iteration 275, loss = 2.66758241
Iteration 276, loss = 2.66534291
Iteration 277, loss = 2.66308725
Iteration 278, loss = 2.66088784
Iteration 279, loss = 2.65861537
Iteration 280, loss = 2.65637173
Iteration 281, loss = 2.65416677
Iteration 282, loss = 2.65190942
Iteration 283, loss = 2.64968395
Iteration 284, loss = 2.64745189
Iteration 285, loss = 2.64522549
Iteration 286, loss = 2.64299350
Iteration 287, loss = 2.64079480
Iteration 288, loss = 2.63857068
Iteration 289, loss = 2.63632773
Iteration 290, loss = 2.63413328
Iteration 291, loss = 2.63190663
Iteration 292, loss = 2.62968628
Iteration 293, loss = 2.62748187
Iteration 294, loss = 2.62525272
Iteration 295, loss = 2.62304543
Iteration 296, loss = 2.62086428
Iteration 297, loss = 2.61863371
Iteration 298, loss = 2.61645860
Iteration 299, loss = 2.61423411
Iteration 300, loss = 2.61206076
Iteration 301, loss = 2.60982117
Iteration 302, loss = 2.60764385
Iteration 303, loss = 2.60543942
Iteration 304, loss = 2.60326866
Iteration 305, loss = 2.60109076
Iteration 306, loss = 2.59888052
Iteration 307, loss = 2.59669723
Iteration 308, loss = 2.59450334
Iteration 309, loss = 2.59231879
Iteration 310, loss = 2.59011844
Iteration 311, loss = 2.58795379
Iteration 312, loss = 2.58576065
Iteration 313, loss = 2.58357359
Iteration 314, loss = 2.58138633
Iteration 315, loss = 2.57920474
Iteration 316, loss = 2.57704129
Iteration 317, loss = 2.57487567
Iteration 318, loss = 2.57268588
Iteration 319, loss = 2.57050572
Iteration 320, loss = 2.56838881
Iteration 321, loss = 2.56617873
Iteration 322, loss = 2.56402055
Iteration 323, loss = 2.56185102
Iteration 324, loss = 2.55969392
Iteration 325, loss = 2.55753912
Iteration 326, loss = 2.55536222
Iteration 327, loss = 2.55323624
Iteration 328, loss = 2.55106475
Iteration 329, loss = 2.54891690
Iteration 330, loss = 2.54676690
Iteration 331, loss = 2.54460904
Iteration 332, loss = 2.54245883
Iteration 333, loss = 2.54033298
Iteration 334, loss = 2.53817246
Iteration 335, loss = 2.53603039
Iteration 336, loss = 2.53391831
Iteration 337, loss = 2.53178044
Iteration 338, loss = 2.52961828
Iteration 339, loss = 2.52750715
Iteration 340, loss = 2.52535070
Iteration 341, loss = 2.52320976
Iteration 342, loss = 2.52109728
Iteration 343, loss = 2.51895922
Iteration 344, loss = 2.51683614
Iteration 345, loss = 2.51470728
Iteration 346, loss = 2.51257539
Iteration 347, loss = 2.51045629
Iteration 348, loss = 2.50834052
Iteration 349, loss = 2.50620893
Iteration 350, loss = 2.50410387
Iteration 351, loss = 2.50196683
Iteration 352, loss = 2.49985309
Iteration 353, loss = 2.49772847
Iteration 354, loss = 2.49561849
Iteration 355, loss = 2.49351616
Iteration 356, loss = 2.49138970
Iteration 357, loss = 2.48929192
Iteration 358, loss = 2.48719376
Iteration 359, loss = 2.48506622
Iteration 360, loss = 2.48298217
Iteration 361, loss = 2.48089034
Iteration 362, loss = 2.47875612
Iteration 363, loss = 2.47667331
Iteration 364, loss = 2.47460574
Iteration 365, loss = 2.47248807
Iteration 366, loss = 2.47038468
Iteration 367, loss = 2.46829547
Iteration 368, loss = 2.46621739
Iteration 369, loss = 2.46410566
Iteration 370, loss = 2.46203352
Iteration 371, loss = 2.45993724
Iteration 372, loss = 2.45786316
Iteration 373, loss = 2.45576166
Iteration 374, loss = 2.45368734
Iteration 375, loss = 2.45160082
Iteration 376, loss = 2.44954408
Iteration 377, loss = 2.44743396
Iteration 378, loss = 2.44536335
Iteration 379, loss = 2.44328295
Iteration 380, loss = 2.44120858
Iteration 381, loss = 2.43914871
Iteration 382, loss = 2.43704341
Iteration 383, loss = 2.43497951
Iteration 384, loss = 2.43290464
Iteration 385, loss = 2.43082949
Iteration 386, loss = 2.42875887
Iteration 387, loss = 2.42668262
Iteration 388, loss = 2.42463037
Iteration 389, loss = 2.42255040
Iteration 390, loss = 2.42050713
Iteration 391, loss = 2.41843526
Iteration 392, loss = 2.41636954
Iteration 393, loss = 2.41429923
Iteration 394, loss = 2.41224963
Iteration 395, loss = 2.41021996
Iteration 396, loss = 2.40813472
Iteration 397, loss = 2.40609565
Iteration 398, loss = 2.40403496
Iteration 399, loss = 2.40194474
Iteration 400, loss = 2.39991103
Iteration 401, loss = 2.39785937
Iteration 402, loss = 2.39578672
Iteration 403, loss = 2.39374658
Iteration 404, loss = 2.39169156
Iteration 405, loss = 2.38964012
Iteration 406, loss = 2.38760096
Iteration 407, loss = 2.38554789
Iteration 408, loss = 2.38350956
Iteration 409, loss = 2.38145616
Iteration 410, loss = 2.37942479
Iteration 411, loss = 2.37738321
Iteration 412, loss = 2.37535473
Iteration 413, loss = 2.37330932
Iteration 414, loss = 2.37129270
Iteration 415, loss = 2.36925249
Iteration 416, loss = 2.36719373
Iteration 417, loss = 2.36517874
Iteration 418, loss = 2.36313176
Iteration 419, loss = 2.36111302
Iteration 420, loss = 2.35908341
Iteration 421, loss = 2.35702449
Iteration 422, loss = 2.35502227
Iteration 423, loss = 2.35297665
Iteration 424, loss = 2.35095228
Iteration 425, loss = 2.34893661
Iteration 426, loss = 2.34689225
Iteration 427, loss = 2.34487368
Iteration 428, loss = 2.34285566
Iteration 429, loss = 2.34080687
Iteration 430, loss = 2.33879103
Iteration 431, loss = 2.33678665
Iteration 432, loss = 2.33474205
Iteration 433, loss = 2.33276044
Iteration 434, loss = 2.33071849
Iteration 435, loss = 2.32871749
Iteration 436, loss = 2.32669814
Iteration 437, loss = 2.32468625
Iteration 438, loss = 2.32266971
Iteration 439, loss = 2.32065508
Iteration 440, loss = 2.31865331
Iteration 441, loss = 2.31664414
Iteration 442, loss = 2.31464106
Iteration 443, loss = 2.31265761
Iteration 444, loss = 2.31063005
Iteration 445, loss = 2.30861490
Iteration 446, loss = 2.30661503
Iteration 447, loss = 2.30461697
Iteration 448, loss = 2.30261996
Iteration 449, loss = 2.30061147
Iteration 450, loss = 2.29863094
Iteration 451, loss = 2.29661666
Iteration 452, loss = 2.29463427
Iteration 453, loss = 2.29262077
Iteration 454, loss = 2.29063658
Iteration 455, loss = 2.28865194
Iteration 456, loss = 2.28665648
Iteration 457, loss = 2.28469331
Iteration 458, loss = 2.28268522
Iteration 459, loss = 2.28070123
Iteration 460, loss = 2.27869616
Iteration 461, loss = 2.27673424
Iteration 462, loss = 2.27473402
Iteration 463, loss = 2.27276335
Iteration 464, loss = 2.27076749
Iteration 465, loss = 2.26882709
Iteration 466, loss = 2.26681113
Iteration 467, loss = 2.26485731
Iteration 468, loss = 2.26289171
Iteration 469, loss = 2.26089321
Iteration 470, loss = 2.25891698
Iteration 471, loss = 2.25693073
Iteration 472, loss = 2.25497852
Iteration 473, loss = 2.25302371
Iteration 474, loss = 2.25102386
Iteration 475, loss = 2.24907294
Iteration 476, loss = 2.24708890
Iteration 477, loss = 2.24515491
Iteration 478, loss = 2.24318014
Iteration 479, loss = 2.24120778
Iteration 480, loss = 2.23924266
Iteration 481, loss = 2.23727813
Iteration 482, loss = 2.23531759
Iteration 483, loss = 2.23336912
Iteration 484, loss = 2.23140919
Iteration 485, loss = 2.22946041
Iteration 486, loss = 2.22748864
Iteration 487, loss = 2.22553168
Iteration 488, loss = 2.22358731
Iteration 489, loss = 2.22164242
Iteration 490, loss = 2.21969505
Iteration 491, loss = 2.21773656
Iteration 492, loss = 2.21580553
Iteration 493, loss = 2.21384525
Iteration 494, loss = 2.21188794
Iteration 495, loss = 2.20995947
Iteration 496, loss = 2.20799493
Iteration 497, loss = 2.20604761
Iteration 498, loss = 2.20411363
Iteration 499, loss = 2.20216611
Iteration 500, loss = 2.20021835
Iteration 501, loss = 2.19830364
Iteration 502, loss = 2.19635360
Iteration 503, loss = 2.19441564
Iteration 504, loss = 2.19247701
Iteration 505, loss = 2.19053096
Iteration 506, loss = 2.18859596
Iteration 507, loss = 2.18666183
Iteration 508, loss = 2.18472958
Iteration 509, loss = 2.18280601
Iteration 510, loss = 2.18084746
Iteration 511, loss = 2.17893984
Iteration 512, loss = 2.17701487
Iteration 513, loss = 2.17508122
Iteration 514, loss = 2.17316037
Iteration 515, loss = 2.17122309
Iteration 516, loss = 2.16930938
Iteration 517, loss = 2.16737781
Iteration 518, loss = 2.16545942
Iteration 519, loss = 2.16353674
Iteration 520, loss = 2.16161925
Iteration 521, loss = 2.15970319
Iteration 522, loss = 2.15777416
Iteration 523, loss = 2.15586712
Iteration 524, loss = 2.15394071
Iteration 525, loss = 2.15203118
Iteration 526, loss = 2.15010419
Iteration 527, loss = 2.14820256
Iteration 528, loss = 2.14629175
Iteration 529, loss = 2.14437534
Iteration 530, loss = 2.14245891
Iteration 531, loss = 2.14056248
Iteration 532, loss = 2.13865560
Iteration 533, loss = 2.13674829
Iteration 534, loss = 2.13484392
Iteration 535, loss = 2.13295110
Iteration 536, loss = 2.13103175
Iteration 537, loss = 2.12913754
Iteration 538, loss = 2.12724337
Iteration 539, loss = 2.12533542
Iteration 540, loss = 2.12343361
Iteration 541, loss = 2.12153184
Iteration 542, loss = 2.11964641
Iteration 543, loss = 2.11773432
Iteration 544, loss = 2.11583982
Iteration 545, loss = 2.11394925
Iteration 546, loss = 2.11207503
Iteration 547, loss = 2.11016944
Iteration 548, loss = 2.10827891
Iteration 549, loss = 2.10639863
Iteration 550, loss = 2.10453151
Iteration 551, loss = 2.10262989
Iteration 552, loss = 2.10073713
Iteration 553, loss = 2.09885383
Iteration 554, loss = 2.09697053
Iteration 555, loss = 2.09509188
Iteration 556, loss = 2.09319438
Iteration 557, loss = 2.09132485
Iteration 558, loss = 2.08945750
Iteration 559, loss = 2.08756626
Iteration 560, loss = 2.08571036
Iteration 561, loss = 2.08381410
Iteration 562, loss = 2.08193460
Iteration 563, loss = 2.08006055
Iteration 564, loss = 2.07819097
Iteration 565, loss = 2.07632526
Iteration 566, loss = 2.07443306
Iteration 567, loss = 2.07258387
Iteration 568, loss = 2.07071922
Iteration 569, loss = 2.06885096
Iteration 570, loss = 2.06699046
Iteration 571, loss = 2.06511746
Iteration 572, loss = 2.06326261
Iteration 573, loss = 2.06140211
Iteration 574, loss = 2.05954923
Iteration 575, loss = 2.05768167
Iteration 576, loss = 2.05584287
Iteration 577, loss = 2.05395930
Iteration 578, loss = 2.05211379
Iteration 579, loss = 2.05024760
Iteration 580, loss = 2.04839166
Iteration 581, loss = 2.04654331
Iteration 582, loss = 2.04469111
Iteration 583, loss = 2.04283628
Iteration 584, loss = 2.04098144
Iteration 585, loss = 2.03913125
Iteration 586, loss = 2.03729750
Iteration 587, loss = 2.03543699
Iteration 588, loss = 2.03359813
Iteration 589, loss = 2.03174887
Iteration 590, loss = 2.02989498
Iteration 591, loss = 2.02806736
Iteration 592, loss = 2.02621890
Iteration 593, loss = 2.02438693
Iteration 594, loss = 2.02253275
Iteration 595, loss = 2.02070866
Iteration 596, loss = 2.01889241
Iteration 597, loss = 2.01704426
Iteration 598, loss = 2.01520519
Iteration 599, loss = 2.01337627
Iteration 600, loss = 2.01153965
Iteration 601, loss = 2.00970664
Iteration 602, loss = 2.00787193
Iteration 603, loss = 2.00605696
Iteration 604, loss = 2.00422315
Iteration 605, loss = 2.00240615
Iteration 606, loss = 2.00058100
Iteration 607, loss = 1.99876292
Iteration 608, loss = 1.99693107
Iteration 609, loss = 1.99511847
Iteration 610, loss = 1.99329355
Iteration 611, loss = 1.99148541
Iteration 612, loss = 1.98967953
Iteration 613, loss = 1.98784382
Iteration 614, loss = 1.98601377
Iteration 615, loss = 1.98419546
Iteration 616, loss = 1.98237715
Iteration 617, loss = 1.98056773
Iteration 618, loss = 1.97875380
Iteration 619, loss = 1.97695017
Iteration 620, loss = 1.97513423
Iteration 621, loss = 1.97333235
Iteration 622, loss = 1.97152947
Iteration 623, loss = 1.96973619
Iteration 624, loss = 1.96791761
Iteration 625, loss = 1.96611157
Iteration 626, loss = 1.96430806
Iteration 627, loss = 1.96251095
Iteration 628, loss = 1.96072461
Iteration 629, loss = 1.95891623
Iteration 630, loss = 1.95713486
Iteration 631, loss = 1.95533348
Iteration 632, loss = 1.95353618
Iteration 633, loss = 1.95175003
Iteration 634, loss = 1.94993994
Iteration 635, loss = 1.94815335
Iteration 636, loss = 1.94636654
Iteration 637, loss = 1.94457611
Iteration 638, loss = 1.94279424
Iteration 639, loss = 1.94100185
Iteration 640, loss = 1.93923130
Iteration 641, loss = 1.93743349
Iteration 642, loss = 1.93565738
Iteration 643, loss = 1.93387617
Iteration 644, loss = 1.93209811
Iteration 645, loss = 1.93032061
Iteration 646, loss = 1.92854523
Iteration 647, loss = 1.92677114
Iteration 648, loss = 1.92499753
Iteration 649, loss = 1.92320725
Iteration 650, loss = 1.92144154
Iteration 651, loss = 1.91965687
Iteration 652, loss = 1.91789146
Iteration 653, loss = 1.91610401
Iteration 654, loss = 1.91435760
Iteration 655, loss = 1.91259865
Iteration 656, loss = 1.91082091
Iteration 657, loss = 1.90906848
Iteration 658, loss = 1.90729614
Iteration 659, loss = 1.90554336
Iteration 660, loss = 1.90377187
Iteration 661, loss = 1.90202534
Iteration 662, loss = 1.90027433
Iteration 663, loss = 1.89849916
Iteration 664, loss = 1.89676162
Iteration 665, loss = 1.89499853
Iteration 666, loss = 1.89324623
Iteration 667, loss = 1.89148602
Iteration 668, loss = 1.88973564
Iteration 669, loss = 1.88799644
Iteration 670, loss = 1.88625170
Iteration 671, loss = 1.88450283
Iteration 672, loss = 1.88274355
Iteration 673, loss = 1.88101814
Iteration 674, loss = 1.87926695
Iteration 675, loss = 1.87750232
Iteration 676, loss = 1.87577193
Iteration 677, loss = 1.87404549
Iteration 678, loss = 1.87230467
Iteration 679, loss = 1.87055837
Iteration 680, loss = 1.86882496
Iteration 681, loss = 1.86710021
Iteration 682, loss = 1.86536512
Iteration 683, loss = 1.86362469
Iteration 684, loss = 1.86190854
Iteration 685, loss = 1.86017893
Iteration 686, loss = 1.85843606
Iteration 687, loss = 1.85671123
Iteration 688, loss = 1.85499626
Iteration 689, loss = 1.85327920
Iteration 690, loss = 1.85154376
Iteration 691, loss = 1.84982671
Iteration 692, loss = 1.84811942
Iteration 693, loss = 1.84637949
Iteration 694, loss = 1.84465969
Iteration 695, loss = 1.84293058
Iteration 696, loss = 1.84123501
Iteration 697, loss = 1.83951235
Iteration 698, loss = 1.83780511
Iteration 699, loss = 1.83609305
Iteration 700, loss = 1.83438996
Iteration 701, loss = 1.83267075
Iteration 702, loss = 1.83096131
Iteration 703, loss = 1.82926134
Iteration 704, loss = 1.82756506
Iteration 705, loss = 1.82585886
Iteration 706, loss = 1.82417469
Iteration 707, loss = 1.82245942
Iteration 708, loss = 1.82075252
Iteration 709, loss = 1.81904875
Iteration 710, loss = 1.81735615
Iteration 711, loss = 1.81566433
Iteration 712, loss = 1.81396346
Iteration 713, loss = 1.81227168
Iteration 714, loss = 1.81058398
Iteration 715, loss = 1.80888102
Iteration 716, loss = 1.80719433
Iteration 717, loss = 1.80550099
Iteration 718, loss = 1.80382051
Iteration 719, loss = 1.80213315
Iteration 720, loss = 1.80044055
Iteration 721, loss = 1.79876444
Iteration 722, loss = 1.79708222
Iteration 723, loss = 1.79540365
Iteration 724, loss = 1.79371300
Iteration 725, loss = 1.79204043
Iteration 726, loss = 1.79036444
Iteration 727, loss = 1.78869656
Iteration 728, loss = 1.78701865
Iteration 729, loss = 1.78533926
Iteration 730, loss = 1.78368938
Iteration 731, loss = 1.78200759
Iteration 732, loss = 1.78034190
Iteration 733, loss = 1.77866397
Iteration 734, loss = 1.77700023
Iteration 735, loss = 1.77533453
Iteration 736, loss = 1.77366266
Iteration 737, loss = 1.77202257
Iteration 738, loss = 1.77035273
Iteration 739, loss = 1.76868794
Iteration 740, loss = 1.76703356
Iteration 741, loss = 1.76536845
Iteration 742, loss = 1.76370606
Iteration 743, loss = 1.76205966
Iteration 744, loss = 1.76040062
Iteration 745, loss = 1.75874926
Iteration 746, loss = 1.75710817
Iteration 747, loss = 1.75544822
Iteration 748, loss = 1.75379842
Iteration 749, loss = 1.75215626
Iteration 750, loss = 1.75050123
Iteration 751, loss = 1.74886564
Iteration 752, loss = 1.74721315
Iteration 753, loss = 1.74555418
Iteration 754, loss = 1.74392516
Iteration 755, loss = 1.74227842
Iteration 756, loss = 1.74065023
Iteration 757, loss = 1.73901099
Iteration 758, loss = 1.73736582
Iteration 759, loss = 1.73571689
Iteration 760, loss = 1.73408680
Iteration 761, loss = 1.73245534
Iteration 762, loss = 1.73082764
Iteration 763, loss = 1.72919806
Iteration 764, loss = 1.72756412
Iteration 765, loss = 1.72592861
Iteration 766, loss = 1.72430445
Iteration 767, loss = 1.72268774
Iteration 768, loss = 1.72105108
Iteration 769, loss = 1.71944733
Iteration 770, loss = 1.71782717
Iteration 771, loss = 1.71620990
Iteration 772, loss = 1.71458429
Iteration 773, loss = 1.71296180
Iteration 774, loss = 1.71134632
Iteration 775, loss = 1.70973176
Iteration 776, loss = 1.70811903
Iteration 777, loss = 1.70650675
Iteration 778, loss = 1.70491345
Iteration 779, loss = 1.70329273
Iteration 780, loss = 1.70167717
Iteration 781, loss = 1.70009064
Iteration 782, loss = 1.69846047
Iteration 783, loss = 1.69687566
Iteration 784, loss = 1.69525202
Iteration 785, loss = 1.69366148
Iteration 786, loss = 1.69206975
Iteration 787, loss = 1.69046111
Iteration 788, loss = 1.68886262
Iteration 789, loss = 1.68727081
Iteration 790, loss = 1.68568167
Iteration 791, loss = 1.68408058
Iteration 792, loss = 1.68249881
Iteration 793, loss = 1.68091526
Iteration 794, loss = 1.67932628
Iteration 795, loss = 1.67772701
Iteration 796, loss = 1.67614617
Iteration 797, loss = 1.67455082
Iteration 798, loss = 1.67298133
Iteration 799, loss = 1.67138800
Iteration 800, loss = 1.66982965
Iteration 801, loss = 1.66822369
Iteration 802, loss = 1.66663921
Iteration 803, loss = 1.66506292
Iteration 804, loss = 1.66349122
Iteration 805, loss = 1.66190161
Iteration 806, loss = 1.66033701
Iteration 807, loss = 1.65876354
Iteration 808, loss = 1.65718456
Iteration 809, loss = 1.65562625
Iteration 810, loss = 1.65406036
Iteration 811, loss = 1.65248689
Iteration 812, loss = 1.65092135
Iteration 813, loss = 1.64936893
Iteration 814, loss = 1.64778037
Iteration 815, loss = 1.64623037
Iteration 816, loss = 1.64466225
Iteration 817, loss = 1.64310748
Iteration 818, loss = 1.64154781
Iteration 819, loss = 1.63997682
Iteration 820, loss = 1.63843230
Iteration 821, loss = 1.63687293
Iteration 822, loss = 1.63533122
Iteration 823, loss = 1.63377309
Iteration 824, loss = 1.63222393
Iteration 825, loss = 1.63067943
Iteration 826, loss = 1.62913212
Iteration 827, loss = 1.62758548
Iteration 828, loss = 1.62603389
Iteration 829, loss = 1.62448909
Iteration 830, loss = 1.62295851
Iteration 831, loss = 1.62140695
Iteration 832, loss = 1.61986925
Iteration 833, loss = 1.61834416
Iteration 834, loss = 1.61679731
Iteration 835, loss = 1.61526530
Iteration 836, loss = 1.61373263
Iteration 837, loss = 1.61218742
Iteration 838, loss = 1.61065657
Iteration 839, loss = 1.60912706
Iteration 840, loss = 1.60759702
Iteration 841, loss = 1.60607656
Iteration 842, loss = 1.60455318
Iteration 843, loss = 1.60303511
Iteration 844, loss = 1.60149252
Iteration 845, loss = 1.59997823
Iteration 846, loss = 1.59844384
Iteration 847, loss = 1.59693495
Iteration 848, loss = 1.59541497
Iteration 849, loss = 1.59388424
Iteration 850, loss = 1.59238091
Iteration 851, loss = 1.59086116
Iteration 852, loss = 1.58933933
Iteration 853, loss = 1.58781301
Iteration 854, loss = 1.58630524
Iteration 855, loss = 1.58479341
Iteration 856, loss = 1.58327626
Iteration 857, loss = 1.58177178
Iteration 858, loss = 1.58026292
Iteration 859, loss = 1.57874087
Iteration 860, loss = 1.57724572
Iteration 861, loss = 1.57574489
Iteration 862, loss = 1.57423477
Iteration 863, loss = 1.57273827
Iteration 864, loss = 1.57124058
Iteration 865, loss = 1.56973940
Iteration 866, loss = 1.56823679
Iteration 867, loss = 1.56675226
Iteration 868, loss = 1.56526264
Iteration 869, loss = 1.56377050
Iteration 870, loss = 1.56227981
Iteration 871, loss = 1.56078681
Iteration 872, loss = 1.55929259
Iteration 873, loss = 1.55780154
Iteration 874, loss = 1.55631445
Iteration 875, loss = 1.55483432
Iteration 876, loss = 1.55336158
Iteration 877, loss = 1.55187463
Iteration 878, loss = 1.55038347
Iteration 879, loss = 1.54890611
Iteration 880, loss = 1.54742996
Iteration 881, loss = 1.54596359
Iteration 882, loss = 1.54447367
Iteration 883, loss = 1.54300250
Iteration 884, loss = 1.54153516
Iteration 885, loss = 1.54005917
Iteration 886, loss = 1.53858554
Iteration 887, loss = 1.53711695
Iteration 888, loss = 1.53565340
Iteration 889, loss = 1.53417859
Iteration 890, loss = 1.53270965
Iteration 891, loss = 1.53125509
Iteration 892, loss = 1.52979619
Iteration 893, loss = 1.52833140
Iteration 894, loss = 1.52687441
Iteration 895, loss = 1.52541832
Iteration 896, loss = 1.52396017
Iteration 897, loss = 1.52249039
Iteration 898, loss = 1.52103823
Iteration 899, loss = 1.51959478
Iteration 900, loss = 1.51814127
Iteration 901, loss = 1.51668988
Iteration 902, loss = 1.51524388
Iteration 903, loss = 1.51378077
Iteration 904, loss = 1.51236285
Iteration 905, loss = 1.51090372
Iteration 906, loss = 1.50945525
Iteration 907, loss = 1.50802136
Iteration 908, loss = 1.50658153
Iteration 909, loss = 1.50514162
Iteration 910, loss = 1.50369651
Iteration 911, loss = 1.50226219
Iteration 912, loss = 1.50082533
Iteration 913, loss = 1.49940247
Iteration 914, loss = 1.49796874
Iteration 915, loss = 1.49652607
Iteration 916, loss = 1.49509907
Iteration 917, loss = 1.49367456
Iteration 918, loss = 1.49223943
Iteration 919, loss = 1.49081698
Iteration 920, loss = 1.48938015
Iteration 921, loss = 1.48795991
Iteration 922, loss = 1.48652930
Iteration 923, loss = 1.48511482
Iteration 924, loss = 1.48369544
Iteration 925, loss = 1.48226938
Iteration 926, loss = 1.48086407
Iteration 927, loss = 1.47945029
Iteration 928, loss = 1.47802611
Iteration 929, loss = 1.47661798
Iteration 930, loss = 1.47519365
Iteration 931, loss = 1.47378816
Iteration 932, loss = 1.47239536
Iteration 933, loss = 1.47097081
Iteration 934, loss = 1.46956455
Iteration 935, loss = 1.46815137
Iteration 936, loss = 1.46674970
Iteration 937, loss = 1.46533928
Iteration 938, loss = 1.46394676
Iteration 939, loss = 1.46254767
Iteration 940, loss = 1.46114520
Iteration 941, loss = 1.45975301
Iteration 942, loss = 1.45835152
Iteration 943, loss = 1.45695909
Iteration 944, loss = 1.45555615
Iteration 945, loss = 1.45416099
Iteration 946, loss = 1.45278474
Iteration 947, loss = 1.45137863
Iteration 948, loss = 1.45000613
Iteration 949, loss = 1.44861398
Iteration 950, loss = 1.44722691
Iteration 951, loss = 1.44583265
Iteration 952, loss = 1.44445514
Iteration 953, loss = 1.44306867
Iteration 954, loss = 1.44168329
Iteration 955, loss = 1.44029877
Iteration 956, loss = 1.43892019
Iteration 957, loss = 1.43754074
Iteration 958, loss = 1.43616490
Iteration 959, loss = 1.43478738
Iteration 960, loss = 1.43341613
Iteration 961, loss = 1.43203018
Iteration 962, loss = 1.43065694
Iteration 963, loss = 1.42929432
Iteration 964, loss = 1.42790893
Iteration 965, loss = 1.42654853
Iteration 966, loss = 1.42517885
Iteration 967, loss = 1.42381517
Iteration 968, loss = 1.42245877
Iteration 969, loss = 1.42107650
Iteration 970, loss = 1.41973094
Iteration 971, loss = 1.41836002
Iteration 972, loss = 1.41700628
Iteration 973, loss = 1.41564478
Iteration 974, loss = 1.41430065
Iteration 975, loss = 1.41292670
Iteration 976, loss = 1.41158988
Iteration 977, loss = 1.41023094
Iteration 978, loss = 1.40887273
Iteration 979, loss = 1.40753173
Iteration 980, loss = 1.40619063
Iteration 981, loss = 1.40482610
Iteration 982, loss = 1.40349615
Iteration 983, loss = 1.40214195
Iteration 984, loss = 1.40080955
Iteration 985, loss = 1.39947055
Iteration 986, loss = 1.39811141
Iteration 987, loss = 1.39677837
Iteration 988, loss = 1.39545526
Iteration 989, loss = 1.39411504
Iteration 990, loss = 1.39278327
Iteration 991, loss = 1.39144558
Iteration 992, loss = 1.39010945
Iteration 993, loss = 1.38878039
Iteration 994, loss = 1.38744682
Iteration 995, loss = 1.38611114
Iteration 996, loss = 1.38480020
Iteration 997, loss = 1.38346614
Iteration 998, loss = 1.38213625
Iteration 999, loss = 1.38081006
Iteration 1000, loss = 1.37949575
Iteration 1001, loss = 1.37817968
Iteration 1002, loss = 1.37685449
Iteration 1003, loss = 1.37553102
Iteration 1004, loss = 1.37421845
Iteration 1005, loss = 1.37289780
Iteration 1006, loss = 1.37159354
Iteration 1007, loss = 1.37028265
Iteration 1008, loss = 1.36895903
Iteration 1009, loss = 1.36764829
Iteration 1010, loss = 1.36634116
Iteration 1011, loss = 1.36502817
Iteration 1012, loss = 1.36372730
Iteration 1013, loss = 1.36242007
Iteration 1014, loss = 1.36111986
Iteration 1015, loss = 1.35981547
Iteration 1016, loss = 1.35849349
Iteration 1017, loss = 1.35721246
Iteration 1018, loss = 1.35589919
Iteration 1019, loss = 1.35461048
Iteration 1020, loss = 1.35332189
Iteration 1021, loss = 1.35200477
Iteration 1022, loss = 1.35072072
Iteration 1023, loss = 1.34942777
Iteration 1024, loss = 1.34813318
Iteration 1025, loss = 1.34683614
Iteration 1026, loss = 1.34554398
Iteration 1027, loss = 1.34426466
Iteration 1028, loss = 1.34298154
Iteration 1029, loss = 1.34168626
Iteration 1030, loss = 1.34040637
Iteration 1031, loss = 1.33914123
Iteration 1032, loss = 1.33783887
Iteration 1033, loss = 1.33655281
Iteration 1034, loss = 1.33528462
Iteration 1035, loss = 1.33399834
Iteration 1036, loss = 1.33271233
Iteration 1037, loss = 1.33144103
Iteration 1038, loss = 1.33015385
Iteration 1039, loss = 1.32888654
Iteration 1040, loss = 1.32761255
Iteration 1041, loss = 1.32634392
Iteration 1042, loss = 1.32506512
Iteration 1043, loss = 1.32379292
Iteration 1044, loss = 1.32253822
Iteration 1045, loss = 1.32125161
Iteration 1046, loss = 1.32000024
Iteration 1047, loss = 1.31872346
Iteration 1048, loss = 1.31746169
Iteration 1049, loss = 1.31620470
Iteration 1050, loss = 1.31493390
Iteration 1051, loss = 1.31369184
Iteration 1052, loss = 1.31242486
Iteration 1053, loss = 1.31116650
Iteration 1054, loss = 1.30990852
Iteration 1055, loss = 1.30866212
Iteration 1056, loss = 1.30739142
Iteration 1057, loss = 1.30614482
Iteration 1058, loss = 1.30489665
Iteration 1059, loss = 1.30365630
Iteration 1060, loss = 1.30239018
Iteration 1061, loss = 1.30115229
Iteration 1062, loss = 1.29990556
Iteration 1063, loss = 1.29867103
Iteration 1064, loss = 1.29742002
Iteration 1065, loss = 1.29617534
Iteration 1066, loss = 1.29493841
Iteration 1067, loss = 1.29371085
Iteration 1068, loss = 1.29246027
Iteration 1069, loss = 1.29122632
Iteration 1070, loss = 1.28998767
Iteration 1071, loss = 1.28875423
Iteration 1072, loss = 1.28753117
Iteration 1073, loss = 1.28629064
Iteration 1074, loss = 1.28506595
Iteration 1075, loss = 1.28384539
Iteration 1076, loss = 1.28260137
Iteration 1077, loss = 1.28137476
Iteration 1078, loss = 1.28014455
Iteration 1079, loss = 1.27892597
Iteration 1080, loss = 1.27769103
Iteration 1081, loss = 1.27648664
Iteration 1082, loss = 1.27525403
Iteration 1083, loss = 1.27402669
Iteration 1084, loss = 1.27281031
Iteration 1085, loss = 1.27158846
Iteration 1086, loss = 1.27037183
Iteration 1087, loss = 1.26917265
Iteration 1088, loss = 1.26794044
Iteration 1089, loss = 1.26673470
Iteration 1090, loss = 1.26551329
Iteration 1091, loss = 1.26430687
Iteration 1092, loss = 1.26309235
Iteration 1093, loss = 1.26189969
Iteration 1094, loss = 1.26067930
Iteration 1095, loss = 1.25948106
Iteration 1096, loss = 1.25827575
Iteration 1097, loss = 1.25708607
Iteration 1098, loss = 1.25587231
Iteration 1099, loss = 1.25467396
Iteration 1100, loss = 1.25347198
Iteration 1101, loss = 1.25229207
Iteration 1102, loss = 1.25108279
Iteration 1103, loss = 1.24989675
Iteration 1104, loss = 1.24869785
Iteration 1105, loss = 1.24749838
Iteration 1106, loss = 1.24632033
Iteration 1107, loss = 1.24512550
Iteration 1108, loss = 1.24392840
Iteration 1109, loss = 1.24273685
Iteration 1110, loss = 1.24156242
Iteration 1111, loss = 1.24037413
Iteration 1112, loss = 1.23918214
Iteration 1113, loss = 1.23799578
Iteration 1114, loss = 1.23681252
Iteration 1115, loss = 1.23562309
Iteration 1116, loss = 1.23445814
Iteration 1117, loss = 1.23326458
Iteration 1118, loss = 1.23208657
Iteration 1119, loss = 1.23091549
Iteration 1120, loss = 1.22974692
Iteration 1121, loss = 1.22856914
Iteration 1122, loss = 1.22740057
Iteration 1123, loss = 1.22621744
Iteration 1124, loss = 1.22505954
Iteration 1125, loss = 1.22387594
Iteration 1126, loss = 1.22271607
Iteration 1127, loss = 1.22154039
Iteration 1128, loss = 1.22037965
Iteration 1129, loss = 1.21920634
Iteration 1130, loss = 1.21804439
Iteration 1131, loss = 1.21689578
Iteration 1132, loss = 1.21572742
Iteration 1133, loss = 1.21456670
Iteration 1134, loss = 1.21341770
Iteration 1135, loss = 1.21226088
Iteration 1136, loss = 1.21110283
Iteration 1137, loss = 1.20995516
Iteration 1138, loss = 1.20879902
Iteration 1139, loss = 1.20764598
Iteration 1140, loss = 1.20650399
Iteration 1141, loss = 1.20535341
Iteration 1142, loss = 1.20421039
Iteration 1143, loss = 1.20306700
Iteration 1144, loss = 1.20191791
Iteration 1145, loss = 1.20077559
Iteration 1146, loss = 1.19963830
Iteration 1147, loss = 1.19849121
Iteration 1148, loss = 1.19735233
Iteration 1149, loss = 1.19621159
Iteration 1150, loss = 1.19507174
Iteration 1151, loss = 1.19394059
Iteration 1152, loss = 1.19281083
Iteration 1153, loss = 1.19166042
Iteration 1154, loss = 1.19053201
Iteration 1155, loss = 1.18939229
Iteration 1156, loss = 1.18826931
Iteration 1157, loss = 1.18713339
Iteration 1158, loss = 1.18599025
Iteration 1159, loss = 1.18487978
Iteration 1160, loss = 1.18374611
Iteration 1161, loss = 1.18261095
Iteration 1162, loss = 1.18149905
Iteration 1163, loss = 1.18035903
Iteration 1164, loss = 1.17923819
Iteration 1165, loss = 1.17811576
Iteration 1166, loss = 1.17700237
Iteration 1167, loss = 1.17586682
Iteration 1168, loss = 1.17474548
Iteration 1169, loss = 1.17363529
Iteration 1170, loss = 1.17252519
Iteration 1171, loss = 1.17140633
Iteration 1172, loss = 1.17029179
Iteration 1173, loss = 1.16917626
Iteration 1174, loss = 1.16806643
Iteration 1175, loss = 1.16695226
Iteration 1176, loss = 1.16584065
Iteration 1177, loss = 1.16473477
Iteration 1178, loss = 1.16362603
Iteration 1179, loss = 1.16251392
Iteration 1180, loss = 1.16141794
Iteration 1181, loss = 1.16031163
Iteration 1182, loss = 1.15921220
Iteration 1183, loss = 1.15810764
Iteration 1184, loss = 1.15700238
Iteration 1185, loss = 1.15591031
Iteration 1186, loss = 1.15481685
Iteration 1187, loss = 1.15372571
Iteration 1188, loss = 1.15263062
Iteration 1189, loss = 1.15153414
Iteration 1190, loss = 1.15044687
Iteration 1191, loss = 1.14934846
Iteration 1192, loss = 1.14826387
Iteration 1193, loss = 1.14716494
Iteration 1194, loss = 1.14608189
Iteration 1195, loss = 1.14500897
Iteration 1196, loss = 1.14391177
Iteration 1197, loss = 1.14282528
Iteration 1198, loss = 1.14174732
Iteration 1199, loss = 1.14066473
Iteration 1200, loss = 1.13958752
Iteration 1201, loss = 1.13849462
Iteration 1202, loss = 1.13743496
Iteration 1203, loss = 1.13634917
Iteration 1204, loss = 1.13527407
Iteration 1205, loss = 1.13419128
Iteration 1206, loss = 1.13312253
Iteration 1207, loss = 1.13204566
Iteration 1208, loss = 1.13097254
Iteration 1209, loss = 1.12990357
Iteration 1210, loss = 1.12883118
Iteration 1211, loss = 1.12778020
Iteration 1212, loss = 1.12670006
Iteration 1213, loss = 1.12564079
Iteration 1214, loss = 1.12456855
Iteration 1215, loss = 1.12351371
Iteration 1216, loss = 1.12244330
Iteration 1217, loss = 1.12138102
Iteration 1218, loss = 1.12031270
Iteration 1219, loss = 1.11926289
Iteration 1220, loss = 1.11819215
Iteration 1221, loss = 1.11714369
Iteration 1222, loss = 1.11608127
Iteration 1223, loss = 1.11502997
Iteration 1224, loss = 1.11397399
Iteration 1225, loss = 1.11292053
Iteration 1226, loss = 1.11186390
Iteration 1227, loss = 1.11081951
Iteration 1228, loss = 1.10976780
Iteration 1229, loss = 1.10870849
Iteration 1230, loss = 1.10765872
Iteration 1231, loss = 1.10662505
Iteration 1232, loss = 1.10556967
Iteration 1233, loss = 1.10453389
Iteration 1234, loss = 1.10348662
Iteration 1235, loss = 1.10244451
Iteration 1236, loss = 1.10140322
Iteration 1237, loss = 1.10035790
Iteration 1238, loss = 1.09932238
Iteration 1239, loss = 1.09829316
Iteration 1240, loss = 1.09725503
Iteration 1241, loss = 1.09622269
Iteration 1242, loss = 1.09517410
Iteration 1243, loss = 1.09414371
Iteration 1244, loss = 1.09311533
Iteration 1245, loss = 1.09209032
Iteration 1246, loss = 1.09104606
Iteration 1247, loss = 1.09002646
Iteration 1248, loss = 1.08900598
Iteration 1249, loss = 1.08797599
Iteration 1250, loss = 1.08695723
Iteration 1251, loss = 1.08592420
Iteration 1252, loss = 1.08490388
Iteration 1253, loss = 1.08389025
Iteration 1254, loss = 1.08286241
Iteration 1255, loss = 1.08184653
Iteration 1256, loss = 1.08082467
Iteration 1257, loss = 1.07980596
Iteration 1258, loss = 1.07881761
Iteration 1259, loss = 1.07777057
Iteration 1260, loss = 1.07677761
Iteration 1261, loss = 1.07575812
Iteration 1262, loss = 1.07473429
Iteration 1263, loss = 1.07371840
Iteration 1264, loss = 1.07272254
Iteration 1265, loss = 1.07171076
Iteration 1266, loss = 1.07069734
Iteration 1267, loss = 1.06969689
Iteration 1268, loss = 1.06869206
Iteration 1269, loss = 1.06768653
Iteration 1270, loss = 1.06668231
Iteration 1271, loss = 1.06567693
Iteration 1272, loss = 1.06467197
Iteration 1273, loss = 1.06368160
Iteration 1274, loss = 1.06266757
Iteration 1275, loss = 1.06167469
Iteration 1276, loss = 1.06067574
Iteration 1277, loss = 1.05967892
Iteration 1278, loss = 1.05869168
Iteration 1279, loss = 1.05769068
Iteration 1280, loss = 1.05669837
Iteration 1281, loss = 1.05570832
Iteration 1282, loss = 1.05470864
Iteration 1283, loss = 1.05372744
Iteration 1284, loss = 1.05273589
Iteration 1285, loss = 1.05176794
Iteration 1286, loss = 1.05077126
Iteration 1287, loss = 1.04978168
Iteration 1288, loss = 1.04879649
Iteration 1289, loss = 1.04781603
Iteration 1290, loss = 1.04684096
Iteration 1291, loss = 1.04586283
Iteration 1292, loss = 1.04489187
Iteration 1293, loss = 1.04390946
Iteration 1294, loss = 1.04293071
Iteration 1295, loss = 1.04195657
Iteration 1296, loss = 1.04097768
Iteration 1297, loss = 1.04001393
Iteration 1298, loss = 1.03903215
Iteration 1299, loss = 1.03806001
Iteration 1300, loss = 1.03709076
Iteration 1301, loss = 1.03612537
Iteration 1302, loss = 1.03515210
Iteration 1303, loss = 1.03419067
Iteration 1304, loss = 1.03323797
Iteration 1305, loss = 1.03226053
Iteration 1306, loss = 1.03130554
Iteration 1307, loss = 1.03033859
Iteration 1308, loss = 1.02937900
Iteration 1309, loss = 1.02840680
Iteration 1310, loss = 1.02745926
Iteration 1311, loss = 1.02650399
Iteration 1312, loss = 1.02555041
Iteration 1313, loss = 1.02458693
Iteration 1314, loss = 1.02363086
Iteration 1315, loss = 1.02267651
Iteration 1316, loss = 1.02172303
Iteration 1317, loss = 1.02077433
Iteration 1318, loss = 1.01982001
Iteration 1319, loss = 1.01887704
Iteration 1320, loss = 1.01792956
Iteration 1321, loss = 1.01697503
Iteration 1322, loss = 1.01603336
Iteration 1323, loss = 1.01509222
Iteration 1324, loss = 1.01415576
Iteration 1325, loss = 1.01320836
Iteration 1326, loss = 1.01226613
Iteration 1327, loss = 1.01131483
Iteration 1328, loss = 1.01037846
Iteration 1329, loss = 1.00943301
Iteration 1330, loss = 1.00849690
Iteration 1331, loss = 1.00755447
Iteration 1332, loss = 1.00662072
Iteration 1333, loss = 1.00569354
Iteration 1334, loss = 1.00475748
Iteration 1335, loss = 1.00381690
Iteration 1336, loss = 1.00287896
Iteration 1337, loss = 1.00194797
Iteration 1338, loss = 1.00102419
Iteration 1339, loss = 1.00007881
Iteration 1340, loss = 0.99916228
Iteration 1341, loss = 0.99823352
Iteration 1342, loss = 0.99731768
Iteration 1343, loss = 0.99638619
Iteration 1344, loss = 0.99546412
Iteration 1345, loss = 0.99454183
Iteration 1346, loss = 0.99362343
Iteration 1347, loss = 0.99269961
Iteration 1348, loss = 0.99177738
Iteration 1349, loss = 0.99086059
Iteration 1350, loss = 0.98994633
Iteration 1351, loss = 0.98903220
Iteration 1352, loss = 0.98811080
Iteration 1353, loss = 0.98720042
Iteration 1354, loss = 0.98628277
Iteration 1355, loss = 0.98537326
Iteration 1356, loss = 0.98446232
Iteration 1357, loss = 0.98354919
Iteration 1358, loss = 0.98264336
Iteration 1359, loss = 0.98173739
Iteration 1360, loss = 0.98083102
Iteration 1361, loss = 0.97992127
Iteration 1362, loss = 0.97903187
Iteration 1363, loss = 0.97811273
Iteration 1364, loss = 0.97721731
Iteration 1365, loss = 0.97631450
Iteration 1366, loss = 0.97541788
Iteration 1367, loss = 0.97451380
Iteration 1368, loss = 0.97362882
Iteration 1369, loss = 0.97271626
Iteration 1370, loss = 0.97182600
Iteration 1371, loss = 0.97092670
Iteration 1372, loss = 0.97004067
Iteration 1373, loss = 0.96913623
Iteration 1374, loss = 0.96824528
Iteration 1375, loss = 0.96735258
Iteration 1376, loss = 0.96645851
Iteration 1377, loss = 0.96557085
Iteration 1378, loss = 0.96467517
Iteration 1379, loss = 0.96378630
Iteration 1380, loss = 0.96289759
Iteration 1381, loss = 0.96201542
Iteration 1382, loss = 0.96111848
Iteration 1383, loss = 0.96023879
Iteration 1384, loss = 0.95935527
Iteration 1385, loss = 0.95847711
Iteration 1386, loss = 0.95759344
Iteration 1387, loss = 0.95670701
Iteration 1388, loss = 0.95583988
Iteration 1389, loss = 0.95496748
Iteration 1390, loss = 0.95408004
Iteration 1391, loss = 0.95321116
Iteration 1392, loss = 0.95234067
Iteration 1393, loss = 0.95147113
Iteration 1394, loss = 0.95058160
Iteration 1395, loss = 0.94972189
Iteration 1396, loss = 0.94885161
Iteration 1397, loss = 0.94798250
Iteration 1398, loss = 0.94711191
Iteration 1399, loss = 0.94624720
Iteration 1400, loss = 0.94537757
Iteration 1401, loss = 0.94451466
Iteration 1402, loss = 0.94364441
Iteration 1403, loss = 0.94278822
Iteration 1404, loss = 0.94191259
Iteration 1405, loss = 0.94105221
Iteration 1406, loss = 0.94019402
Iteration 1407, loss = 0.93933958
Iteration 1408, loss = 0.93848294
Iteration 1409, loss = 0.93762606
Iteration 1410, loss = 0.93676887
Iteration 1411, loss = 0.93591808
Iteration 1412, loss = 0.93506601
Iteration 1413, loss = 0.93421547
Iteration 1414, loss = 0.93336636
Iteration 1415, loss = 0.93251245
Iteration 1416, loss = 0.93165932
Iteration 1417, loss = 0.93080873
Iteration 1418, loss = 0.92996572
Iteration 1419, loss = 0.92912503
Iteration 1420, loss = 0.92826842
Iteration 1421, loss = 0.92742496
Iteration 1422, loss = 0.92658065
Iteration 1423, loss = 0.92572729
Iteration 1424, loss = 0.92489975
Iteration 1425, loss = 0.92404147
Iteration 1426, loss = 0.92321265
Iteration 1427, loss = 0.92236552
Iteration 1428, loss = 0.92152096
Iteration 1429, loss = 0.92068291
Iteration 1430, loss = 0.91984239
Iteration 1431, loss = 0.91901076
Iteration 1432, loss = 0.91817874
Iteration 1433, loss = 0.91734249
Iteration 1434, loss = 0.91651143
Iteration 1435, loss = 0.91567765
Iteration 1436, loss = 0.91483001
Iteration 1437, loss = 0.91400542
Iteration 1438, loss = 0.91318380
Iteration 1439, loss = 0.91235156
Iteration 1440, loss = 0.91152686
Iteration 1441, loss = 0.91069378
Iteration 1442, loss = 0.90986896
Iteration 1443, loss = 0.90904468
Iteration 1444, loss = 0.90822135
Iteration 1445, loss = 0.90740658
Iteration 1446, loss = 0.90657692
Iteration 1447, loss = 0.90576185
Iteration 1448, loss = 0.90494315
Iteration 1449, loss = 0.90411327
Iteration 1450, loss = 0.90330233
Iteration 1451, loss = 0.90247762
Iteration 1452, loss = 0.90166073
Iteration 1453, loss = 0.90085227
Iteration 1454, loss = 0.90003599
Iteration 1455, loss = 0.89921666
Iteration 1456, loss = 0.89840871
Iteration 1457, loss = 0.89759324
Iteration 1458, loss = 0.89678548
Iteration 1459, loss = 0.89597380
Iteration 1460, loss = 0.89515832
Iteration 1461, loss = 0.89434456
Iteration 1462, loss = 0.89354435
Iteration 1463, loss = 0.89273752
Iteration 1464, loss = 0.89193804
Iteration 1465, loss = 0.89112145
Iteration 1466, loss = 0.89031921
Iteration 1467, loss = 0.88951408
Iteration 1468, loss = 0.88871695
Iteration 1469, loss = 0.88791186
Iteration 1470, loss = 0.88711747
Iteration 1471, loss = 0.88631830
Iteration 1472, loss = 0.88551915
Iteration 1473, loss = 0.88472523
Iteration 1474, loss = 0.88393191
Iteration 1475, loss = 0.88314688
Iteration 1476, loss = 0.88233996
Iteration 1477, loss = 0.88154625
Iteration 1478, loss = 0.88075357
Iteration 1479, loss = 0.87996514
Iteration 1480, loss = 0.87917152
Iteration 1481, loss = 0.87838294
Iteration 1482, loss = 0.87759898
Iteration 1483, loss = 0.87681051
Iteration 1484, loss = 0.87601760
Iteration 1485, loss = 0.87523949
Iteration 1486, loss = 0.87445581
Iteration 1487, loss = 0.87366222
Iteration 1488, loss = 0.87288609
Iteration 1489, loss = 0.87210297
Iteration 1490, loss = 0.87131935
Iteration 1491, loss = 0.87054091
Iteration 1492, loss = 0.86976688
Iteration 1493, loss = 0.86898473
Iteration 1494, loss = 0.86820155
Iteration 1495, loss = 0.86743332
Iteration 1496, loss = 0.86665060
Iteration 1497, loss = 0.86588728
Iteration 1498, loss = 0.86510838
Iteration 1499, loss = 0.86433451
Iteration 1500, loss = 0.86355912
Iteration 1501, loss = 0.86278506
Iteration 1502, loss = 0.86202323
Iteration 1503, loss = 0.86125318
Iteration 1504, loss = 0.86048277
Iteration 1505, loss = 0.85971031
Iteration 1506, loss = 0.85894612
Iteration 1507, loss = 0.85818272
Iteration 1508, loss = 0.85741823
Iteration 1509, loss = 0.85665307
Iteration 1510, loss = 0.85589337
Iteration 1511, loss = 0.85512722
Iteration 1512, loss = 0.85436646
Iteration 1513, loss = 0.85360765
Iteration 1514, loss = 0.85284609
Iteration 1515, loss = 0.85207671
Iteration 1516, loss = 0.85133045
Iteration 1517, loss = 0.85056741
Iteration 1518, loss = 0.84981706
Iteration 1519, loss = 0.84905865
Iteration 1520, loss = 0.84829642
Iteration 1521, loss = 0.84754598
Iteration 1522, loss = 0.84679219
Iteration 1523, loss = 0.84604533
Iteration 1524, loss = 0.84529040
Iteration 1525, loss = 0.84454534
Iteration 1526, loss = 0.84379017
Iteration 1527, loss = 0.84304763
Iteration 1528, loss = 0.84229788
Iteration 1529, loss = 0.84156156
Iteration 1530, loss = 0.84081101
Iteration 1531, loss = 0.84006030
Iteration 1532, loss = 0.83931523
Iteration 1533, loss = 0.83858280
Iteration 1534, loss = 0.83783461
Iteration 1535, loss = 0.83709392
Iteration 1536, loss = 0.83635154
Iteration 1537, loss = 0.83560532
Iteration 1538, loss = 0.83486670
Iteration 1539, loss = 0.83413159
Iteration 1540, loss = 0.83339653
Iteration 1541, loss = 0.83266267
Iteration 1542, loss = 0.83192801
Iteration 1543, loss = 0.83119350
Iteration 1544, loss = 0.83046288
Iteration 1545, loss = 0.82971948
Iteration 1546, loss = 0.82898696
Iteration 1547, loss = 0.82825333
Iteration 1548, loss = 0.82752564
Iteration 1549, loss = 0.82679122
Iteration 1550, loss = 0.82606659
Iteration 1551, loss = 0.82534126
Iteration 1552, loss = 0.82461050
Iteration 1553, loss = 0.82388668
Iteration 1554, loss = 0.82315747
Iteration 1555, loss = 0.82242732
Iteration 1556, loss = 0.82170779
Iteration 1557, loss = 0.82098609
Iteration 1558, loss = 0.82026186
Iteration 1559, loss = 0.81954101
Iteration 1560, loss = 0.81881732
Iteration 1561, loss = 0.81809876
Iteration 1562, loss = 0.81738008
Iteration 1563, loss = 0.81666145
Iteration 1564, loss = 0.81594488
Iteration 1565, loss = 0.81522832
Iteration 1566, loss = 0.81451163
Iteration 1567, loss = 0.81380028
Iteration 1568, loss = 0.81308207
Iteration 1569, loss = 0.81237450
Iteration 1570, loss = 0.81165547
Iteration 1571, loss = 0.81094913
Iteration 1572, loss = 0.81023682
Iteration 1573, loss = 0.80953255
Iteration 1574, loss = 0.80882547
Iteration 1575, loss = 0.80811741
Iteration 1576, loss = 0.80741099
Iteration 1577, loss = 0.80671243
Iteration 1578, loss = 0.80600214
Iteration 1579, loss = 0.80529344
Iteration 1580, loss = 0.80459378
Iteration 1581, loss = 0.80389046
Iteration 1582, loss = 0.80318274
Iteration 1583, loss = 0.80248625
Iteration 1584, loss = 0.80178260
Iteration 1585, loss = 0.80109033
Iteration 1586, loss = 0.80038657
Iteration 1587, loss = 0.79968214
Iteration 1588, loss = 0.79898168
Iteration 1589, loss = 0.79828237
Iteration 1590, loss = 0.79758974
Iteration 1591, loss = 0.79689394
Iteration 1592, loss = 0.79620063
Iteration 1593, loss = 0.79551117
Iteration 1594, loss = 0.79481415
Iteration 1595, loss = 0.79412285
Iteration 1596, loss = 0.79342838
Iteration 1597, loss = 0.79273903
Iteration 1598, loss = 0.79205948
Iteration 1599, loss = 0.79136849
Iteration 1600, loss = 0.79067944
Iteration 1601, loss = 0.78999186
Iteration 1602, loss = 0.78931578
Iteration 1603, loss = 0.78862386
Iteration 1604, loss = 0.78793923
Iteration 1605, loss = 0.78725887
Iteration 1606, loss = 0.78658502
Iteration 1607, loss = 0.78589527
Iteration 1608, loss = 0.78521496
Iteration 1609, loss = 0.78453677
Iteration 1610, loss = 0.78385904
Iteration 1611, loss = 0.78318617
Iteration 1612, loss = 0.78249421
Iteration 1613, loss = 0.78182729
Iteration 1614, loss = 0.78115077
Iteration 1615, loss = 0.78046597
Iteration 1616, loss = 0.77978991
Iteration 1617, loss = 0.77911966
Iteration 1618, loss = 0.77845896
Iteration 1619, loss = 0.77776858
Iteration 1620, loss = 0.77710309
Iteration 1621, loss = 0.77642295
Iteration 1622, loss = 0.77575943
Iteration 1623, loss = 0.77508420
Iteration 1624, loss = 0.77442064
Iteration 1625, loss = 0.77374979
Iteration 1626, loss = 0.77308998
Iteration 1627, loss = 0.77242061
Iteration 1628, loss = 0.77175771
Iteration 1629, loss = 0.77109294
Iteration 1630, loss = 0.77042949
Iteration 1631, loss = 0.76976171
Iteration 1632, loss = 0.76910443
Iteration 1633, loss = 0.76844361
Iteration 1634, loss = 0.76777705
Iteration 1635, loss = 0.76711849
Iteration 1636, loss = 0.76645716
Iteration 1637, loss = 0.76580927
Iteration 1638, loss = 0.76514008
Iteration 1639, loss = 0.76448720
Iteration 1640, loss = 0.76383151
Iteration 1641, loss = 0.76317142
Iteration 1642, loss = 0.76251373
Iteration 1643, loss = 0.76185093
Iteration 1644, loss = 0.76119273
Iteration 1645, loss = 0.76054771
Iteration 1646, loss = 0.75989784
Iteration 1647, loss = 0.75924102
Iteration 1648, loss = 0.75858704
Iteration 1649, loss = 0.75794810
Iteration 1650, loss = 0.75728881
Iteration 1651, loss = 0.75664166
Iteration 1652, loss = 0.75599040
Iteration 1653, loss = 0.75535028
Iteration 1654, loss = 0.75470147
Iteration 1655, loss = 0.75405468
Iteration 1656, loss = 0.75341782
Iteration 1657, loss = 0.75277525
Iteration 1658, loss = 0.75213782
Iteration 1659, loss = 0.75148775
Iteration 1660, loss = 0.75085027
Iteration 1661, loss = 0.75021401
Iteration 1662, loss = 0.74957643
Iteration 1663, loss = 0.74893438
Iteration 1664, loss = 0.74829971
Iteration 1665, loss = 0.74766349
Iteration 1666, loss = 0.74702415
Iteration 1667, loss = 0.74638812
Iteration 1668, loss = 0.74576122
Iteration 1669, loss = 0.74511491
Iteration 1670, loss = 0.74448300
Iteration 1671, loss = 0.74384623
Iteration 1672, loss = 0.74321784
Iteration 1673, loss = 0.74258603
Iteration 1674, loss = 0.74196060
Iteration 1675, loss = 0.74132638
Iteration 1676, loss = 0.74069446
Iteration 1677, loss = 0.74006291
Iteration 1678, loss = 0.73944477
Iteration 1679, loss = 0.73881675
Iteration 1680, loss = 0.73818163
Iteration 1681, loss = 0.73756046
Iteration 1682, loss = 0.73693363
Iteration 1683, loss = 0.73631186
Iteration 1684, loss = 0.73568309
Iteration 1685, loss = 0.73506318
Iteration 1686, loss = 0.73444515
Iteration 1687, loss = 0.73382374
Iteration 1688, loss = 0.73320143
Iteration 1689, loss = 0.73258826
Iteration 1690, loss = 0.73197179
Iteration 1691, loss = 0.73134991
Iteration 1692, loss = 0.73072628
Iteration 1693, loss = 0.73010847
Iteration 1694, loss = 0.72949759
Iteration 1695, loss = 0.72887586
Iteration 1696, loss = 0.72826023
Iteration 1697, loss = 0.72765246
Iteration 1698, loss = 0.72702935
Iteration 1699, loss = 0.72641809
Iteration 1700, loss = 0.72580315
Iteration 1701, loss = 0.72520062
Iteration 1702, loss = 0.72458205
Iteration 1703, loss = 0.72396933
Iteration 1704, loss = 0.72337327
Iteration 1705, loss = 0.72275757
Iteration 1706, loss = 0.72214280
Iteration 1707, loss = 0.72153887
Iteration 1708, loss = 0.72093860
Iteration 1709, loss = 0.72033094
Iteration 1710, loss = 0.71972996
Iteration 1711, loss = 0.71912252
Iteration 1712, loss = 0.71851624
Iteration 1713, loss = 0.71791395
Iteration 1714, loss = 0.71731243
Iteration 1715, loss = 0.71671947
Iteration 1716, loss = 0.71611553
Iteration 1717, loss = 0.71551662
Iteration 1718, loss = 0.71492248
Iteration 1719, loss = 0.71431523
Iteration 1720, loss = 0.71372336
Iteration 1721, loss = 0.71312516
Iteration 1722, loss = 0.71252496
Iteration 1723, loss = 0.71193764
Iteration 1724, loss = 0.71133868
Iteration 1725, loss = 0.71074471
Iteration 1726, loss = 0.71015107
Iteration 1727, loss = 0.70955762
Iteration 1728, loss = 0.70896360
Iteration 1729, loss = 0.70838130
Iteration 1730, loss = 0.70778395
Iteration 1731, loss = 0.70718673
Iteration 1732, loss = 0.70660098
Iteration 1733, loss = 0.70601048
Iteration 1734, loss = 0.70542298
Iteration 1735, loss = 0.70483524
Iteration 1736, loss = 0.70425425
Iteration 1737, loss = 0.70366752
Iteration 1738, loss = 0.70307036
Iteration 1739, loss = 0.70249519
Iteration 1740, loss = 0.70191605
Iteration 1741, loss = 0.70133547
Iteration 1742, loss = 0.70073983
Iteration 1743, loss = 0.70015936
Iteration 1744, loss = 0.69958361
Iteration 1745, loss = 0.69899452
Iteration 1746, loss = 0.69842336
Iteration 1747, loss = 0.69784548
Iteration 1748, loss = 0.69726239
Iteration 1749, loss = 0.69667716
Iteration 1750, loss = 0.69609884
Iteration 1751, loss = 0.69553031
Iteration 1752, loss = 0.69495151
Iteration 1753, loss = 0.69437349
Iteration 1754, loss = 0.69379744
Iteration 1755, loss = 0.69322258
Iteration 1756, loss = 0.69264953
Iteration 1757, loss = 0.69207646
Iteration 1758, loss = 0.69150430
Iteration 1759, loss = 0.69093234
Iteration 1760, loss = 0.69036467
Iteration 1761, loss = 0.68979108
Iteration 1762, loss = 0.68921982
Iteration 1763, loss = 0.68864827
Iteration 1764, loss = 0.68808337
Iteration 1765, loss = 0.68751374
Iteration 1766, loss = 0.68695600
Iteration 1767, loss = 0.68637814
Iteration 1768, loss = 0.68582322
Iteration 1769, loss = 0.68525446
Iteration 1770, loss = 0.68469401
Iteration 1771, loss = 0.68413351
Iteration 1772, loss = 0.68356987
Iteration 1773, loss = 0.68300576
Iteration 1774, loss = 0.68244627
Iteration 1775, loss = 0.68188135
Iteration 1776, loss = 0.68132764
Iteration 1777, loss = 0.68076206
Iteration 1778, loss = 0.68020543
Iteration 1779, loss = 0.67964469
Iteration 1780, loss = 0.67908984
Iteration 1781, loss = 0.67852936
Iteration 1782, loss = 0.67796711
Iteration 1783, loss = 0.67741613
Iteration 1784, loss = 0.67685757
Iteration 1785, loss = 0.67630877
Iteration 1786, loss = 0.67575277
Iteration 1787, loss = 0.67519503
Iteration 1788, loss = 0.67464780
Iteration 1789, loss = 0.67409165
Iteration 1790, loss = 0.67354235
Iteration 1791, loss = 0.67299273
Iteration 1792, loss = 0.67244445
Iteration 1793, loss = 0.67189825
Iteration 1794, loss = 0.67135215
Iteration 1795, loss = 0.67080383
Iteration 1796, loss = 0.67025878
Iteration 1797, loss = 0.66971692
Iteration 1798, loss = 0.66916585
Iteration 1799, loss = 0.66862131
Iteration 1800, loss = 0.66807793
Iteration 1801, loss = 0.66753746
Iteration 1802, loss = 0.66699070
Iteration 1803, loss = 0.66645146
Iteration 1804, loss = 0.66590553
Iteration 1805, loss = 0.66536541
Iteration 1806, loss = 0.66482172
Iteration 1807, loss = 0.66428296
Iteration 1808, loss = 0.66374107
Iteration 1809, loss = 0.66320599
Iteration 1810, loss = 0.66266347
Iteration 1811, loss = 0.66212288
Iteration 1812, loss = 0.66158376
Iteration 1813, loss = 0.66104801
Iteration 1814, loss = 0.66051102
Iteration 1815, loss = 0.65997320
Iteration 1816, loss = 0.65943682
Iteration 1817, loss = 0.65890427
Iteration 1818, loss = 0.65837012
Iteration 1819, loss = 0.65783266
Iteration 1820, loss = 0.65730757
Iteration 1821, loss = 0.65677262
Iteration 1822, loss = 0.65623934
Iteration 1823, loss = 0.65570666
Iteration 1824, loss = 0.65517889
Iteration 1825, loss = 0.65464639
Iteration 1826, loss = 0.65411960
Iteration 1827, loss = 0.65358889
Iteration 1828, loss = 0.65306164
Iteration 1829, loss = 0.65253236
Iteration 1830, loss = 0.65200759
Iteration 1831, loss = 0.65147779
Iteration 1832, loss = 0.65095569
Iteration 1833, loss = 0.65043143
Iteration 1834, loss = 0.64989971
Iteration 1835, loss = 0.64937577
Iteration 1836, loss = 0.64885350
Iteration 1837, loss = 0.64833014
Iteration 1838, loss = 0.64780646
Iteration 1839, loss = 0.64728724
Iteration 1840, loss = 0.64676246
Iteration 1841, loss = 0.64625099
Iteration 1842, loss = 0.64572508
Iteration 1843, loss = 0.64520141
Iteration 1844, loss = 0.64468938
Iteration 1845, loss = 0.64416893
Iteration 1846, loss = 0.64364665
Iteration 1847, loss = 0.64313580
Iteration 1848, loss = 0.64261850
Iteration 1849, loss = 0.64209767
Iteration 1850, loss = 0.64159487
Iteration 1851, loss = 0.64107002
Iteration 1852, loss = 0.64056014
Iteration 1853, loss = 0.64004407
Iteration 1854, loss = 0.63953043
Iteration 1855, loss = 0.63902920
Iteration 1856, loss = 0.63851075
Iteration 1857, loss = 0.63800306
Iteration 1858, loss = 0.63749139
Iteration 1859, loss = 0.63698380
Iteration 1860, loss = 0.63647852
Iteration 1861, loss = 0.63596943
Iteration 1862, loss = 0.63546372
Iteration 1863, loss = 0.63495102
Iteration 1864, loss = 0.63445181
Iteration 1865, loss = 0.63394237
Iteration 1866, loss = 0.63343865
Iteration 1867, loss = 0.63293740
Iteration 1868, loss = 0.63243058
Iteration 1869, loss = 0.63192244
Iteration 1870, loss = 0.63142095
Iteration 1871, loss = 0.63092206
Iteration 1872, loss = 0.63041776
Iteration 1873, loss = 0.62991970
Iteration 1874, loss = 0.62942314
Iteration 1875, loss = 0.62892246
Iteration 1876, loss = 0.62842855
Iteration 1877, loss = 0.62791889
Iteration 1878, loss = 0.62741973
Iteration 1879, loss = 0.62692660
Iteration 1880, loss = 0.62642993
Iteration 1881, loss = 0.62593351
Iteration 1882, loss = 0.62544044
Iteration 1883, loss = 0.62493512
Iteration 1884, loss = 0.62444545
Iteration 1885, loss = 0.62394882
Iteration 1886, loss = 0.62345363
Iteration 1887, loss = 0.62296241
Iteration 1888, loss = 0.62246799
Iteration 1889, loss = 0.62197757
Iteration 1890, loss = 0.62148116
Iteration 1891, loss = 0.62099295
Iteration 1892, loss = 0.62050706
Iteration 1893, loss = 0.62001223
Iteration 1894, loss = 0.61952878
Iteration 1895, loss = 0.61903682
Iteration 1896, loss = 0.61855022
Iteration 1897, loss = 0.61805752
Iteration 1898, loss = 0.61757159
Iteration 1899, loss = 0.61708682
Iteration 1900, loss = 0.61659592
Iteration 1901, loss = 0.61611220
Iteration 1902, loss = 0.61563049
Iteration 1903, loss = 0.61514131
Iteration 1904, loss = 0.61465736
Iteration 1905, loss = 0.61417433
Iteration 1906, loss = 0.61368972
Iteration 1907, loss = 0.61320956
Iteration 1908, loss = 0.61272171
Iteration 1909, loss = 0.61224382
Iteration 1910, loss = 0.61176590
Iteration 1911, loss = 0.61128111
Iteration 1912, loss = 0.61080309
Iteration 1913, loss = 0.61032687
Iteration 1914, loss = 0.60984249
Iteration 1915, loss = 0.60936602
Iteration 1916, loss = 0.60889329
Iteration 1917, loss = 0.60842018
Iteration 1918, loss = 0.60793959
Iteration 1919, loss = 0.60746066
Iteration 1920, loss = 0.60698510
Iteration 1921, loss = 0.60651496
Iteration 1922, loss = 0.60604126
Iteration 1923, loss = 0.60556293
Iteration 1924, loss = 0.60509144
Iteration 1925, loss = 0.60461948
Iteration 1926, loss = 0.60414565
Iteration 1927, loss = 0.60367368
Iteration 1928, loss = 0.60320072
Iteration 1929, loss = 0.60273639
Iteration 1930, loss = 0.60225991
Iteration 1931, loss = 0.60179130
Iteration 1932, loss = 0.60132073
Iteration 1933, loss = 0.60085221
Iteration 1934, loss = 0.60038239
Iteration 1935, loss = 0.59991366
Iteration 1936, loss = 0.59945365
Iteration 1937, loss = 0.59898013
Iteration 1938, loss = 0.59851125
Iteration 1939, loss = 0.59805681
Iteration 1940, loss = 0.59758920
Iteration 1941, loss = 0.59712526
Iteration 1942, loss = 0.59665748
Iteration 1943, loss = 0.59619538
Iteration 1944, loss = 0.59573289
Iteration 1945, loss = 0.59527155
Iteration 1946, loss = 0.59481181
Iteration 1947, loss = 0.59434789
Iteration 1948, loss = 0.59389335
Iteration 1949, loss = 0.59342685
Iteration 1950, loss = 0.59296967
Iteration 1951, loss = 0.59251284
Iteration 1952, loss = 0.59205069
Iteration 1953, loss = 0.59159340
Iteration 1954, loss = 0.59113725
Iteration 1955, loss = 0.59068300
Iteration 1956, loss = 0.59022443
Iteration 1957, loss = 0.58976610
Iteration 1958, loss = 0.58930782
Iteration 1959, loss = 0.58885334
Iteration 1960, loss = 0.58839667
Iteration 1961, loss = 0.58795089
Iteration 1962, loss = 0.58748813
Iteration 1963, loss = 0.58703884
Iteration 1964, loss = 0.58658418
Iteration 1965, loss = 0.58613653
Iteration 1966, loss = 0.58568290
Iteration 1967, loss = 0.58523486
Iteration 1968, loss = 0.58478495
Iteration 1969, loss = 0.58433599
Iteration 1970, loss = 0.58388159
Iteration 1971, loss = 0.58343568
Iteration 1972, loss = 0.58298496
Iteration 1973, loss = 0.58253629
Iteration 1974, loss = 0.58209805
Iteration 1975, loss = 0.58164928
Iteration 1976, loss = 0.58120004
Iteration 1977, loss = 0.58075554
Iteration 1978, loss = 0.58031137
Iteration 1979, loss = 0.57987007
Iteration 1980, loss = 0.57942091
Iteration 1981, loss = 0.57897494
Iteration 1982, loss = 0.57853547
Iteration 1983, loss = 0.57809618
Iteration 1984, loss = 0.57765267
Iteration 1985, loss = 0.57720949
Iteration 1986, loss = 0.57676580
Iteration 1987, loss = 0.57633510
Iteration 1988, loss = 0.57588937
Iteration 1989, loss = 0.57544840
Iteration 1990, loss = 0.57501025
Iteration 1991, loss = 0.57457309
Iteration 1992, loss = 0.57413987
Iteration 1993, loss = 0.57369459
Iteration 1994, loss = 0.57325995
Iteration 1995, loss = 0.57282614
Iteration 1996, loss = 0.57238602
Iteration 1997, loss = 0.57195257
Iteration 1998, loss = 0.57151322
Iteration 1999, loss = 0.57107982
Iteration 2000, loss = 0.57065117
Iteration 2001, loss = 0.57020893
Iteration 2002, loss = 0.56977971
Iteration 2003, loss = 0.56934339
Iteration 2004, loss = 0.56891028
Iteration 2005, loss = 0.56847648
Iteration 2006, loss = 0.56804248
Iteration 2007, loss = 0.56761140
Iteration 2008, loss = 0.56718473
Iteration 2009, loss = 0.56675033
Iteration 2010, loss = 0.56632293
Iteration 2011, loss = 0.56589608
Iteration 2012, loss = 0.56546516
Iteration 2013, loss = 0.56503481
Iteration 2014, loss = 0.56460539
Iteration 2015, loss = 0.56418136
Iteration 2016, loss = 0.56375511
Iteration 2017, loss = 0.56332772
Iteration 2018, loss = 0.56290209
Iteration 2019, loss = 0.56247638
Iteration 2020, loss = 0.56205053
Iteration 2021, loss = 0.56163191
Iteration 2022, loss = 0.56120814
Iteration 2023, loss = 0.56078400
Iteration 2024, loss = 0.56035614
Iteration 2025, loss = 0.55993709
Iteration 2026, loss = 0.55951943
Iteration 2027, loss = 0.55909701
Iteration 2028, loss = 0.55867579
Iteration 2029, loss = 0.55826120
Iteration 2030, loss = 0.55783763
Iteration 2031, loss = 0.55741515
Iteration 2032, loss = 0.55700082
Iteration 2033, loss = 0.55658078
Iteration 2034, loss = 0.55616627
Iteration 2035, loss = 0.55573986
Iteration 2036, loss = 0.55532620
Iteration 2037, loss = 0.55490530
Iteration 2038, loss = 0.55449441
Iteration 2039, loss = 0.55407566
Iteration 2040, loss = 0.55366840
Iteration 2041, loss = 0.55324732
Iteration 2042, loss = 0.55283397
Iteration 2043, loss = 0.55241823
Iteration 2044, loss = 0.55200216
Iteration 2045, loss = 0.55159163
Iteration 2046, loss = 0.55118026
Iteration 2047, loss = 0.55077196
Iteration 2048, loss = 0.55035123
Iteration 2049, loss = 0.54994198
Iteration 2050, loss = 0.54953106
Iteration 2051, loss = 0.54911912
Iteration 2052, loss = 0.54871946
Iteration 2053, loss = 0.54830510
Iteration 2054, loss = 0.54789863
Iteration 2055, loss = 0.54748775
Iteration 2056, loss = 0.54708147
Iteration 2057, loss = 0.54667636
Iteration 2058, loss = 0.54627379
Iteration 2059, loss = 0.54586312
Iteration 2060, loss = 0.54546284
Iteration 2061, loss = 0.54505731
Iteration 2062, loss = 0.54465444
Iteration 2063, loss = 0.54424553
Iteration 2064, loss = 0.54384421
Iteration 2065, loss = 0.54344684
Iteration 2066, loss = 0.54304005
Iteration 2067, loss = 0.54263837
Iteration 2068, loss = 0.54223531
Iteration 2069, loss = 0.54183797
Iteration 2070, loss = 0.54143666
Iteration 2071, loss = 0.54102873
Iteration 2072, loss = 0.54062986
Iteration 2073, loss = 0.54023172
Iteration 2074, loss = 0.53982948
Iteration 2075, loss = 0.53943250
Iteration 2076, loss = 0.53903131
Iteration 2077, loss = 0.53863385
Iteration 2078, loss = 0.53823342
Iteration 2079, loss = 0.53783931
Iteration 2080, loss = 0.53744372
Iteration 2081, loss = 0.53704464
Iteration 2082, loss = 0.53665141
Iteration 2083, loss = 0.53625756
Iteration 2084, loss = 0.53586343
Iteration 2085, loss = 0.53546851
Iteration 2086, loss = 0.53507382
Iteration 2087, loss = 0.53467955
Iteration 2088, loss = 0.53428826
Iteration 2089, loss = 0.53389563
Iteration 2090, loss = 0.53350308
Iteration 2091, loss = 0.53310878
Iteration 2092, loss = 0.53272181
Iteration 2093, loss = 0.53232838
Iteration 2094, loss = 0.53193698
Iteration 2095, loss = 0.53154619
Iteration 2096, loss = 0.53115685
Iteration 2097, loss = 0.53076918
Iteration 2098, loss = 0.53038088
Iteration 2099, loss = 0.52999061
Iteration 2100, loss = 0.52960521
Iteration 2101, loss = 0.52921728
Iteration 2102, loss = 0.52882784
Iteration 2103, loss = 0.52844332
Iteration 2104, loss = 0.52805648
Iteration 2105, loss = 0.52767132
Iteration 2106, loss = 0.52728455
Iteration 2107, loss = 0.52690059
Iteration 2108, loss = 0.52651520
Iteration 2109, loss = 0.52612973
Iteration 2110, loss = 0.52574856
Iteration 2111, loss = 0.52536256
Iteration 2112, loss = 0.52497925
Iteration 2113, loss = 0.52459601
Iteration 2114, loss = 0.52421464
Iteration 2115, loss = 0.52383402
Iteration 2116, loss = 0.52345396
Iteration 2117, loss = 0.52307781
Iteration 2118, loss = 0.52268784
Iteration 2119, loss = 0.52231639
Iteration 2120, loss = 0.52193298
Iteration 2121, loss = 0.52155242
Iteration 2122, loss = 0.52117443
Iteration 2123, loss = 0.52079505
Iteration 2124, loss = 0.52042223
Iteration 2125, loss = 0.52004098
Iteration 2126, loss = 0.51966485
Iteration 2127, loss = 0.51928783
Iteration 2128, loss = 0.51891069
Iteration 2129, loss = 0.51853362
Iteration 2130, loss = 0.51816069
Iteration 2131, loss = 0.51778481
Iteration 2132, loss = 0.51740863
Iteration 2133, loss = 0.51703583
Iteration 2134, loss = 0.51666216
Iteration 2135, loss = 0.51628577
Iteration 2136, loss = 0.51591494
Iteration 2137, loss = 0.51554259
Iteration 2138, loss = 0.51516498
Iteration 2139, loss = 0.51479574
Iteration 2140, loss = 0.51442306
Iteration 2141, loss = 0.51405327
Iteration 2142, loss = 0.51368025
Iteration 2143, loss = 0.51330778
Iteration 2144, loss = 0.51293652
Iteration 2145, loss = 0.51256569
Iteration 2146, loss = 0.51219860
Iteration 2147, loss = 0.51182629
Iteration 2148, loss = 0.51146131
Iteration 2149, loss = 0.51108915
Iteration 2150, loss = 0.51072155
Iteration 2151, loss = 0.51035912
Iteration 2152, loss = 0.50998898
Iteration 2153, loss = 0.50962479
Iteration 2154, loss = 0.50925575
Iteration 2155, loss = 0.50889094
Iteration 2156, loss = 0.50852989
Iteration 2157, loss = 0.50816552
Iteration 2158, loss = 0.50780121
Iteration 2159, loss = 0.50743963
Iteration 2160, loss = 0.50707790
Iteration 2161, loss = 0.50671323
Iteration 2162, loss = 0.50635391
Iteration 2163, loss = 0.50599265
Iteration 2164, loss = 0.50562915
Iteration 2165, loss = 0.50526767
Iteration 2166, loss = 0.50490627
Iteration 2167, loss = 0.50455194
Iteration 2168, loss = 0.50418654
Iteration 2169, loss = 0.50382782
Iteration 2170, loss = 0.50346989
Iteration 2171, loss = 0.50310773
Iteration 2172, loss = 0.50275082
Iteration 2173, loss = 0.50239096
Iteration 2174, loss = 0.50203679
Iteration 2175, loss = 0.50167578
Iteration 2176, loss = 0.50131613
Iteration 2177, loss = 0.50096285
Iteration 2178, loss = 0.50060698
Iteration 2179, loss = 0.50024620
Iteration 2180, loss = 0.49989270
Iteration 2181, loss = 0.49953617
Iteration 2182, loss = 0.49918647
Iteration 2183, loss = 0.49882752
Iteration 2184, loss = 0.49847406
Iteration 2185, loss = 0.49812125
Iteration 2186, loss = 0.49776860
Iteration 2187, loss = 0.49741393
Iteration 2188, loss = 0.49706363
Iteration 2189, loss = 0.49671571
Iteration 2190, loss = 0.49636418
Iteration 2191, loss = 0.49601297
Iteration 2192, loss = 0.49566140
Iteration 2193, loss = 0.49531164
Iteration 2194, loss = 0.49495872
Iteration 2195, loss = 0.49461652
Iteration 2196, loss = 0.49426243
Iteration 2197, loss = 0.49391411
Iteration 2198, loss = 0.49356624
Iteration 2199, loss = 0.49322088
Iteration 2200, loss = 0.49287270
Iteration 2201, loss = 0.49252282
Iteration 2202, loss = 0.49217888
Iteration 2203, loss = 0.49183273
Iteration 2204, loss = 0.49148716
Iteration 2205, loss = 0.49114220
Iteration 2206, loss = 0.49080024
Iteration 2207, loss = 0.49045140
Iteration 2208, loss = 0.49010709
Iteration 2209, loss = 0.48976075
Iteration 2210, loss = 0.48941922
Iteration 2211, loss = 0.48907664
Iteration 2212, loss = 0.48873292
Iteration 2213, loss = 0.48838890
Iteration 2214, loss = 0.48804511
Iteration 2215, loss = 0.48770576
Iteration 2216, loss = 0.48736341
Iteration 2217, loss = 0.48702541
Iteration 2218, loss = 0.48667588
Iteration 2219, loss = 0.48633617
Iteration 2220, loss = 0.48599185
Iteration 2221, loss = 0.48565581
Iteration 2222, loss = 0.48531261
Iteration 2223, loss = 0.48497503
Iteration 2224, loss = 0.48463336
Iteration 2225, loss = 0.48429283
Iteration 2226, loss = 0.48395546
Iteration 2227, loss = 0.48362089
Iteration 2228, loss = 0.48327924
Iteration 2229, loss = 0.48294335
Iteration 2230, loss = 0.48260863
Iteration 2231, loss = 0.48227376
Iteration 2232, loss = 0.48193743
Iteration 2233, loss = 0.48159900
Iteration 2234, loss = 0.48126502
Iteration 2235, loss = 0.48092976
Iteration 2236, loss = 0.48059441
Iteration 2237, loss = 0.48026129
Iteration 2238, loss = 0.47992832
Iteration 2239, loss = 0.47959437
Iteration 2240, loss = 0.47925775
Iteration 2241, loss = 0.47892470
Iteration 2242, loss = 0.47859220
Iteration 2243, loss = 0.47826028
Iteration 2244, loss = 0.47793066
Iteration 2245, loss = 0.47760028
Iteration 2246, loss = 0.47726999
Iteration 2247, loss = 0.47693753
Iteration 2248, loss = 0.47660734
Iteration 2249, loss = 0.47628123
Iteration 2250, loss = 0.47594779
Iteration 2251, loss = 0.47561834
Iteration 2252, loss = 0.47529043
Iteration 2253, loss = 0.47495926
Iteration 2254, loss = 0.47463316
Iteration 2255, loss = 0.47430198
Iteration 2256, loss = 0.47397775
Iteration 2257, loss = 0.47365531
Iteration 2258, loss = 0.47332334
Iteration 2259, loss = 0.47299538
Iteration 2260, loss = 0.47267335
Iteration 2261, loss = 0.47234562
Iteration 2262, loss = 0.47202126
Iteration 2263, loss = 0.47169738
Iteration 2264, loss = 0.47137266
Iteration 2265, loss = 0.47105162
Iteration 2266, loss = 0.47072364
Iteration 2267, loss = 0.47039984
Iteration 2268, loss = 0.47008182
Iteration 2269, loss = 0.46976151
Iteration 2270, loss = 0.46943767
Iteration 2271, loss = 0.46911439
Iteration 2272, loss = 0.46879031
Iteration 2273, loss = 0.46847162
Iteration 2274, loss = 0.46815429
Iteration 2275, loss = 0.46783070
Iteration 2276, loss = 0.46750962
Iteration 2277, loss = 0.46719142
Iteration 2278, loss = 0.46687470
Iteration 2279, loss = 0.46655163
Iteration 2280, loss = 0.46623498
Iteration 2281, loss = 0.46591429
Iteration 2282, loss = 0.46559594
Iteration 2283, loss = 0.46527833
Iteration 2284, loss = 0.46496399
Iteration 2285, loss = 0.46464427
Iteration 2286, loss = 0.46432704
Iteration 2287, loss = 0.46401695
Iteration 2288, loss = 0.46369603
Iteration 2289, loss = 0.46337957
Iteration 2290, loss = 0.46306613
Iteration 2291, loss = 0.46274933
Iteration 2292, loss = 0.46243587
Iteration 2293, loss = 0.46212277
Iteration 2294, loss = 0.46180867
Iteration 2295, loss = 0.46149572
Iteration 2296, loss = 0.46118002
Iteration 2297, loss = 0.46087050
Iteration 2298, loss = 0.46055952
Iteration 2299, loss = 0.46024361
Iteration 2300, loss = 0.45993629
Iteration 2301, loss = 0.45962077
Iteration 2302, loss = 0.45930752
Iteration 2303, loss = 0.45899879
Iteration 2304, loss = 0.45868569
Iteration 2305, loss = 0.45837701
Iteration 2306, loss = 0.45807206
Iteration 2307, loss = 0.45775901
Iteration 2308, loss = 0.45744999
Iteration 2309, loss = 0.45714195
Iteration 2310, loss = 0.45683115
Iteration 2311, loss = 0.45652692
Iteration 2312, loss = 0.45621728
Iteration 2313, loss = 0.45591001
Iteration 2314, loss = 0.45560038
Iteration 2315, loss = 0.45529848
Iteration 2316, loss = 0.45498836
Iteration 2317, loss = 0.45468065
Iteration 2318, loss = 0.45437975
Iteration 2319, loss = 0.45407411
Iteration 2320, loss = 0.45376817
Iteration 2321, loss = 0.45346089
Iteration 2322, loss = 0.45315493
Iteration 2323, loss = 0.45285575
Iteration 2324, loss = 0.45255315
Iteration 2325, loss = 0.45224610
Iteration 2326, loss = 0.45194342
Iteration 2327, loss = 0.45164153
Iteration 2328, loss = 0.45134138
Iteration 2329, loss = 0.45103966
Iteration 2330, loss = 0.45073740
Iteration 2331, loss = 0.45043663
Iteration 2332, loss = 0.45013775
Iteration 2333, loss = 0.44983711
Iteration 2334, loss = 0.44953776
Iteration 2335, loss = 0.44923441
Iteration 2336, loss = 0.44893563
Iteration 2337, loss = 0.44863521
Iteration 2338, loss = 0.44833865
Iteration 2339, loss = 0.44803882
Iteration 2340, loss = 0.44774277
Iteration 2341, loss = 0.44743956
Iteration 2342, loss = 0.44714227
Iteration 2343, loss = 0.44684323
Iteration 2344, loss = 0.44654733
Iteration 2345, loss = 0.44625038
Iteration 2346, loss = 0.44595440
Iteration 2347, loss = 0.44565818
Iteration 2348, loss = 0.44536520
Iteration 2349, loss = 0.44506298
Iteration 2350, loss = 0.44476806
Iteration 2351, loss = 0.44446981
Iteration 2352, loss = 0.44417663
Iteration 2353, loss = 0.44388315
Iteration 2354, loss = 0.44359144
Iteration 2355, loss = 0.44329389
Iteration 2356, loss = 0.44299944
Iteration 2357, loss = 0.44270333
Iteration 2358, loss = 0.44241231
Iteration 2359, loss = 0.44212012
Iteration 2360, loss = 0.44182856
Iteration 2361, loss = 0.44153712
Iteration 2362, loss = 0.44124489
Iteration 2363, loss = 0.44095067
Iteration 2364, loss = 0.44065970
Iteration 2365, loss = 0.44036814
Iteration 2366, loss = 0.44007748
Iteration 2367, loss = 0.43978513
Iteration 2368, loss = 0.43949513
Iteration 2369, loss = 0.43920753
Iteration 2370, loss = 0.43891779
Iteration 2371, loss = 0.43862758
Iteration 2372, loss = 0.43833895
Iteration 2373, loss = 0.43805054
Iteration 2374, loss = 0.43776368
Iteration 2375, loss = 0.43747767
Iteration 2376, loss = 0.43718426
Iteration 2377, loss = 0.43689988
Iteration 2378, loss = 0.43661161
Iteration 2379, loss = 0.43632572
Iteration 2380, loss = 0.43603685
Iteration 2381, loss = 0.43575306
Iteration 2382, loss = 0.43546805
Iteration 2383, loss = 0.43517767
Iteration 2384, loss = 0.43489350
Iteration 2385, loss = 0.43460927
Iteration 2386, loss = 0.43432258
Iteration 2387, loss = 0.43403924
Iteration 2388, loss = 0.43375324
Iteration 2389, loss = 0.43346833
Iteration 2390, loss = 0.43318525
Iteration 2391, loss = 0.43290063
Iteration 2392, loss = 0.43261777
Iteration 2393, loss = 0.43233831
Iteration 2394, loss = 0.43205704
Iteration 2395, loss = 0.43177160
Iteration 2396, loss = 0.43148969
Iteration 2397, loss = 0.43120732
Iteration 2398, loss = 0.43092520
Iteration 2399, loss = 0.43064423
Iteration 2400, loss = 0.43036559
Iteration 2401, loss = 0.43008530
Iteration 2402, loss = 0.42980608
Iteration 2403, loss = 0.42952941
Iteration 2404, loss = 0.42924713
Iteration 2405, loss = 0.42896841
Iteration 2406, loss = 0.42869081
Iteration 2407, loss = 0.42840730
Iteration 2408, loss = 0.42813182
Iteration 2409, loss = 0.42785345
Iteration 2410, loss = 0.42757830
Iteration 2411, loss = 0.42730497
Iteration 2412, loss = 0.42702368
Iteration 2413, loss = 0.42674552
Iteration 2414, loss = 0.42647235
Iteration 2415, loss = 0.42619377
Iteration 2416, loss = 0.42591752
Iteration 2417, loss = 0.42564682
Iteration 2418, loss = 0.42536948
Iteration 2419, loss = 0.42509310
Iteration 2420, loss = 0.42482139
Iteration 2421, loss = 0.42454747
Iteration 2422, loss = 0.42427109
Iteration 2423, loss = 0.42399793
Iteration 2424, loss = 0.42372413
Iteration 2425, loss = 0.42345371
Iteration 2426, loss = 0.42317834
Iteration 2427, loss = 0.42290556
Iteration 2428, loss = 0.42263133
Iteration 2429, loss = 0.42236225
Iteration 2430, loss = 0.42208724
Iteration 2431, loss = 0.42181322
Iteration 2432, loss = 0.42154228
Iteration 2433, loss = 0.42126830
Iteration 2434, loss = 0.42100201
Iteration 2435, loss = 0.42072243
Iteration 2436, loss = 0.42045569
Iteration 2437, loss = 0.42018077
Iteration 2438, loss = 0.41991391
Iteration 2439, loss = 0.41964087
Iteration 2440, loss = 0.41937694
Iteration 2441, loss = 0.41910388
Iteration 2442, loss = 0.41883271
Iteration 2443, loss = 0.41856630
Iteration 2444, loss = 0.41829826
Iteration 2445, loss = 0.41803199
Iteration 2446, loss = 0.41776522
Iteration 2447, loss = 0.41749601
Iteration 2448, loss = 0.41722778
Iteration 2449, loss = 0.41696455
Iteration 2450, loss = 0.41669999
Iteration 2451, loss = 0.41643636
Iteration 2452, loss = 0.41616791
Iteration 2453, loss = 0.41590126
Iteration 2454, loss = 0.41563403
Iteration 2455, loss = 0.41536990
Iteration 2456, loss = 0.41510070
Iteration 2457, loss = 0.41483827
Iteration 2458, loss = 0.41457052
Iteration 2459, loss = 0.41430783
Iteration 2460, loss = 0.41404040
Iteration 2461, loss = 0.41377994
Iteration 2462, loss = 0.41351386
Iteration 2463, loss = 0.41325139
Iteration 2464, loss = 0.41298601
Iteration 2465, loss = 0.41272796
Iteration 2466, loss = 0.41246440
Iteration 2467, loss = 0.41220214
Iteration 2468, loss = 0.41194224
Iteration 2469, loss = 0.41168177
Iteration 2470, loss = 0.41141978
Iteration 2471, loss = 0.41115844
Iteration 2472, loss = 0.41089751
Iteration 2473, loss = 0.41063989
Iteration 2474, loss = 0.41037805
Iteration 2475, loss = 0.41012504
Iteration 2476, loss = 0.40985910
Iteration 2477, loss = 0.40959789
Iteration 2478, loss = 0.40934016
Iteration 2479, loss = 0.40908223
Iteration 2480, loss = 0.40882370
Iteration 2481, loss = 0.40856756
Iteration 2482, loss = 0.40830842
Iteration 2483, loss = 0.40804930
Iteration 2484, loss = 0.40779474
Iteration 2485, loss = 0.40753800
Iteration 2486, loss = 0.40727874
Iteration 2487, loss = 0.40701986
Iteration 2488, loss = 0.40676498
Iteration 2489, loss = 0.40650813
Iteration 2490, loss = 0.40625325
Iteration 2491, loss = 0.40599778
Iteration 2492, loss = 0.40574015
Iteration 2493, loss = 0.40548499
Iteration 2494, loss = 0.40523486
Iteration 2495, loss = 0.40497733
Iteration 2496, loss = 0.40472148
Iteration 2497, loss = 0.40446739
Iteration 2498, loss = 0.40421458
Iteration 2499, loss = 0.40396199
Iteration 2500, loss = 0.40370791
Iteration 2501, loss = 0.40345435
Iteration 2502, loss = 0.40320417
Iteration 2503, loss = 0.40294903
Iteration 2504, loss = 0.40269626
Iteration 2505, loss = 0.40244478
Iteration 2506, loss = 0.40219157
Iteration 2507, loss = 0.40194301
Iteration 2508, loss = 0.40168861
Iteration 2509, loss = 0.40143869
Iteration 2510, loss = 0.40118737
Iteration 2511, loss = 0.40093861
Iteration 2512, loss = 0.40068663
Iteration 2513, loss = 0.40043749
Iteration 2514, loss = 0.40018742
Iteration 2515, loss = 0.39994203
Iteration 2516, loss = 0.39969291
Iteration 2517, loss = 0.39944220
Iteration 2518, loss = 0.39919423
Iteration 2519, loss = 0.39894489
Iteration 2520, loss = 0.39870064
Iteration 2521, loss = 0.39845005
Iteration 2522, loss = 0.39820144
Iteration 2523, loss = 0.39795233
Iteration 2524, loss = 0.39770960
Iteration 2525, loss = 0.39745885
Iteration 2526, loss = 0.39721841
Iteration 2527, loss = 0.39697009
Iteration 2528, loss = 0.39672264
Iteration 2529, loss = 0.39647652
Iteration 2530, loss = 0.39623423
Iteration 2531, loss = 0.39598799
Iteration 2532, loss = 0.39574375
Iteration 2533, loss = 0.39550131
Iteration 2534, loss = 0.39525304
Iteration 2535, loss = 0.39501088
Iteration 2536, loss = 0.39476913
Iteration 2537, loss = 0.39452225
Iteration 2538, loss = 0.39428474
Iteration 2539, loss = 0.39403772
Iteration 2540, loss = 0.39379471
Iteration 2541, loss = 0.39355344
Iteration 2542, loss = 0.39330875
Iteration 2543, loss = 0.39306625
Iteration 2544, loss = 0.39282222
Iteration 2545, loss = 0.39257896
Iteration 2546, loss = 0.39234274
Iteration 2547, loss = 0.39209653
Iteration 2548, loss = 0.39185685
Iteration 2549, loss = 0.39161792
Iteration 2550, loss = 0.39137592
Iteration 2551, loss = 0.39113986
Iteration 2552, loss = 0.39089762
Iteration 2553, loss = 0.39065837
Iteration 2554, loss = 0.39041995
Iteration 2555, loss = 0.39018047
Iteration 2556, loss = 0.38994127
Iteration 2557, loss = 0.38970241
Iteration 2558, loss = 0.38946618
Iteration 2559, loss = 0.38922696
Iteration 2560, loss = 0.38898946
Iteration 2561, loss = 0.38875163
Iteration 2562, loss = 0.38851835
Iteration 2563, loss = 0.38827902
Iteration 2564, loss = 0.38803907
Iteration 2565, loss = 0.38780404
Iteration 2566, loss = 0.38756308
Iteration 2567, loss = 0.38732774
Iteration 2568, loss = 0.38709669
Iteration 2569, loss = 0.38685727
Iteration 2570, loss = 0.38662051
Iteration 2571, loss = 0.38638550
Iteration 2572, loss = 0.38614698
Iteration 2573, loss = 0.38591082
Iteration 2574, loss = 0.38567767
Iteration 2575, loss = 0.38544040
Iteration 2576, loss = 0.38520507
Iteration 2577, loss = 0.38497143
Iteration 2578, loss = 0.38473848
Iteration 2579, loss = 0.38450583
Iteration 2580, loss = 0.38427005
Iteration 2581, loss = 0.38403444
Iteration 2582, loss = 0.38380032
Iteration 2583, loss = 0.38356890
Iteration 2584, loss = 0.38333468
Iteration 2585, loss = 0.38310230
Iteration 2586, loss = 0.38286979
Iteration 2587, loss = 0.38264048
Iteration 2588, loss = 0.38240401
Iteration 2589, loss = 0.38217363
Iteration 2590, loss = 0.38194207
Iteration 2591, loss = 0.38171387
Iteration 2592, loss = 0.38148172
Iteration 2593, loss = 0.38124924
Iteration 2594, loss = 0.38102163
Iteration 2595, loss = 0.38078954
Iteration 2596, loss = 0.38056247
Iteration 2597, loss = 0.38033107
Iteration 2598, loss = 0.38010012
Iteration 2599, loss = 0.37987088
Iteration 2600, loss = 0.37964142
Iteration 2601, loss = 0.37941370
Iteration 2602, loss = 0.37918362
Iteration 2603, loss = 0.37895535
Iteration 2604, loss = 0.37872938
Iteration 2605, loss = 0.37849755
Iteration 2606, loss = 0.37827429
Iteration 2607, loss = 0.37804472
Iteration 2608, loss = 0.37781872
Iteration 2609, loss = 0.37759302
Iteration 2610, loss = 0.37736569
Iteration 2611, loss = 0.37713866
Iteration 2612, loss = 0.37691433
Iteration 2613, loss = 0.37668807
Iteration 2614, loss = 0.37646042
Iteration 2615, loss = 0.37623788
Iteration 2616, loss = 0.37600794
Iteration 2617, loss = 0.37578519
Iteration 2618, loss = 0.37555750
Iteration 2619, loss = 0.37533564
Iteration 2620, loss = 0.37510799
Iteration 2621, loss = 0.37488544
Iteration 2622, loss = 0.37466129
Iteration 2623, loss = 0.37443730
Iteration 2624, loss = 0.37421565
Iteration 2625, loss = 0.37398888
Iteration 2626, loss = 0.37376841
Iteration 2627, loss = 0.37354794
Iteration 2628, loss = 0.37332159
Iteration 2629, loss = 0.37309858
Iteration 2630, loss = 0.37288008
Iteration 2631, loss = 0.37265933
Iteration 2632, loss = 0.37243561
Iteration 2633, loss = 0.37221476
Iteration 2634, loss = 0.37199463
Iteration 2635, loss = 0.37177092
Iteration 2636, loss = 0.37155271
Iteration 2637, loss = 0.37133381
Iteration 2638, loss = 0.37110866
Iteration 2639, loss = 0.37088968
Iteration 2640, loss = 0.37066811
Iteration 2641, loss = 0.37045164
Iteration 2642, loss = 0.37022822
Iteration 2643, loss = 0.37001045
Iteration 2644, loss = 0.36978802
Iteration 2645, loss = 0.36957273
Iteration 2646, loss = 0.36935111
Iteration 2647, loss = 0.36913532
Iteration 2648, loss = 0.36891661
Iteration 2649, loss = 0.36869664
Iteration 2650, loss = 0.36847822
Iteration 2651, loss = 0.36826434
Iteration 2652, loss = 0.36804370
Iteration 2653, loss = 0.36782581
Iteration 2654, loss = 0.36761176
Iteration 2655, loss = 0.36739573
Iteration 2656, loss = 0.36717906
Iteration 2657, loss = 0.36696063
Iteration 2658, loss = 0.36674454
Iteration 2659, loss = 0.36652921
Iteration 2660, loss = 0.36631182
Iteration 2661, loss = 0.36610167
Iteration 2662, loss = 0.36588472
Iteration 2663, loss = 0.36566902
Iteration 2664, loss = 0.36545236
Iteration 2665, loss = 0.36523850
Iteration 2666, loss = 0.36502543
Iteration 2667, loss = 0.36480946
Iteration 2668, loss = 0.36459625
Iteration 2669, loss = 0.36438380
Iteration 2670, loss = 0.36416842
Iteration 2671, loss = 0.36395476
Iteration 2672, loss = 0.36374267
Iteration 2673, loss = 0.36352807
Iteration 2674, loss = 0.36331506
Iteration 2675, loss = 0.36310327
Iteration 2676, loss = 0.36288789
Iteration 2677, loss = 0.36267857
Iteration 2678, loss = 0.36246247
Iteration 2679, loss = 0.36225248
Iteration 2680, loss = 0.36203888
Iteration 2681, loss = 0.36182762
Iteration 2682, loss = 0.36161566
Iteration 2683, loss = 0.36140234
Iteration 2684, loss = 0.36119514
Iteration 2685, loss = 0.36098262
Iteration 2686, loss = 0.36077260
Iteration 2687, loss = 0.36056134
Iteration 2688, loss = 0.36034828
Iteration 2689, loss = 0.36013926
Iteration 2690, loss = 0.35992668
Iteration 2691, loss = 0.35971875
Iteration 2692, loss = 0.35950886
Iteration 2693, loss = 0.35930050
Iteration 2694, loss = 0.35909225
Iteration 2695, loss = 0.35888320
Iteration 2696, loss = 0.35867487
Iteration 2697, loss = 0.35846722
Iteration 2698, loss = 0.35825524
Iteration 2699, loss = 0.35805005
Iteration 2700, loss = 0.35784077
Iteration 2701, loss = 0.35763415
Iteration 2702, loss = 0.35742624
Iteration 2703, loss = 0.35721928
Iteration 2704, loss = 0.35700963
Iteration 2705, loss = 0.35680532
Iteration 2706, loss = 0.35659759
Iteration 2707, loss = 0.35638975
Iteration 2708, loss = 0.35618753
Iteration 2709, loss = 0.35597821
Iteration 2710, loss = 0.35577391
Iteration 2711, loss = 0.35556758
Iteration 2712, loss = 0.35536409
Iteration 2713, loss = 0.35515578
Iteration 2714, loss = 0.35494832
Iteration 2715, loss = 0.35474729
Iteration 2716, loss = 0.35454236
Iteration 2717, loss = 0.35433518
Iteration 2718, loss = 0.35413054
Iteration 2719, loss = 0.35392784
Iteration 2720, loss = 0.35372540
Iteration 2721, loss = 0.35351991
Iteration 2722, loss = 0.35331790
Iteration 2723, loss = 0.35311094
Iteration 2724, loss = 0.35291337
Iteration 2725, loss = 0.35270598
Iteration 2726, loss = 0.35250554
Iteration 2727, loss = 0.35230286
Iteration 2728, loss = 0.35210234
Iteration 2729, loss = 0.35189894
Iteration 2730, loss = 0.35169747
Iteration 2731, loss = 0.35149608
Iteration 2732, loss = 0.35129758
Iteration 2733, loss = 0.35109420
Iteration 2734, loss = 0.35089213
Iteration 2735, loss = 0.35069253
Iteration 2736, loss = 0.35049392
Iteration 2737, loss = 0.35029233
Iteration 2738, loss = 0.35009240
Iteration 2739, loss = 0.34989199
Iteration 2740, loss = 0.34969363
Iteration 2741, loss = 0.34949331
Iteration 2742, loss = 0.34929542
Iteration 2743, loss = 0.34909445
Iteration 2744, loss = 0.34889388
Iteration 2745, loss = 0.34869527
Iteration 2746, loss = 0.34849970
Iteration 2747, loss = 0.34829870
Iteration 2748, loss = 0.34809792
Iteration 2749, loss = 0.34790257
Iteration 2750, loss = 0.34770540
Iteration 2751, loss = 0.34750633
Iteration 2752, loss = 0.34730932
Iteration 2753, loss = 0.34711223
Iteration 2754, loss = 0.34691555
Iteration 2755, loss = 0.34672104
Iteration 2756, loss = 0.34652039
Iteration 2757, loss = 0.34632401
Iteration 2758, loss = 0.34612728
Iteration 2759, loss = 0.34593186
Iteration 2760, loss = 0.34573823
Iteration 2761, loss = 0.34554120
Iteration 2762, loss = 0.34534389
Iteration 2763, loss = 0.34514814
Iteration 2764, loss = 0.34495210
Iteration 2765, loss = 0.34475971
Iteration 2766, loss = 0.34456185
Iteration 2767, loss = 0.34436740
Iteration 2768, loss = 0.34417179
Iteration 2769, loss = 0.34397924
Iteration 2770, loss = 0.34378061
Iteration 2771, loss = 0.34358802
Iteration 2772, loss = 0.34339331
Iteration 2773, loss = 0.34320007
Iteration 2774, loss = 0.34300506
Iteration 2775, loss = 0.34281049
Iteration 2776, loss = 0.34261778
Iteration 2777, loss = 0.34242617
Iteration 2778, loss = 0.34223191
Iteration 2779, loss = 0.34203832
Iteration 2780, loss = 0.34184719
Iteration 2781, loss = 0.34165358
Iteration 2782, loss = 0.34146225
Iteration 2783, loss = 0.34126892
Iteration 2784, loss = 0.34107852
Iteration 2785, loss = 0.34088699
Iteration 2786, loss = 0.34069666
Iteration 2787, loss = 0.34050586
Iteration 2788, loss = 0.34031384
Iteration 2789, loss = 0.34012234
Iteration 2790, loss = 0.33993139
Iteration 2791, loss = 0.33974133
Iteration 2792, loss = 0.33955138
Iteration 2793, loss = 0.33936374
Iteration 2794, loss = 0.33916972
Iteration 2795, loss = 0.33898085
Iteration 2796, loss = 0.33879183
Iteration 2797, loss = 0.33860378
Iteration 2798, loss = 0.33841122
Iteration 2799, loss = 0.33822239
Iteration 2800, loss = 0.33803099
Iteration 2801, loss = 0.33784462
Iteration 2802, loss = 0.33765217
Iteration 2803, loss = 0.33746020
Iteration 2804, loss = 0.33727623
Iteration 2805, loss = 0.33708483
Iteration 2806, loss = 0.33689654
Iteration 2807, loss = 0.33671045
Iteration 2808, loss = 0.33652105
Iteration 2809, loss = 0.33633283
Iteration 2810, loss = 0.33614450
Iteration 2811, loss = 0.33595695
Iteration 2812, loss = 0.33577299
Iteration 2813, loss = 0.33558489
Iteration 2814, loss = 0.33539840
Iteration 2815, loss = 0.33521142
Iteration 2816, loss = 0.33502525
Iteration 2817, loss = 0.33483794
Iteration 2818, loss = 0.33465161
Iteration 2819, loss = 0.33446784
Iteration 2820, loss = 0.33428249
Iteration 2821, loss = 0.33409921
Iteration 2822, loss = 0.33391106
Iteration 2823, loss = 0.33372624
Iteration 2824, loss = 0.33354061
Iteration 2825, loss = 0.33335798
Iteration 2826, loss = 0.33316939
Iteration 2827, loss = 0.33298647
Iteration 2828, loss = 0.33280074
Iteration 2829, loss = 0.33261741
Iteration 2830, loss = 0.33243264
Iteration 2831, loss = 0.33224948
Iteration 2832, loss = 0.33206746
Iteration 2833, loss = 0.33188191
Iteration 2834, loss = 0.33169719
Iteration 2835, loss = 0.33151646
Iteration 2836, loss = 0.33133084
Iteration 2837, loss = 0.33114922
Iteration 2838, loss = 0.33096856
Iteration 2839, loss = 0.33078696
Iteration 2840, loss = 0.33060450
Iteration 2841, loss = 0.33042065
Iteration 2842, loss = 0.33023890
Iteration 2843, loss = 0.33005674
Iteration 2844, loss = 0.32987674
Iteration 2845, loss = 0.32969356
Iteration 2846, loss = 0.32951473
Iteration 2847, loss = 0.32933428
Iteration 2848, loss = 0.32915106
Iteration 2849, loss = 0.32897155
Iteration 2850, loss = 0.32879169
Iteration 2851, loss = 0.32861169
Iteration 2852, loss = 0.32843265
Iteration 2853, loss = 0.32825197
Iteration 2854, loss = 0.32807068
Iteration 2855, loss = 0.32789383
Iteration 2856, loss = 0.32771301
Iteration 2857, loss = 0.32753408
Iteration 2858, loss = 0.32735331
Iteration 2859, loss = 0.32717838
Iteration 2860, loss = 0.32699778
Iteration 2861, loss = 0.32681868
Iteration 2862, loss = 0.32664023
Iteration 2863, loss = 0.32646366
Iteration 2864, loss = 0.32628524
Iteration 2865, loss = 0.32610713
Iteration 2866, loss = 0.32593033
Iteration 2867, loss = 0.32575272
Iteration 2868, loss = 0.32557428
Iteration 2869, loss = 0.32539575
Iteration 2870, loss = 0.32522147
Iteration 2871, loss = 0.32504260
Iteration 2872, loss = 0.32486591
Iteration 2873, loss = 0.32468687
Iteration 2874, loss = 0.32451157
Iteration 2875, loss = 0.32433520
Iteration 2876, loss = 0.32416091
Iteration 2877, loss = 0.32398354
Iteration 2878, loss = 0.32380730
Iteration 2879, loss = 0.32362934
Iteration 2880, loss = 0.32345596
Iteration 2881, loss = 0.32328274
Iteration 2882, loss = 0.32310532
Iteration 2883, loss = 0.32293040
Iteration 2884, loss = 0.32275461
Iteration 2885, loss = 0.32258121
Iteration 2886, loss = 0.32240533
Iteration 2887, loss = 0.32223179
Iteration 2888, loss = 0.32205675
Iteration 2889, loss = 0.32188382
Iteration 2890, loss = 0.32170906
Iteration 2891, loss = 0.32153650
Iteration 2892, loss = 0.32136239
Iteration 2893, loss = 0.32119039
Iteration 2894, loss = 0.32101504
Iteration 2895, loss = 0.32084143
Iteration 2896, loss = 0.32066976
Iteration 2897, loss = 0.32049613
Iteration 2898, loss = 0.32032483
Iteration 2899, loss = 0.32015262
Iteration 2900, loss = 0.31997986
Iteration 2901, loss = 0.31980730
Iteration 2902, loss = 0.31963383
Iteration 2903, loss = 0.31946143
Iteration 2904, loss = 0.31929154
Iteration 2905, loss = 0.31911740
Iteration 2906, loss = 0.31894861
Iteration 2907, loss = 0.31877509
Iteration 2908, loss = 0.31860443
Iteration 2909, loss = 0.31843259
Iteration 2910, loss = 0.31826224
Iteration 2911, loss = 0.31809135
Iteration 2912, loss = 0.31791872
Iteration 2913, loss = 0.31774873
Iteration 2914, loss = 0.31757938
Iteration 2915, loss = 0.31740739
Iteration 2916, loss = 0.31723890
Iteration 2917, loss = 0.31706950
Iteration 2918, loss = 0.31689804
Iteration 2919, loss = 0.31672827
Iteration 2920, loss = 0.31656000
Iteration 2921, loss = 0.31639035
Iteration 2922, loss = 0.31622071
Iteration 2923, loss = 0.31605097
Iteration 2924, loss = 0.31588224
Iteration 2925, loss = 0.31571453
Iteration 2926, loss = 0.31554679
Iteration 2927, loss = 0.31537776
Iteration 2928, loss = 0.31521094
Iteration 2929, loss = 0.31504332
Iteration 2930, loss = 0.31487631
Iteration 2931, loss = 0.31470617
Iteration 2932, loss = 0.31454033
Iteration 2933, loss = 0.31437433
Iteration 2934, loss = 0.31420603
Iteration 2935, loss = 0.31403928
Iteration 2936, loss = 0.31387256
Iteration 2937, loss = 0.31370727
Iteration 2938, loss = 0.31354032
Iteration 2939, loss = 0.31337426
Iteration 2940, loss = 0.31320767
Iteration 2941, loss = 0.31304072
Iteration 2942, loss = 0.31287697
Iteration 2943, loss = 0.31270949
Iteration 2944, loss = 0.31254509
Iteration 2945, loss = 0.31238105
Iteration 2946, loss = 0.31221494
Iteration 2947, loss = 0.31205017
Iteration 2948, loss = 0.31188693
Iteration 2949, loss = 0.31172187
Iteration 2950, loss = 0.31155643
Iteration 2951, loss = 0.31139297
Iteration 2952, loss = 0.31122763
Iteration 2953, loss = 0.31106409
Iteration 2954, loss = 0.31090125
Iteration 2955, loss = 0.31073646
Iteration 2956, loss = 0.31057215
Iteration 2957, loss = 0.31041026
Iteration 2958, loss = 0.31024551
Iteration 2959, loss = 0.31008179
Iteration 2960, loss = 0.30991916
Iteration 2961, loss = 0.30975674
Iteration 2962, loss = 0.30959217
Iteration 2963, loss = 0.30943065
Iteration 2964, loss = 0.30926745
Iteration 2965, loss = 0.30910569
Iteration 2966, loss = 0.30894254
Iteration 2967, loss = 0.30878183
Iteration 2968, loss = 0.30862049
Iteration 2969, loss = 0.30845637
Iteration 2970, loss = 0.30829436
Iteration 2971, loss = 0.30813541
Iteration 2972, loss = 0.30797168
Iteration 2973, loss = 0.30781186
Iteration 2974, loss = 0.30764943
Iteration 2975, loss = 0.30748988
Iteration 2976, loss = 0.30732865
Iteration 2977, loss = 0.30716747
Iteration 2978, loss = 0.30700671
Iteration 2979, loss = 0.30684697
Iteration 2980, loss = 0.30668692
Iteration 2981, loss = 0.30652566
Iteration 2982, loss = 0.30636773
Iteration 2983, loss = 0.30620674
Iteration 2984, loss = 0.30604667
Iteration 2985, loss = 0.30588645
Iteration 2986, loss = 0.30572906
Iteration 2987, loss = 0.30556804
Iteration 2988, loss = 0.30540898
Iteration 2989, loss = 0.30524979
Iteration 2990, loss = 0.30509040
Iteration 2991, loss = 0.30493027
Iteration 2992, loss = 0.30477019
Iteration 2993, loss = 0.30461294
Iteration 2994, loss = 0.30445264
Iteration 2995, loss = 0.30429554
Iteration 2996, loss = 0.30413673
Iteration 2997, loss = 0.30397720
Iteration 2998, loss = 0.30381969
Iteration 2999, loss = 0.30366238
Iteration 3000, loss = 0.30350609
Iteration 3001, loss = 0.30334781
Iteration 3002, loss = 0.30318965
Iteration 3003, loss = 0.30303342
Iteration 3004, loss = 0.30287561
Iteration 3005, loss = 0.30271903
Iteration 3006, loss = 0.30256125
Iteration 3007, loss = 0.30240544
Iteration 3008, loss = 0.30224827
Iteration 3009, loss = 0.30209228
Iteration 3010, loss = 0.30193668
Iteration 3011, loss = 0.30177960
Iteration 3012, loss = 0.30162451
Iteration 3013, loss = 0.30146761
Iteration 3014, loss = 0.30131336
Iteration 3015, loss = 0.30115760
Iteration 3016, loss = 0.30100326
Iteration 3017, loss = 0.30084804
Iteration 3018, loss = 0.30069263
Iteration 3019, loss = 0.30054265
Iteration 3020, loss = 0.30038412
Iteration 3021, loss = 0.30023156
Iteration 3022, loss = 0.30007582
Iteration 3023, loss = 0.29992314
Iteration 3024, loss = 0.29976954
Iteration 3025, loss = 0.29961559
Iteration 3026, loss = 0.29946025
Iteration 3027, loss = 0.29931053
Iteration 3028, loss = 0.29915556
Iteration 3029, loss = 0.29900206
Iteration 3030, loss = 0.29884818
Iteration 3031, loss = 0.29869493
Iteration 3032, loss = 0.29854140
Iteration 3033, loss = 0.29838866
Iteration 3034, loss = 0.29823675
Iteration 3035, loss = 0.29808222
Iteration 3036, loss = 0.29792799
Iteration 3037, loss = 0.29778058
Iteration 3038, loss = 0.29762263
Iteration 3039, loss = 0.29747227
Iteration 3040, loss = 0.29732013
Iteration 3041, loss = 0.29716755
Iteration 3042, loss = 0.29701642
Iteration 3043, loss = 0.29686443
Iteration 3044, loss = 0.29671142
Iteration 3045, loss = 0.29656178
Iteration 3046, loss = 0.29640914
Iteration 3047, loss = 0.29625858
Iteration 3048, loss = 0.29610692
Iteration 3049, loss = 0.29595885
Iteration 3050, loss = 0.29580788
Iteration 3051, loss = 0.29565336
Iteration 3052, loss = 0.29550500
Iteration 3053, loss = 0.29535360
Iteration 3054, loss = 0.29520231
Iteration 3055, loss = 0.29505207
Iteration 3056, loss = 0.29490282
Iteration 3057, loss = 0.29475186
Iteration 3058, loss = 0.29460236
Iteration 3059, loss = 0.29445473
Iteration 3060, loss = 0.29430202
Iteration 3061, loss = 0.29415392
Iteration 3062, loss = 0.29400386
Iteration 3063, loss = 0.29385699
Iteration 3064, loss = 0.29370680
Iteration 3065, loss = 0.29355788
Iteration 3066, loss = 0.29341126
Iteration 3067, loss = 0.29326195
Iteration 3068, loss = 0.29311399
Iteration 3069, loss = 0.29296567
Iteration 3070, loss = 0.29281874
Iteration 3071, loss = 0.29267185
Iteration 3072, loss = 0.29252239
Iteration 3073, loss = 0.29237407
Iteration 3074, loss = 0.29222693
Iteration 3075, loss = 0.29207883
Iteration 3076, loss = 0.29193224
Iteration 3077, loss = 0.29178628
Iteration 3078, loss = 0.29163893
Iteration 3079, loss = 0.29149103
Iteration 3080, loss = 0.29134274
Iteration 3081, loss = 0.29119814
Iteration 3082, loss = 0.29104821
Iteration 3083, loss = 0.29090349
Iteration 3084, loss = 0.29075656
Iteration 3085, loss = 0.29060928
Iteration 3086, loss = 0.29046237
Iteration 3087, loss = 0.29031401
Iteration 3088, loss = 0.29017272
Iteration 3089, loss = 0.29002399
Iteration 3090, loss = 0.28987767
Iteration 3091, loss = 0.28973348
Iteration 3092, loss = 0.28958648
Iteration 3093, loss = 0.28944254
Iteration 3094, loss = 0.28929430
Iteration 3095, loss = 0.28914944
Iteration 3096, loss = 0.28900347
Iteration 3097, loss = 0.28885848
Iteration 3098, loss = 0.28871266
Iteration 3099, loss = 0.28856834
Iteration 3100, loss = 0.28842342
Iteration 3101, loss = 0.28827996
Iteration 3102, loss = 0.28813578
Iteration 3103, loss = 0.28799047
Iteration 3104, loss = 0.28784608
Iteration 3105, loss = 0.28770110
Iteration 3106, loss = 0.28755917
Iteration 3107, loss = 0.28741677
Iteration 3108, loss = 0.28727051
Iteration 3109, loss = 0.28712630
Iteration 3110, loss = 0.28698577
Iteration 3111, loss = 0.28684166
Iteration 3112, loss = 0.28669949
Iteration 3113, loss = 0.28655620
Iteration 3114, loss = 0.28641488
Iteration 3115, loss = 0.28627237
Iteration 3116, loss = 0.28612885
Iteration 3117, loss = 0.28598742
Iteration 3118, loss = 0.28584711
Iteration 3119, loss = 0.28570451
Iteration 3120, loss = 0.28556218
Iteration 3121, loss = 0.28542176
Iteration 3122, loss = 0.28527893
Iteration 3123, loss = 0.28513783
Iteration 3124, loss = 0.28499509
Iteration 3125, loss = 0.28485455
Iteration 3126, loss = 0.28471329
Iteration 3127, loss = 0.28457257
Iteration 3128, loss = 0.28443158
Iteration 3129, loss = 0.28429052
Iteration 3130, loss = 0.28414884
Iteration 3131, loss = 0.28401106
Iteration 3132, loss = 0.28386770
Iteration 3133, loss = 0.28372807
Iteration 3134, loss = 0.28358874
Iteration 3135, loss = 0.28344766
Iteration 3136, loss = 0.28330670
Iteration 3137, loss = 0.28316834
Iteration 3138, loss = 0.28302866
Iteration 3139, loss = 0.28288871
Iteration 3140, loss = 0.28274980
Iteration 3141, loss = 0.28261021
Iteration 3142, loss = 0.28246846
Iteration 3143, loss = 0.28233195
Iteration 3144, loss = 0.28218938
Iteration 3145, loss = 0.28205197
Iteration 3146, loss = 0.28191425
Iteration 3147, loss = 0.28177397
Iteration 3148, loss = 0.28163521
Iteration 3149, loss = 0.28149842
Iteration 3150, loss = 0.28136013
Iteration 3151, loss = 0.28122021
Iteration 3152, loss = 0.28108245
Iteration 3153, loss = 0.28094546
Iteration 3154, loss = 0.28080859
Iteration 3155, loss = 0.28067225
Iteration 3156, loss = 0.28053280
Iteration 3157, loss = 0.28039642
Iteration 3158, loss = 0.28025808
Iteration 3159, loss = 0.28012361
Iteration 3160, loss = 0.27998384
Iteration 3161, loss = 0.27984771
Iteration 3162, loss = 0.27970964
Iteration 3163, loss = 0.27957408
Iteration 3164, loss = 0.27943956
Iteration 3165, loss = 0.27930128
Iteration 3166, loss = 0.27916453
Iteration 3167, loss = 0.27902915
Iteration 3168, loss = 0.27889347
Iteration 3169, loss = 0.27875858
Iteration 3170, loss = 0.27862060
Iteration 3171, loss = 0.27848506
Iteration 3172, loss = 0.27835009
Iteration 3173, loss = 0.27821347
Iteration 3174, loss = 0.27807799
Iteration 3175, loss = 0.27794268
Iteration 3176, loss = 0.27780838
Iteration 3177, loss = 0.27767275
Iteration 3178, loss = 0.27753647
Iteration 3179, loss = 0.27740369
Iteration 3180, loss = 0.27726718
Iteration 3181, loss = 0.27713163
Iteration 3182, loss = 0.27699927
Iteration 3183, loss = 0.27686156
Iteration 3184, loss = 0.27672877
Iteration 3185, loss = 0.27659471
Iteration 3186, loss = 0.27645943
Iteration 3187, loss = 0.27632567
Iteration 3188, loss = 0.27619240
Iteration 3189, loss = 0.27605665
Iteration 3190, loss = 0.27592289
Iteration 3191, loss = 0.27579123
Iteration 3192, loss = 0.27565622
Iteration 3193, loss = 0.27552332
Iteration 3194, loss = 0.27538928
Iteration 3195, loss = 0.27525714
Iteration 3196, loss = 0.27512459
Iteration 3197, loss = 0.27499230
Iteration 3198, loss = 0.27485968
Iteration 3199, loss = 0.27472454
Iteration 3200, loss = 0.27459383
Iteration 3201, loss = 0.27446192
Iteration 3202, loss = 0.27433002
Iteration 3203, loss = 0.27419796
Iteration 3204, loss = 0.27406642
Iteration 3205, loss = 0.27393328
Iteration 3206, loss = 0.27380190
Iteration 3207, loss = 0.27367001
Iteration 3208, loss = 0.27353842
Iteration 3209, loss = 0.27340702
Iteration 3210, loss = 0.27327666
Iteration 3211, loss = 0.27314504
Iteration 3212, loss = 0.27301429
Iteration 3213, loss = 0.27288183
Iteration 3214, loss = 0.27274940
Iteration 3215, loss = 0.27262068
Iteration 3216, loss = 0.27248760
Iteration 3217, loss = 0.27235913
Iteration 3218, loss = 0.27222688
Iteration 3219, loss = 0.27209645
Iteration 3220, loss = 0.27196667
Iteration 3221, loss = 0.27183655
Iteration 3222, loss = 0.27170699
Iteration 3223, loss = 0.27157755
Iteration 3224, loss = 0.27144666
Iteration 3225, loss = 0.27131588
Iteration 3226, loss = 0.27118910
Iteration 3227, loss = 0.27105615
Iteration 3228, loss = 0.27092879
Iteration 3229, loss = 0.27079974
Iteration 3230, loss = 0.27066798
Iteration 3231, loss = 0.27054057
Iteration 3232, loss = 0.27041151
Iteration 3233, loss = 0.27028358
Iteration 3234, loss = 0.27015375
Iteration 3235, loss = 0.27002571
Iteration 3236, loss = 0.26989652
Iteration 3237, loss = 0.26976992
Iteration 3238, loss = 0.26964040
Iteration 3239, loss = 0.26951157
Iteration 3240, loss = 0.26938378
Iteration 3241, loss = 0.26925663
Iteration 3242, loss = 0.26912873
Iteration 3243, loss = 0.26899897
Iteration 3244, loss = 0.26887184
Iteration 3245, loss = 0.26874472
Iteration 3246, loss = 0.26861675
Iteration 3247, loss = 0.26849056
Iteration 3248, loss = 0.26836263
Iteration 3249, loss = 0.26823537
Iteration 3250, loss = 0.26810797
Iteration 3251, loss = 0.26798107
Iteration 3252, loss = 0.26785528
Iteration 3253, loss = 0.26772758
Iteration 3254, loss = 0.26760165
Iteration 3255, loss = 0.26747538
Iteration 3256, loss = 0.26734843
Iteration 3257, loss = 0.26722190
Iteration 3258, loss = 0.26709649
Iteration 3259, loss = 0.26696818
Iteration 3260, loss = 0.26684233
Iteration 3261, loss = 0.26671772
Iteration 3262, loss = 0.26659080
Iteration 3263, loss = 0.26646449
Iteration 3264, loss = 0.26633931
Iteration 3265, loss = 0.26621438
Iteration 3266, loss = 0.26608765
Iteration 3267, loss = 0.26596204
Iteration 3268, loss = 0.26583760
Iteration 3269, loss = 0.26571193
Iteration 3270, loss = 0.26558566
Iteration 3271, loss = 0.26545995
Iteration 3272, loss = 0.26533530
Iteration 3273, loss = 0.26520921
Iteration 3274, loss = 0.26508563
Iteration 3275, loss = 0.26496021
Iteration 3276, loss = 0.26483529
Iteration 3277, loss = 0.26470931
Iteration 3278, loss = 0.26458609
Iteration 3279, loss = 0.26446221
Iteration 3280, loss = 0.26433961
Iteration 3281, loss = 0.26421265
Iteration 3282, loss = 0.26408854
Iteration 3283, loss = 0.26396405
Iteration 3284, loss = 0.26384116
Iteration 3285, loss = 0.26371913
Iteration 3286, loss = 0.26359314
Iteration 3287, loss = 0.26347070
Iteration 3288, loss = 0.26334693
Iteration 3289, loss = 0.26322364
Iteration 3290, loss = 0.26310061
Iteration 3291, loss = 0.26297841
Iteration 3292, loss = 0.26285500
Iteration 3293, loss = 0.26273232
Iteration 3294, loss = 0.26260998
Iteration 3295, loss = 0.26248574
Iteration 3296, loss = 0.26236462
Iteration 3297, loss = 0.26224121
Iteration 3298, loss = 0.26211883
Iteration 3299, loss = 0.26199827
Iteration 3300, loss = 0.26187624
Iteration 3301, loss = 0.26175295
Iteration 3302, loss = 0.26163103
Iteration 3303, loss = 0.26151088
Iteration 3304, loss = 0.26138935
Iteration 3305, loss = 0.26126560
Iteration 3306, loss = 0.26114511
Iteration 3307, loss = 0.26102302
Iteration 3308, loss = 0.26090236
Iteration 3309, loss = 0.26078101
Iteration 3310, loss = 0.26066048
Iteration 3311, loss = 0.26054039
Iteration 3312, loss = 0.26041966
Iteration 3313, loss = 0.26029856
Iteration 3314, loss = 0.26017791
Iteration 3315, loss = 0.26005713
Iteration 3316, loss = 0.25993710
Iteration 3317, loss = 0.25981534
Iteration 3318, loss = 0.25969557
Iteration 3319, loss = 0.25957597
Iteration 3320, loss = 0.25945516
Iteration 3321, loss = 0.25933374
Iteration 3322, loss = 0.25921373
Iteration 3323, loss = 0.25909414
Iteration 3324, loss = 0.25897436
Iteration 3325, loss = 0.25885396
Iteration 3326, loss = 0.25873335
Iteration 3327, loss = 0.25861397
Iteration 3328, loss = 0.25849408
Iteration 3329, loss = 0.25837486
Iteration 3330, loss = 0.25825461
Iteration 3331, loss = 0.25813613
Iteration 3332, loss = 0.25801628
Iteration 3333, loss = 0.25789704
Iteration 3334, loss = 0.25777695
Iteration 3335, loss = 0.25765829
Iteration 3336, loss = 0.25754138
Iteration 3337, loss = 0.25742136
Iteration 3338, loss = 0.25730231
Iteration 3339, loss = 0.25718523
Iteration 3340, loss = 0.25706763
Iteration 3341, loss = 0.25694836
Iteration 3342, loss = 0.25683041
Iteration 3343, loss = 0.25671081
Iteration 3344, loss = 0.25659457
Iteration 3345, loss = 0.25647793
Iteration 3346, loss = 0.25635995
Iteration 3347, loss = 0.25624235
Iteration 3348, loss = 0.25612307
Iteration 3349, loss = 0.25600723
Iteration 3350, loss = 0.25588923
Iteration 3351, loss = 0.25577219
Iteration 3352, loss = 0.25565590
Iteration 3353, loss = 0.25553756
Iteration 3354, loss = 0.25542173
Iteration 3355, loss = 0.25530392
Iteration 3356, loss = 0.25518739
Iteration 3357, loss = 0.25507046
Iteration 3358, loss = 0.25495420
Iteration 3359, loss = 0.25483770
Iteration 3360, loss = 0.25472144
Iteration 3361, loss = 0.25460331
Iteration 3362, loss = 0.25448801
Iteration 3363, loss = 0.25437162
Iteration 3364, loss = 0.25425520
Iteration 3365, loss = 0.25413918
Iteration 3366, loss = 0.25402158
Iteration 3367, loss = 0.25390912
Iteration 3368, loss = 0.25379116
Iteration 3369, loss = 0.25367701
Iteration 3370, loss = 0.25356092
Iteration 3371, loss = 0.25344440
Iteration 3372, loss = 0.25332960
Iteration 3373, loss = 0.25321301
Iteration 3374, loss = 0.25309950
Iteration 3375, loss = 0.25298333
Iteration 3376, loss = 0.25286959
Iteration 3377, loss = 0.25275510
Iteration 3378, loss = 0.25264016
Iteration 3379, loss = 0.25252355
Iteration 3380, loss = 0.25241047
Iteration 3381, loss = 0.25229617
Iteration 3382, loss = 0.25218146
Iteration 3383, loss = 0.25206774
Iteration 3384, loss = 0.25195366
Iteration 3385, loss = 0.25183785
Iteration 3386, loss = 0.25172373
Iteration 3387, loss = 0.25160871
Iteration 3388, loss = 0.25149711
Iteration 3389, loss = 0.25138116
Iteration 3390, loss = 0.25126871
Iteration 3391, loss = 0.25115475
Iteration 3392, loss = 0.25104070
Iteration 3393, loss = 0.25092803
Iteration 3394, loss = 0.25081421
Iteration 3395, loss = 0.25070128
Iteration 3396, loss = 0.25058633
Iteration 3397, loss = 0.25047357
Iteration 3398, loss = 0.25035977
Iteration 3399, loss = 0.25024723
Iteration 3400, loss = 0.25013444
Iteration 3401, loss = 0.25002390
Iteration 3402, loss = 0.24990831
Iteration 3403, loss = 0.24979800
Iteration 3404, loss = 0.24968349
Iteration 3405, loss = 0.24957194
Iteration 3406, loss = 0.24945997
Iteration 3407, loss = 0.24934769
Iteration 3408, loss = 0.24923717
Iteration 3409, loss = 0.24912371
Iteration 3410, loss = 0.24901318
Iteration 3411, loss = 0.24890184
Iteration 3412, loss = 0.24878906
Iteration 3413, loss = 0.24867719
Iteration 3414, loss = 0.24856544
Iteration 3415, loss = 0.24845349
Iteration 3416, loss = 0.24834206
Iteration 3417, loss = 0.24823035
Iteration 3418, loss = 0.24811848
Iteration 3419, loss = 0.24800753
Iteration 3420, loss = 0.24789622
Iteration 3421, loss = 0.24778374
Iteration 3422, loss = 0.24767414
Iteration 3423, loss = 0.24756363
Iteration 3424, loss = 0.24745136
Iteration 3425, loss = 0.24734030
Iteration 3426, loss = 0.24723238
Iteration 3427, loss = 0.24712120
Iteration 3428, loss = 0.24700914
Iteration 3429, loss = 0.24689947
Iteration 3430, loss = 0.24678930
Iteration 3431, loss = 0.24667887
Iteration 3432, loss = 0.24657014
Iteration 3433, loss = 0.24645863
Iteration 3434, loss = 0.24634949
Iteration 3435, loss = 0.24624085
Iteration 3436, loss = 0.24613136
Iteration 3437, loss = 0.24602033
Iteration 3438, loss = 0.24591091
Iteration 3439, loss = 0.24580167
Iteration 3440, loss = 0.24569160
Iteration 3441, loss = 0.24558323
Iteration 3442, loss = 0.24547503
Iteration 3443, loss = 0.24536525
Iteration 3444, loss = 0.24525653
Iteration 3445, loss = 0.24514762
Iteration 3446, loss = 0.24503920
Iteration 3447, loss = 0.24493106
Iteration 3448, loss = 0.24482124
Iteration 3449, loss = 0.24471374
Iteration 3450, loss = 0.24460513
Iteration 3451, loss = 0.24449694
Iteration 3452, loss = 0.24438916
Iteration 3453, loss = 0.24428099
Iteration 3454, loss = 0.24417088
Iteration 3455, loss = 0.24406471
Iteration 3456, loss = 0.24395620
Iteration 3457, loss = 0.24384855
Iteration 3458, loss = 0.24374092
Iteration 3459, loss = 0.24363282
Iteration 3460, loss = 0.24352348
Iteration 3461, loss = 0.24341833
Iteration 3462, loss = 0.24331005
Iteration 3463, loss = 0.24320232
Iteration 3464, loss = 0.24309461
Iteration 3465, loss = 0.24298633
Iteration 3466, loss = 0.24288258
Iteration 3467, loss = 0.24277258
Iteration 3468, loss = 0.24266671
Iteration 3469, loss = 0.24255950
Iteration 3470, loss = 0.24245236
Iteration 3471, loss = 0.24234554
Iteration 3472, loss = 0.24223933
Iteration 3473, loss = 0.24213094
Iteration 3474, loss = 0.24202435
Iteration 3475, loss = 0.24191839
Iteration 3476, loss = 0.24181225
Iteration 3477, loss = 0.24170630
Iteration 3478, loss = 0.24159900
Iteration 3479, loss = 0.24149352
Iteration 3480, loss = 0.24138687
Iteration 3481, loss = 0.24128005
Iteration 3482, loss = 0.24117452
Iteration 3483, loss = 0.24106696
Iteration 3484, loss = 0.24096302
Iteration 3485, loss = 0.24085647
Iteration 3486, loss = 0.24075113
Iteration 3487, loss = 0.24064492
Iteration 3488, loss = 0.24053983
Iteration 3489, loss = 0.24043530
Iteration 3490, loss = 0.24032987
Iteration 3491, loss = 0.24022417
Iteration 3492, loss = 0.24011995
Iteration 3493, loss = 0.24001466
Iteration 3494, loss = 0.23991057
Iteration 3495, loss = 0.23980528
Iteration 3496, loss = 0.23970127
Iteration 3497, loss = 0.23959569
Iteration 3498, loss = 0.23949235
Iteration 3499, loss = 0.23938911
Iteration 3500, loss = 0.23928328
Iteration 3501, loss = 0.23918013
Iteration 3502, loss = 0.23907485
Iteration 3503, loss = 0.23897257
Iteration 3504, loss = 0.23886805
Iteration 3505, loss = 0.23876423
Iteration 3506, loss = 0.23866135
Iteration 3507, loss = 0.23855638
Iteration 3508, loss = 0.23845393
Iteration 3509, loss = 0.23834888
Iteration 3510, loss = 0.23824634
Iteration 3511, loss = 0.23814311
Iteration 3512, loss = 0.23803908
Iteration 3513, loss = 0.23793852
Iteration 3514, loss = 0.23783301
Iteration 3515, loss = 0.23772965
Iteration 3516, loss = 0.23762669
Iteration 3517, loss = 0.23752366
Iteration 3518, loss = 0.23742136
Iteration 3519, loss = 0.23731893
Iteration 3520, loss = 0.23721546
Iteration 3521, loss = 0.23711389
Iteration 3522, loss = 0.23701147
Iteration 3523, loss = 0.23690754
Iteration 3524, loss = 0.23680602
Iteration 3525, loss = 0.23670311
Iteration 3526, loss = 0.23660110
Iteration 3527, loss = 0.23649992
Iteration 3528, loss = 0.23639739
Iteration 3529, loss = 0.23629464
Iteration 3530, loss = 0.23619350
Iteration 3531, loss = 0.23609120
Iteration 3532, loss = 0.23598879
Iteration 3533, loss = 0.23588792
Iteration 3534, loss = 0.23578518
Iteration 3535, loss = 0.23568434
Iteration 3536, loss = 0.23558301
Iteration 3537, loss = 0.23548094
Iteration 3538, loss = 0.23537933
Iteration 3539, loss = 0.23527784
Iteration 3540, loss = 0.23517582
Iteration 3541, loss = 0.23507543
Iteration 3542, loss = 0.23497420
Iteration 3543, loss = 0.23487226
Iteration 3544, loss = 0.23477229
Iteration 3545, loss = 0.23467197
Iteration 3546, loss = 0.23457036
Iteration 3547, loss = 0.23446854
Iteration 3548, loss = 0.23436784
Iteration 3549, loss = 0.23426854
Iteration 3550, loss = 0.23416712
Iteration 3551, loss = 0.23406731
Iteration 3552, loss = 0.23396589
Iteration 3553, loss = 0.23386631
Iteration 3554, loss = 0.23376588
Iteration 3555, loss = 0.23366691
Iteration 3556, loss = 0.23356589
Iteration 3557, loss = 0.23346599
Iteration 3558, loss = 0.23336602
Iteration 3559, loss = 0.23326527
Iteration 3560, loss = 0.23316643
Iteration 3561, loss = 0.23306662
Iteration 3562, loss = 0.23296636
Iteration 3563, loss = 0.23286716
Iteration 3564, loss = 0.23276785
Iteration 3565, loss = 0.23266974
Iteration 3566, loss = 0.23257022
Iteration 3567, loss = 0.23247114
Iteration 3568, loss = 0.23237163
Iteration 3569, loss = 0.23227368
Iteration 3570, loss = 0.23217374
Iteration 3571, loss = 0.23207501
Iteration 3572, loss = 0.23197555
Iteration 3573, loss = 0.23187749
Iteration 3574, loss = 0.23177851
Iteration 3575, loss = 0.23168019
Iteration 3576, loss = 0.23158163
Iteration 3577, loss = 0.23148184
Iteration 3578, loss = 0.23138341
Iteration 3579, loss = 0.23128496
Iteration 3580, loss = 0.23118574
Iteration 3581, loss = 0.23108808
Iteration 3582, loss = 0.23098891
Iteration 3583, loss = 0.23089205
Iteration 3584, loss = 0.23079224
Iteration 3585, loss = 0.23069531
Iteration 3586, loss = 0.23059835
Iteration 3587, loss = 0.23049877
Iteration 3588, loss = 0.23040205
Iteration 3589, loss = 0.23030365
Iteration 3590, loss = 0.23020680
Iteration 3591, loss = 0.23010872
Iteration 3592, loss = 0.23001184
Iteration 3593, loss = 0.22991438
Iteration 3594, loss = 0.22981572
Iteration 3595, loss = 0.22971803
Iteration 3596, loss = 0.22962158
Iteration 3597, loss = 0.22952513
Iteration 3598, loss = 0.22942697
Iteration 3599, loss = 0.22933023
Iteration 3600, loss = 0.22923305
Iteration 3601, loss = 0.22913575
Iteration 3602, loss = 0.22903904
Iteration 3603, loss = 0.22894345
Iteration 3604, loss = 0.22884641
Iteration 3605, loss = 0.22875037
Iteration 3606, loss = 0.22865423
Iteration 3607, loss = 0.22855716
Iteration 3608, loss = 0.22846082
Iteration 3609, loss = 0.22836484
Iteration 3610, loss = 0.22826861
Iteration 3611, loss = 0.22817406
Iteration 3612, loss = 0.22807708
Iteration 3613, loss = 0.22798138
Iteration 3614, loss = 0.22788693
Iteration 3615, loss = 0.22778978
Iteration 3616, loss = 0.22769468
Iteration 3617, loss = 0.22759928
Iteration 3618, loss = 0.22750398
Iteration 3619, loss = 0.22740754
Iteration 3620, loss = 0.22731227
Iteration 3621, loss = 0.22721745
Iteration 3622, loss = 0.22712260
Iteration 3623, loss = 0.22702702
Iteration 3624, loss = 0.22693154
Iteration 3625, loss = 0.22683723
Iteration 3626, loss = 0.22674212
Iteration 3627, loss = 0.22664634
Iteration 3628, loss = 0.22655164
Iteration 3629, loss = 0.22645721
Iteration 3630, loss = 0.22636393
Iteration 3631, loss = 0.22626706
Iteration 3632, loss = 0.22617194
Iteration 3633, loss = 0.22607854
Iteration 3634, loss = 0.22598395
Iteration 3635, loss = 0.22588870
Iteration 3636, loss = 0.22579543
Iteration 3637, loss = 0.22569968
Iteration 3638, loss = 0.22560665
Iteration 3639, loss = 0.22551177
Iteration 3640, loss = 0.22541865
Iteration 3641, loss = 0.22532335
Iteration 3642, loss = 0.22523040
Iteration 3643, loss = 0.22513663
Iteration 3644, loss = 0.22504176
Iteration 3645, loss = 0.22494748
Iteration 3646, loss = 0.22485416
Iteration 3647, loss = 0.22476164
Iteration 3648, loss = 0.22466817
Iteration 3649, loss = 0.22457340
Iteration 3650, loss = 0.22448036
Iteration 3651, loss = 0.22438796
Iteration 3652, loss = 0.22429281
Iteration 3653, loss = 0.22420134
Iteration 3654, loss = 0.22410874
Iteration 3655, loss = 0.22401375
Iteration 3656, loss = 0.22392178
Iteration 3657, loss = 0.22382978
Iteration 3658, loss = 0.22373553
Iteration 3659, loss = 0.22364332
Iteration 3660, loss = 0.22355114
Iteration 3661, loss = 0.22345850
Iteration 3662, loss = 0.22336587
Iteration 3663, loss = 0.22327408
Iteration 3664, loss = 0.22318256
Iteration 3665, loss = 0.22308926
Iteration 3666, loss = 0.22299894
Iteration 3667, loss = 0.22290502
Iteration 3668, loss = 0.22281347
Iteration 3669, loss = 0.22272229
Iteration 3670, loss = 0.22263068
Iteration 3671, loss = 0.22253849
Iteration 3672, loss = 0.22244671
Iteration 3673, loss = 0.22235503
Iteration 3674, loss = 0.22226415
Iteration 3675, loss = 0.22217126
Iteration 3676, loss = 0.22208055
Iteration 3677, loss = 0.22199042
Iteration 3678, loss = 0.22189839
Iteration 3679, loss = 0.22180696
Iteration 3680, loss = 0.22171609
Iteration 3681, loss = 0.22162411
Iteration 3682, loss = 0.22153470
Iteration 3683, loss = 0.22144392
Iteration 3684, loss = 0.22135316
Iteration 3685, loss = 0.22126160
Iteration 3686, loss = 0.22117037
Iteration 3687, loss = 0.22108039
Iteration 3688, loss = 0.22099007
Iteration 3689, loss = 0.22089849
Iteration 3690, loss = 0.22080912
Iteration 3691, loss = 0.22071910
Iteration 3692, loss = 0.22062883
Iteration 3693, loss = 0.22053744
Iteration 3694, loss = 0.22044732
Iteration 3695, loss = 0.22035826
Iteration 3696, loss = 0.22026813
Iteration 3697, loss = 0.22017739
Iteration 3698, loss = 0.22008801
Iteration 3699, loss = 0.21999765
Iteration 3700, loss = 0.21990808
Iteration 3701, loss = 0.21981845
Iteration 3702, loss = 0.21972805
Iteration 3703, loss = 0.21963981
Iteration 3704, loss = 0.21954955
Iteration 3705, loss = 0.21946063
Iteration 3706, loss = 0.21937047
Iteration 3707, loss = 0.21928166
Iteration 3708, loss = 0.21919228
Iteration 3709, loss = 0.21910343
Iteration 3710, loss = 0.21901216
Iteration 3711, loss = 0.21892404
Iteration 3712, loss = 0.21883507
Iteration 3713, loss = 0.21874485
Iteration 3714, loss = 0.21865650
Iteration 3715, loss = 0.21856761
Iteration 3716, loss = 0.21848010
Iteration 3717, loss = 0.21838998
Iteration 3718, loss = 0.21830233
Iteration 3719, loss = 0.21821305
Iteration 3720, loss = 0.21812597
Iteration 3721, loss = 0.21803700
Iteration 3722, loss = 0.21794880
Iteration 3723, loss = 0.21785967
Iteration 3724, loss = 0.21777159
Iteration 3725, loss = 0.21768277
Iteration 3726, loss = 0.21759628
Iteration 3727, loss = 0.21750814
Iteration 3728, loss = 0.21741921
Iteration 3729, loss = 0.21733161
Iteration 3730, loss = 0.21724344
Iteration 3731, loss = 0.21715494
Iteration 3732, loss = 0.21706666
Iteration 3733, loss = 0.21697914
Iteration 3734, loss = 0.21689055
Iteration 3735, loss = 0.21680226
Iteration 3736, loss = 0.21671604
Iteration 3737, loss = 0.21662643
Iteration 3738, loss = 0.21654140
Iteration 3739, loss = 0.21645152
Iteration 3740, loss = 0.21636472
Iteration 3741, loss = 0.21627699
Iteration 3742, loss = 0.21619018
Iteration 3743, loss = 0.21610178
Iteration 3744, loss = 0.21601620
Iteration 3745, loss = 0.21592812
Iteration 3746, loss = 0.21584178
Iteration 3747, loss = 0.21575538
Iteration 3748, loss = 0.21566825
Iteration 3749, loss = 0.21558169
Iteration 3750, loss = 0.21549458
Iteration 3751, loss = 0.21540748
Iteration 3752, loss = 0.21532208
Iteration 3753, loss = 0.21523530
Iteration 3754, loss = 0.21514884
Iteration 3755, loss = 0.21506217
Iteration 3756, loss = 0.21497648
Iteration 3757, loss = 0.21488885
Iteration 3758, loss = 0.21480443
Iteration 3759, loss = 0.21471759
Iteration 3760, loss = 0.21463226
Iteration 3761, loss = 0.21454724
Iteration 3762, loss = 0.21446005
Iteration 3763, loss = 0.21437368
Iteration 3764, loss = 0.21428849
Iteration 3765, loss = 0.21420319
Iteration 3766, loss = 0.21411672
Iteration 3767, loss = 0.21403150
Iteration 3768, loss = 0.21394528
Iteration 3769, loss = 0.21385954
Iteration 3770, loss = 0.21377491
Iteration 3771, loss = 0.21368948
Iteration 3772, loss = 0.21360438
Iteration 3773, loss = 0.21351849
Iteration 3774, loss = 0.21343248
Iteration 3775, loss = 0.21334719
Iteration 3776, loss = 0.21326236
Iteration 3777, loss = 0.21317693
Iteration 3778, loss = 0.21309197
Iteration 3779, loss = 0.21300698
Iteration 3780, loss = 0.21292284
Iteration 3781, loss = 0.21283818
Iteration 3782, loss = 0.21275257
Iteration 3783, loss = 0.21266738
Iteration 3784, loss = 0.21258336
Iteration 3785, loss = 0.21249934
Iteration 3786, loss = 0.21241494
Iteration 3787, loss = 0.21233031
Iteration 3788, loss = 0.21224533
Iteration 3789, loss = 0.21216104
Iteration 3790, loss = 0.21207696
Iteration 3791, loss = 0.21199293
Iteration 3792, loss = 0.21190871
Iteration 3793, loss = 0.21182450
Iteration 3794, loss = 0.21174081
Iteration 3795, loss = 0.21165592
Iteration 3796, loss = 0.21157380
Iteration 3797, loss = 0.21148892
Iteration 3798, loss = 0.21140516
Iteration 3799, loss = 0.21132255
Iteration 3800, loss = 0.21123838
Iteration 3801, loss = 0.21115429
Iteration 3802, loss = 0.21107202
Iteration 3803, loss = 0.21098662
Iteration 3804, loss = 0.21090360
Iteration 3805, loss = 0.21082162
Iteration 3806, loss = 0.21073757
Iteration 3807, loss = 0.21065301
Iteration 3808, loss = 0.21057060
Iteration 3809, loss = 0.21048773
Iteration 3810, loss = 0.21040457
Iteration 3811, loss = 0.21032047
Iteration 3812, loss = 0.21023764
Iteration 3813, loss = 0.21015423
Iteration 3814, loss = 0.21007223
Iteration 3815, loss = 0.20998858
Iteration 3816, loss = 0.20990553
Iteration 3817, loss = 0.20982295
Iteration 3818, loss = 0.20974022
Iteration 3819, loss = 0.20965743
Iteration 3820, loss = 0.20957416
Iteration 3821, loss = 0.20949312
Iteration 3822, loss = 0.20940994
Iteration 3823, loss = 0.20932669
Iteration 3824, loss = 0.20924390
Iteration 3825, loss = 0.20916222
Iteration 3826, loss = 0.20907974
Iteration 3827, loss = 0.20899749
Iteration 3828, loss = 0.20891684
Iteration 3829, loss = 0.20883411
Iteration 3830, loss = 0.20875285
Iteration 3831, loss = 0.20866970
Iteration 3832, loss = 0.20858861
Iteration 3833, loss = 0.20850724
Iteration 3834, loss = 0.20842554
Iteration 3835, loss = 0.20834394
Iteration 3836, loss = 0.20826378
Iteration 3837, loss = 0.20818050
Iteration 3838, loss = 0.20809988
Iteration 3839, loss = 0.20801742
Iteration 3840, loss = 0.20793666
Iteration 3841, loss = 0.20785529
Iteration 3842, loss = 0.20777442
Iteration 3843, loss = 0.20769238
Iteration 3844, loss = 0.20761140
Iteration 3845, loss = 0.20752998
Iteration 3846, loss = 0.20744911
Iteration 3847, loss = 0.20736891
Iteration 3848, loss = 0.20728735
Iteration 3849, loss = 0.20720706
Iteration 3850, loss = 0.20712519
Iteration 3851, loss = 0.20704489
Iteration 3852, loss = 0.20696364
Iteration 3853, loss = 0.20688431
Iteration 3854, loss = 0.20680408
Iteration 3855, loss = 0.20672258
Iteration 3856, loss = 0.20664237
Iteration 3857, loss = 0.20656211
Iteration 3858, loss = 0.20648205
Iteration 3859, loss = 0.20640205
Iteration 3860, loss = 0.20632178
Iteration 3861, loss = 0.20624202
Iteration 3862, loss = 0.20616218
Iteration 3863, loss = 0.20608228
Iteration 3864, loss = 0.20600221
Iteration 3865, loss = 0.20592199
Iteration 3866, loss = 0.20584255
Iteration 3867, loss = 0.20576279
Iteration 3868, loss = 0.20568380
Iteration 3869, loss = 0.20560396
Iteration 3870, loss = 0.20552527
Iteration 3871, loss = 0.20544567
Iteration 3872, loss = 0.20536555
Iteration 3873, loss = 0.20528585
Iteration 3874, loss = 0.20520787
Iteration 3875, loss = 0.20512842
Iteration 3876, loss = 0.20504882
Iteration 3877, loss = 0.20496943
Iteration 3878, loss = 0.20489057
Iteration 3879, loss = 0.20481092
Iteration 3880, loss = 0.20473318
Iteration 3881, loss = 0.20465369
Iteration 3882, loss = 0.20457519
Iteration 3883, loss = 0.20449581
Iteration 3884, loss = 0.20441799
Iteration 3885, loss = 0.20433837
Iteration 3886, loss = 0.20426020
Iteration 3887, loss = 0.20418096
Iteration 3888, loss = 0.20410156
Iteration 3889, loss = 0.20402405
Iteration 3890, loss = 0.20394575
Iteration 3891, loss = 0.20386710
Iteration 3892, loss = 0.20378899
Iteration 3893, loss = 0.20370987
Iteration 3894, loss = 0.20363194
Iteration 3895, loss = 0.20355358
Iteration 3896, loss = 0.20347482
Iteration 3897, loss = 0.20339677
Iteration 3898, loss = 0.20331968
Iteration 3899, loss = 0.20323986
Iteration 3900, loss = 0.20316376
Iteration 3901, loss = 0.20308449
Iteration 3902, loss = 0.20300679
Iteration 3903, loss = 0.20292922
Iteration 3904, loss = 0.20285097
Iteration 3905, loss = 0.20277353
Iteration 3906, loss = 0.20269544
Iteration 3907, loss = 0.20261739
Iteration 3908, loss = 0.20254010
Iteration 3909, loss = 0.20246205
Iteration 3910, loss = 0.20238469
Iteration 3911, loss = 0.20230705
Iteration 3912, loss = 0.20222896
Iteration 3913, loss = 0.20215176
Iteration 3914, loss = 0.20207396
Iteration 3915, loss = 0.20199780
Iteration 3916, loss = 0.20191963
Iteration 3917, loss = 0.20184278
Iteration 3918, loss = 0.20176530
Iteration 3919, loss = 0.20168780
Iteration 3920, loss = 0.20161066
Iteration 3921, loss = 0.20153276
Iteration 3922, loss = 0.20145697
Iteration 3923, loss = 0.20137860
Iteration 3924, loss = 0.20130175
Iteration 3925, loss = 0.20122472
Iteration 3926, loss = 0.20114791
Iteration 3927, loss = 0.20107208
Iteration 3928, loss = 0.20099423
Iteration 3929, loss = 0.20091720
Iteration 3930, loss = 0.20084002
Iteration 3931, loss = 0.20076504
Iteration 3932, loss = 0.20068757
Iteration 3933, loss = 0.20061030
Iteration 3934, loss = 0.20053469
Iteration 3935, loss = 0.20045820
Iteration 3936, loss = 0.20038318
Iteration 3937, loss = 0.20030632
Iteration 3938, loss = 0.20022984
Iteration 3939, loss = 0.20015434
Iteration 3940, loss = 0.20007765
Iteration 3941, loss = 0.20000146
Iteration 3942, loss = 0.19992560
Iteration 3943, loss = 0.19985040
Iteration 3944, loss = 0.19977427
Iteration 3945, loss = 0.19969825
Iteration 3946, loss = 0.19962381
Iteration 3947, loss = 0.19954727
Iteration 3948, loss = 0.19947238
Iteration 3949, loss = 0.19939657
Iteration 3950, loss = 0.19932124
Iteration 3951, loss = 0.19924671
Iteration 3952, loss = 0.19917125
Iteration 3953, loss = 0.19909485
Iteration 3954, loss = 0.19901950
Iteration 3955, loss = 0.19894458
Iteration 3956, loss = 0.19886995
Iteration 3957, loss = 0.19879482
Iteration 3958, loss = 0.19871987
Iteration 3959, loss = 0.19864429
Iteration 3960, loss = 0.19856924
Iteration 3961, loss = 0.19849464
Iteration 3962, loss = 0.19841912
Iteration 3963, loss = 0.19834463
Iteration 3964, loss = 0.19827060
Iteration 3965, loss = 0.19819590
Iteration 3966, loss = 0.19812046
Iteration 3967, loss = 0.19804539
Iteration 3968, loss = 0.19797234
Iteration 3969, loss = 0.19789588
Iteration 3970, loss = 0.19782154
Iteration 3971, loss = 0.19774743
Iteration 3972, loss = 0.19767315
Iteration 3973, loss = 0.19759936
Iteration 3974, loss = 0.19752389
Iteration 3975, loss = 0.19744945
Iteration 3976, loss = 0.19737581
Iteration 3977, loss = 0.19730138
Iteration 3978, loss = 0.19722807
Iteration 3979, loss = 0.19715300
Iteration 3980, loss = 0.19707974
Iteration 3981, loss = 0.19700512
Iteration 3982, loss = 0.19693293
Iteration 3983, loss = 0.19685812
Iteration 3984, loss = 0.19678388
Iteration 3985, loss = 0.19671085
Iteration 3986, loss = 0.19663674
Iteration 3987, loss = 0.19656312
Iteration 3988, loss = 0.19648980
Iteration 3989, loss = 0.19641637
Iteration 3990, loss = 0.19634259
Iteration 3991, loss = 0.19626916
Iteration 3992, loss = 0.19619550
Iteration 3993, loss = 0.19612215
Iteration 3994, loss = 0.19604930
Iteration 3995, loss = 0.19597560
Iteration 3996, loss = 0.19590178
Iteration 3997, loss = 0.19582800
Iteration 3998, loss = 0.19575507
Iteration 3999, loss = 0.19568169
Iteration 4000, loss = 0.19560837
Iteration 4001, loss = 0.19553510
Iteration 4002, loss = 0.19546191
Iteration 4003, loss = 0.19538822
Iteration 4004, loss = 0.19531483
Iteration 4005, loss = 0.19524214
Iteration 4006, loss = 0.19516871
Iteration 4007, loss = 0.19509682
Iteration 4008, loss = 0.19502376
Iteration 4009, loss = 0.19495023
Iteration 4010, loss = 0.19487818
Iteration 4011, loss = 0.19480511
Iteration 4012, loss = 0.19473220
Iteration 4013, loss = 0.19465932
Iteration 4014, loss = 0.19458733
Iteration 4015, loss = 0.19451531
Iteration 4016, loss = 0.19444247
Iteration 4017, loss = 0.19437011
Iteration 4018, loss = 0.19429755
Iteration 4019, loss = 0.19422540
Iteration 4020, loss = 0.19415286
Iteration 4021, loss = 0.19408005
Iteration 4022, loss = 0.19400811
Iteration 4023, loss = 0.19393618
Iteration 4024, loss = 0.19386423
Iteration 4025, loss = 0.19379166
Iteration 4026, loss = 0.19372060
Iteration 4027, loss = 0.19364823
Iteration 4028, loss = 0.19357687
Iteration 4029, loss = 0.19350466
Iteration 4030, loss = 0.19343312
Iteration 4031, loss = 0.19336189
Iteration 4032, loss = 0.19329030
Iteration 4033, loss = 0.19321847
Iteration 4034, loss = 0.19314740
Iteration 4035, loss = 0.19307544
Iteration 4036, loss = 0.19300463
Iteration 4037, loss = 0.19293324
Iteration 4038, loss = 0.19286247
Iteration 4039, loss = 0.19279097
Iteration 4040, loss = 0.19271887
Iteration 4041, loss = 0.19264771
Iteration 4042, loss = 0.19257738
Iteration 4043, loss = 0.19250496
Iteration 4044, loss = 0.19243424
Iteration 4045, loss = 0.19236308
Iteration 4046, loss = 0.19229208
Iteration 4047, loss = 0.19222082
Iteration 4048, loss = 0.19214996
Iteration 4049, loss = 0.19207835
Iteration 4050, loss = 0.19200823
Iteration 4051, loss = 0.19193718
Iteration 4052, loss = 0.19186680
Iteration 4053, loss = 0.19179631
Iteration 4054, loss = 0.19172523
Iteration 4055, loss = 0.19165547
Iteration 4056, loss = 0.19158478
Iteration 4057, loss = 0.19151401
Iteration 4058, loss = 0.19144449
Iteration 4059, loss = 0.19137281
Iteration 4060, loss = 0.19130358
Iteration 4061, loss = 0.19123338
Iteration 4062, loss = 0.19116268
Iteration 4063, loss = 0.19109293
Iteration 4064, loss = 0.19102328
Iteration 4065, loss = 0.19095410
Iteration 4066, loss = 0.19088325
Iteration 4067, loss = 0.19081344
Iteration 4068, loss = 0.19074395
Iteration 4069, loss = 0.19067421
Iteration 4070, loss = 0.19060386
Iteration 4071, loss = 0.19053544
Iteration 4072, loss = 0.19046445
Iteration 4073, loss = 0.19039560
Iteration 4074, loss = 0.19032588
Iteration 4075, loss = 0.19025687
Iteration 4076, loss = 0.19018696
Iteration 4077, loss = 0.19011715
Iteration 4078, loss = 0.19004950
Iteration 4079, loss = 0.18997984
Iteration 4080, loss = 0.18991045
Iteration 4081, loss = 0.18984115
Iteration 4082, loss = 0.18977220
Iteration 4083, loss = 0.18970398
Iteration 4084, loss = 0.18963377
Iteration 4085, loss = 0.18956628
Iteration 4086, loss = 0.18949622
Iteration 4087, loss = 0.18942796
Iteration 4088, loss = 0.18935858
Iteration 4089, loss = 0.18929047
Iteration 4090, loss = 0.18922121
Iteration 4091, loss = 0.18915235
Iteration 4092, loss = 0.18908373
Iteration 4093, loss = 0.18901473
Iteration 4094, loss = 0.18894566
Iteration 4095, loss = 0.18887675
Iteration 4096, loss = 0.18880855
Iteration 4097, loss = 0.18873927
Iteration 4098, loss = 0.18867034
Iteration 4099, loss = 0.18860150
Iteration 4100, loss = 0.18853394
Iteration 4101, loss = 0.18846605
Iteration 4102, loss = 0.18839626
Iteration 4103, loss = 0.18832818
Iteration 4104, loss = 0.18826007
Iteration 4105, loss = 0.18819239
Iteration 4106, loss = 0.18812454
Iteration 4107, loss = 0.18805527
Iteration 4108, loss = 0.18798767
Iteration 4109, loss = 0.18791951
Iteration 4110, loss = 0.18785216
Iteration 4111, loss = 0.18778393
Iteration 4112, loss = 0.18771646
Iteration 4113, loss = 0.18764782
Iteration 4114, loss = 0.18758014
Iteration 4115, loss = 0.18751256
Iteration 4116, loss = 0.18744411
Iteration 4117, loss = 0.18737624
Iteration 4118, loss = 0.18730928
Iteration 4119, loss = 0.18724102
Iteration 4120, loss = 0.18717309
Iteration 4121, loss = 0.18710526
Iteration 4122, loss = 0.18703751
Iteration 4123, loss = 0.18696947
Iteration 4124, loss = 0.18690246
Iteration 4125, loss = 0.18683547
Iteration 4126, loss = 0.18676729
Iteration 4127, loss = 0.18669953
Iteration 4128, loss = 0.18663221
Iteration 4129, loss = 0.18656521
Iteration 4130, loss = 0.18649716
Iteration 4131, loss = 0.18643020
Iteration 4132, loss = 0.18636227
Iteration 4133, loss = 0.18629544
Iteration 4134, loss = 0.18622884
Iteration 4135, loss = 0.18616141
Iteration 4136, loss = 0.18609478
Iteration 4137, loss = 0.18602803
Iteration 4138, loss = 0.18596054
Iteration 4139, loss = 0.18589476
Iteration 4140, loss = 0.18582704
Iteration 4141, loss = 0.18576071
Iteration 4142, loss = 0.18569326
Iteration 4143, loss = 0.18562671
Iteration 4144, loss = 0.18556002
Iteration 4145, loss = 0.18549299
Iteration 4146, loss = 0.18542665
Iteration 4147, loss = 0.18536056
Iteration 4148, loss = 0.18529415
Iteration 4149, loss = 0.18522808
Iteration 4150, loss = 0.18516195
Iteration 4151, loss = 0.18509564
Iteration 4152, loss = 0.18502868
Iteration 4153, loss = 0.18496340
Iteration 4154, loss = 0.18489702
Iteration 4155, loss = 0.18483188
Iteration 4156, loss = 0.18476535
Iteration 4157, loss = 0.18469909
Iteration 4158, loss = 0.18463421
Iteration 4159, loss = 0.18456817
Iteration 4160, loss = 0.18450274
Iteration 4161, loss = 0.18443775
Iteration 4162, loss = 0.18437097
Iteration 4163, loss = 0.18430545
Iteration 4164, loss = 0.18424064
Iteration 4165, loss = 0.18417532
Iteration 4166, loss = 0.18410967
Iteration 4167, loss = 0.18404331
Iteration 4168, loss = 0.18397866
Iteration 4169, loss = 0.18391252
Iteration 4170, loss = 0.18384763
Iteration 4171, loss = 0.18378159
Iteration 4172, loss = 0.18371761
Iteration 4173, loss = 0.18365154
Iteration 4174, loss = 0.18358660
Iteration 4175, loss = 0.18352089
Iteration 4176, loss = 0.18345608
Iteration 4177, loss = 0.18339050
Iteration 4178, loss = 0.18332522
Iteration 4179, loss = 0.18326015
Iteration 4180, loss = 0.18319609
Iteration 4181, loss = 0.18313031
Iteration 4182, loss = 0.18306582
Iteration 4183, loss = 0.18300019
Iteration 4184, loss = 0.18293625
Iteration 4185, loss = 0.18287137
Iteration 4186, loss = 0.18280622
Iteration 4187, loss = 0.18274140
Iteration 4188, loss = 0.18267668
Iteration 4189, loss = 0.18261299
Iteration 4190, loss = 0.18254786
Iteration 4191, loss = 0.18248298
Iteration 4192, loss = 0.18241909
Iteration 4193, loss = 0.18235477
Iteration 4194, loss = 0.18229028
Iteration 4195, loss = 0.18222486
Iteration 4196, loss = 0.18216178
Iteration 4197, loss = 0.18209623
Iteration 4198, loss = 0.18203267
Iteration 4199, loss = 0.18196803
Iteration 4200, loss = 0.18190327
Iteration 4201, loss = 0.18183958
Iteration 4202, loss = 0.18177494
Iteration 4203, loss = 0.18171094
Iteration 4204, loss = 0.18164656
Iteration 4205, loss = 0.18158293
Iteration 4206, loss = 0.18151783
Iteration 4207, loss = 0.18145466
Iteration 4208, loss = 0.18139072
Iteration 4209, loss = 0.18132689
Iteration 4210, loss = 0.18126318
Iteration 4211, loss = 0.18119839
Iteration 4212, loss = 0.18113586
Iteration 4213, loss = 0.18107122
Iteration 4214, loss = 0.18100760
Iteration 4215, loss = 0.18094408
Iteration 4216, loss = 0.18087978
Iteration 4217, loss = 0.18081815
Iteration 4218, loss = 0.18075255
Iteration 4219, loss = 0.18068983
Iteration 4220, loss = 0.18062644
Iteration 4221, loss = 0.18056270
Iteration 4222, loss = 0.18049983
Iteration 4223, loss = 0.18043667
Iteration 4224, loss = 0.18037209
Iteration 4225, loss = 0.18030944
Iteration 4226, loss = 0.18024669
Iteration 4227, loss = 0.18018272
Iteration 4228, loss = 0.18012037
Iteration 4229, loss = 0.18005580
Iteration 4230, loss = 0.17999328
Iteration 4231, loss = 0.17993046
Iteration 4232, loss = 0.17986599
Iteration 4233, loss = 0.17980394
Iteration 4234, loss = 0.17974116
Iteration 4235, loss = 0.17967773
Iteration 4236, loss = 0.17961502
Iteration 4237, loss = 0.17955231
Iteration 4238, loss = 0.17949003
Iteration 4239, loss = 0.17942656
Iteration 4240, loss = 0.17936420
Iteration 4241, loss = 0.17930211
Iteration 4242, loss = 0.17923791
Iteration 4243, loss = 0.17917511
Iteration 4244, loss = 0.17911249
Iteration 4245, loss = 0.17905098
Iteration 4246, loss = 0.17898757
Iteration 4247, loss = 0.17892589
Iteration 4248, loss = 0.17886274
Iteration 4249, loss = 0.17880097
Iteration 4250, loss = 0.17873836
Iteration 4251, loss = 0.17867635
Iteration 4252, loss = 0.17861407
Iteration 4253, loss = 0.17855222
Iteration 4254, loss = 0.17848991
Iteration 4255, loss = 0.17842895
Iteration 4256, loss = 0.17836686
Iteration 4257, loss = 0.17830427
Iteration 4258, loss = 0.17824339
Iteration 4259, loss = 0.17818070
Iteration 4260, loss = 0.17811882
Iteration 4261, loss = 0.17805703
Iteration 4262, loss = 0.17799496
Iteration 4263, loss = 0.17793349
Iteration 4264, loss = 0.17787207
Iteration 4265, loss = 0.17780978
Iteration 4266, loss = 0.17774854
Iteration 4267, loss = 0.17768635
Iteration 4268, loss = 0.17762477
Iteration 4269, loss = 0.17756322
Iteration 4270, loss = 0.17750086
Iteration 4271, loss = 0.17743979
Iteration 4272, loss = 0.17737964
Iteration 4273, loss = 0.17731804
Iteration 4274, loss = 0.17725623
Iteration 4275, loss = 0.17719515
Iteration 4276, loss = 0.17713349
Iteration 4277, loss = 0.17707247
Iteration 4278, loss = 0.17701154
Iteration 4279, loss = 0.17695074
Iteration 4280, loss = 0.17689054
Iteration 4281, loss = 0.17682886
Iteration 4282, loss = 0.17676805
Iteration 4283, loss = 0.17670652
Iteration 4284, loss = 0.17664654
Iteration 4285, loss = 0.17658529
Iteration 4286, loss = 0.17652505
Iteration 4287, loss = 0.17646363
Iteration 4288, loss = 0.17640359
Iteration 4289, loss = 0.17634231
Iteration 4290, loss = 0.17628278
Iteration 4291, loss = 0.17622114
Iteration 4292, loss = 0.17616182
Iteration 4293, loss = 0.17610064
Iteration 4294, loss = 0.17604026
Iteration 4295, loss = 0.17597943
Iteration 4296, loss = 0.17591868
Iteration 4297, loss = 0.17585813
Iteration 4298, loss = 0.17579783
Iteration 4299, loss = 0.17573731
Iteration 4300, loss = 0.17567640
Iteration 4301, loss = 0.17561708
Iteration 4302, loss = 0.17555594
Iteration 4303, loss = 0.17549596
Iteration 4304, loss = 0.17543557
Iteration 4305, loss = 0.17537489
Iteration 4306, loss = 0.17531457
Iteration 4307, loss = 0.17525492
Iteration 4308, loss = 0.17519394
Iteration 4309, loss = 0.17513474
Iteration 4310, loss = 0.17507494
Iteration 4311, loss = 0.17501521
Iteration 4312, loss = 0.17495424
Iteration 4313, loss = 0.17489526
Iteration 4314, loss = 0.17483478
Iteration 4315, loss = 0.17477516
Iteration 4316, loss = 0.17471577
Iteration 4317, loss = 0.17465622
Iteration 4318, loss = 0.17459539
Iteration 4319, loss = 0.17453669
Iteration 4320, loss = 0.17447661
Iteration 4321, loss = 0.17441711
Iteration 4322, loss = 0.17435777
Iteration 4323, loss = 0.17429843
Iteration 4324, loss = 0.17423886
Iteration 4325, loss = 0.17417945
Iteration 4326, loss = 0.17412025
Iteration 4327, loss = 0.17406077
Iteration 4328, loss = 0.17400197
Iteration 4329, loss = 0.17394235
Iteration 4330, loss = 0.17388285
Iteration 4331, loss = 0.17382334
Iteration 4332, loss = 0.17376500
Iteration 4333, loss = 0.17370561
Iteration 4334, loss = 0.17364647
Iteration 4335, loss = 0.17358700
Iteration 4336, loss = 0.17352854
Iteration 4337, loss = 0.17346975
Iteration 4338, loss = 0.17341048
Iteration 4339, loss = 0.17335089
Iteration 4340, loss = 0.17329304
Iteration 4341, loss = 0.17323324
Iteration 4342, loss = 0.17317402
Iteration 4343, loss = 0.17311585
Iteration 4344, loss = 0.17305762
Iteration 4345, loss = 0.17299844
Iteration 4346, loss = 0.17293994
Iteration 4347, loss = 0.17288115
Iteration 4348, loss = 0.17282202
Iteration 4349, loss = 0.17276330
Iteration 4350, loss = 0.17270535
Iteration 4351, loss = 0.17264694
Iteration 4352, loss = 0.17258813
Iteration 4353, loss = 0.17253010
Iteration 4354, loss = 0.17247097
Iteration 4355, loss = 0.17241297
Iteration 4356, loss = 0.17235495
Iteration 4357, loss = 0.17229640
Iteration 4358, loss = 0.17223730
Iteration 4359, loss = 0.17217924
Iteration 4360, loss = 0.17212078
Iteration 4361, loss = 0.17206253
Iteration 4362, loss = 0.17200461
Iteration 4363, loss = 0.17194624
Iteration 4364, loss = 0.17188827
Iteration 4365, loss = 0.17183095
Iteration 4366, loss = 0.17177234
Iteration 4367, loss = 0.17171443
Iteration 4368, loss = 0.17165702
Iteration 4369, loss = 0.17159811
Iteration 4370, loss = 0.17154062
Iteration 4371, loss = 0.17148257
Iteration 4372, loss = 0.17142472
Iteration 4373, loss = 0.17136711
Iteration 4374, loss = 0.17130855
Iteration 4375, loss = 0.17125101
Iteration 4376, loss = 0.17119344
Iteration 4377, loss = 0.17113589
Iteration 4378, loss = 0.17107852
Iteration 4379, loss = 0.17102028
Iteration 4380, loss = 0.17096235
Iteration 4381, loss = 0.17090503
Iteration 4382, loss = 0.17084745
Iteration 4383, loss = 0.17079004
Iteration 4384, loss = 0.17073304
Iteration 4385, loss = 0.17067524
Iteration 4386, loss = 0.17061719
Iteration 4387, loss = 0.17056006
Iteration 4388, loss = 0.17050275
Iteration 4389, loss = 0.17044596
Iteration 4390, loss = 0.17038900
Iteration 4391, loss = 0.17033143
Iteration 4392, loss = 0.17027380
Iteration 4393, loss = 0.17021805
Iteration 4394, loss = 0.17016106
Iteration 4395, loss = 0.17010422
Iteration 4396, loss = 0.17004675
Iteration 4397, loss = 0.16999067
Iteration 4398, loss = 0.16993322
Iteration 4399, loss = 0.16987640
Iteration 4400, loss = 0.16981970
Iteration 4401, loss = 0.16976402
Iteration 4402, loss = 0.16970658
Iteration 4403, loss = 0.16964925
Iteration 4404, loss = 0.16959286
Iteration 4405, loss = 0.16953671
Iteration 4406, loss = 0.16947952
Iteration 4407, loss = 0.16942266
Iteration 4408, loss = 0.16936662
Iteration 4409, loss = 0.16930993
Iteration 4410, loss = 0.16925369
Iteration 4411, loss = 0.16919637
Iteration 4412, loss = 0.16914027
Iteration 4413, loss = 0.16908376
Iteration 4414, loss = 0.16902689
Iteration 4415, loss = 0.16897070
Iteration 4416, loss = 0.16891434
Iteration 4417, loss = 0.16885759
Iteration 4418, loss = 0.16880116
Iteration 4419, loss = 0.16874543
Iteration 4420, loss = 0.16868879
Iteration 4421, loss = 0.16863250
Iteration 4422, loss = 0.16857662
Iteration 4423, loss = 0.16852079
Iteration 4424, loss = 0.16846403
Iteration 4425, loss = 0.16840895
Iteration 4426, loss = 0.16835196
Iteration 4427, loss = 0.16829701
Iteration 4428, loss = 0.16824050
Iteration 4429, loss = 0.16818458
Iteration 4430, loss = 0.16812888
Iteration 4431, loss = 0.16807298
Iteration 4432, loss = 0.16801743
Iteration 4433, loss = 0.16796227
Iteration 4434, loss = 0.16790661
Iteration 4435, loss = 0.16785067
Iteration 4436, loss = 0.16779516
Iteration 4437, loss = 0.16773996
Iteration 4438, loss = 0.16768388
Iteration 4439, loss = 0.16762867
Iteration 4440, loss = 0.16757350
Iteration 4441, loss = 0.16751874
Iteration 4442, loss = 0.16746195
Iteration 4443, loss = 0.16740787
Iteration 4444, loss = 0.16735198
Iteration 4445, loss = 0.16729689
Iteration 4446, loss = 0.16724198
Iteration 4447, loss = 0.16718652
Iteration 4448, loss = 0.16713093
Iteration 4449, loss = 0.16707670
Iteration 4450, loss = 0.16702145
Iteration 4451, loss = 0.16696601
Iteration 4452, loss = 0.16691246
Iteration 4453, loss = 0.16685696
Iteration 4454, loss = 0.16680160
Iteration 4455, loss = 0.16674648
Iteration 4456, loss = 0.16669172
Iteration 4457, loss = 0.16663732
Iteration 4458, loss = 0.16658214
Iteration 4459, loss = 0.16652719
Iteration 4460, loss = 0.16647324
Iteration 4461, loss = 0.16641779
Iteration 4462, loss = 0.16636264
Iteration 4463, loss = 0.16630830
Iteration 4464, loss = 0.16625339
Iteration 4465, loss = 0.16619874
Iteration 4466, loss = 0.16614393
Iteration 4467, loss = 0.16608915
Iteration 4468, loss = 0.16603527
Iteration 4469, loss = 0.16598038
Iteration 4470, loss = 0.16592643
Iteration 4471, loss = 0.16587234
Iteration 4472, loss = 0.16581767
Iteration 4473, loss = 0.16576319
Iteration 4474, loss = 0.16570862
Iteration 4475, loss = 0.16565516
Iteration 4476, loss = 0.16560157
Iteration 4477, loss = 0.16554690
Iteration 4478, loss = 0.16549310
Iteration 4479, loss = 0.16543880
Iteration 4480, loss = 0.16538446
Iteration 4481, loss = 0.16533063
Iteration 4482, loss = 0.16527685
Iteration 4483, loss = 0.16522296
Iteration 4484, loss = 0.16516938
Iteration 4485, loss = 0.16511473
Iteration 4486, loss = 0.16506197
Iteration 4487, loss = 0.16500837
Iteration 4488, loss = 0.16495416
Iteration 4489, loss = 0.16490081
Iteration 4490, loss = 0.16484788
Iteration 4491, loss = 0.16479413
Iteration 4492, loss = 0.16474078
Iteration 4493, loss = 0.16468639
Iteration 4494, loss = 0.16463373
Iteration 4495, loss = 0.16457972
Iteration 4496, loss = 0.16452712
Iteration 4497, loss = 0.16447276
Iteration 4498, loss = 0.16441987
Iteration 4499, loss = 0.16436617
Iteration 4500, loss = 0.16431246
Iteration 4501, loss = 0.16425948
Iteration 4502, loss = 0.16420577
Iteration 4503, loss = 0.16415326
Iteration 4504, loss = 0.16409925
Iteration 4505, loss = 0.16404624
Iteration 4506, loss = 0.16399288
Iteration 4507, loss = 0.16394005
Iteration 4508, loss = 0.16388691
Iteration 4509, loss = 0.16383269
Iteration 4510, loss = 0.16377989
Iteration 4511, loss = 0.16372694
Iteration 4512, loss = 0.16367416
Iteration 4513, loss = 0.16362078
Iteration 4514, loss = 0.16356752
Iteration 4515, loss = 0.16351490
Iteration 4516, loss = 0.16346143
Iteration 4517, loss = 0.16340827
Iteration 4518, loss = 0.16335593
Iteration 4519, loss = 0.16330333
Iteration 4520, loss = 0.16325046
Iteration 4521, loss = 0.16319735
Iteration 4522, loss = 0.16314445
Iteration 4523, loss = 0.16309210
Iteration 4524, loss = 0.16303900
Iteration 4525, loss = 0.16298649
Iteration 4526, loss = 0.16293371
Iteration 4527, loss = 0.16288124
Iteration 4528, loss = 0.16282853
Iteration 4529, loss = 0.16277632
Iteration 4530, loss = 0.16272335
Iteration 4531, loss = 0.16267068
Iteration 4532, loss = 0.16261865
Iteration 4533, loss = 0.16256572
Iteration 4534, loss = 0.16251343
Iteration 4535, loss = 0.16246114
Iteration 4536, loss = 0.16240875
Iteration 4537, loss = 0.16235672
Iteration 4538, loss = 0.16230449
Iteration 4539, loss = 0.16225240
Iteration 4540, loss = 0.16220036
Iteration 4541, loss = 0.16214760
Iteration 4542, loss = 0.16209472
Iteration 4543, loss = 0.16204358
Iteration 4544, loss = 0.16199066
Iteration 4545, loss = 0.16193893
Iteration 4546, loss = 0.16188648
Iteration 4547, loss = 0.16183556
Iteration 4548, loss = 0.16178346
Iteration 4549, loss = 0.16173136
Iteration 4550, loss = 0.16167916
Iteration 4551, loss = 0.16162732
Iteration 4552, loss = 0.16157595
Iteration 4553, loss = 0.16152388
Iteration 4554, loss = 0.16147244
Iteration 4555, loss = 0.16141990
Iteration 4556, loss = 0.16136807
Iteration 4557, loss = 0.16131676
Iteration 4558, loss = 0.16126509
Iteration 4559, loss = 0.16121418
Iteration 4560, loss = 0.16116194
Iteration 4561, loss = 0.16111043
Iteration 4562, loss = 0.16105904
Iteration 4563, loss = 0.16100695
Iteration 4564, loss = 0.16095536
Iteration 4565, loss = 0.16090425
Iteration 4566, loss = 0.16085243
Iteration 4567, loss = 0.16080121
Iteration 4568, loss = 0.16074962
Iteration 4569, loss = 0.16069850
Iteration 4570, loss = 0.16064731
Iteration 4571, loss = 0.16059532
Iteration 4572, loss = 0.16054493
Iteration 4573, loss = 0.16049279
Iteration 4574, loss = 0.16044221
Iteration 4575, loss = 0.16039083
Iteration 4576, loss = 0.16034012
Iteration 4577, loss = 0.16028938
Iteration 4578, loss = 0.16023745
Iteration 4579, loss = 0.16018683
Iteration 4580, loss = 0.16013588
Iteration 4581, loss = 0.16008491
Iteration 4582, loss = 0.16003410
Iteration 4583, loss = 0.15998342
Iteration 4584, loss = 0.15993355
Iteration 4585, loss = 0.15988199
Iteration 4586, loss = 0.15983187
Iteration 4587, loss = 0.15978091
Iteration 4588, loss = 0.15972999
Iteration 4589, loss = 0.15967982
Iteration 4590, loss = 0.15962908
Iteration 4591, loss = 0.15957771
Iteration 4592, loss = 0.15952771
Iteration 4593, loss = 0.15947769
Iteration 4594, loss = 0.15942677
Iteration 4595, loss = 0.15937576
Iteration 4596, loss = 0.15932561
Iteration 4597, loss = 0.15927582
Iteration 4598, loss = 0.15922441
Iteration 4599, loss = 0.15917449
Iteration 4600, loss = 0.15912432
Iteration 4601, loss = 0.15907398
Iteration 4602, loss = 0.15902376
Iteration 4603, loss = 0.15897361
Iteration 4604, loss = 0.15892322
Iteration 4605, loss = 0.15887298
Iteration 4606, loss = 0.15882311
Iteration 4607, loss = 0.15877261
Iteration 4608, loss = 0.15872253
Iteration 4609, loss = 0.15867244
Iteration 4610, loss = 0.15862182
Iteration 4611, loss = 0.15857187
Iteration 4612, loss = 0.15852220
Iteration 4613, loss = 0.15847149
Iteration 4614, loss = 0.15842166
Iteration 4615, loss = 0.15837177
Iteration 4616, loss = 0.15832200
Iteration 4617, loss = 0.15827231
Iteration 4618, loss = 0.15822241
Iteration 4619, loss = 0.15817229
Iteration 4620, loss = 0.15812335
Iteration 4621, loss = 0.15807347
Iteration 4622, loss = 0.15802371
Iteration 4623, loss = 0.15797434
Iteration 4624, loss = 0.15792422
Iteration 4625, loss = 0.15787629
Iteration 4626, loss = 0.15782552
Iteration 4627, loss = 0.15777670
Iteration 4628, loss = 0.15772650
Iteration 4629, loss = 0.15767797
Iteration 4630, loss = 0.15762814
Iteration 4631, loss = 0.15757925
Iteration 4632, loss = 0.15752928
Iteration 4633, loss = 0.15747994
Iteration 4634, loss = 0.15743145
Iteration 4635, loss = 0.15738168
Iteration 4636, loss = 0.15733274
Iteration 4637, loss = 0.15728307
Iteration 4638, loss = 0.15723386
Iteration 4639, loss = 0.15718488
Iteration 4640, loss = 0.15713580
Iteration 4641, loss = 0.15708615
Iteration 4642, loss = 0.15703760
Iteration 4643, loss = 0.15698815
Iteration 4644, loss = 0.15693881
Iteration 4645, loss = 0.15688964
Iteration 4646, loss = 0.15684041
Iteration 4647, loss = 0.15679155
Iteration 4648, loss = 0.15674289
Iteration 4649, loss = 0.15669321
Iteration 4650, loss = 0.15664407
Iteration 4651, loss = 0.15659594
Iteration 4652, loss = 0.15654740
Iteration 4653, loss = 0.15649818
Iteration 4654, loss = 0.15644936
Iteration 4655, loss = 0.15640066
Iteration 4656, loss = 0.15635263
Iteration 4657, loss = 0.15630385
Iteration 4658, loss = 0.15625548
Iteration 4659, loss = 0.15620620
Iteration 4660, loss = 0.15615738
Iteration 4661, loss = 0.15610943
Iteration 4662, loss = 0.15605987
Iteration 4663, loss = 0.15601160
Iteration 4664, loss = 0.15596304
Iteration 4665, loss = 0.15591430
Iteration 4666, loss = 0.15586587
Iteration 4667, loss = 0.15581789
Iteration 4668, loss = 0.15576921
Iteration 4669, loss = 0.15572052
Iteration 4670, loss = 0.15567193
Iteration 4671, loss = 0.15562392
Iteration 4672, loss = 0.15557552
Iteration 4673, loss = 0.15552713
Iteration 4674, loss = 0.15547876
Iteration 4675, loss = 0.15543073
Iteration 4676, loss = 0.15538292
Iteration 4677, loss = 0.15533435
Iteration 4678, loss = 0.15528622
Iteration 4679, loss = 0.15523788
Iteration 4680, loss = 0.15519009
Iteration 4681, loss = 0.15514139
Iteration 4682, loss = 0.15509389
Iteration 4683, loss = 0.15504552
Iteration 4684, loss = 0.15499794
Iteration 4685, loss = 0.15494972
Iteration 4686, loss = 0.15490244
Iteration 4687, loss = 0.15485397
Iteration 4688, loss = 0.15480675
Iteration 4689, loss = 0.15475798
Iteration 4690, loss = 0.15471090
Iteration 4691, loss = 0.15466332
Iteration 4692, loss = 0.15461555
Iteration 4693, loss = 0.15456773
Iteration 4694, loss = 0.15451982
Iteration 4695, loss = 0.15447227
Iteration 4696, loss = 0.15442427
Iteration 4697, loss = 0.15437750
Iteration 4698, loss = 0.15432982
Iteration 4699, loss = 0.15428201
Iteration 4700, loss = 0.15423430
Iteration 4701, loss = 0.15418696
Iteration 4702, loss = 0.15414003
Iteration 4703, loss = 0.15409220
Iteration 4704, loss = 0.15404493
Iteration 4705, loss = 0.15399758
Iteration 4706, loss = 0.15395056
Iteration 4707, loss = 0.15390362
Iteration 4708, loss = 0.15385576
Iteration 4709, loss = 0.15380920
Iteration 4710, loss = 0.15376181
Iteration 4711, loss = 0.15371436
Iteration 4712, loss = 0.15366669
Iteration 4713, loss = 0.15361963
Iteration 4714, loss = 0.15357225
Iteration 4715, loss = 0.15352565
Iteration 4716, loss = 0.15347793
Iteration 4717, loss = 0.15343111
Iteration 4718, loss = 0.15338354
Iteration 4719, loss = 0.15333727
Iteration 4720, loss = 0.15328996
Iteration 4721, loss = 0.15324289
Iteration 4722, loss = 0.15319673
Iteration 4723, loss = 0.15314957
Iteration 4724, loss = 0.15310246
Iteration 4725, loss = 0.15305524
Iteration 4726, loss = 0.15300860
Iteration 4727, loss = 0.15296201
Iteration 4728, loss = 0.15291444
Iteration 4729, loss = 0.15286777
Iteration 4730, loss = 0.15282114
Iteration 4731, loss = 0.15277437
Iteration 4732, loss = 0.15272758
Iteration 4733, loss = 0.15268075
Iteration 4734, loss = 0.15263384
Iteration 4735, loss = 0.15258722
Iteration 4736, loss = 0.15254120
Iteration 4737, loss = 0.15249464
Iteration 4738, loss = 0.15244761
Iteration 4739, loss = 0.15240092
Iteration 4740, loss = 0.15235472
Iteration 4741, loss = 0.15230834
Iteration 4742, loss = 0.15226178
Iteration 4743, loss = 0.15221551
Iteration 4744, loss = 0.15216879
Iteration 4745, loss = 0.15212273
Iteration 4746, loss = 0.15207598
Iteration 4747, loss = 0.15202992
Iteration 4748, loss = 0.15198343
Iteration 4749, loss = 0.15193834
Iteration 4750, loss = 0.15189079
Iteration 4751, loss = 0.15184533
Iteration 4752, loss = 0.15179904
Iteration 4753, loss = 0.15175238
Iteration 4754, loss = 0.15170677
Iteration 4755, loss = 0.15166073
Iteration 4756, loss = 0.15161521
Iteration 4757, loss = 0.15156903
Iteration 4758, loss = 0.15152334
Iteration 4759, loss = 0.15147670
Iteration 4760, loss = 0.15143104
Iteration 4761, loss = 0.15138561
Iteration 4762, loss = 0.15133963
Iteration 4763, loss = 0.15129398
Iteration 4764, loss = 0.15124844
Iteration 4765, loss = 0.15120267
Iteration 4766, loss = 0.15115649
Iteration 4767, loss = 0.15111087
Iteration 4768, loss = 0.15106539
Iteration 4769, loss = 0.15102020
Iteration 4770, loss = 0.15097415
Iteration 4771, loss = 0.15092887
Iteration 4772, loss = 0.15088287
Iteration 4773, loss = 0.15083745
Iteration 4774, loss = 0.15079220
Iteration 4775, loss = 0.15074657
Iteration 4776, loss = 0.15070119
Iteration 4777, loss = 0.15065569
Iteration 4778, loss = 0.15061053
Iteration 4779, loss = 0.15056480
Iteration 4780, loss = 0.15052023
Iteration 4781, loss = 0.15047415
Iteration 4782, loss = 0.15042956
Iteration 4783, loss = 0.15038403
Iteration 4784, loss = 0.15033863
Iteration 4785, loss = 0.15029340
Iteration 4786, loss = 0.15024900
Iteration 4787, loss = 0.15020316
Iteration 4788, loss = 0.15015850
Iteration 4789, loss = 0.15011343
Iteration 4790, loss = 0.15006798
Iteration 4791, loss = 0.15002314
Iteration 4792, loss = 0.14997875
Iteration 4793, loss = 0.14993394
Iteration 4794, loss = 0.14988908
Iteration 4795, loss = 0.14984310
Iteration 4796, loss = 0.14979890
Iteration 4797, loss = 0.14975401
Iteration 4798, loss = 0.14970884
Iteration 4799, loss = 0.14966459
Iteration 4800, loss = 0.14961949
Iteration 4801, loss = 0.14957528
Iteration 4802, loss = 0.14953003
Iteration 4803, loss = 0.14948498
Iteration 4804, loss = 0.14944049
Iteration 4805, loss = 0.14939617
Iteration 4806, loss = 0.14935148
Iteration 4807, loss = 0.14930641
Iteration 4808, loss = 0.14926184
Iteration 4809, loss = 0.14921745
Iteration 4810, loss = 0.14917229
Iteration 4811, loss = 0.14912728
Iteration 4812, loss = 0.14908294
Iteration 4813, loss = 0.14903871
Iteration 4814, loss = 0.14899393
Iteration 4815, loss = 0.14894971
Iteration 4816, loss = 0.14890549
Iteration 4817, loss = 0.14886088
Iteration 4818, loss = 0.14881681
Iteration 4819, loss = 0.14877263
Iteration 4820, loss = 0.14872778
Iteration 4821, loss = 0.14868359
Iteration 4822, loss = 0.14863910
Iteration 4823, loss = 0.14859606
Iteration 4824, loss = 0.14855105
Iteration 4825, loss = 0.14850706
Iteration 4826, loss = 0.14846283
Iteration 4827, loss = 0.14841904
Iteration 4828, loss = 0.14837505
Iteration 4829, loss = 0.14833114
Iteration 4830, loss = 0.14828721
Iteration 4831, loss = 0.14824289
Iteration 4832, loss = 0.14819863
Iteration 4833, loss = 0.14815476
Iteration 4834, loss = 0.14811087
Iteration 4835, loss = 0.14806729
Iteration 4836, loss = 0.14802299
Iteration 4837, loss = 0.14797926
Iteration 4838, loss = 0.14793533
Iteration 4839, loss = 0.14789197
Iteration 4840, loss = 0.14784793
Iteration 4841, loss = 0.14780428
Iteration 4842, loss = 0.14776005
Iteration 4843, loss = 0.14771629
Iteration 4844, loss = 0.14767284
Iteration 4845, loss = 0.14762890
Iteration 4846, loss = 0.14758560
Iteration 4847, loss = 0.14754135
Iteration 4848, loss = 0.14749795
Iteration 4849, loss = 0.14745438
Iteration 4850, loss = 0.14741033
Iteration 4851, loss = 0.14736723
Iteration 4852, loss = 0.14732271
Iteration 4853, loss = 0.14727996
Iteration 4854, loss = 0.14723623
Iteration 4855, loss = 0.14719271
Iteration 4856, loss = 0.14714903
Iteration 4857, loss = 0.14710621
Iteration 4858, loss = 0.14706223
Iteration 4859, loss = 0.14701895
Iteration 4860, loss = 0.14697607
Iteration 4861, loss = 0.14693239
Iteration 4862, loss = 0.14688905
Iteration 4863, loss = 0.14684585
Iteration 4864, loss = 0.14680246
Iteration 4865, loss = 0.14675982
Iteration 4866, loss = 0.14671605
Iteration 4867, loss = 0.14667345
Iteration 4868, loss = 0.14663037
Iteration 4869, loss = 0.14658673
Iteration 4870, loss = 0.14654355
Iteration 4871, loss = 0.14650055
Iteration 4872, loss = 0.14645735
Iteration 4873, loss = 0.14641456
Iteration 4874, loss = 0.14637121
Iteration 4875, loss = 0.14632823
Iteration 4876, loss = 0.14628531
Iteration 4877, loss = 0.14624217
Iteration 4878, loss = 0.14619926
Iteration 4879, loss = 0.14615645
Iteration 4880, loss = 0.14611377
Iteration 4881, loss = 0.14607185
Iteration 4882, loss = 0.14602741
Iteration 4883, loss = 0.14598467
Iteration 4884, loss = 0.14594280
Iteration 4885, loss = 0.14590006
Iteration 4886, loss = 0.14585672
Iteration 4887, loss = 0.14581450
Iteration 4888, loss = 0.14577113
Iteration 4889, loss = 0.14572852
Iteration 4890, loss = 0.14568572
Iteration 4891, loss = 0.14564310
Iteration 4892, loss = 0.14560054
Iteration 4893, loss = 0.14555788
Iteration 4894, loss = 0.14551519
Iteration 4895, loss = 0.14547285
Iteration 4896, loss = 0.14543072
Iteration 4897, loss = 0.14538703
Iteration 4898, loss = 0.14534502
Iteration 4899, loss = 0.14530237
Iteration 4900, loss = 0.14526043
Iteration 4901, loss = 0.14521746
Iteration 4902, loss = 0.14517468
Iteration 4903, loss = 0.14513246
Iteration 4904, loss = 0.14509055
Iteration 4905, loss = 0.14504754
Iteration 4906, loss = 0.14500586
Iteration 4907, loss = 0.14496337
Iteration 4908, loss = 0.14492085
Iteration 4909, loss = 0.14487861
Iteration 4910, loss = 0.14483679
Iteration 4911, loss = 0.14479467
Iteration 4912, loss = 0.14475206
Iteration 4913, loss = 0.14471043
Iteration 4914, loss = 0.14466775
Iteration 4915, loss = 0.14462592
Iteration 4916, loss = 0.14458408
Iteration 4917, loss = 0.14454165
Iteration 4918, loss = 0.14449954
Iteration 4919, loss = 0.14445836
Iteration 4920, loss = 0.14441624
Iteration 4921, loss = 0.14437406
Iteration 4922, loss = 0.14433246
Iteration 4923, loss = 0.14429038
Iteration 4924, loss = 0.14424874
Iteration 4925, loss = 0.14420689
Iteration 4926, loss = 0.14416515
Iteration 4927, loss = 0.14412286
Iteration 4928, loss = 0.14408186
Iteration 4929, loss = 0.14403999
Iteration 4930, loss = 0.14399792
Iteration 4931, loss = 0.14395616
Iteration 4932, loss = 0.14391435
Iteration 4933, loss = 0.14387272
Iteration 4934, loss = 0.14383087
Iteration 4935, loss = 0.14378958
Iteration 4936, loss = 0.14374773
Iteration 4937, loss = 0.14370636
Iteration 4938, loss = 0.14366492
Iteration 4939, loss = 0.14362311
Iteration 4940, loss = 0.14358150
Iteration 4941, loss = 0.14353977
Iteration 4942, loss = 0.14349840
Iteration 4943, loss = 0.14345712
Iteration 4944, loss = 0.14341590
Iteration 4945, loss = 0.14337392
Iteration 4946, loss = 0.14333259
Iteration 4947, loss = 0.14329140
Iteration 4948, loss = 0.14324978
Iteration 4949, loss = 0.14320903
Iteration 4950, loss = 0.14316717
Iteration 4951, loss = 0.14312611
Iteration 4952, loss = 0.14308516
Iteration 4953, loss = 0.14304379
Iteration 4954, loss = 0.14300231
Iteration 4955, loss = 0.14296108
Iteration 4956, loss = 0.14292040
Iteration 4957, loss = 0.14287908
Iteration 4958, loss = 0.14283821
Iteration 4959, loss = 0.14279724
Iteration 4960, loss = 0.14275520
Iteration 4961, loss = 0.14271472
Iteration 4962, loss = 0.14267378
Iteration 4963, loss = 0.14263211
Iteration 4964, loss = 0.14259186
Iteration 4965, loss = 0.14255062
Iteration 4966, loss = 0.14250975
Iteration 4967, loss = 0.14246884
Iteration 4968, loss = 0.14242804
Iteration 4969, loss = 0.14238706
Iteration 4970, loss = 0.14234664
Iteration 4971, loss = 0.14230551
Iteration 4972, loss = 0.14226431
Iteration 4973, loss = 0.14222356
Iteration 4974, loss = 0.14218344
Iteration 4975, loss = 0.14214314
Iteration 4976, loss = 0.14210197
Iteration 4977, loss = 0.14206102
Iteration 4978, loss = 0.14202060
Iteration 4979, loss = 0.14197955
Iteration 4980, loss = 0.14193897
Iteration 4981, loss = 0.14189856
Iteration 4982, loss = 0.14185815
Iteration 4983, loss = 0.14181743
Iteration 4984, loss = 0.14177721
Iteration 4985, loss = 0.14173627
Iteration 4986, loss = 0.14169601
Iteration 4987, loss = 0.14165512
Iteration 4988, loss = 0.14161596
Iteration 4989, loss = 0.14157493
Iteration 4990, loss = 0.14153426
Iteration 4991, loss = 0.14149359
Iteration 4992, loss = 0.14145404
Iteration 4993, loss = 0.14141353
Iteration 4994, loss = 0.14137330
Iteration 4995, loss = 0.14133306
Iteration 4996, loss = 0.14129232
Iteration 4997, loss = 0.14125190
Iteration 4998, loss = 0.14121244
Iteration 4999, loss = 0.14117214
Iteration 5000, loss = 0.14113155
Iteration 5001, loss = 0.14109097
Iteration 5002, loss = 0.14105126
Iteration 5003, loss = 0.14101166
Iteration 5004, loss = 0.14097112
Iteration 5005, loss = 0.14093091
Iteration 5006, loss = 0.14089118
Iteration 5007, loss = 0.14085065
Iteration 5008, loss = 0.14081004
Iteration 5009, loss = 0.14077023
Iteration 5010, loss = 0.14073058
Iteration 5011, loss = 0.14069053
Iteration 5012, loss = 0.14065047
Iteration 5013, loss = 0.14061024
Iteration 5014, loss = 0.14057048
Iteration 5015, loss = 0.14053049
Iteration 5016, loss = 0.14049040
Iteration 5017, loss = 0.14045129
Iteration 5018, loss = 0.14041127
Iteration 5019, loss = 0.14037142
Iteration 5020, loss = 0.14033188
Iteration 5021, loss = 0.14029188
Iteration 5022, loss = 0.14025214
Iteration 5023, loss = 0.14021255
Iteration 5024, loss = 0.14017262
Iteration 5025, loss = 0.14013299
Iteration 5026, loss = 0.14009305
Iteration 5027, loss = 0.14005363
Iteration 5028, loss = 0.14001419
Iteration 5029, loss = 0.13997522
Iteration 5030, loss = 0.13993506
Iteration 5031, loss = 0.13989595
Iteration 5032, loss = 0.13985598
Iteration 5033, loss = 0.13981636
Iteration 5034, loss = 0.13977760
Iteration 5035, loss = 0.13973772
Iteration 5036, loss = 0.13969896
Iteration 5037, loss = 0.13965932
Iteration 5038, loss = 0.13961956
Iteration 5039, loss = 0.13958016
Iteration 5040, loss = 0.13954107
Iteration 5041, loss = 0.13950187
Iteration 5042, loss = 0.13946277
Iteration 5043, loss = 0.13942344
Iteration 5044, loss = 0.13938415
Iteration 5045, loss = 0.13934564
Iteration 5046, loss = 0.13930619
Iteration 5047, loss = 0.13926715
Iteration 5048, loss = 0.13922798
Iteration 5049, loss = 0.13918888
Iteration 5050, loss = 0.13914999
Iteration 5051, loss = 0.13911087
Iteration 5052, loss = 0.13907145
Iteration 5053, loss = 0.13903241
Iteration 5054, loss = 0.13899382
Iteration 5055, loss = 0.13895407
Iteration 5056, loss = 0.13891540
Iteration 5057, loss = 0.13887615
Iteration 5058, loss = 0.13883733
Iteration 5059, loss = 0.13879860
Iteration 5060, loss = 0.13875925
Iteration 5061, loss = 0.13872072
Iteration 5062, loss = 0.13868197
Iteration 5063, loss = 0.13864302
Iteration 5064, loss = 0.13860478
Iteration 5065, loss = 0.13856527
Iteration 5066, loss = 0.13852701
Iteration 5067, loss = 0.13848825
Iteration 5068, loss = 0.13844946
Iteration 5069, loss = 0.13841081
Iteration 5070, loss = 0.13837212
Iteration 5071, loss = 0.13833353
Iteration 5072, loss = 0.13829485
Iteration 5073, loss = 0.13825678
Iteration 5074, loss = 0.13821744
Iteration 5075, loss = 0.13817931
Iteration 5076, loss = 0.13814029
Iteration 5077, loss = 0.13810269
Iteration 5078, loss = 0.13806345
Iteration 5079, loss = 0.13802499
Iteration 5080, loss = 0.13798627
Iteration 5081, loss = 0.13794789
Iteration 5082, loss = 0.13790915
Iteration 5083, loss = 0.13787087
Iteration 5084, loss = 0.13783257
Iteration 5085, loss = 0.13779368
Iteration 5086, loss = 0.13775554
Iteration 5087, loss = 0.13771667
Iteration 5088, loss = 0.13767830
Iteration 5089, loss = 0.13764003
Iteration 5090, loss = 0.13760143
Iteration 5091, loss = 0.13756354
Iteration 5092, loss = 0.13752508
Iteration 5093, loss = 0.13748650
Iteration 5094, loss = 0.13744880
Iteration 5095, loss = 0.13740978
Iteration 5096, loss = 0.13737190
Iteration 5097, loss = 0.13733371
Iteration 5098, loss = 0.13729556
Iteration 5099, loss = 0.13725729
Iteration 5100, loss = 0.13721943
Iteration 5101, loss = 0.13718101
Iteration 5102, loss = 0.13714267
Iteration 5103, loss = 0.13710465
Iteration 5104, loss = 0.13706690
Iteration 5105, loss = 0.13702857
Iteration 5106, loss = 0.13699100
Iteration 5107, loss = 0.13695223
Iteration 5108, loss = 0.13691484
Iteration 5109, loss = 0.13687712
Iteration 5110, loss = 0.13683936
Iteration 5111, loss = 0.13680119
Iteration 5112, loss = 0.13676344
Iteration 5113, loss = 0.13672588
Iteration 5114, loss = 0.13668784
Iteration 5115, loss = 0.13665003
Iteration 5116, loss = 0.13661226
Iteration 5117, loss = 0.13657500
Iteration 5118, loss = 0.13653722
Iteration 5119, loss = 0.13649910
Iteration 5120, loss = 0.13646172
Iteration 5121, loss = 0.13642374
Iteration 5122, loss = 0.13638563
Iteration 5123, loss = 0.13634822
Iteration 5124, loss = 0.13631038
Iteration 5125, loss = 0.13627282
Iteration 5126, loss = 0.13623565
Iteration 5127, loss = 0.13619752
Iteration 5128, loss = 0.13616018
Iteration 5129, loss = 0.13612239
Iteration 5130, loss = 0.13608434
Iteration 5131, loss = 0.13604697
Iteration 5132, loss = 0.13600964
Iteration 5133, loss = 0.13597204
Iteration 5134, loss = 0.13593468
Iteration 5135, loss = 0.13589698
Iteration 5136, loss = 0.13585969
Iteration 5137, loss = 0.13582247
Iteration 5138, loss = 0.13578491
Iteration 5139, loss = 0.13574712
Iteration 5140, loss = 0.13570966
Iteration 5141, loss = 0.13567248
Iteration 5142, loss = 0.13563527
Iteration 5143, loss = 0.13559796
Iteration 5144, loss = 0.13556039
Iteration 5145, loss = 0.13552326
Iteration 5146, loss = 0.13548624
Iteration 5147, loss = 0.13544901
Iteration 5148, loss = 0.13541201
Iteration 5149, loss = 0.13537459
Iteration 5150, loss = 0.13533695
Iteration 5151, loss = 0.13529996
Iteration 5152, loss = 0.13526285
Iteration 5153, loss = 0.13522544
Iteration 5154, loss = 0.13518855
Iteration 5155, loss = 0.13515195
Iteration 5156, loss = 0.13511481
Iteration 5157, loss = 0.13507756
Iteration 5158, loss = 0.13504075
Iteration 5159, loss = 0.13500353
Iteration 5160, loss = 0.13496709
Iteration 5161, loss = 0.13492961
Iteration 5162, loss = 0.13489248
Iteration 5163, loss = 0.13485582
Iteration 5164, loss = 0.13481907
Iteration 5165, loss = 0.13478189
Iteration 5166, loss = 0.13474507
Iteration 5167, loss = 0.13470837
Iteration 5168, loss = 0.13467200
Iteration 5169, loss = 0.13463407
Iteration 5170, loss = 0.13459796
Iteration 5171, loss = 0.13456108
Iteration 5172, loss = 0.13452448
Iteration 5173, loss = 0.13448790
Iteration 5174, loss = 0.13445079
Iteration 5175, loss = 0.13441434
Iteration 5176, loss = 0.13437757
Iteration 5177, loss = 0.13434070
Iteration 5178, loss = 0.13430426
Iteration 5179, loss = 0.13426763
Iteration 5180, loss = 0.13423084
Iteration 5181, loss = 0.13419455
Iteration 5182, loss = 0.13415787
Iteration 5183, loss = 0.13412184
Iteration 5184, loss = 0.13408477
Iteration 5185, loss = 0.13404879
Iteration 5186, loss = 0.13401242
Iteration 5187, loss = 0.13397528
Iteration 5188, loss = 0.13393968
Iteration 5189, loss = 0.13390275
Iteration 5190, loss = 0.13386691
Iteration 5191, loss = 0.13383041
Iteration 5192, loss = 0.13379405
Iteration 5193, loss = 0.13375792
Iteration 5194, loss = 0.13372165
Iteration 5195, loss = 0.13368509
Iteration 5196, loss = 0.13364916
Iteration 5197, loss = 0.13361291
Iteration 5198, loss = 0.13357645
Iteration 5199, loss = 0.13354033
Iteration 5200, loss = 0.13350421
Iteration 5201, loss = 0.13346857
Iteration 5202, loss = 0.13343223
Iteration 5203, loss = 0.13339604
Iteration 5204, loss = 0.13335964
Iteration 5205, loss = 0.13332386
Iteration 5206, loss = 0.13328777
Iteration 5207, loss = 0.13325151
Iteration 5208, loss = 0.13321567
Iteration 5209, loss = 0.13317981
Iteration 5210, loss = 0.13314344
Iteration 5211, loss = 0.13310731
Iteration 5212, loss = 0.13307165
Iteration 5213, loss = 0.13303579
Iteration 5214, loss = 0.13299968
Iteration 5215, loss = 0.13296392
Iteration 5216, loss = 0.13292768
Iteration 5217, loss = 0.13289203
Iteration 5218, loss = 0.13285584
Iteration 5219, loss = 0.13282029
Iteration 5220, loss = 0.13278441
Iteration 5221, loss = 0.13274876
Iteration 5222, loss = 0.13271259
Iteration 5223, loss = 0.13267710
Iteration 5224, loss = 0.13264143
Iteration 5225, loss = 0.13260569
Iteration 5226, loss = 0.13256971
Iteration 5227, loss = 0.13253416
Iteration 5228, loss = 0.13249875
Iteration 5229, loss = 0.13246243
Iteration 5230, loss = 0.13242671
Iteration 5231, loss = 0.13239118
Iteration 5232, loss = 0.13235607
Iteration 5233, loss = 0.13232004
Iteration 5234, loss = 0.13228449
Iteration 5235, loss = 0.13224924
Iteration 5236, loss = 0.13221354
Iteration 5237, loss = 0.13217799
Iteration 5238, loss = 0.13214234
Iteration 5239, loss = 0.13210657
Iteration 5240, loss = 0.13207130
Iteration 5241, loss = 0.13203564
Iteration 5242, loss = 0.13199973
Iteration 5243, loss = 0.13196443
Iteration 5244, loss = 0.13192923
Iteration 5245, loss = 0.13189407
Iteration 5246, loss = 0.13185871
Iteration 5247, loss = 0.13182275
Iteration 5248, loss = 0.13178764
Iteration 5249, loss = 0.13175230
Iteration 5250, loss = 0.13171718
Iteration 5251, loss = 0.13168171
Iteration 5252, loss = 0.13164657
Iteration 5253, loss = 0.13161106
Iteration 5254, loss = 0.13157588
Iteration 5255, loss = 0.13154084
Iteration 5256, loss = 0.13150550
Iteration 5257, loss = 0.13147004
Iteration 5258, loss = 0.13143566
Iteration 5259, loss = 0.13139955
Iteration 5260, loss = 0.13136475
Iteration 5261, loss = 0.13132984
Iteration 5262, loss = 0.13129436
Iteration 5263, loss = 0.13125962
Iteration 5264, loss = 0.13122469
Iteration 5265, loss = 0.13118948
Iteration 5266, loss = 0.13115445
Iteration 5267, loss = 0.13111961
Iteration 5268, loss = 0.13108475
Iteration 5269, loss = 0.13104950
Iteration 5270, loss = 0.13101466
Iteration 5271, loss = 0.13097952
Iteration 5272, loss = 0.13094465
Iteration 5273, loss = 0.13090969
Iteration 5274, loss = 0.13087448
Iteration 5275, loss = 0.13084000
Iteration 5276, loss = 0.13080480
Iteration 5277, loss = 0.13077025
Iteration 5278, loss = 0.13073490
Iteration 5279, loss = 0.13070040
Iteration 5280, loss = 0.13066554
Iteration 5281, loss = 0.13063103
Iteration 5282, loss = 0.13059613
Iteration 5283, loss = 0.13056176
Iteration 5284, loss = 0.13052688
Iteration 5285, loss = 0.13049240
Iteration 5286, loss = 0.13045768
Iteration 5287, loss = 0.13042295
Iteration 5288, loss = 0.13038795
Iteration 5289, loss = 0.13035331
Iteration 5290, loss = 0.13031855
Iteration 5291, loss = 0.13028371
Iteration 5292, loss = 0.13024914
Iteration 5293, loss = 0.13021452
Iteration 5294, loss = 0.13017957
Iteration 5295, loss = 0.13014527
Iteration 5296, loss = 0.13011043
Iteration 5297, loss = 0.13007628
Iteration 5298, loss = 0.13004128
Iteration 5299, loss = 0.13000669
Iteration 5300, loss = 0.12997218
Iteration 5301, loss = 0.12993815
Iteration 5302, loss = 0.12990321
Iteration 5303, loss = 0.12986900
Iteration 5304, loss = 0.12983488
Iteration 5305, loss = 0.12979996
Iteration 5306, loss = 0.12976569
Iteration 5307, loss = 0.12973143
Iteration 5308, loss = 0.12969682
Iteration 5309, loss = 0.12966256
Iteration 5310, loss = 0.12962869
Iteration 5311, loss = 0.12959424
Iteration 5312, loss = 0.12955943
Iteration 5313, loss = 0.12952543
Iteration 5314, loss = 0.12949084
Iteration 5315, loss = 0.12945688
Iteration 5316, loss = 0.12942256
Iteration 5317, loss = 0.12938860
Iteration 5318, loss = 0.12935445
Iteration 5319, loss = 0.12931997
Iteration 5320, loss = 0.12928568
Iteration 5321, loss = 0.12925171
Iteration 5322, loss = 0.12921770
Iteration 5323, loss = 0.12918365
Iteration 5324, loss = 0.12914934
Iteration 5325, loss = 0.12911546
Iteration 5326, loss = 0.12908158
Iteration 5327, loss = 0.12904743
Iteration 5328, loss = 0.12901340
Iteration 5329, loss = 0.12897929
Iteration 5330, loss = 0.12894590
Iteration 5331, loss = 0.12891153
Iteration 5332, loss = 0.12887813
Iteration 5333, loss = 0.12884352
Iteration 5334, loss = 0.12880982
Iteration 5335, loss = 0.12877606
Iteration 5336, loss = 0.12874248
Iteration 5337, loss = 0.12870844
Iteration 5338, loss = 0.12867441
Iteration 5339, loss = 0.12864108
Iteration 5340, loss = 0.12860741
Iteration 5341, loss = 0.12857333
Iteration 5342, loss = 0.12853979
Iteration 5343, loss = 0.12850599
Iteration 5344, loss = 0.12847287
Iteration 5345, loss = 0.12843953
Iteration 5346, loss = 0.12840540
Iteration 5347, loss = 0.12837161
Iteration 5348, loss = 0.12833847
Iteration 5349, loss = 0.12830508
Iteration 5350, loss = 0.12827154
Iteration 5351, loss = 0.12823784
Iteration 5352, loss = 0.12820406
Iteration 5353, loss = 0.12817090
Iteration 5354, loss = 0.12813723
Iteration 5355, loss = 0.12810387
Iteration 5356, loss = 0.12807046
Iteration 5357, loss = 0.12803710
Iteration 5358, loss = 0.12800338
Iteration 5359, loss = 0.12797003
Iteration 5360, loss = 0.12793639
Iteration 5361, loss = 0.12790312
Iteration 5362, loss = 0.12786991
Iteration 5363, loss = 0.12783653
Iteration 5364, loss = 0.12780290
Iteration 5365, loss = 0.12777011
Iteration 5366, loss = 0.12773627
Iteration 5367, loss = 0.12770332
Iteration 5368, loss = 0.12766999
Iteration 5369, loss = 0.12763674
Iteration 5370, loss = 0.12760355
Iteration 5371, loss = 0.12757035
Iteration 5372, loss = 0.12753734
Iteration 5373, loss = 0.12750466
Iteration 5374, loss = 0.12747092
Iteration 5375, loss = 0.12743783
Iteration 5376, loss = 0.12740472
Iteration 5377, loss = 0.12737170
Iteration 5378, loss = 0.12733837
Iteration 5379, loss = 0.12730551
Iteration 5380, loss = 0.12727237
Iteration 5381, loss = 0.12723910
Iteration 5382, loss = 0.12720618
Iteration 5383, loss = 0.12717305
Iteration 5384, loss = 0.12713980
Iteration 5385, loss = 0.12710726
Iteration 5386, loss = 0.12707406
Iteration 5387, loss = 0.12704138
Iteration 5388, loss = 0.12700832
Iteration 5389, loss = 0.12697578
Iteration 5390, loss = 0.12694213
Iteration 5391, loss = 0.12690934
Iteration 5392, loss = 0.12687651
Iteration 5393, loss = 0.12684382
Iteration 5394, loss = 0.12681054
Iteration 5395, loss = 0.12677813
Iteration 5396, loss = 0.12674505
Iteration 5397, loss = 0.12671243
Iteration 5398, loss = 0.12667931
Iteration 5399, loss = 0.12664661
Iteration 5400, loss = 0.12661412
Iteration 5401, loss = 0.12658131
Iteration 5402, loss = 0.12654816
Iteration 5403, loss = 0.12651582
Iteration 5404, loss = 0.12648298
Iteration 5405, loss = 0.12645042
Iteration 5406, loss = 0.12641773
Iteration 5407, loss = 0.12638470
Iteration 5408, loss = 0.12635240
Iteration 5409, loss = 0.12631957
Iteration 5410, loss = 0.12628712
Iteration 5411, loss = 0.12625481
Iteration 5412, loss = 0.12622180
Iteration 5413, loss = 0.12618883
Iteration 5414, loss = 0.12615688
Iteration 5415, loss = 0.12612410
Iteration 5416, loss = 0.12609156
Iteration 5417, loss = 0.12605904
Iteration 5418, loss = 0.12602692
Iteration 5419, loss = 0.12599434
Iteration 5420, loss = 0.12596199
Iteration 5421, loss = 0.12592950
Iteration 5422, loss = 0.12589739
Iteration 5423, loss = 0.12586470
Iteration 5424, loss = 0.12583230
Iteration 5425, loss = 0.12579986
Iteration 5426, loss = 0.12576739
Iteration 5427, loss = 0.12573537
Iteration 5428, loss = 0.12570281
Iteration 5429, loss = 0.12567039
Iteration 5430, loss = 0.12563807
Iteration 5431, loss = 0.12560576
Iteration 5432, loss = 0.12557340
Iteration 5433, loss = 0.12554137
Iteration 5434, loss = 0.12550875
Iteration 5435, loss = 0.12547633
Iteration 5436, loss = 0.12544426
Iteration 5437, loss = 0.12541226
Iteration 5438, loss = 0.12537964
Iteration 5439, loss = 0.12534803
Iteration 5440, loss = 0.12531539
Iteration 5441, loss = 0.12528326
Iteration 5442, loss = 0.12525136
Iteration 5443, loss = 0.12521913
Iteration 5444, loss = 0.12518696
Iteration 5445, loss = 0.12515466
Iteration 5446, loss = 0.12512229
Iteration 5447, loss = 0.12509073
Iteration 5448, loss = 0.12505804
Iteration 5449, loss = 0.12502643
Iteration 5450, loss = 0.12499386
Iteration 5451, loss = 0.12496180
Iteration 5452, loss = 0.12493010
Iteration 5453, loss = 0.12489818
Iteration 5454, loss = 0.12486606
Iteration 5455, loss = 0.12483403
Iteration 5456, loss = 0.12480215
Iteration 5457, loss = 0.12477026
Iteration 5458, loss = 0.12473880
Iteration 5459, loss = 0.12470602
Iteration 5460, loss = 0.12467484
Iteration 5461, loss = 0.12464281
Iteration 5462, loss = 0.12461119
Iteration 5463, loss = 0.12457963
Iteration 5464, loss = 0.12454775
Iteration 5465, loss = 0.12451622
Iteration 5466, loss = 0.12448445
Iteration 5467, loss = 0.12445298
Iteration 5468, loss = 0.12442135
Iteration 5469, loss = 0.12438970
Iteration 5470, loss = 0.12435802
Iteration 5471, loss = 0.12432671
Iteration 5472, loss = 0.12429513
Iteration 5473, loss = 0.12426314
Iteration 5474, loss = 0.12423139
Iteration 5475, loss = 0.12419992
Iteration 5476, loss = 0.12416817
Iteration 5477, loss = 0.12413656
Iteration 5478, loss = 0.12410451
Iteration 5479, loss = 0.12407327
Iteration 5480, loss = 0.12404174
Iteration 5481, loss = 0.12401000
Iteration 5482, loss = 0.12397837
Iteration 5483, loss = 0.12394671
Iteration 5484, loss = 0.12391545
Iteration 5485, loss = 0.12388368
Iteration 5486, loss = 0.12385222
Iteration 5487, loss = 0.12382055
Iteration 5488, loss = 0.12378916
Iteration 5489, loss = 0.12375744
Iteration 5490, loss = 0.12372649
Iteration 5491, loss = 0.12369485
Iteration 5492, loss = 0.12366332
Iteration 5493, loss = 0.12363187
Iteration 5494, loss = 0.12360078
Iteration 5495, loss = 0.12356913
Iteration 5496, loss = 0.12353791
Iteration 5497, loss = 0.12350698
Iteration 5498, loss = 0.12347500
Iteration 5499, loss = 0.12344390
Iteration 5500, loss = 0.12341282
Iteration 5501, loss = 0.12338219
Iteration 5502, loss = 0.12335022
Iteration 5503, loss = 0.12331913
Iteration 5504, loss = 0.12328761
Iteration 5505, loss = 0.12325676
Iteration 5506, loss = 0.12322529
Iteration 5507, loss = 0.12319397
Iteration 5508, loss = 0.12316326
Iteration 5509, loss = 0.12313194
Iteration 5510, loss = 0.12310115
Iteration 5511, loss = 0.12306957
Iteration 5512, loss = 0.12303848
Iteration 5513, loss = 0.12300777
Iteration 5514, loss = 0.12297636
Iteration 5515, loss = 0.12294598
Iteration 5516, loss = 0.12291462
Iteration 5517, loss = 0.12288377
Iteration 5518, loss = 0.12285290
Iteration 5519, loss = 0.12282159
Iteration 5520, loss = 0.12279081
Iteration 5521, loss = 0.12275966
Iteration 5522, loss = 0.12272882
Iteration 5523, loss = 0.12269801
Iteration 5524, loss = 0.12266688
Iteration 5525, loss = 0.12263646
Iteration 5526, loss = 0.12260551
Iteration 5527, loss = 0.12257428
Iteration 5528, loss = 0.12254342
Iteration 5529, loss = 0.12251293
Iteration 5530, loss = 0.12248190
Iteration 5531, loss = 0.12245125
Iteration 5532, loss = 0.12242024
Iteration 5533, loss = 0.12238959
Iteration 5534, loss = 0.12235901
Iteration 5535, loss = 0.12232835
Iteration 5536, loss = 0.12229778
Iteration 5537, loss = 0.12226659
Iteration 5538, loss = 0.12223628
Iteration 5539, loss = 0.12220542
Iteration 5540, loss = 0.12217492
Iteration 5541, loss = 0.12214448
Iteration 5542, loss = 0.12211338
Iteration 5543, loss = 0.12208320
Iteration 5544, loss = 0.12205267
Iteration 5545, loss = 0.12202205
Iteration 5546, loss = 0.12199143
Iteration 5547, loss = 0.12196049
Iteration 5548, loss = 0.12192988
Iteration 5549, loss = 0.12189932
Iteration 5550, loss = 0.12186900
Iteration 5551, loss = 0.12183812
Iteration 5552, loss = 0.12180789
Iteration 5553, loss = 0.12177753
Iteration 5554, loss = 0.12174707
Iteration 5555, loss = 0.12171643
Iteration 5556, loss = 0.12168617
Iteration 5557, loss = 0.12165570
Iteration 5558, loss = 0.12162519
Iteration 5559, loss = 0.12159500
Iteration 5560, loss = 0.12156495
Iteration 5561, loss = 0.12153444
Iteration 5562, loss = 0.12150375
Iteration 5563, loss = 0.12147331
Iteration 5564, loss = 0.12144321
Iteration 5565, loss = 0.12141288
Iteration 5566, loss = 0.12138222
Iteration 5567, loss = 0.12135206
Iteration 5568, loss = 0.12132164
Iteration 5569, loss = 0.12129154
Iteration 5570, loss = 0.12126133
Iteration 5571, loss = 0.12123102
Iteration 5572, loss = 0.12120084
Iteration 5573, loss = 0.12117077
Iteration 5574, loss = 0.12114058
Iteration 5575, loss = 0.12111034
Iteration 5576, loss = 0.12108046
Iteration 5577, loss = 0.12104999
Iteration 5578, loss = 0.12102034
Iteration 5579, loss = 0.12099005
Iteration 5580, loss = 0.12096025
Iteration 5581, loss = 0.12092957
Iteration 5582, loss = 0.12089998
Iteration 5583, loss = 0.12086990
Iteration 5584, loss = 0.12083970
Iteration 5585, loss = 0.12080975
Iteration 5586, loss = 0.12077981
Iteration 5587, loss = 0.12074983
Iteration 5588, loss = 0.12071950
Iteration 5589, loss = 0.12068984
Iteration 5590, loss = 0.12065974
Iteration 5591, loss = 0.12063027
Iteration 5592, loss = 0.12060012
Iteration 5593, loss = 0.12057022
Iteration 5594, loss = 0.12054041
Iteration 5595, loss = 0.12051073
Iteration 5596, loss = 0.12048089
Iteration 5597, loss = 0.12045075
Iteration 5598, loss = 0.12042100
Iteration 5599, loss = 0.12039123
Iteration 5600, loss = 0.12036158
Iteration 5601, loss = 0.12033166
Iteration 5602, loss = 0.12030201
Iteration 5603, loss = 0.12027195
Iteration 5604, loss = 0.12024205
Iteration 5605, loss = 0.12021264
Iteration 5606, loss = 0.12018281
Iteration 5607, loss = 0.12015327
Iteration 5608, loss = 0.12012368
Iteration 5609, loss = 0.12009370
Iteration 5610, loss = 0.12006433
Iteration 5611, loss = 0.12003476
Iteration 5612, loss = 0.12000486
Iteration 5613, loss = 0.11997543
Iteration 5614, loss = 0.11994605
Iteration 5615, loss = 0.11991613
Iteration 5616, loss = 0.11988662
Iteration 5617, loss = 0.11985676
Iteration 5618, loss = 0.11982734
Iteration 5619, loss = 0.11979742
Iteration 5620, loss = 0.11976778
Iteration 5621, loss = 0.11973805
Iteration 5622, loss = 0.11970893
Iteration 5623, loss = 0.11967905
Iteration 5624, loss = 0.11964957
Iteration 5625, loss = 0.11962010
Iteration 5626, loss = 0.11959030
Iteration 5627, loss = 0.11956074
Iteration 5628, loss = 0.11953121
Iteration 5629, loss = 0.11950156
Iteration 5630, loss = 0.11947242
Iteration 5631, loss = 0.11944287
Iteration 5632, loss = 0.11941341
Iteration 5633, loss = 0.11938403
Iteration 5634, loss = 0.11935470
Iteration 5635, loss = 0.11932508
Iteration 5636, loss = 0.11929566
Iteration 5637, loss = 0.11926685
Iteration 5638, loss = 0.11923733
Iteration 5639, loss = 0.11920804
Iteration 5640, loss = 0.11917866
Iteration 5641, loss = 0.11914945
Iteration 5642, loss = 0.11912040
Iteration 5643, loss = 0.11909136
Iteration 5644, loss = 0.11906248
Iteration 5645, loss = 0.11903277
Iteration 5646, loss = 0.11900380
Iteration 5647, loss = 0.11897467
Iteration 5648, loss = 0.11894574
Iteration 5649, loss = 0.11891658
Iteration 5650, loss = 0.11888716
Iteration 5651, loss = 0.11885852
Iteration 5652, loss = 0.11882902
Iteration 5653, loss = 0.11880012
Iteration 5654, loss = 0.11877113
Iteration 5655, loss = 0.11874218
Iteration 5656, loss = 0.11871352
Iteration 5657, loss = 0.11868452
Iteration 5658, loss = 0.11865531
Iteration 5659, loss = 0.11862672
Iteration 5660, loss = 0.11859756
Iteration 5661, loss = 0.11856872
Iteration 5662, loss = 0.11853977
Iteration 5663, loss = 0.11851078
Iteration 5664, loss = 0.11848170
Iteration 5665, loss = 0.11845284
Iteration 5666, loss = 0.11842439
Iteration 5667, loss = 0.11839501
Iteration 5668, loss = 0.11836634
Iteration 5669, loss = 0.11833767
Iteration 5670, loss = 0.11830842
Iteration 5671, loss = 0.11828005
Iteration 5672, loss = 0.11825104
Iteration 5673, loss = 0.11822209
Iteration 5674, loss = 0.11819357
Iteration 5675, loss = 0.11816426
Iteration 5676, loss = 0.11813601
Iteration 5677, loss = 0.11810683
Iteration 5678, loss = 0.11807814
Iteration 5679, loss = 0.11804936
Iteration 5680, loss = 0.11802075
Iteration 5681, loss = 0.11799194
Iteration 5682, loss = 0.11796371
Iteration 5683, loss = 0.11793473
Iteration 5684, loss = 0.11790591
Iteration 5685, loss = 0.11787732
Iteration 5686, loss = 0.11784861
Iteration 5687, loss = 0.11782011
Iteration 5688, loss = 0.11779143
Iteration 5689, loss = 0.11776316
Iteration 5690, loss = 0.11773420
Iteration 5691, loss = 0.11770596
Iteration 5692, loss = 0.11767764
Iteration 5693, loss = 0.11764866
Iteration 5694, loss = 0.11762014
Iteration 5695, loss = 0.11759159
Iteration 5696, loss = 0.11756384
Iteration 5697, loss = 0.11753472
Iteration 5698, loss = 0.11750635
Iteration 5699, loss = 0.11747730
Iteration 5700, loss = 0.11744922
Iteration 5701, loss = 0.11742105
Iteration 5702, loss = 0.11739219
Iteration 5703, loss = 0.11736402
Iteration 5704, loss = 0.11733595
Iteration 5705, loss = 0.11730727
Iteration 5706, loss = 0.11727908
Iteration 5707, loss = 0.11725078
Iteration 5708, loss = 0.11722221
Iteration 5709, loss = 0.11719424
Iteration 5710, loss = 0.11716614
Iteration 5711, loss = 0.11713775
Iteration 5712, loss = 0.11710977
Iteration 5713, loss = 0.11708121
Iteration 5714, loss = 0.11705302
Iteration 5715, loss = 0.11702481
Iteration 5716, loss = 0.11699664
Iteration 5717, loss = 0.11696827
Iteration 5718, loss = 0.11694005
Iteration 5719, loss = 0.11691172
Iteration 5720, loss = 0.11688381
Iteration 5721, loss = 0.11685583
Iteration 5722, loss = 0.11682736
Iteration 5723, loss = 0.11679919
Iteration 5724, loss = 0.11677082
Iteration 5725, loss = 0.11674293
Iteration 5726, loss = 0.11671478
Iteration 5727, loss = 0.11668651
Iteration 5728, loss = 0.11665829
Iteration 5729, loss = 0.11663029
Iteration 5730, loss = 0.11660224
Iteration 5731, loss = 0.11657441
Iteration 5732, loss = 0.11654599
Iteration 5733, loss = 0.11651790
Iteration 5734, loss = 0.11649030
Iteration 5735, loss = 0.11646229
Iteration 5736, loss = 0.11643438
Iteration 5737, loss = 0.11640624
Iteration 5738, loss = 0.11637848
Iteration 5739, loss = 0.11635060
Iteration 5740, loss = 0.11632277
Iteration 5741, loss = 0.11629430
Iteration 5742, loss = 0.11626639
Iteration 5743, loss = 0.11623871
Iteration 5744, loss = 0.11621081
Iteration 5745, loss = 0.11618281
Iteration 5746, loss = 0.11615542
Iteration 5747, loss = 0.11612704
Iteration 5748, loss = 0.11609927
Iteration 5749, loss = 0.11607145
Iteration 5750, loss = 0.11604360
Iteration 5751, loss = 0.11601562
Iteration 5752, loss = 0.11598796
Iteration 5753, loss = 0.11596030
Iteration 5754, loss = 0.11593249
Iteration 5755, loss = 0.11590470
Iteration 5756, loss = 0.11587710
Iteration 5757, loss = 0.11584922
Iteration 5758, loss = 0.11582137
Iteration 5759, loss = 0.11579348
Iteration 5760, loss = 0.11576594
Iteration 5761, loss = 0.11573833
Iteration 5762, loss = 0.11571050
Iteration 5763, loss = 0.11568281
Iteration 5764, loss = 0.11565546
Iteration 5765, loss = 0.11562810
Iteration 5766, loss = 0.11560029
Iteration 5767, loss = 0.11557251
Iteration 5768, loss = 0.11554519
Iteration 5769, loss = 0.11551727
Iteration 5770, loss = 0.11549026
Iteration 5771, loss = 0.11546226
Iteration 5772, loss = 0.11543501
Iteration 5773, loss = 0.11540758
Iteration 5774, loss = 0.11537977
Iteration 5775, loss = 0.11535250
Iteration 5776, loss = 0.11532535
Iteration 5777, loss = 0.11529736
Iteration 5778, loss = 0.11526965
Iteration 5779, loss = 0.11524267
Iteration 5780, loss = 0.11521498
Iteration 5781, loss = 0.11518751
Iteration 5782, loss = 0.11515994
Iteration 5783, loss = 0.11513237
Iteration 5784, loss = 0.11510459
Iteration 5785, loss = 0.11507773
Iteration 5786, loss = 0.11505029
Iteration 5787, loss = 0.11502285
Iteration 5788, loss = 0.11499559
Iteration 5789, loss = 0.11496829
Iteration 5790, loss = 0.11494088
Iteration 5791, loss = 0.11491366
Iteration 5792, loss = 0.11488669
Iteration 5793, loss = 0.11485892
Iteration 5794, loss = 0.11483178
Iteration 5795, loss = 0.11480436
Iteration 5796, loss = 0.11477701
Iteration 5797, loss = 0.11475063
Iteration 5798, loss = 0.11472300
Iteration 5799, loss = 0.11469513
Iteration 5800, loss = 0.11466804
Iteration 5801, loss = 0.11464092
Iteration 5802, loss = 0.11461359
Iteration 5803, loss = 0.11458679
Iteration 5804, loss = 0.11455916
Iteration 5805, loss = 0.11453202
Iteration 5806, loss = 0.11450499
Iteration 5807, loss = 0.11447779
Iteration 5808, loss = 0.11445105
Iteration 5809, loss = 0.11442400
Iteration 5810, loss = 0.11439650
Iteration 5811, loss = 0.11436964
Iteration 5812, loss = 0.11434264
Iteration 5813, loss = 0.11431508
Iteration 5814, loss = 0.11428838
Iteration 5815, loss = 0.11426194
Iteration 5816, loss = 0.11423432
Iteration 5817, loss = 0.11420736
Iteration 5818, loss = 0.11418042
Iteration 5819, loss = 0.11415358
Iteration 5820, loss = 0.11412678
Iteration 5821, loss = 0.11409936
Iteration 5822, loss = 0.11407269
Iteration 5823, loss = 0.11404531
Iteration 5824, loss = 0.11401875
Iteration 5825, loss = 0.11399154
Iteration 5826, loss = 0.11396449
Iteration 5827, loss = 0.11393780
Iteration 5828, loss = 0.11391089
Iteration 5829, loss = 0.11388403
Iteration 5830, loss = 0.11385683
Iteration 5831, loss = 0.11383038
Iteration 5832, loss = 0.11380288
Iteration 5833, loss = 0.11377635
Iteration 5834, loss = 0.11374921
Iteration 5835, loss = 0.11372240
Iteration 5836, loss = 0.11369559
Iteration 5837, loss = 0.11366873
Iteration 5838, loss = 0.11364197
Iteration 5839, loss = 0.11361491
Iteration 5840, loss = 0.11358808
Iteration 5841, loss = 0.11356168
Iteration 5842, loss = 0.11353461
Iteration 5843, loss = 0.11350769
Iteration 5844, loss = 0.11348118
Iteration 5845, loss = 0.11345420
Iteration 5846, loss = 0.11342731
Iteration 5847, loss = 0.11340061
Iteration 5848, loss = 0.11337385
Iteration 5849, loss = 0.11334748
Iteration 5850, loss = 0.11332086
Iteration 5851, loss = 0.11329408
Iteration 5852, loss = 0.11326737
Iteration 5853, loss = 0.11324046
Iteration 5854, loss = 0.11321385
Iteration 5855, loss = 0.11318725
Iteration 5856, loss = 0.11316059
Iteration 5857, loss = 0.11313353
Iteration 5858, loss = 0.11310748
Iteration 5859, loss = 0.11308073
Iteration 5860, loss = 0.11305423
Iteration 5861, loss = 0.11302747
Iteration 5862, loss = 0.11300072
Iteration 5863, loss = 0.11297434
Iteration 5864, loss = 0.11294770
Iteration 5865, loss = 0.11292102
Iteration 5866, loss = 0.11289408
Iteration 5867, loss = 0.11286784
Iteration 5868, loss = 0.11284136
Iteration 5869, loss = 0.11281511
Iteration 5870, loss = 0.11278851
Iteration 5871, loss = 0.11276190
Iteration 5872, loss = 0.11273550
Iteration 5873, loss = 0.11270909
Iteration 5874, loss = 0.11268268
Iteration 5875, loss = 0.11265619
Iteration 5876, loss = 0.11263009
Iteration 5877, loss = 0.11260383
Iteration 5878, loss = 0.11257776
Iteration 5879, loss = 0.11255083
Iteration 5880, loss = 0.11252488
Iteration 5881, loss = 0.11249856
Iteration 5882, loss = 0.11247246
Iteration 5883, loss = 0.11244568
Iteration 5884, loss = 0.11241988
Iteration 5885, loss = 0.11239344
Iteration 5886, loss = 0.11236794
Iteration 5887, loss = 0.11234094
Iteration 5888, loss = 0.11231530
Iteration 5889, loss = 0.11228922
Iteration 5890, loss = 0.11226272
Iteration 5891, loss = 0.11223633
Iteration 5892, loss = 0.11221040
Iteration 5893, loss = 0.11218429
Iteration 5894, loss = 0.11215833
Iteration 5895, loss = 0.11213242
Iteration 5896, loss = 0.11210572
Iteration 5897, loss = 0.11207990
Iteration 5898, loss = 0.11205368
Iteration 5899, loss = 0.11202772
Iteration 5900, loss = 0.11200141
Iteration 5901, loss = 0.11197559
Iteration 5902, loss = 0.11194920
Iteration 5903, loss = 0.11192324
Iteration 5904, loss = 0.11189740
Iteration 5905, loss = 0.11187116
Iteration 5906, loss = 0.11184503
Iteration 5907, loss = 0.11181888
Iteration 5908, loss = 0.11179280
Iteration 5909, loss = 0.11176689
Iteration 5910, loss = 0.11174089
Iteration 5911, loss = 0.11171503
Iteration 5912, loss = 0.11168870
Iteration 5913, loss = 0.11166304
Iteration 5914, loss = 0.11163666
Iteration 5915, loss = 0.11161106
Iteration 5916, loss = 0.11158515
Iteration 5917, loss = 0.11155921
Iteration 5918, loss = 0.11153329
Iteration 5919, loss = 0.11150730
Iteration 5920, loss = 0.11148148
Iteration 5921, loss = 0.11145574
Iteration 5922, loss = 0.11142969
Iteration 5923, loss = 0.11140401
Iteration 5924, loss = 0.11137812
Iteration 5925, loss = 0.11135259
Iteration 5926, loss = 0.11132644
Iteration 5927, loss = 0.11130088
Iteration 5928, loss = 0.11127514
Iteration 5929, loss = 0.11124937
Iteration 5930, loss = 0.11122362
Iteration 5931, loss = 0.11119762
Iteration 5932, loss = 0.11117190
Iteration 5933, loss = 0.11114629
Iteration 5934, loss = 0.11112092
Iteration 5935, loss = 0.11109483
Iteration 5936, loss = 0.11106949
Iteration 5937, loss = 0.11104374
Iteration 5938, loss = 0.11101827
Iteration 5939, loss = 0.11099248
Iteration 5940, loss = 0.11096686
Iteration 5941, loss = 0.11094118
Iteration 5942, loss = 0.11091558
Iteration 5943, loss = 0.11088990
Iteration 5944, loss = 0.11086431
Iteration 5945, loss = 0.11083852
Iteration 5946, loss = 0.11081304
Iteration 5947, loss = 0.11078728
Iteration 5948, loss = 0.11076187
Iteration 5949, loss = 0.11073628
Iteration 5950, loss = 0.11071061
Iteration 5951, loss = 0.11068501
Iteration 5952, loss = 0.11065961
Iteration 5953, loss = 0.11063437
Iteration 5954, loss = 0.11060875
Iteration 5955, loss = 0.11058304
Iteration 5956, loss = 0.11055759
Iteration 5957, loss = 0.11053241
Iteration 5958, loss = 0.11050667
Iteration 5959, loss = 0.11048110
Iteration 5960, loss = 0.11045604
Iteration 5961, loss = 0.11043074
Iteration 5962, loss = 0.11040542
Iteration 5963, loss = 0.11038030
Iteration 5964, loss = 0.11035455
Iteration 5965, loss = 0.11032960
Iteration 5966, loss = 0.11030405
Iteration 5967, loss = 0.11027911
Iteration 5968, loss = 0.11025371
Iteration 5969, loss = 0.11022836
Iteration 5970, loss = 0.11020295
Iteration 5971, loss = 0.11017754
Iteration 5972, loss = 0.11015219
Iteration 5973, loss = 0.11012721
Iteration 5974, loss = 0.11010179
Iteration 5975, loss = 0.11007654
Iteration 5976, loss = 0.11005126
Iteration 5977, loss = 0.11002584
Iteration 5978, loss = 0.11000077
Iteration 5979, loss = 0.10997567
Iteration 5980, loss = 0.10995022
Iteration 5981, loss = 0.10992482
Iteration 5982, loss = 0.10989990
Iteration 5983, loss = 0.10987435
Iteration 5984, loss = 0.10984961
Iteration 5985, loss = 0.10982446
Iteration 5986, loss = 0.10979929
Iteration 5987, loss = 0.10977408
Iteration 5988, loss = 0.10974889
Iteration 5989, loss = 0.10972364
Iteration 5990, loss = 0.10969904
Iteration 5991, loss = 0.10967358
Iteration 5992, loss = 0.10964883
Iteration 5993, loss = 0.10962333
Iteration 5994, loss = 0.10959879
Iteration 5995, loss = 0.10957343
Iteration 5996, loss = 0.10954844
Iteration 5997, loss = 0.10952352
Iteration 5998, loss = 0.10949832
Iteration 5999, loss = 0.10947365
Iteration 6000, loss = 0.10944832
Iteration 6001, loss = 0.10942362
Iteration 6002, loss = 0.10939861
Iteration 6003, loss = 0.10937356
Iteration 6004, loss = 0.10934887
Iteration 6005, loss = 0.10932386
Iteration 6006, loss = 0.10929876
Iteration 6007, loss = 0.10927421
Iteration 6008, loss = 0.10924922
Iteration 6009, loss = 0.10922421
Iteration 6010, loss = 0.10919914
Iteration 6011, loss = 0.10917411
Iteration 6012, loss = 0.10914911
Iteration 6013, loss = 0.10912424
Iteration 6014, loss = 0.10909950
Iteration 6015, loss = 0.10907445
Iteration 6016, loss = 0.10904958
Iteration 6017, loss = 0.10902446
Iteration 6018, loss = 0.10899989
Iteration 6019, loss = 0.10897479
Iteration 6020, loss = 0.10895022
Iteration 6021, loss = 0.10892556
Iteration 6022, loss = 0.10890053
Iteration 6023, loss = 0.10887576
Iteration 6024, loss = 0.10885093
Iteration 6025, loss = 0.10882651
Iteration 6026, loss = 0.10880154
Iteration 6027, loss = 0.10877702
Iteration 6028, loss = 0.10875248
Iteration 6029, loss = 0.10872778
Iteration 6030, loss = 0.10870289
Iteration 6031, loss = 0.10867839
Iteration 6032, loss = 0.10865355
Iteration 6033, loss = 0.10862930
Iteration 6034, loss = 0.10860457
Iteration 6035, loss = 0.10858009
Iteration 6036, loss = 0.10855554
Iteration 6037, loss = 0.10853095
Iteration 6038, loss = 0.10850646
Iteration 6039, loss = 0.10848163
Iteration 6040, loss = 0.10845742
Iteration 6041, loss = 0.10843292
Iteration 6042, loss = 0.10840834
Iteration 6043, loss = 0.10838373
Iteration 6044, loss = 0.10835931
Iteration 6045, loss = 0.10833531
Iteration 6046, loss = 0.10831027
Iteration 6047, loss = 0.10828597
Iteration 6048, loss = 0.10826182
Iteration 6049, loss = 0.10823695
Iteration 6050, loss = 0.10821261
Iteration 6051, loss = 0.10818802
Iteration 6052, loss = 0.10816345
Iteration 6053, loss = 0.10813913
Iteration 6054, loss = 0.10811451
Iteration 6055, loss = 0.10809004
Iteration 6056, loss = 0.10806580
Iteration 6057, loss = 0.10804125
Iteration 6058, loss = 0.10801686
Iteration 6059, loss = 0.10799260
Iteration 6060, loss = 0.10796809
Iteration 6061, loss = 0.10794394
Iteration 6062, loss = 0.10791930
Iteration 6063, loss = 0.10789486
Iteration 6064, loss = 0.10787038
Iteration 6065, loss = 0.10784630
Iteration 6066, loss = 0.10782178
Iteration 6067, loss = 0.10779728
Iteration 6068, loss = 0.10777340
Iteration 6069, loss = 0.10774871
Iteration 6070, loss = 0.10772453
Iteration 6071, loss = 0.10770027
Iteration 6072, loss = 0.10767567
Iteration 6073, loss = 0.10765178
Iteration 6074, loss = 0.10762756
Iteration 6075, loss = 0.10760288
Iteration 6076, loss = 0.10757899
Iteration 6077, loss = 0.10755475
Iteration 6078, loss = 0.10753064
Iteration 6079, loss = 0.10750599
Iteration 6080, loss = 0.10748224
Iteration 6081, loss = 0.10745766
Iteration 6082, loss = 0.10743376
Iteration 6083, loss = 0.10740978
Iteration 6084, loss = 0.10738551
Iteration 6085, loss = 0.10736128
Iteration 6086, loss = 0.10733706
Iteration 6087, loss = 0.10731290
Iteration 6088, loss = 0.10728902
Iteration 6089, loss = 0.10726496
Iteration 6090, loss = 0.10724101
Iteration 6091, loss = 0.10721666
Iteration 6092, loss = 0.10719294
Iteration 6093, loss = 0.10716895
Iteration 6094, loss = 0.10714497
Iteration 6095, loss = 0.10712090
Iteration 6096, loss = 0.10709695
Iteration 6097, loss = 0.10707300
Iteration 6098, loss = 0.10704916
Iteration 6099, loss = 0.10702534
Iteration 6100, loss = 0.10700140
Iteration 6101, loss = 0.10697737
Iteration 6102, loss = 0.10695341
Iteration 6103, loss = 0.10692983
Iteration 6104, loss = 0.10690597
Iteration 6105, loss = 0.10688193
Iteration 6106, loss = 0.10685778
Iteration 6107, loss = 0.10683391
Iteration 6108, loss = 0.10681063
Iteration 6109, loss = 0.10678678
Iteration 6110, loss = 0.10676256
Iteration 6111, loss = 0.10673888
Iteration 6112, loss = 0.10671521
Iteration 6113, loss = 0.10669124
Iteration 6114, loss = 0.10666782
Iteration 6115, loss = 0.10664404
Iteration 6116, loss = 0.10662034
Iteration 6117, loss = 0.10659655
Iteration 6118, loss = 0.10657275
Iteration 6119, loss = 0.10654923
Iteration 6120, loss = 0.10652518
Iteration 6121, loss = 0.10650161
Iteration 6122, loss = 0.10647801
Iteration 6123, loss = 0.10645431
Iteration 6124, loss = 0.10643073
Iteration 6125, loss = 0.10640715
Iteration 6126, loss = 0.10638333
Iteration 6127, loss = 0.10636007
Iteration 6128, loss = 0.10633621
Iteration 6129, loss = 0.10631273
Iteration 6130, loss = 0.10628909
Iteration 6131, loss = 0.10626497
Iteration 6132, loss = 0.10624173
Iteration 6133, loss = 0.10621810
Iteration 6134, loss = 0.10619436
Iteration 6135, loss = 0.10617059
Iteration 6136, loss = 0.10614724
Iteration 6137, loss = 0.10612375
Iteration 6138, loss = 0.10610021
Iteration 6139, loss = 0.10607652
Iteration 6140, loss = 0.10605271
Iteration 6141, loss = 0.10602942
Iteration 6142, loss = 0.10600579
Iteration 6143, loss = 0.10598249
Iteration 6144, loss = 0.10595893
Iteration 6145, loss = 0.10593545
Iteration 6146, loss = 0.10591209
Iteration 6147, loss = 0.10588842
Iteration 6148, loss = 0.10586485
Iteration 6149, loss = 0.10584159
Iteration 6150, loss = 0.10581787
Iteration 6151, loss = 0.10579432
Iteration 6152, loss = 0.10577104
Iteration 6153, loss = 0.10574758
Iteration 6154, loss = 0.10572408
Iteration 6155, loss = 0.10570096
Iteration 6156, loss = 0.10567759
Iteration 6157, loss = 0.10565405
Iteration 6158, loss = 0.10563063
Iteration 6159, loss = 0.10560746
Iteration 6160, loss = 0.10558414
Iteration 6161, loss = 0.10556074
Iteration 6162, loss = 0.10553771
Iteration 6163, loss = 0.10551413
Iteration 6164, loss = 0.10549110
Iteration 6165, loss = 0.10546801
Iteration 6166, loss = 0.10544433
Iteration 6167, loss = 0.10542118
Iteration 6168, loss = 0.10539839
Iteration 6169, loss = 0.10537468
Iteration 6170, loss = 0.10535153
Iteration 6171, loss = 0.10532823
Iteration 6172, loss = 0.10530516
Iteration 6173, loss = 0.10528169
Iteration 6174, loss = 0.10525894
Iteration 6175, loss = 0.10523561
Iteration 6176, loss = 0.10521244
Iteration 6177, loss = 0.10518891
Iteration 6178, loss = 0.10516616
Iteration 6179, loss = 0.10514289
Iteration 6180, loss = 0.10511976
Iteration 6181, loss = 0.10509657
Iteration 6182, loss = 0.10507340
Iteration 6183, loss = 0.10505033
Iteration 6184, loss = 0.10502706
Iteration 6185, loss = 0.10500376
Iteration 6186, loss = 0.10498071
Iteration 6187, loss = 0.10495768
Iteration 6188, loss = 0.10493423
Iteration 6189, loss = 0.10491137
Iteration 6190, loss = 0.10488841
Iteration 6191, loss = 0.10486512
Iteration 6192, loss = 0.10484222
Iteration 6193, loss = 0.10481940
Iteration 6194, loss = 0.10479584
Iteration 6195, loss = 0.10477304
Iteration 6196, loss = 0.10475016
Iteration 6197, loss = 0.10472680
Iteration 6198, loss = 0.10470386
Iteration 6199, loss = 0.10468077
Iteration 6200, loss = 0.10465766
Iteration 6201, loss = 0.10463492
Iteration 6202, loss = 0.10461189
Iteration 6203, loss = 0.10458880
Iteration 6204, loss = 0.10456593
Iteration 6205, loss = 0.10454317
Iteration 6206, loss = 0.10452023
Iteration 6207, loss = 0.10449720
Iteration 6208, loss = 0.10447410
Iteration 6209, loss = 0.10445149
Iteration 6210, loss = 0.10442855
Iteration 6211, loss = 0.10440579
Iteration 6212, loss = 0.10438319
Iteration 6213, loss = 0.10435993
Iteration 6214, loss = 0.10433729
Iteration 6215, loss = 0.10431456
Iteration 6216, loss = 0.10429189
Iteration 6217, loss = 0.10426925
Iteration 6218, loss = 0.10424657
Iteration 6219, loss = 0.10422356
Iteration 6220, loss = 0.10420110
Iteration 6221, loss = 0.10417829
Iteration 6222, loss = 0.10415563
Iteration 6223, loss = 0.10413287
Iteration 6224, loss = 0.10411052
Iteration 6225, loss = 0.10408788
Iteration 6226, loss = 0.10406502
Iteration 6227, loss = 0.10404266
Iteration 6228, loss = 0.10401986
Iteration 6229, loss = 0.10399725
Iteration 6230, loss = 0.10397480
Iteration 6231, loss = 0.10395216
Iteration 6232, loss = 0.10392918
Iteration 6233, loss = 0.10390651
Iteration 6234, loss = 0.10388401
Iteration 6235, loss = 0.10386130
Iteration 6236, loss = 0.10383902
Iteration 6237, loss = 0.10381636
Iteration 6238, loss = 0.10379382
Iteration 6239, loss = 0.10377076
Iteration 6240, loss = 0.10374831
Iteration 6241, loss = 0.10372594
Iteration 6242, loss = 0.10370352
Iteration 6243, loss = 0.10368059
Iteration 6244, loss = 0.10365801
Iteration 6245, loss = 0.10363567
Iteration 6246, loss = 0.10361281
Iteration 6247, loss = 0.10359050
Iteration 6248, loss = 0.10356782
Iteration 6249, loss = 0.10354514
Iteration 6250, loss = 0.10352267
Iteration 6251, loss = 0.10349999
Iteration 6252, loss = 0.10347749
Iteration 6253, loss = 0.10345490
Iteration 6254, loss = 0.10343238
Iteration 6255, loss = 0.10340997
Iteration 6256, loss = 0.10338744
Iteration 6257, loss = 0.10336486
Iteration 6258, loss = 0.10334251
Iteration 6259, loss = 0.10332024
Iteration 6260, loss = 0.10329749
Iteration 6261, loss = 0.10327562
Iteration 6262, loss = 0.10325276
Iteration 6263, loss = 0.10323061
Iteration 6264, loss = 0.10320793
Iteration 6265, loss = 0.10318588
Iteration 6266, loss = 0.10316332
Iteration 6267, loss = 0.10314103
Iteration 6268, loss = 0.10311864
Iteration 6269, loss = 0.10309607
Iteration 6270, loss = 0.10307375
Iteration 6271, loss = 0.10305162
Iteration 6272, loss = 0.10302928
Iteration 6273, loss = 0.10300687
Iteration 6274, loss = 0.10298488
Iteration 6275, loss = 0.10296242
Iteration 6276, loss = 0.10294022
Iteration 6277, loss = 0.10291761
Iteration 6278, loss = 0.10289556
Iteration 6279, loss = 0.10287312
Iteration 6280, loss = 0.10285076
Iteration 6281, loss = 0.10282867
Iteration 6282, loss = 0.10280647
Iteration 6283, loss = 0.10278405
Iteration 6284, loss = 0.10276204
Iteration 6285, loss = 0.10273954
Iteration 6286, loss = 0.10271773
Iteration 6287, loss = 0.10269524
Iteration 6288, loss = 0.10267333
Iteration 6289, loss = 0.10265108
Iteration 6290, loss = 0.10262908
Iteration 6291, loss = 0.10260674
Iteration 6292, loss = 0.10258486
Iteration 6293, loss = 0.10256248
Iteration 6294, loss = 0.10254071
Iteration 6295, loss = 0.10251853
Iteration 6296, loss = 0.10249617
Iteration 6297, loss = 0.10247451
Iteration 6298, loss = 0.10245231
Iteration 6299, loss = 0.10243038
Iteration 6300, loss = 0.10240827
Iteration 6301, loss = 0.10238633
Iteration 6302, loss = 0.10236399
Iteration 6303, loss = 0.10234211
Iteration 6304, loss = 0.10231994
Iteration 6305, loss = 0.10229830
Iteration 6306, loss = 0.10227599
Iteration 6307, loss = 0.10225416
Iteration 6308, loss = 0.10223198
Iteration 6309, loss = 0.10221005
Iteration 6310, loss = 0.10218815
Iteration 6311, loss = 0.10216605
Iteration 6312, loss = 0.10214436
Iteration 6313, loss = 0.10212248
Iteration 6314, loss = 0.10210035
Iteration 6315, loss = 0.10207829
Iteration 6316, loss = 0.10205641
Iteration 6317, loss = 0.10203465
Iteration 6318, loss = 0.10201255
Iteration 6319, loss = 0.10199078
Iteration 6320, loss = 0.10196896
Iteration 6321, loss = 0.10194720
Iteration 6322, loss = 0.10192514
Iteration 6323, loss = 0.10190372
Iteration 6324, loss = 0.10188171
Iteration 6325, loss = 0.10185962
Iteration 6326, loss = 0.10183835
Iteration 6327, loss = 0.10181636
Iteration 6328, loss = 0.10179460
Iteration 6329, loss = 0.10177266
Iteration 6330, loss = 0.10175118
Iteration 6331, loss = 0.10172931
Iteration 6332, loss = 0.10170741
Iteration 6333, loss = 0.10168578
Iteration 6334, loss = 0.10166404
Iteration 6335, loss = 0.10164212
Iteration 6336, loss = 0.10162026
Iteration 6337, loss = 0.10159862
Iteration 6338, loss = 0.10157678
Iteration 6339, loss = 0.10155526
Iteration 6340, loss = 0.10153353
Iteration 6341, loss = 0.10151174
Iteration 6342, loss = 0.10148987
Iteration 6343, loss = 0.10146840
Iteration 6344, loss = 0.10144678
Iteration 6345, loss = 0.10142472
Iteration 6346, loss = 0.10140357
Iteration 6347, loss = 0.10138163
Iteration 6348, loss = 0.10136016
Iteration 6349, loss = 0.10133866
Iteration 6350, loss = 0.10131701
Iteration 6351, loss = 0.10129531
Iteration 6352, loss = 0.10127381
Iteration 6353, loss = 0.10125238
Iteration 6354, loss = 0.10123091
Iteration 6355, loss = 0.10120908
Iteration 6356, loss = 0.10118801
Iteration 6357, loss = 0.10116603
Iteration 6358, loss = 0.10114469
Iteration 6359, loss = 0.10112334
Iteration 6360, loss = 0.10110155
Iteration 6361, loss = 0.10108015
Iteration 6362, loss = 0.10105860
Iteration 6363, loss = 0.10103731
Iteration 6364, loss = 0.10101612
Iteration 6365, loss = 0.10099454
Iteration 6366, loss = 0.10097306
Iteration 6367, loss = 0.10095176
Iteration 6368, loss = 0.10093019
Iteration 6369, loss = 0.10090856
Iteration 6370, loss = 0.10088759
Iteration 6371, loss = 0.10086599
Iteration 6372, loss = 0.10084457
Iteration 6373, loss = 0.10082290
Iteration 6374, loss = 0.10080149
Iteration 6375, loss = 0.10078037
Iteration 6376, loss = 0.10075890
Iteration 6377, loss = 0.10073695
Iteration 6378, loss = 0.10071580
Iteration 6379, loss = 0.10069451
Iteration 6380, loss = 0.10067341
Iteration 6381, loss = 0.10065132
Iteration 6382, loss = 0.10063069
Iteration 6383, loss = 0.10060907
Iteration 6384, loss = 0.10058775
Iteration 6385, loss = 0.10056652
Iteration 6386, loss = 0.10054504
Iteration 6387, loss = 0.10052375
Iteration 6388, loss = 0.10050243
Iteration 6389, loss = 0.10048121
Iteration 6390, loss = 0.10045981
Iteration 6391, loss = 0.10043875
Iteration 6392, loss = 0.10041738
Iteration 6393, loss = 0.10039611
Iteration 6394, loss = 0.10037489
Iteration 6395, loss = 0.10035338
Iteration 6396, loss = 0.10033234
Iteration 6397, loss = 0.10031106
Iteration 6398, loss = 0.10028985
Iteration 6399, loss = 0.10026871
Iteration 6400, loss = 0.10024758
Iteration 6401, loss = 0.10022646
Iteration 6402, loss = 0.10020516
Iteration 6403, loss = 0.10018422
Iteration 6404, loss = 0.10016313
Iteration 6405, loss = 0.10014190
Iteration 6406, loss = 0.10012070
Iteration 6407, loss = 0.10009994
Iteration 6408, loss = 0.10007865
Iteration 6409, loss = 0.10005757
Iteration 6410, loss = 0.10003664
Iteration 6411, loss = 0.10001533
Iteration 6412, loss = 0.09999433
Iteration 6413, loss = 0.09997338
Iteration 6414, loss = 0.09995208
Iteration 6415, loss = 0.09993082
Iteration 6416, loss = 0.09991012
Iteration 6417, loss = 0.09988894
Iteration 6418, loss = 0.09986753
Iteration 6419, loss = 0.09984681
Iteration 6420, loss = 0.09982589
Iteration 6421, loss = 0.09980441
Iteration 6422, loss = 0.09978381
Iteration 6423, loss = 0.09976251
Iteration 6424, loss = 0.09974165
Iteration 6425, loss = 0.09972056
Iteration 6426, loss = 0.09969991
Iteration 6427, loss = 0.09967864
Iteration 6428, loss = 0.09965782
Iteration 6429, loss = 0.09963690
Iteration 6430, loss = 0.09961567
Iteration 6431, loss = 0.09959528
Iteration 6432, loss = 0.09957410
Iteration 6433, loss = 0.09955267
Iteration 6434, loss = 0.09953171
Iteration 6435, loss = 0.09951092
Iteration 6436, loss = 0.09949001
Iteration 6437, loss = 0.09946929
Iteration 6438, loss = 0.09944781
Iteration 6439, loss = 0.09942721
Iteration 6440, loss = 0.09940634
Iteration 6441, loss = 0.09938546
Iteration 6442, loss = 0.09936453
Iteration 6443, loss = 0.09934374
Iteration 6444, loss = 0.09932311
Iteration 6445, loss = 0.09930201
Iteration 6446, loss = 0.09928115
Iteration 6447, loss = 0.09926053
Iteration 6448, loss = 0.09923980
Iteration 6449, loss = 0.09921868
Iteration 6450, loss = 0.09919792
Iteration 6451, loss = 0.09917730
Iteration 6452, loss = 0.09915680
Iteration 6453, loss = 0.09913569
Iteration 6454, loss = 0.09911509
Iteration 6455, loss = 0.09909413
Iteration 6456, loss = 0.09907364
Iteration 6457, loss = 0.09905287
Iteration 6458, loss = 0.09903205
Iteration 6459, loss = 0.09901116
Iteration 6460, loss = 0.09899072
Iteration 6461, loss = 0.09897015
Iteration 6462, loss = 0.09894925
Iteration 6463, loss = 0.09892863
Iteration 6464, loss = 0.09890806
Iteration 6465, loss = 0.09888750
Iteration 6466, loss = 0.09886652
Iteration 6467, loss = 0.09884628
Iteration 6468, loss = 0.09882556
Iteration 6469, loss = 0.09880492
Iteration 6470, loss = 0.09878445
Iteration 6471, loss = 0.09876399
Iteration 6472, loss = 0.09874341
Iteration 6473, loss = 0.09872255
Iteration 6474, loss = 0.09870222
Iteration 6475, loss = 0.09868150
Iteration 6476, loss = 0.09866108
Iteration 6477, loss = 0.09864044
Iteration 6478, loss = 0.09861987
Iteration 6479, loss = 0.09859951
Iteration 6480, loss = 0.09857866
Iteration 6481, loss = 0.09855826
Iteration 6482, loss = 0.09853778
Iteration 6483, loss = 0.09851749
Iteration 6484, loss = 0.09849690
Iteration 6485, loss = 0.09847616
Iteration 6486, loss = 0.09845558
Iteration 6487, loss = 0.09843526
Iteration 6488, loss = 0.09841497
Iteration 6489, loss = 0.09839428
Iteration 6490, loss = 0.09837392
Iteration 6491, loss = 0.09835341
Iteration 6492, loss = 0.09833314
Iteration 6493, loss = 0.09831267
Iteration 6494, loss = 0.09829220
Iteration 6495, loss = 0.09827208
Iteration 6496, loss = 0.09825165
Iteration 6497, loss = 0.09823133
Iteration 6498, loss = 0.09821093
Iteration 6499, loss = 0.09819067
Iteration 6500, loss = 0.09817016
Iteration 6501, loss = 0.09814971
Iteration 6502, loss = 0.09812951
Iteration 6503, loss = 0.09810929
Iteration 6504, loss = 0.09808879
Iteration 6505, loss = 0.09806859
Iteration 6506, loss = 0.09804842
Iteration 6507, loss = 0.09802831
Iteration 6508, loss = 0.09800777
Iteration 6509, loss = 0.09798757
Iteration 6510, loss = 0.09796726
Iteration 6511, loss = 0.09794701
Iteration 6512, loss = 0.09792719
Iteration 6513, loss = 0.09790670
Iteration 6514, loss = 0.09788651
Iteration 6515, loss = 0.09786639
Iteration 6516, loss = 0.09784626
Iteration 6517, loss = 0.09782616
Iteration 6518, loss = 0.09780582
Iteration 6519, loss = 0.09778567
Iteration 6520, loss = 0.09776546
Iteration 6521, loss = 0.09774534
Iteration 6522, loss = 0.09772527
Iteration 6523, loss = 0.09770495
Iteration 6524, loss = 0.09768499
Iteration 6525, loss = 0.09766457
Iteration 6526, loss = 0.09764454
Iteration 6527, loss = 0.09762457
Iteration 6528, loss = 0.09760422
Iteration 6529, loss = 0.09758411
Iteration 6530, loss = 0.09756397
Iteration 6531, loss = 0.09754385
Iteration 6532, loss = 0.09752415
Iteration 6533, loss = 0.09750340
Iteration 6534, loss = 0.09748355
Iteration 6535, loss = 0.09746342
Iteration 6536, loss = 0.09744334
Iteration 6537, loss = 0.09742343
Iteration 6538, loss = 0.09740330
Iteration 6539, loss = 0.09738323
Iteration 6540, loss = 0.09736334
Iteration 6541, loss = 0.09734312
Iteration 6542, loss = 0.09732319
Iteration 6543, loss = 0.09730345
Iteration 6544, loss = 0.09728331
Iteration 6545, loss = 0.09726323
Iteration 6546, loss = 0.09724340
Iteration 6547, loss = 0.09722345
Iteration 6548, loss = 0.09720355
Iteration 6549, loss = 0.09718351
Iteration 6550, loss = 0.09716396
Iteration 6551, loss = 0.09714389
Iteration 6552, loss = 0.09712376
Iteration 6553, loss = 0.09710383
Iteration 6554, loss = 0.09708401
Iteration 6555, loss = 0.09706417
Iteration 6556, loss = 0.09704447
Iteration 6557, loss = 0.09702467
Iteration 6558, loss = 0.09700455
Iteration 6559, loss = 0.09698476
Iteration 6560, loss = 0.09696473
Iteration 6561, loss = 0.09694483
Iteration 6562, loss = 0.09692534
Iteration 6563, loss = 0.09690510
Iteration 6564, loss = 0.09688540
Iteration 6565, loss = 0.09686551
Iteration 6566, loss = 0.09684567
Iteration 6567, loss = 0.09682575
Iteration 6568, loss = 0.09680605
Iteration 6569, loss = 0.09678650
Iteration 6570, loss = 0.09676648
Iteration 6571, loss = 0.09674683
Iteration 6572, loss = 0.09672707
Iteration 6573, loss = 0.09670719
Iteration 6574, loss = 0.09668755
Iteration 6575, loss = 0.09666789
Iteration 6576, loss = 0.09664785
Iteration 6577, loss = 0.09662821
Iteration 6578, loss = 0.09660854
Iteration 6579, loss = 0.09658862
Iteration 6580, loss = 0.09656905
Iteration 6581, loss = 0.09654948
Iteration 6582, loss = 0.09652958
Iteration 6583, loss = 0.09650979
Iteration 6584, loss = 0.09649040
Iteration 6585, loss = 0.09647060
Iteration 6586, loss = 0.09645080
Iteration 6587, loss = 0.09643144
Iteration 6588, loss = 0.09641142
Iteration 6589, loss = 0.09639195
Iteration 6590, loss = 0.09637233
Iteration 6591, loss = 0.09635289
Iteration 6592, loss = 0.09633317
Iteration 6593, loss = 0.09631349
Iteration 6594, loss = 0.09629395
Iteration 6595, loss = 0.09627467
Iteration 6596, loss = 0.09625488
Iteration 6597, loss = 0.09623536
Iteration 6598, loss = 0.09621575
Iteration 6599, loss = 0.09619650
Iteration 6600, loss = 0.09617675
Iteration 6601, loss = 0.09615705
Iteration 6602, loss = 0.09613765
Iteration 6603, loss = 0.09611818
Iteration 6604, loss = 0.09609858
Iteration 6605, loss = 0.09607919
Iteration 6606, loss = 0.09605962
Iteration 6607, loss = 0.09604029
Iteration 6608, loss = 0.09602084
Iteration 6609, loss = 0.09600137
Iteration 6610, loss = 0.09598182
Iteration 6611, loss = 0.09596250
Iteration 6612, loss = 0.09594297
Iteration 6613, loss = 0.09592354
Iteration 6614, loss = 0.09590409
Iteration 6615, loss = 0.09588432
Iteration 6616, loss = 0.09586505
Iteration 6617, loss = 0.09584551
Iteration 6618, loss = 0.09582640
Iteration 6619, loss = 0.09580671
Iteration 6620, loss = 0.09578722
Iteration 6621, loss = 0.09576790
Iteration 6622, loss = 0.09574840
Iteration 6623, loss = 0.09572887
Iteration 6624, loss = 0.09570944
Iteration 6625, loss = 0.09569028
Iteration 6626, loss = 0.09567096
Iteration 6627, loss = 0.09565166
Iteration 6628, loss = 0.09563207
Iteration 6629, loss = 0.09561290
Iteration 6630, loss = 0.09559346
Iteration 6631, loss = 0.09557399
Iteration 6632, loss = 0.09555477
Iteration 6633, loss = 0.09553544
Iteration 6634, loss = 0.09551622
Iteration 6635, loss = 0.09549703
Iteration 6636, loss = 0.09547792
Iteration 6637, loss = 0.09545835
Iteration 6638, loss = 0.09543918
Iteration 6639, loss = 0.09541989
Iteration 6640, loss = 0.09540066
Iteration 6641, loss = 0.09538118
Iteration 6642, loss = 0.09536220
Iteration 6643, loss = 0.09534255
Iteration 6644, loss = 0.09532375
Iteration 6645, loss = 0.09530421
Iteration 6646, loss = 0.09528520
Iteration 6647, loss = 0.09526569
Iteration 6648, loss = 0.09524643
Iteration 6649, loss = 0.09522742
Iteration 6650, loss = 0.09520808
Iteration 6651, loss = 0.09518898
Iteration 6652, loss = 0.09516980
Iteration 6653, loss = 0.09515049
Iteration 6654, loss = 0.09513145
Iteration 6655, loss = 0.09511239
Iteration 6656, loss = 0.09509313
Iteration 6657, loss = 0.09507388
Iteration 6658, loss = 0.09505494
Iteration 6659, loss = 0.09503579
Iteration 6660, loss = 0.09501678
Iteration 6661, loss = 0.09499774
Iteration 6662, loss = 0.09497857
Iteration 6663, loss = 0.09495967
Iteration 6664, loss = 0.09494063
Iteration 6665, loss = 0.09492153
Iteration 6666, loss = 0.09490261
Iteration 6667, loss = 0.09488361
Iteration 6668, loss = 0.09486447
Iteration 6669, loss = 0.09484562
Iteration 6670, loss = 0.09482656
Iteration 6671, loss = 0.09480746
Iteration 6672, loss = 0.09478845
Iteration 6673, loss = 0.09476974
Iteration 6674, loss = 0.09475042
Iteration 6675, loss = 0.09473179
Iteration 6676, loss = 0.09471260
Iteration 6677, loss = 0.09469368
Iteration 6678, loss = 0.09467464
Iteration 6679, loss = 0.09465545
Iteration 6680, loss = 0.09463664
Iteration 6681, loss = 0.09461789
Iteration 6682, loss = 0.09459863
Iteration 6683, loss = 0.09457965
Iteration 6684, loss = 0.09456097
Iteration 6685, loss = 0.09454186
Iteration 6686, loss = 0.09452272
Iteration 6687, loss = 0.09450380
Iteration 6688, loss = 0.09448508
Iteration 6689, loss = 0.09446624
Iteration 6690, loss = 0.09444719
Iteration 6691, loss = 0.09442833
Iteration 6692, loss = 0.09440947
Iteration 6693, loss = 0.09439058
Iteration 6694, loss = 0.09437159
Iteration 6695, loss = 0.09435272
Iteration 6696, loss = 0.09433401
Iteration 6697, loss = 0.09431540
Iteration 6698, loss = 0.09429612
Iteration 6699, loss = 0.09427756
Iteration 6700, loss = 0.09425860
Iteration 6701, loss = 0.09423984
Iteration 6702, loss = 0.09422106
Iteration 6703, loss = 0.09420238
Iteration 6704, loss = 0.09418376
Iteration 6705, loss = 0.09416475
Iteration 6706, loss = 0.09414593
Iteration 6707, loss = 0.09412755
Iteration 6708, loss = 0.09410851
Iteration 6709, loss = 0.09408988
Iteration 6710, loss = 0.09407123
Iteration 6711, loss = 0.09405245
Iteration 6712, loss = 0.09403361
Iteration 6713, loss = 0.09401483
Iteration 6714, loss = 0.09399626
Iteration 6715, loss = 0.09397738
Iteration 6716, loss = 0.09395873
Iteration 6717, loss = 0.09394005
Iteration 6718, loss = 0.09392119
Iteration 6719, loss = 0.09390275
Iteration 6720, loss = 0.09388381
Iteration 6721, loss = 0.09386534
Iteration 6722, loss = 0.09384661
Iteration 6723, loss = 0.09382795
Iteration 6724, loss = 0.09380923
Iteration 6725, loss = 0.09379046
Iteration 6726, loss = 0.09377199
Iteration 6727, loss = 0.09375331
Iteration 6728, loss = 0.09373473
Iteration 6729, loss = 0.09371619
Iteration 6730, loss = 0.09369747
Iteration 6731, loss = 0.09367898
Iteration 6732, loss = 0.09366024
Iteration 6733, loss = 0.09364150
Iteration 6734, loss = 0.09362310
Iteration 6735, loss = 0.09360452
Iteration 6736, loss = 0.09358596
Iteration 6737, loss = 0.09356743
Iteration 6738, loss = 0.09354883
Iteration 6739, loss = 0.09353014
Iteration 6740, loss = 0.09351157
Iteration 6741, loss = 0.09349342
Iteration 6742, loss = 0.09347481
Iteration 6743, loss = 0.09345605
Iteration 6744, loss = 0.09343782
Iteration 6745, loss = 0.09341911
Iteration 6746, loss = 0.09340074
Iteration 6747, loss = 0.09338221
Iteration 6748, loss = 0.09336382
Iteration 6749, loss = 0.09334544
Iteration 6750, loss = 0.09332695
Iteration 6751, loss = 0.09330837
Iteration 6752, loss = 0.09329003
Iteration 6753, loss = 0.09327143
Iteration 6754, loss = 0.09325303
Iteration 6755, loss = 0.09323459
Iteration 6756, loss = 0.09321608
Iteration 6757, loss = 0.09319773
Iteration 6758, loss = 0.09317935
Iteration 6759, loss = 0.09316098
Iteration 6760, loss = 0.09314252
Iteration 6761, loss = 0.09312455
Iteration 6762, loss = 0.09310553
Iteration 6763, loss = 0.09308732
Iteration 6764, loss = 0.09306864
Iteration 6765, loss = 0.09305055
Iteration 6766, loss = 0.09303214
Iteration 6767, loss = 0.09301378
Iteration 6768, loss = 0.09299558
Iteration 6769, loss = 0.09297720
Iteration 6770, loss = 0.09295854
Iteration 6771, loss = 0.09294047
Iteration 6772, loss = 0.09292221
Iteration 6773, loss = 0.09290360
Iteration 6774, loss = 0.09288542
Iteration 6775, loss = 0.09286727
Iteration 6776, loss = 0.09284891
Iteration 6777, loss = 0.09283046
Iteration 6778, loss = 0.09281230
Iteration 6779, loss = 0.09279412
Iteration 6780, loss = 0.09277572
Iteration 6781, loss = 0.09275749
Iteration 6782, loss = 0.09273928
Iteration 6783, loss = 0.09272112
Iteration 6784, loss = 0.09270291
Iteration 6785, loss = 0.09268449
Iteration 6786, loss = 0.09266661
Iteration 6787, loss = 0.09264813
Iteration 6788, loss = 0.09263000
Iteration 6789, loss = 0.09261222
Iteration 6790, loss = 0.09259381
Iteration 6791, loss = 0.09257559
Iteration 6792, loss = 0.09255730
Iteration 6793, loss = 0.09253910
Iteration 6794, loss = 0.09252110
Iteration 6795, loss = 0.09250278
Iteration 6796, loss = 0.09248468
Iteration 6797, loss = 0.09246654
Iteration 6798, loss = 0.09244866
Iteration 6799, loss = 0.09243023
Iteration 6800, loss = 0.09241201
Iteration 6801, loss = 0.09239415
Iteration 6802, loss = 0.09237582
Iteration 6803, loss = 0.09235784
Iteration 6804, loss = 0.09233979
Iteration 6805, loss = 0.09232151
Iteration 6806, loss = 0.09230326
Iteration 6807, loss = 0.09228537
Iteration 6808, loss = 0.09226732
Iteration 6809, loss = 0.09224897
Iteration 6810, loss = 0.09223103
Iteration 6811, loss = 0.09221285
Iteration 6812, loss = 0.09219472
Iteration 6813, loss = 0.09217654
Iteration 6814, loss = 0.09215884
Iteration 6815, loss = 0.09214081
Iteration 6816, loss = 0.09212293
Iteration 6817, loss = 0.09210438
Iteration 6818, loss = 0.09208664
Iteration 6819, loss = 0.09206856
Iteration 6820, loss = 0.09205063
Iteration 6821, loss = 0.09203291
Iteration 6822, loss = 0.09201483
Iteration 6823, loss = 0.09199699
Iteration 6824, loss = 0.09197915
Iteration 6825, loss = 0.09196113
Iteration 6826, loss = 0.09194356
Iteration 6827, loss = 0.09192562
Iteration 6828, loss = 0.09190772
Iteration 6829, loss = 0.09188971
Iteration 6830, loss = 0.09187173
Iteration 6831, loss = 0.09185403
Iteration 6832, loss = 0.09183603
Iteration 6833, loss = 0.09181835
Iteration 6834, loss = 0.09180058
Iteration 6835, loss = 0.09178256
Iteration 6836, loss = 0.09176463
Iteration 6837, loss = 0.09174700
Iteration 6838, loss = 0.09172903
Iteration 6839, loss = 0.09171132
Iteration 6840, loss = 0.09169332
Iteration 6841, loss = 0.09167518
Iteration 6842, loss = 0.09165759
Iteration 6843, loss = 0.09163961
Iteration 6844, loss = 0.09162188
Iteration 6845, loss = 0.09160419
Iteration 6846, loss = 0.09158626
Iteration 6847, loss = 0.09156840
Iteration 6848, loss = 0.09155076
Iteration 6849, loss = 0.09153266
Iteration 6850, loss = 0.09151488
Iteration 6851, loss = 0.09149719
Iteration 6852, loss = 0.09147939
Iteration 6853, loss = 0.09146144
Iteration 6854, loss = 0.09144376
Iteration 6855, loss = 0.09142604
Iteration 6856, loss = 0.09140819
Iteration 6857, loss = 0.09139060
Iteration 6858, loss = 0.09137257
Iteration 6859, loss = 0.09135526
Iteration 6860, loss = 0.09133742
Iteration 6861, loss = 0.09131978
Iteration 6862, loss = 0.09130202
Iteration 6863, loss = 0.09128431
Iteration 6864, loss = 0.09126681
Iteration 6865, loss = 0.09124904
Iteration 6866, loss = 0.09123140
Iteration 6867, loss = 0.09121418
Iteration 6868, loss = 0.09119663
Iteration 6869, loss = 0.09117903
Iteration 6870, loss = 0.09116118
Iteration 6871, loss = 0.09114356
Iteration 6872, loss = 0.09112588
Iteration 6873, loss = 0.09110846
Iteration 6874, loss = 0.09109071
Iteration 6875, loss = 0.09107339
Iteration 6876, loss = 0.09105554
Iteration 6877, loss = 0.09103793
Iteration 6878, loss = 0.09102045
Iteration 6879, loss = 0.09100284
Iteration 6880, loss = 0.09098515
Iteration 6881, loss = 0.09096759
Iteration 6882, loss = 0.09095010
Iteration 6883, loss = 0.09093258
Iteration 6884, loss = 0.09091484
Iteration 6885, loss = 0.09089715
Iteration 6886, loss = 0.09087977
Iteration 6887, loss = 0.09086227
Iteration 6888, loss = 0.09084480
Iteration 6889, loss = 0.09082715
Iteration 6890, loss = 0.09080967
Iteration 6891, loss = 0.09079201
Iteration 6892, loss = 0.09077463
Iteration 6893, loss = 0.09075706
Iteration 6894, loss = 0.09073963
Iteration 6895, loss = 0.09072226
Iteration 6896, loss = 0.09070480
Iteration 6897, loss = 0.09068730
Iteration 6898, loss = 0.09067002
Iteration 6899, loss = 0.09065227
Iteration 6900, loss = 0.09063495
Iteration 6901, loss = 0.09061762
Iteration 6902, loss = 0.09060005
Iteration 6903, loss = 0.09058276
Iteration 6904, loss = 0.09056507
Iteration 6905, loss = 0.09054793
Iteration 6906, loss = 0.09053040
Iteration 6907, loss = 0.09051318
Iteration 6908, loss = 0.09049542
Iteration 6909, loss = 0.09047836
Iteration 6910, loss = 0.09046076
Iteration 6911, loss = 0.09044338
Iteration 6912, loss = 0.09042613
Iteration 6913, loss = 0.09040854
Iteration 6914, loss = 0.09039159
Iteration 6915, loss = 0.09037417
Iteration 6916, loss = 0.09035670
Iteration 6917, loss = 0.09033932
Iteration 6918, loss = 0.09032203
Iteration 6919, loss = 0.09030467
Iteration 6920, loss = 0.09028741
Iteration 6921, loss = 0.09027027
Iteration 6922, loss = 0.09025291
Iteration 6923, loss = 0.09023566
Iteration 6924, loss = 0.09021816
Iteration 6925, loss = 0.09020111
Iteration 6926, loss = 0.09018378
Iteration 6927, loss = 0.09016663
Iteration 6928, loss = 0.09014938
Iteration 6929, loss = 0.09013176
Iteration 6930, loss = 0.09011472
Iteration 6931, loss = 0.09009774
Iteration 6932, loss = 0.09008032
Iteration 6933, loss = 0.09006299
Iteration 6934, loss = 0.09004594
Iteration 6935, loss = 0.09002893
Iteration 6936, loss = 0.09001163
Iteration 6937, loss = 0.08999445
Iteration 6938, loss = 0.08997716
Iteration 6939, loss = 0.08996016
Iteration 6940, loss = 0.08994306
Iteration 6941, loss = 0.08992573
Iteration 6942, loss = 0.08990852
Iteration 6943, loss = 0.08989132
Iteration 6944, loss = 0.08987425
Iteration 6945, loss = 0.08985706
Iteration 6946, loss = 0.08984014
Iteration 6947, loss = 0.08982271
Iteration 6948, loss = 0.08980551
Iteration 6949, loss = 0.08978854
Iteration 6950, loss = 0.08977146
Iteration 6951, loss = 0.08975436
Iteration 6952, loss = 0.08973709
Iteration 6953, loss = 0.08972024
Iteration 6954, loss = 0.08970317
Iteration 6955, loss = 0.08968599
Iteration 6956, loss = 0.08966894
Iteration 6957, loss = 0.08965190
Iteration 6958, loss = 0.08963491
Iteration 6959, loss = 0.08961791
Iteration 6960, loss = 0.08960097
Iteration 6961, loss = 0.08958387
Iteration 6962, loss = 0.08956695
Iteration 6963, loss = 0.08954981
Iteration 6964, loss = 0.08953301
Iteration 6965, loss = 0.08951588
Iteration 6966, loss = 0.08949906
Iteration 6967, loss = 0.08948208
Iteration 6968, loss = 0.08946494
Iteration 6969, loss = 0.08944800
Iteration 6970, loss = 0.08943120
Iteration 6971, loss = 0.08941391
Iteration 6972, loss = 0.08939698
Iteration 6973, loss = 0.08938018
Iteration 6974, loss = 0.08936323
Iteration 6975, loss = 0.08934623
Iteration 6976, loss = 0.08932921
Iteration 6977, loss = 0.08931216
Iteration 6978, loss = 0.08929508
Iteration 6979, loss = 0.08927823
Iteration 6980, loss = 0.08926141
Iteration 6981, loss = 0.08924443
Iteration 6982, loss = 0.08922748
Iteration 6983, loss = 0.08921062
Iteration 6984, loss = 0.08919361
Iteration 6985, loss = 0.08917654
Iteration 6986, loss = 0.08915963
Iteration 6987, loss = 0.08914296
Iteration 6988, loss = 0.08912624
Iteration 6989, loss = 0.08910897
Iteration 6990, loss = 0.08909261
Iteration 6991, loss = 0.08907543
Iteration 6992, loss = 0.08905868
Iteration 6993, loss = 0.08904176
Iteration 6994, loss = 0.08902497
Iteration 6995, loss = 0.08900810
Iteration 6996, loss = 0.08899134
Iteration 6997, loss = 0.08897442
Iteration 6998, loss = 0.08895798
Iteration 6999, loss = 0.08894096
Iteration 7000, loss = 0.08892415
Iteration 7001, loss = 0.08890753
Iteration 7002, loss = 0.08889085
Iteration 7003, loss = 0.08887409
Iteration 7004, loss = 0.08885714
Iteration 7005, loss = 0.08884062
Iteration 7006, loss = 0.08882362
Iteration 7007, loss = 0.08880720
Iteration 7008, loss = 0.08879019
Iteration 7009, loss = 0.08877371
Iteration 7010, loss = 0.08875686
Iteration 7011, loss = 0.08873991
Iteration 7012, loss = 0.08872346
Iteration 7013, loss = 0.08870652
Iteration 7014, loss = 0.08868994
Iteration 7015, loss = 0.08867310
Iteration 7016, loss = 0.08865637
Iteration 7017, loss = 0.08863967
Iteration 7018, loss = 0.08862312
Iteration 7019, loss = 0.08860630
Iteration 7020, loss = 0.08858984
Iteration 7021, loss = 0.08857293
Iteration 7022, loss = 0.08855633
Iteration 7023, loss = 0.08853976
Iteration 7024, loss = 0.08852308
Iteration 7025, loss = 0.08850657
Iteration 7026, loss = 0.08848973
Iteration 7027, loss = 0.08847335
Iteration 7028, loss = 0.08845667
Iteration 7029, loss = 0.08844003
Iteration 7030, loss = 0.08842340
Iteration 7031, loss = 0.08840673
Iteration 7032, loss = 0.08839012
Iteration 7033, loss = 0.08837346
Iteration 7034, loss = 0.08835702
Iteration 7035, loss = 0.08834041
Iteration 7036, loss = 0.08832396
Iteration 7037, loss = 0.08830715
Iteration 7038, loss = 0.08829071
Iteration 7039, loss = 0.08827397
Iteration 7040, loss = 0.08825771
Iteration 7041, loss = 0.08824084
Iteration 7042, loss = 0.08822448
Iteration 7043, loss = 0.08820799
Iteration 7044, loss = 0.08819165
Iteration 7045, loss = 0.08817493
Iteration 7046, loss = 0.08815845
Iteration 7047, loss = 0.08814186
Iteration 7048, loss = 0.08812553
Iteration 7049, loss = 0.08810892
Iteration 7050, loss = 0.08809241
Iteration 7051, loss = 0.08807621
Iteration 7052, loss = 0.08805964
Iteration 7053, loss = 0.08804305
Iteration 7054, loss = 0.08802652
Iteration 7055, loss = 0.08801020
Iteration 7056, loss = 0.08799363
Iteration 7057, loss = 0.08797726
Iteration 7058, loss = 0.08796067
Iteration 7059, loss = 0.08794448
Iteration 7060, loss = 0.08792800
Iteration 7061, loss = 0.08791167
Iteration 7062, loss = 0.08789532
Iteration 7063, loss = 0.08787882
Iteration 7064, loss = 0.08786242
Iteration 7065, loss = 0.08784584
Iteration 7066, loss = 0.08782959
Iteration 7067, loss = 0.08781336
Iteration 7068, loss = 0.08779684
Iteration 7069, loss = 0.08778033
Iteration 7070, loss = 0.08776417
Iteration 7071, loss = 0.08774769
Iteration 7072, loss = 0.08773148
Iteration 7073, loss = 0.08771489
Iteration 7074, loss = 0.08769867
Iteration 7075, loss = 0.08768228
Iteration 7076, loss = 0.08766612
Iteration 7077, loss = 0.08764944
Iteration 7078, loss = 0.08763323
Iteration 7079, loss = 0.08761696
Iteration 7080, loss = 0.08760054
Iteration 7081, loss = 0.08758435
Iteration 7082, loss = 0.08756784
Iteration 7083, loss = 0.08755165
Iteration 7084, loss = 0.08753538
Iteration 7085, loss = 0.08751917
Iteration 7086, loss = 0.08750282
Iteration 7087, loss = 0.08748651
Iteration 7088, loss = 0.08747018
Iteration 7089, loss = 0.08745397
Iteration 7090, loss = 0.08743769
Iteration 7091, loss = 0.08742150
Iteration 7092, loss = 0.08740521
Iteration 7093, loss = 0.08738901
Iteration 7094, loss = 0.08737287
Iteration 7095, loss = 0.08735637
Iteration 7096, loss = 0.08734040
Iteration 7097, loss = 0.08732396
Iteration 7098, loss = 0.08730791
Iteration 7099, loss = 0.08729141
Iteration 7100, loss = 0.08727532
Iteration 7101, loss = 0.08725913
Iteration 7102, loss = 0.08724330
Iteration 7103, loss = 0.08722680
Iteration 7104, loss = 0.08721064
Iteration 7105, loss = 0.08719443
Iteration 7106, loss = 0.08717840
Iteration 7107, loss = 0.08716233
Iteration 7108, loss = 0.08714613
Iteration 7109, loss = 0.08712994
Iteration 7110, loss = 0.08711362
Iteration 7111, loss = 0.08709776
Iteration 7112, loss = 0.08708152
Iteration 7113, loss = 0.08706588
Iteration 7114, loss = 0.08704964
Iteration 7115, loss = 0.08703335
Iteration 7116, loss = 0.08701716
Iteration 7117, loss = 0.08700131
Iteration 7118, loss = 0.08698527
Iteration 7119, loss = 0.08696911
Iteration 7120, loss = 0.08695318
Iteration 7121, loss = 0.08693736
Iteration 7122, loss = 0.08692105
Iteration 7123, loss = 0.08690497
Iteration 7124, loss = 0.08688892
Iteration 7125, loss = 0.08687273
Iteration 7126, loss = 0.08685655
Iteration 7127, loss = 0.08684060
Iteration 7128, loss = 0.08682463
Iteration 7129, loss = 0.08680870
Iteration 7130, loss = 0.08679235
Iteration 7131, loss = 0.08677628
Iteration 7132, loss = 0.08676045
Iteration 7133, loss = 0.08674435
Iteration 7134, loss = 0.08672824
Iteration 7135, loss = 0.08671229
Iteration 7136, loss = 0.08669636
Iteration 7137, loss = 0.08668050
Iteration 7138, loss = 0.08666442
Iteration 7139, loss = 0.08664831
Iteration 7140, loss = 0.08663235
Iteration 7141, loss = 0.08661657
Iteration 7142, loss = 0.08660058
Iteration 7143, loss = 0.08658449
Iteration 7144, loss = 0.08656876
Iteration 7145, loss = 0.08655263
Iteration 7146, loss = 0.08653670
Iteration 7147, loss = 0.08652061
Iteration 7148, loss = 0.08650494
Iteration 7149, loss = 0.08648878
Iteration 7150, loss = 0.08647285
Iteration 7151, loss = 0.08645733
Iteration 7152, loss = 0.08644127
Iteration 7153, loss = 0.08642529
Iteration 7154, loss = 0.08640935
Iteration 7155, loss = 0.08639368
Iteration 7156, loss = 0.08637791
Iteration 7157, loss = 0.08636214
Iteration 7158, loss = 0.08634612
Iteration 7159, loss = 0.08633024
Iteration 7160, loss = 0.08631451
Iteration 7161, loss = 0.08629865
Iteration 7162, loss = 0.08628293
Iteration 7163, loss = 0.08626704
Iteration 7164, loss = 0.08625129
Iteration 7165, loss = 0.08623539
Iteration 7166, loss = 0.08621957
Iteration 7167, loss = 0.08620367
Iteration 7168, loss = 0.08618774
Iteration 7169, loss = 0.08617220
Iteration 7170, loss = 0.08615636
Iteration 7171, loss = 0.08614056
Iteration 7172, loss = 0.08612483
Iteration 7173, loss = 0.08610914
Iteration 7174, loss = 0.08609325
Iteration 7175, loss = 0.08607737
Iteration 7176, loss = 0.08606178
Iteration 7177, loss = 0.08604605
Iteration 7178, loss = 0.08603045
Iteration 7179, loss = 0.08601442
Iteration 7180, loss = 0.08599870
Iteration 7181, loss = 0.08598314
Iteration 7182, loss = 0.08596728
Iteration 7183, loss = 0.08595148
Iteration 7184, loss = 0.08593577
Iteration 7185, loss = 0.08592017
Iteration 7186, loss = 0.08590431
Iteration 7187, loss = 0.08588871
Iteration 7188, loss = 0.08587309
Iteration 7189, loss = 0.08585748
Iteration 7190, loss = 0.08584170
Iteration 7191, loss = 0.08582600
Iteration 7192, loss = 0.08581031
Iteration 7193, loss = 0.08579488
Iteration 7194, loss = 0.08577915
Iteration 7195, loss = 0.08576347
Iteration 7196, loss = 0.08574784
Iteration 7197, loss = 0.08573224
Iteration 7198, loss = 0.08571655
Iteration 7199, loss = 0.08570093
Iteration 7200, loss = 0.08568529
Iteration 7201, loss = 0.08566971
Iteration 7202, loss = 0.08565418
Iteration 7203, loss = 0.08563862
Iteration 7204, loss = 0.08562293
Iteration 7205, loss = 0.08560746
Iteration 7206, loss = 0.08559190
Iteration 7207, loss = 0.08557626
Iteration 7208, loss = 0.08556071
Iteration 7209, loss = 0.08554524
Iteration 7210, loss = 0.08552939
Iteration 7211, loss = 0.08551387
Iteration 7212, loss = 0.08549846
Iteration 7213, loss = 0.08548292
Iteration 7214, loss = 0.08546754
Iteration 7215, loss = 0.08545176
Iteration 7216, loss = 0.08543631
Iteration 7217, loss = 0.08542096
Iteration 7218, loss = 0.08540530
Iteration 7219, loss = 0.08538974
Iteration 7220, loss = 0.08537453
Iteration 7221, loss = 0.08535882
Iteration 7222, loss = 0.08534307
Iteration 7223, loss = 0.08532773
Iteration 7224, loss = 0.08531230
Iteration 7225, loss = 0.08529682
Iteration 7226, loss = 0.08528123
Iteration 7227, loss = 0.08526573
Iteration 7228, loss = 0.08525030
Iteration 7229, loss = 0.08523474
Iteration 7230, loss = 0.08521966
Iteration 7231, loss = 0.08520392
Iteration 7232, loss = 0.08518848
Iteration 7233, loss = 0.08517304
Iteration 7234, loss = 0.08515741
Iteration 7235, loss = 0.08514233
Iteration 7236, loss = 0.08512688
Iteration 7237, loss = 0.08511139
Iteration 7238, loss = 0.08509600
Iteration 7239, loss = 0.08508069
Iteration 7240, loss = 0.08506526
Iteration 7241, loss = 0.08504978
Iteration 7242, loss = 0.08503455
Iteration 7243, loss = 0.08501930
Iteration 7244, loss = 0.08500378
Iteration 7245, loss = 0.08498859
Iteration 7246, loss = 0.08497331
Iteration 7247, loss = 0.08495769
Iteration 7248, loss = 0.08494234
Iteration 7249, loss = 0.08492686
Iteration 7250, loss = 0.08491166
Iteration 7251, loss = 0.08489622
Iteration 7252, loss = 0.08488090
Iteration 7253, loss = 0.08486565
Iteration 7254, loss = 0.08485007
Iteration 7255, loss = 0.08483483
Iteration 7256, loss = 0.08481936
Iteration 7257, loss = 0.08480400
Iteration 7258, loss = 0.08478883
Iteration 7259, loss = 0.08477358
Iteration 7260, loss = 0.08475815
Iteration 7261, loss = 0.08474299
Iteration 7262, loss = 0.08472748
Iteration 7263, loss = 0.08471246
Iteration 7264, loss = 0.08469698
Iteration 7265, loss = 0.08468168
Iteration 7266, loss = 0.08466667
Iteration 7267, loss = 0.08465131
Iteration 7268, loss = 0.08463587
Iteration 7269, loss = 0.08462068
Iteration 7270, loss = 0.08460537
Iteration 7271, loss = 0.08459018
Iteration 7272, loss = 0.08457479
Iteration 7273, loss = 0.08455964
Iteration 7274, loss = 0.08454431
Iteration 7275, loss = 0.08452914
Iteration 7276, loss = 0.08451392
Iteration 7277, loss = 0.08449861
Iteration 7278, loss = 0.08448328
Iteration 7279, loss = 0.08446825
Iteration 7280, loss = 0.08445301
Iteration 7281, loss = 0.08443758
Iteration 7282, loss = 0.08442247
Iteration 7283, loss = 0.08440744
Iteration 7284, loss = 0.08439222
Iteration 7285, loss = 0.08437703
Iteration 7286, loss = 0.08436183
Iteration 7287, loss = 0.08434657
Iteration 7288, loss = 0.08433157
Iteration 7289, loss = 0.08431618
Iteration 7290, loss = 0.08430127
Iteration 7291, loss = 0.08428591
Iteration 7292, loss = 0.08427080
Iteration 7293, loss = 0.08425577
Iteration 7294, loss = 0.08424048
Iteration 7295, loss = 0.08422547
Iteration 7296, loss = 0.08421026
Iteration 7297, loss = 0.08419535
Iteration 7298, loss = 0.08418005
Iteration 7299, loss = 0.08416499
Iteration 7300, loss = 0.08414997
Iteration 7301, loss = 0.08413491
Iteration 7302, loss = 0.08411982
Iteration 7303, loss = 0.08410476
Iteration 7304, loss = 0.08408977
Iteration 7305, loss = 0.08407466
Iteration 7306, loss = 0.08405956
Iteration 7307, loss = 0.08404436
Iteration 7308, loss = 0.08402968
Iteration 7309, loss = 0.08401441
Iteration 7310, loss = 0.08399936
Iteration 7311, loss = 0.08398434
Iteration 7312, loss = 0.08396923
Iteration 7313, loss = 0.08395418
Iteration 7314, loss = 0.08393920
Iteration 7315, loss = 0.08392419
Iteration 7316, loss = 0.08390912
Iteration 7317, loss = 0.08389433
Iteration 7318, loss = 0.08387922
Iteration 7319, loss = 0.08386418
Iteration 7320, loss = 0.08384920
Iteration 7321, loss = 0.08383408
Iteration 7322, loss = 0.08381926
Iteration 7323, loss = 0.08380435
Iteration 7324, loss = 0.08378935
Iteration 7325, loss = 0.08377436
Iteration 7326, loss = 0.08375950
Iteration 7327, loss = 0.08374457
Iteration 7328, loss = 0.08372975
Iteration 7329, loss = 0.08371485
Iteration 7330, loss = 0.08369975
Iteration 7331, loss = 0.08368488
Iteration 7332, loss = 0.08367000
Iteration 7333, loss = 0.08365520
Iteration 7334, loss = 0.08364039
Iteration 7335, loss = 0.08362540
Iteration 7336, loss = 0.08361054
Iteration 7337, loss = 0.08359568
Iteration 7338, loss = 0.08358070
Iteration 7339, loss = 0.08356581
Iteration 7340, loss = 0.08355100
Iteration 7341, loss = 0.08353619
Iteration 7342, loss = 0.08352123
Iteration 7343, loss = 0.08350642
Iteration 7344, loss = 0.08349160
Iteration 7345, loss = 0.08347693
Iteration 7346, loss = 0.08346188
Iteration 7347, loss = 0.08344711
Iteration 7348, loss = 0.08343240
Iteration 7349, loss = 0.08341748
Iteration 7350, loss = 0.08340275
Iteration 7351, loss = 0.08338794
Iteration 7352, loss = 0.08337322
Iteration 7353, loss = 0.08335849
Iteration 7354, loss = 0.08334344
Iteration 7355, loss = 0.08332886
Iteration 7356, loss = 0.08331399
Iteration 7357, loss = 0.08329914
Iteration 7358, loss = 0.08328452
Iteration 7359, loss = 0.08326994
Iteration 7360, loss = 0.08325494
Iteration 7361, loss = 0.08324017
Iteration 7362, loss = 0.08322566
Iteration 7363, loss = 0.08321061
Iteration 7364, loss = 0.08319607
Iteration 7365, loss = 0.08318118
Iteration 7366, loss = 0.08316655
Iteration 7367, loss = 0.08315175
Iteration 7368, loss = 0.08313696
Iteration 7369, loss = 0.08312225
Iteration 7370, loss = 0.08310761
Iteration 7371, loss = 0.08309273
Iteration 7372, loss = 0.08307809
Iteration 7373, loss = 0.08306335
Iteration 7374, loss = 0.08304877
Iteration 7375, loss = 0.08303402
Iteration 7376, loss = 0.08301955
Iteration 7377, loss = 0.08300459
Iteration 7378, loss = 0.08299011
Iteration 7379, loss = 0.08297522
Iteration 7380, loss = 0.08296065
Iteration 7381, loss = 0.08294619
Iteration 7382, loss = 0.08293138
Iteration 7383, loss = 0.08291677
Iteration 7384, loss = 0.08290238
Iteration 7385, loss = 0.08288757
Iteration 7386, loss = 0.08287281
Iteration 7387, loss = 0.08285832
Iteration 7388, loss = 0.08284363
Iteration 7389, loss = 0.08282905
Iteration 7390, loss = 0.08281436
Iteration 7391, loss = 0.08279990
Iteration 7392, loss = 0.08278538
Iteration 7393, loss = 0.08277066
Iteration 7394, loss = 0.08275612
Iteration 7395, loss = 0.08274139
Iteration 7396, loss = 0.08272711
Iteration 7397, loss = 0.08271254
Iteration 7398, loss = 0.08269768
Iteration 7399, loss = 0.08268307
Iteration 7400, loss = 0.08266873
Iteration 7401, loss = 0.08265395
Iteration 7402, loss = 0.08263939
Iteration 7403, loss = 0.08262500
Iteration 7404, loss = 0.08261019
Iteration 7405, loss = 0.08259569
Iteration 7406, loss = 0.08258132
Iteration 7407, loss = 0.08256637
Iteration 7408, loss = 0.08255197
Iteration 7409, loss = 0.08253759
Iteration 7410, loss = 0.08252299
Iteration 7411, loss = 0.08250857
Iteration 7412, loss = 0.08249394
Iteration 7413, loss = 0.08247945
Iteration 7414, loss = 0.08246501
Iteration 7415, loss = 0.08245041
Iteration 7416, loss = 0.08243625
Iteration 7417, loss = 0.08242152
Iteration 7418, loss = 0.08240713
Iteration 7419, loss = 0.08239260
Iteration 7420, loss = 0.08237818
Iteration 7421, loss = 0.08236376
Iteration 7422, loss = 0.08234924
Iteration 7423, loss = 0.08233491
Iteration 7424, loss = 0.08232048
Iteration 7425, loss = 0.08230605
Iteration 7426, loss = 0.08229168
Iteration 7427, loss = 0.08227720
Iteration 7428, loss = 0.08226290
Iteration 7429, loss = 0.08224854
Iteration 7430, loss = 0.08223421
Iteration 7431, loss = 0.08221980
Iteration 7432, loss = 0.08220532
Iteration 7433, loss = 0.08219096
Iteration 7434, loss = 0.08217644
Iteration 7435, loss = 0.08216206
Iteration 7436, loss = 0.08214780
Iteration 7437, loss = 0.08213344
Iteration 7438, loss = 0.08211891
Iteration 7439, loss = 0.08210475
Iteration 7440, loss = 0.08209038
Iteration 7441, loss = 0.08207588
Iteration 7442, loss = 0.08206150
Iteration 7443, loss = 0.08204710
Iteration 7444, loss = 0.08203297
Iteration 7445, loss = 0.08201847
Iteration 7446, loss = 0.08200401
Iteration 7447, loss = 0.08198971
Iteration 7448, loss = 0.08197574
Iteration 7449, loss = 0.08196131
Iteration 7450, loss = 0.08194672
Iteration 7451, loss = 0.08193248
Iteration 7452, loss = 0.08191833
Iteration 7453, loss = 0.08190391
Iteration 7454, loss = 0.08188977
Iteration 7455, loss = 0.08187521
Iteration 7456, loss = 0.08186127
Iteration 7457, loss = 0.08184674
Iteration 7458, loss = 0.08183240
Iteration 7459, loss = 0.08181809
Iteration 7460, loss = 0.08180394
Iteration 7461, loss = 0.08178972
Iteration 7462, loss = 0.08177543
Iteration 7463, loss = 0.08176125
Iteration 7464, loss = 0.08174682
Iteration 7465, loss = 0.08173268
Iteration 7466, loss = 0.08171854
Iteration 7467, loss = 0.08170416
Iteration 7468, loss = 0.08169003
Iteration 7469, loss = 0.08167578
Iteration 7470, loss = 0.08166150
Iteration 7471, loss = 0.08164728
Iteration 7472, loss = 0.08163303
Iteration 7473, loss = 0.08161878
Iteration 7474, loss = 0.08160471
Iteration 7475, loss = 0.08159039
Iteration 7476, loss = 0.08157623
Iteration 7477, loss = 0.08156198
Iteration 7478, loss = 0.08154761
Iteration 7479, loss = 0.08153354
Iteration 7480, loss = 0.08151941
Iteration 7481, loss = 0.08150524
Iteration 7482, loss = 0.08149088
Iteration 7483, loss = 0.08147692
Iteration 7484, loss = 0.08146266
Iteration 7485, loss = 0.08144838
Iteration 7486, loss = 0.08143434
Iteration 7487, loss = 0.08142016
Iteration 7488, loss = 0.08140603
Iteration 7489, loss = 0.08139190
Iteration 7490, loss = 0.08137788
Iteration 7491, loss = 0.08136364
Iteration 7492, loss = 0.08134935
Iteration 7493, loss = 0.08133541
Iteration 7494, loss = 0.08132116
Iteration 7495, loss = 0.08130704
Iteration 7496, loss = 0.08129291
Iteration 7497, loss = 0.08127903
Iteration 7498, loss = 0.08126494
Iteration 7499, loss = 0.08125060
Iteration 7500, loss = 0.08123647
Iteration 7501, loss = 0.08122256
Iteration 7502, loss = 0.08120832
Iteration 7503, loss = 0.08119429
Iteration 7504, loss = 0.08118016
Iteration 7505, loss = 0.08116612
Iteration 7506, loss = 0.08115202
Iteration 7507, loss = 0.08113799
Iteration 7508, loss = 0.08112403
Iteration 7509, loss = 0.08111007
Iteration 7510, loss = 0.08109570
Iteration 7511, loss = 0.08108189
Iteration 7512, loss = 0.08106779
Iteration 7513, loss = 0.08105364
Iteration 7514, loss = 0.08103968
Iteration 7515, loss = 0.08102566
Iteration 7516, loss = 0.08101160
Iteration 7517, loss = 0.08099758
Iteration 7518, loss = 0.08098370
Iteration 7519, loss = 0.08096949
Iteration 7520, loss = 0.08095557
Iteration 7521, loss = 0.08094151
Iteration 7522, loss = 0.08092755
Iteration 7523, loss = 0.08091347
Iteration 7524, loss = 0.08089953
Iteration 7525, loss = 0.08088573
Iteration 7526, loss = 0.08087152
Iteration 7527, loss = 0.08085770
Iteration 7528, loss = 0.08084366
Iteration 7529, loss = 0.08082973
Iteration 7530, loss = 0.08081596
Iteration 7531, loss = 0.08080195
Iteration 7532, loss = 0.08078806
Iteration 7533, loss = 0.08077419
Iteration 7534, loss = 0.08075997
Iteration 7535, loss = 0.08074617
Iteration 7536, loss = 0.08073228
Iteration 7537, loss = 0.08071829
Iteration 7538, loss = 0.08070441
Iteration 7539, loss = 0.08069057
Iteration 7540, loss = 0.08067647
Iteration 7541, loss = 0.08066260
Iteration 7542, loss = 0.08064880
Iteration 7543, loss = 0.08063473
Iteration 7544, loss = 0.08062090
Iteration 7545, loss = 0.08060694
Iteration 7546, loss = 0.08059308
Iteration 7547, loss = 0.08057907
Iteration 7548, loss = 0.08056544
Iteration 7549, loss = 0.08055142
Iteration 7550, loss = 0.08053765
Iteration 7551, loss = 0.08052365
Iteration 7552, loss = 0.08050982
Iteration 7553, loss = 0.08049604
Iteration 7554, loss = 0.08048214
Iteration 7555, loss = 0.08046832
Iteration 7556, loss = 0.08045460
Iteration 7557, loss = 0.08044072
Iteration 7558, loss = 0.08042670
Iteration 7559, loss = 0.08041309
Iteration 7560, loss = 0.08039936
Iteration 7561, loss = 0.08038539
Iteration 7562, loss = 0.08037158
Iteration 7563, loss = 0.08035781
Iteration 7564, loss = 0.08034399
Iteration 7565, loss = 0.08033023
Iteration 7566, loss = 0.08031627
Iteration 7567, loss = 0.08030263
Iteration 7568, loss = 0.08028900
Iteration 7569, loss = 0.08027512
Iteration 7570, loss = 0.08026140
Iteration 7571, loss = 0.08024763
Iteration 7572, loss = 0.08023390
Iteration 7573, loss = 0.08022012
Iteration 7574, loss = 0.08020639
Iteration 7575, loss = 0.08019263
Iteration 7576, loss = 0.08017881
Iteration 7577, loss = 0.08016523
Iteration 7578, loss = 0.08015146
Iteration 7579, loss = 0.08013785
Iteration 7580, loss = 0.08012398
Iteration 7581, loss = 0.08011034
Iteration 7582, loss = 0.08009665
Iteration 7583, loss = 0.08008312
Iteration 7584, loss = 0.08006927
Iteration 7585, loss = 0.08005561
Iteration 7586, loss = 0.08004211
Iteration 7587, loss = 0.08002842
Iteration 7588, loss = 0.08001478
Iteration 7589, loss = 0.08000098
Iteration 7590, loss = 0.07998733
Iteration 7591, loss = 0.07997374
Iteration 7592, loss = 0.07996025
Iteration 7593, loss = 0.07994641
Iteration 7594, loss = 0.07993281
Iteration 7595, loss = 0.07991930
Iteration 7596, loss = 0.07990566
Iteration 7597, loss = 0.07989195
Iteration 7598, loss = 0.07987835
Iteration 7599, loss = 0.07986483
Iteration 7600, loss = 0.07985124
Iteration 7601, loss = 0.07983768
Iteration 7602, loss = 0.07982393
Iteration 7603, loss = 0.07981036
Iteration 7604, loss = 0.07979663
Iteration 7605, loss = 0.07978315
Iteration 7606, loss = 0.07976963
Iteration 7607, loss = 0.07975593
Iteration 7608, loss = 0.07974225
Iteration 7609, loss = 0.07972887
Iteration 7610, loss = 0.07971531
Iteration 7611, loss = 0.07970144
Iteration 7612, loss = 0.07968808
Iteration 7613, loss = 0.07967439
Iteration 7614, loss = 0.07966086
Iteration 7615, loss = 0.07964735
Iteration 7616, loss = 0.07963382
Iteration 7617, loss = 0.07962021
Iteration 7618, loss = 0.07960668
Iteration 7619, loss = 0.07959317
Iteration 7620, loss = 0.07957958
Iteration 7621, loss = 0.07956605
Iteration 7622, loss = 0.07955263
Iteration 7623, loss = 0.07953900
Iteration 7624, loss = 0.07952565
Iteration 7625, loss = 0.07951201
Iteration 7626, loss = 0.07949859
Iteration 7627, loss = 0.07948502
Iteration 7628, loss = 0.07947161
Iteration 7629, loss = 0.07945805
Iteration 7630, loss = 0.07944467
Iteration 7631, loss = 0.07943127
Iteration 7632, loss = 0.07941770
Iteration 7633, loss = 0.07940422
Iteration 7634, loss = 0.07939063
Iteration 7635, loss = 0.07937716
Iteration 7636, loss = 0.07936394
Iteration 7637, loss = 0.07935020
Iteration 7638, loss = 0.07933697
Iteration 7639, loss = 0.07932328
Iteration 7640, loss = 0.07930989
Iteration 7641, loss = 0.07929644
Iteration 7642, loss = 0.07928301
Iteration 7643, loss = 0.07926947
Iteration 7644, loss = 0.07925613
Iteration 7645, loss = 0.07924248
Iteration 7646, loss = 0.07922923
Iteration 7647, loss = 0.07921587
Iteration 7648, loss = 0.07920237
Iteration 7649, loss = 0.07918899
Iteration 7650, loss = 0.07917538
Iteration 7651, loss = 0.07916205
Iteration 7652, loss = 0.07914869
Iteration 7653, loss = 0.07913528
Iteration 7654, loss = 0.07912190
Iteration 7655, loss = 0.07910863
Iteration 7656, loss = 0.07909519
Iteration 7657, loss = 0.07908179
Iteration 7658, loss = 0.07906846
Iteration 7659, loss = 0.07905508
Iteration 7660, loss = 0.07904167
Iteration 7661, loss = 0.07902827
Iteration 7662, loss = 0.07901509
Iteration 7663, loss = 0.07900168
Iteration 7664, loss = 0.07898838
Iteration 7665, loss = 0.07897494
Iteration 7666, loss = 0.07896176
Iteration 7667, loss = 0.07894833
Iteration 7668, loss = 0.07893500
Iteration 7669, loss = 0.07892179
Iteration 7670, loss = 0.07890841
Iteration 7671, loss = 0.07889503
Iteration 7672, loss = 0.07888179
Iteration 7673, loss = 0.07886843
Iteration 7674, loss = 0.07885513
Iteration 7675, loss = 0.07884177
Iteration 7676, loss = 0.07882840
Iteration 7677, loss = 0.07881530
Iteration 7678, loss = 0.07880193
Iteration 7679, loss = 0.07878851
Iteration 7680, loss = 0.07877526
Iteration 7681, loss = 0.07876190
Iteration 7682, loss = 0.07874858
Iteration 7683, loss = 0.07873539
Iteration 7684, loss = 0.07872205
Iteration 7685, loss = 0.07870876
Iteration 7686, loss = 0.07869546
Iteration 7687, loss = 0.07868216
Iteration 7688, loss = 0.07866902
Iteration 7689, loss = 0.07865569
Iteration 7690, loss = 0.07864242
Iteration 7691, loss = 0.07862901
Iteration 7692, loss = 0.07861588
Iteration 7693, loss = 0.07860258
Iteration 7694, loss = 0.07858944
Iteration 7695, loss = 0.07857616
Iteration 7696, loss = 0.07856317
Iteration 7697, loss = 0.07854983
Iteration 7698, loss = 0.07853655
Iteration 7699, loss = 0.07852348
Iteration 7700, loss = 0.07851022
Iteration 7701, loss = 0.07849699
Iteration 7702, loss = 0.07848420
Iteration 7703, loss = 0.07847067
Iteration 7704, loss = 0.07845750
Iteration 7705, loss = 0.07844447
Iteration 7706, loss = 0.07843134
Iteration 7707, loss = 0.07841815
Iteration 7708, loss = 0.07840483
Iteration 7709, loss = 0.07839173
Iteration 7710, loss = 0.07837867
Iteration 7711, loss = 0.07836547
Iteration 7712, loss = 0.07835235
Iteration 7713, loss = 0.07833917
Iteration 7714, loss = 0.07832607
Iteration 7715, loss = 0.07831281
Iteration 7716, loss = 0.07829980
Iteration 7717, loss = 0.07828667
Iteration 7718, loss = 0.07827354
Iteration 7719, loss = 0.07826043
Iteration 7720, loss = 0.07824731
Iteration 7721, loss = 0.07823409
Iteration 7722, loss = 0.07822104
Iteration 7723, loss = 0.07820805
Iteration 7724, loss = 0.07819499
Iteration 7725, loss = 0.07818175
Iteration 7726, loss = 0.07816875
Iteration 7727, loss = 0.07815574
Iteration 7728, loss = 0.07814272
Iteration 7729, loss = 0.07812959
Iteration 7730, loss = 0.07811659
Iteration 7731, loss = 0.07810370
Iteration 7732, loss = 0.07809068
Iteration 7733, loss = 0.07807757
Iteration 7734, loss = 0.07806455
Iteration 7735, loss = 0.07805149
Iteration 7736, loss = 0.07803855
Iteration 7737, loss = 0.07802562
Iteration 7738, loss = 0.07801261
Iteration 7739, loss = 0.07799953
Iteration 7740, loss = 0.07798646
Iteration 7741, loss = 0.07797356
Iteration 7742, loss = 0.07796051
Iteration 7743, loss = 0.07794753
Iteration 7744, loss = 0.07793462
Iteration 7745, loss = 0.07792161
Iteration 7746, loss = 0.07790872
Iteration 7747, loss = 0.07789588
Iteration 7748, loss = 0.07788292
Iteration 7749, loss = 0.07786987
Iteration 7750, loss = 0.07785700
Iteration 7751, loss = 0.07784424
Iteration 7752, loss = 0.07783117
Iteration 7753, loss = 0.07781834
Iteration 7754, loss = 0.07780528
Iteration 7755, loss = 0.07779232
Iteration 7756, loss = 0.07777951
Iteration 7757, loss = 0.07776645
Iteration 7758, loss = 0.07775372
Iteration 7759, loss = 0.07774053
Iteration 7760, loss = 0.07772780
Iteration 7761, loss = 0.07771469
Iteration 7762, loss = 0.07770175
Iteration 7763, loss = 0.07768876
Iteration 7764, loss = 0.07767595
Iteration 7765, loss = 0.07766302
Iteration 7766, loss = 0.07765010
Iteration 7767, loss = 0.07763710
Iteration 7768, loss = 0.07762420
Iteration 7769, loss = 0.07761126
Iteration 7770, loss = 0.07759830
Iteration 7771, loss = 0.07758542
Iteration 7772, loss = 0.07757273
Iteration 7773, loss = 0.07755980
Iteration 7774, loss = 0.07754695
Iteration 7775, loss = 0.07753392
Iteration 7776, loss = 0.07752115
Iteration 7777, loss = 0.07750828
Iteration 7778, loss = 0.07749550
Iteration 7779, loss = 0.07748250
Iteration 7780, loss = 0.07746977
Iteration 7781, loss = 0.07745694
Iteration 7782, loss = 0.07744403
Iteration 7783, loss = 0.07743126
Iteration 7784, loss = 0.07741843
Iteration 7785, loss = 0.07740567
Iteration 7786, loss = 0.07739285
Iteration 7787, loss = 0.07738013
Iteration 7788, loss = 0.07736734
Iteration 7789, loss = 0.07735444
Iteration 7790, loss = 0.07734161
Iteration 7791, loss = 0.07732890
Iteration 7792, loss = 0.07731616
Iteration 7793, loss = 0.07730316
Iteration 7794, loss = 0.07729045
Iteration 7795, loss = 0.07727762
Iteration 7796, loss = 0.07726475
Iteration 7797, loss = 0.07725223
Iteration 7798, loss = 0.07723930
Iteration 7799, loss = 0.07722656
Iteration 7800, loss = 0.07721382
Iteration 7801, loss = 0.07720114
Iteration 7802, loss = 0.07718811
Iteration 7803, loss = 0.07717537
Iteration 7804, loss = 0.07716262
Iteration 7805, loss = 0.07714990
Iteration 7806, loss = 0.07713703
Iteration 7807, loss = 0.07712438
Iteration 7808, loss = 0.07711155
Iteration 7809, loss = 0.07709901
Iteration 7810, loss = 0.07708617
Iteration 7811, loss = 0.07707354
Iteration 7812, loss = 0.07706067
Iteration 7813, loss = 0.07704801
Iteration 7814, loss = 0.07703546
Iteration 7815, loss = 0.07702261
Iteration 7816, loss = 0.07701008
Iteration 7817, loss = 0.07699740
Iteration 7818, loss = 0.07698462
Iteration 7819, loss = 0.07697196
Iteration 7820, loss = 0.07695928
Iteration 7821, loss = 0.07694668
Iteration 7822, loss = 0.07693392
Iteration 7823, loss = 0.07692121
Iteration 7824, loss = 0.07690858
Iteration 7825, loss = 0.07689595
Iteration 7826, loss = 0.07688341
Iteration 7827, loss = 0.07687069
Iteration 7828, loss = 0.07685813
Iteration 7829, loss = 0.07684537
Iteration 7830, loss = 0.07683291
Iteration 7831, loss = 0.07682033
Iteration 7832, loss = 0.07680774
Iteration 7833, loss = 0.07679504
Iteration 7834, loss = 0.07678233
Iteration 7835, loss = 0.07676981
Iteration 7836, loss = 0.07675729
Iteration 7837, loss = 0.07674440
Iteration 7838, loss = 0.07673222
Iteration 7839, loss = 0.07671933
Iteration 7840, loss = 0.07670673
Iteration 7841, loss = 0.07669422
Iteration 7842, loss = 0.07668152
Iteration 7843, loss = 0.07666891
Iteration 7844, loss = 0.07665616
Iteration 7845, loss = 0.07664374
Iteration 7846, loss = 0.07663142
Iteration 7847, loss = 0.07661844
Iteration 7848, loss = 0.07660599
Iteration 7849, loss = 0.07659337
Iteration 7850, loss = 0.07658107
Iteration 7851, loss = 0.07656830
Iteration 7852, loss = 0.07655590
Iteration 7853, loss = 0.07654330
Iteration 7854, loss = 0.07653067
Iteration 7855, loss = 0.07651813
Iteration 7856, loss = 0.07650586
Iteration 7857, loss = 0.07649318
Iteration 7858, loss = 0.07648055
Iteration 7859, loss = 0.07646816
Iteration 7860, loss = 0.07645573
Iteration 7861, loss = 0.07644323
Iteration 7862, loss = 0.07643055
Iteration 7863, loss = 0.07641811
Iteration 7864, loss = 0.07640566
Iteration 7865, loss = 0.07639314
Iteration 7866, loss = 0.07638069
Iteration 7867, loss = 0.07636822
Iteration 7868, loss = 0.07635561
Iteration 7869, loss = 0.07634324
Iteration 7870, loss = 0.07633064
Iteration 7871, loss = 0.07631812
Iteration 7872, loss = 0.07630567
Iteration 7873, loss = 0.07629315
Iteration 7874, loss = 0.07628084
Iteration 7875, loss = 0.07626826
Iteration 7876, loss = 0.07625579
Iteration 7877, loss = 0.07624357
Iteration 7878, loss = 0.07623087
Iteration 7879, loss = 0.07621845
Iteration 7880, loss = 0.07620595
Iteration 7881, loss = 0.07619368
Iteration 7882, loss = 0.07618145
Iteration 7883, loss = 0.07616868
Iteration 7884, loss = 0.07615629
Iteration 7885, loss = 0.07614408
Iteration 7886, loss = 0.07613165
Iteration 7887, loss = 0.07611914
Iteration 7888, loss = 0.07610694
Iteration 7889, loss = 0.07609439
Iteration 7890, loss = 0.07608222
Iteration 7891, loss = 0.07606961
Iteration 7892, loss = 0.07605718
Iteration 7893, loss = 0.07604494
Iteration 7894, loss = 0.07603257
Iteration 7895, loss = 0.07602001
Iteration 7896, loss = 0.07600762
Iteration 7897, loss = 0.07599538
Iteration 7898, loss = 0.07598296
Iteration 7899, loss = 0.07597052
Iteration 7900, loss = 0.07595817
Iteration 7901, loss = 0.07594569
Iteration 7902, loss = 0.07593334
Iteration 7903, loss = 0.07592111
Iteration 7904, loss = 0.07590883
Iteration 7905, loss = 0.07589638
Iteration 7906, loss = 0.07588397
Iteration 7907, loss = 0.07587182
Iteration 7908, loss = 0.07585937
Iteration 7909, loss = 0.07584712
Iteration 7910, loss = 0.07583463
Iteration 7911, loss = 0.07582245
Iteration 7912, loss = 0.07581009
Iteration 7913, loss = 0.07579794
Iteration 7914, loss = 0.07578557
Iteration 7915, loss = 0.07577325
Iteration 7916, loss = 0.07576101
Iteration 7917, loss = 0.07574864
Iteration 7918, loss = 0.07573629
Iteration 7919, loss = 0.07572404
Iteration 7920, loss = 0.07571188
Iteration 7921, loss = 0.07569943
Iteration 7922, loss = 0.07568721
Iteration 7923, loss = 0.07567498
Iteration 7924, loss = 0.07566267
Iteration 7925, loss = 0.07565034
Iteration 7926, loss = 0.07563808
Iteration 7927, loss = 0.07562593
Iteration 7928, loss = 0.07561369
Iteration 7929, loss = 0.07560136
Iteration 7930, loss = 0.07558923
Iteration 7931, loss = 0.07557702
Iteration 7932, loss = 0.07556467
Iteration 7933, loss = 0.07555243
Iteration 7934, loss = 0.07554014
Iteration 7935, loss = 0.07552794
Iteration 7936, loss = 0.07551579
Iteration 7937, loss = 0.07550370
Iteration 7938, loss = 0.07549108
Iteration 7939, loss = 0.07547908
Iteration 7940, loss = 0.07546681
Iteration 7941, loss = 0.07545457
Iteration 7942, loss = 0.07544239
Iteration 7943, loss = 0.07543017
Iteration 7944, loss = 0.07541796
Iteration 7945, loss = 0.07540585
Iteration 7946, loss = 0.07539356
Iteration 7947, loss = 0.07538154
Iteration 7948, loss = 0.07536926
Iteration 7949, loss = 0.07535721
Iteration 7950, loss = 0.07534511
Iteration 7951, loss = 0.07533279
Iteration 7952, loss = 0.07532066
Iteration 7953, loss = 0.07530854
Iteration 7954, loss = 0.07529628
Iteration 7955, loss = 0.07528425
Iteration 7956, loss = 0.07527200
Iteration 7957, loss = 0.07526013
Iteration 7958, loss = 0.07524776
Iteration 7959, loss = 0.07523559
Iteration 7960, loss = 0.07522349
Iteration 7961, loss = 0.07521127
Iteration 7962, loss = 0.07519933
Iteration 7963, loss = 0.07518720
Iteration 7964, loss = 0.07517495
Iteration 7965, loss = 0.07516297
Iteration 7966, loss = 0.07515073
Iteration 7967, loss = 0.07513884
Iteration 7968, loss = 0.07512668
Iteration 7969, loss = 0.07511457
Iteration 7970, loss = 0.07510265
Iteration 7971, loss = 0.07509054
Iteration 7972, loss = 0.07507831
Iteration 7973, loss = 0.07506632
Iteration 7974, loss = 0.07505435
Iteration 7975, loss = 0.07504229
Iteration 7976, loss = 0.07503011
Iteration 7977, loss = 0.07501826
Iteration 7978, loss = 0.07500606
Iteration 7979, loss = 0.07499401
Iteration 7980, loss = 0.07498196
Iteration 7981, loss = 0.07496997
Iteration 7982, loss = 0.07495787
Iteration 7983, loss = 0.07494588
Iteration 7984, loss = 0.07493374
Iteration 7985, loss = 0.07492167
Iteration 7986, loss = 0.07490962
Iteration 7987, loss = 0.07489762
Iteration 7988, loss = 0.07488565
Iteration 7989, loss = 0.07487358
Iteration 7990, loss = 0.07486139
Iteration 7991, loss = 0.07484950
Iteration 7992, loss = 0.07483743
Iteration 7993, loss = 0.07482541
Iteration 7994, loss = 0.07481326
Iteration 7995, loss = 0.07480141
Iteration 7996, loss = 0.07478940
Iteration 7997, loss = 0.07477739
Iteration 7998, loss = 0.07476540
Iteration 7999, loss = 0.07475356
Iteration 8000, loss = 0.07474141
Iteration 8001, loss = 0.07472947
Iteration 8002, loss = 0.07471742
Iteration 8003, loss = 0.07470553
Iteration 8004, loss = 0.07469363
Iteration 8005, loss = 0.07468162
Iteration 8006, loss = 0.07466966
Iteration 8007, loss = 0.07465771
Iteration 8008, loss = 0.07464590
Iteration 8009, loss = 0.07463397
Iteration 8010, loss = 0.07462191
Iteration 8011, loss = 0.07461004
Iteration 8012, loss = 0.07459812
Iteration 8013, loss = 0.07458613
Iteration 8014, loss = 0.07457419
Iteration 8015, loss = 0.07456233
Iteration 8016, loss = 0.07455035
Iteration 8017, loss = 0.07453846
Iteration 8018, loss = 0.07452667
Iteration 8019, loss = 0.07451475
Iteration 8020, loss = 0.07450291
Iteration 8021, loss = 0.07449109
Iteration 8022, loss = 0.07447906
Iteration 8023, loss = 0.07446730
Iteration 8024, loss = 0.07445533
Iteration 8025, loss = 0.07444352
Iteration 8026, loss = 0.07443177
Iteration 8027, loss = 0.07441994
Iteration 8028, loss = 0.07440801
Iteration 8029, loss = 0.07439617
Iteration 8030, loss = 0.07438432
Iteration 8031, loss = 0.07437238
Iteration 8032, loss = 0.07436053
Iteration 8033, loss = 0.07434850
Iteration 8034, loss = 0.07433685
Iteration 8035, loss = 0.07432498
Iteration 8036, loss = 0.07431296
Iteration 8037, loss = 0.07430110
Iteration 8038, loss = 0.07428918
Iteration 8039, loss = 0.07427755
Iteration 8040, loss = 0.07426554
Iteration 8041, loss = 0.07425363
Iteration 8042, loss = 0.07424186
Iteration 8043, loss = 0.07423006
Iteration 8044, loss = 0.07421831
Iteration 8045, loss = 0.07420628
Iteration 8046, loss = 0.07419443
Iteration 8047, loss = 0.07418253
Iteration 8048, loss = 0.07417099
Iteration 8049, loss = 0.07415910
Iteration 8050, loss = 0.07414718
Iteration 8051, loss = 0.07413522
Iteration 8052, loss = 0.07412358
Iteration 8053, loss = 0.07411157
Iteration 8054, loss = 0.07409973
Iteration 8055, loss = 0.07408816
Iteration 8056, loss = 0.07407633
Iteration 8057, loss = 0.07406448
Iteration 8058, loss = 0.07405277
Iteration 8059, loss = 0.07404084
Iteration 8060, loss = 0.07402933
Iteration 8061, loss = 0.07401740
Iteration 8062, loss = 0.07400567
Iteration 8063, loss = 0.07399395
Iteration 8064, loss = 0.07398220
Iteration 8065, loss = 0.07397039
Iteration 8066, loss = 0.07395874
Iteration 8067, loss = 0.07394695
Iteration 8068, loss = 0.07393528
Iteration 8069, loss = 0.07392329
Iteration 8070, loss = 0.07391155
Iteration 8071, loss = 0.07389993
Iteration 8072, loss = 0.07388803
Iteration 8073, loss = 0.07387640
Iteration 8074, loss = 0.07386462
Iteration 8075, loss = 0.07385322
Iteration 8076, loss = 0.07384112
Iteration 8077, loss = 0.07382942
Iteration 8078, loss = 0.07381774
Iteration 8079, loss = 0.07380593
Iteration 8080, loss = 0.07379437
Iteration 8081, loss = 0.07378257
Iteration 8082, loss = 0.07377094
Iteration 8083, loss = 0.07375911
Iteration 8084, loss = 0.07374767
Iteration 8085, loss = 0.07373581
Iteration 8086, loss = 0.07372421
Iteration 8087, loss = 0.07371255
Iteration 8088, loss = 0.07370094
Iteration 8089, loss = 0.07368910
Iteration 8090, loss = 0.07367754
Iteration 8091, loss = 0.07366583
Iteration 8092, loss = 0.07365419
Iteration 8093, loss = 0.07364264
Iteration 8094, loss = 0.07363080
Iteration 8095, loss = 0.07361910
Iteration 8096, loss = 0.07360765
Iteration 8097, loss = 0.07359598
Iteration 8098, loss = 0.07358414
Iteration 8099, loss = 0.07357259
Iteration 8100, loss = 0.07356109
Iteration 8101, loss = 0.07354936
Iteration 8102, loss = 0.07353789
Iteration 8103, loss = 0.07352614
Iteration 8104, loss = 0.07351459
Iteration 8105, loss = 0.07350283
Iteration 8106, loss = 0.07349132
Iteration 8107, loss = 0.07347967
Iteration 8108, loss = 0.07346813
Iteration 8109, loss = 0.07345645
Iteration 8110, loss = 0.07344472
Iteration 8111, loss = 0.07343314
Iteration 8112, loss = 0.07342159
Iteration 8113, loss = 0.07341007
Iteration 8114, loss = 0.07339845
Iteration 8115, loss = 0.07338689
Iteration 8116, loss = 0.07337528
Iteration 8117, loss = 0.07336380
Iteration 8118, loss = 0.07335206
Iteration 8119, loss = 0.07334056
Iteration 8120, loss = 0.07332893
Iteration 8121, loss = 0.07331758
Iteration 8122, loss = 0.07330588
Iteration 8123, loss = 0.07329434
Iteration 8124, loss = 0.07328285
Iteration 8125, loss = 0.07327142
Iteration 8126, loss = 0.07325981
Iteration 8127, loss = 0.07324849
Iteration 8128, loss = 0.07323667
Iteration 8129, loss = 0.07322528
Iteration 8130, loss = 0.07321391
Iteration 8131, loss = 0.07320209
Iteration 8132, loss = 0.07319080
Iteration 8133, loss = 0.07317915
Iteration 8134, loss = 0.07316759
Iteration 8135, loss = 0.07315610
Iteration 8136, loss = 0.07314478
Iteration 8137, loss = 0.07313314
Iteration 8138, loss = 0.07312172
Iteration 8139, loss = 0.07311011
Iteration 8140, loss = 0.07309861
Iteration 8141, loss = 0.07308713
Iteration 8142, loss = 0.07307580
Iteration 8143, loss = 0.07306426
Iteration 8144, loss = 0.07305292
Iteration 8145, loss = 0.07304122
Iteration 8146, loss = 0.07302980
Iteration 8147, loss = 0.07301845
Iteration 8148, loss = 0.07300709
Iteration 8149, loss = 0.07299548
Iteration 8150, loss = 0.07298407
Iteration 8151, loss = 0.07297243
Iteration 8152, loss = 0.07296123
Iteration 8153, loss = 0.07294973
Iteration 8154, loss = 0.07293817
Iteration 8155, loss = 0.07292681
Iteration 8156, loss = 0.07291538
Iteration 8157, loss = 0.07290388
Iteration 8158, loss = 0.07289250
Iteration 8159, loss = 0.07288099
Iteration 8160, loss = 0.07286965
Iteration 8161, loss = 0.07285809
Iteration 8162, loss = 0.07284679
Iteration 8163, loss = 0.07283560
Iteration 8164, loss = 0.07282394
Iteration 8165, loss = 0.07281258
Iteration 8166, loss = 0.07280119
Iteration 8167, loss = 0.07278995
Iteration 8168, loss = 0.07277836
Iteration 8169, loss = 0.07276711
Iteration 8170, loss = 0.07275549
Iteration 8171, loss = 0.07274427
Iteration 8172, loss = 0.07273294
Iteration 8173, loss = 0.07272156
Iteration 8174, loss = 0.07271008
Iteration 8175, loss = 0.07269868
Iteration 8176, loss = 0.07268737
Iteration 8177, loss = 0.07267609
Iteration 8178, loss = 0.07266466
Iteration 8179, loss = 0.07265330
Iteration 8180, loss = 0.07264209
Iteration 8181, loss = 0.07263069
Iteration 8182, loss = 0.07261917
Iteration 8183, loss = 0.07260782
Iteration 8184, loss = 0.07259660
Iteration 8185, loss = 0.07258518
Iteration 8186, loss = 0.07257374
Iteration 8187, loss = 0.07256244
Iteration 8188, loss = 0.07255112
Iteration 8189, loss = 0.07253975
Iteration 8190, loss = 0.07252834
Iteration 8191, loss = 0.07251717
Iteration 8192, loss = 0.07250580
Iteration 8193, loss = 0.07249447
Iteration 8194, loss = 0.07248313
Iteration 8195, loss = 0.07247186
Iteration 8196, loss = 0.07246045
Iteration 8197, loss = 0.07244922
Iteration 8198, loss = 0.07243782
Iteration 8199, loss = 0.07242669
Iteration 8200, loss = 0.07241542
Iteration 8201, loss = 0.07240407
Iteration 8202, loss = 0.07239285
Iteration 8203, loss = 0.07238164
Iteration 8204, loss = 0.07237025
Iteration 8205, loss = 0.07235904
Iteration 8206, loss = 0.07234778
Iteration 8207, loss = 0.07233644
Iteration 8208, loss = 0.07232513
Iteration 8209, loss = 0.07231377
Iteration 8210, loss = 0.07230264
Iteration 8211, loss = 0.07229145
Iteration 8212, loss = 0.07228007
Iteration 8213, loss = 0.07226904
Iteration 8214, loss = 0.07225762
Iteration 8215, loss = 0.07224657
Iteration 8216, loss = 0.07223530
Iteration 8217, loss = 0.07222408
Iteration 8218, loss = 0.07221280
Iteration 8219, loss = 0.07220164
Iteration 8220, loss = 0.07219062
Iteration 8221, loss = 0.07217915
Iteration 8222, loss = 0.07216796
Iteration 8223, loss = 0.07215664
Iteration 8224, loss = 0.07214550
Iteration 8225, loss = 0.07213442
Iteration 8226, loss = 0.07212317
Iteration 8227, loss = 0.07211195
Iteration 8228, loss = 0.07210064
Iteration 8229, loss = 0.07208943
Iteration 8230, loss = 0.07207837
Iteration 8231, loss = 0.07206700
Iteration 8232, loss = 0.07205592
Iteration 8233, loss = 0.07204479
Iteration 8234, loss = 0.07203349
Iteration 8235, loss = 0.07202240
Iteration 8236, loss = 0.07201118
Iteration 8237, loss = 0.07200007
Iteration 8238, loss = 0.07198892
Iteration 8239, loss = 0.07197772
Iteration 8240, loss = 0.07196664
Iteration 8241, loss = 0.07195548
Iteration 8242, loss = 0.07194438
Iteration 8243, loss = 0.07193321
Iteration 8244, loss = 0.07192211
Iteration 8245, loss = 0.07191108
Iteration 8246, loss = 0.07189978
Iteration 8247, loss = 0.07188868
Iteration 8248, loss = 0.07187747
Iteration 8249, loss = 0.07186642
Iteration 8250, loss = 0.07185532
Iteration 8251, loss = 0.07184414
Iteration 8252, loss = 0.07183301
Iteration 8253, loss = 0.07182205
Iteration 8254, loss = 0.07181089
Iteration 8255, loss = 0.07179956
Iteration 8256, loss = 0.07178859
Iteration 8257, loss = 0.07177741
Iteration 8258, loss = 0.07176624
Iteration 8259, loss = 0.07175521
Iteration 8260, loss = 0.07174407
Iteration 8261, loss = 0.07173293
Iteration 8262, loss = 0.07172193
Iteration 8263, loss = 0.07171079
Iteration 8264, loss = 0.07169967
Iteration 8265, loss = 0.07168866
Iteration 8266, loss = 0.07167766
Iteration 8267, loss = 0.07166657
Iteration 8268, loss = 0.07165561
Iteration 8269, loss = 0.07164454
Iteration 8270, loss = 0.07163359
Iteration 8271, loss = 0.07162256
Iteration 8272, loss = 0.07161163
Iteration 8273, loss = 0.07160045
Iteration 8274, loss = 0.07158946
Iteration 8275, loss = 0.07157836
Iteration 8276, loss = 0.07156749
Iteration 8277, loss = 0.07155629
Iteration 8278, loss = 0.07154539
Iteration 8279, loss = 0.07153430
Iteration 8280, loss = 0.07152327
Iteration 8281, loss = 0.07151226
Iteration 8282, loss = 0.07150120
Iteration 8283, loss = 0.07149027
Iteration 8284, loss = 0.07147896
Iteration 8285, loss = 0.07146799
Iteration 8286, loss = 0.07145694
Iteration 8287, loss = 0.07144588
Iteration 8288, loss = 0.07143481
Iteration 8289, loss = 0.07142396
Iteration 8290, loss = 0.07141289
Iteration 8291, loss = 0.07140183
Iteration 8292, loss = 0.07139067
Iteration 8293, loss = 0.07137967
Iteration 8294, loss = 0.07136886
Iteration 8295, loss = 0.07135792
Iteration 8296, loss = 0.07134673
Iteration 8297, loss = 0.07133582
Iteration 8298, loss = 0.07132484
Iteration 8299, loss = 0.07131386
Iteration 8300, loss = 0.07130294
Iteration 8301, loss = 0.07129208
Iteration 8302, loss = 0.07128096
Iteration 8303, loss = 0.07127009
Iteration 8304, loss = 0.07125912
Iteration 8305, loss = 0.07124813
Iteration 8306, loss = 0.07123726
Iteration 8307, loss = 0.07122626
Iteration 8308, loss = 0.07121538
Iteration 8309, loss = 0.07120438
Iteration 8310, loss = 0.07119359
Iteration 8311, loss = 0.07118256
Iteration 8312, loss = 0.07117167
Iteration 8313, loss = 0.07116062
Iteration 8314, loss = 0.07114979
Iteration 8315, loss = 0.07113871
Iteration 8316, loss = 0.07112802
Iteration 8317, loss = 0.07111694
Iteration 8318, loss = 0.07110605
Iteration 8319, loss = 0.07109512
Iteration 8320, loss = 0.07108409
Iteration 8321, loss = 0.07107319
Iteration 8322, loss = 0.07106227
Iteration 8323, loss = 0.07105163
Iteration 8324, loss = 0.07104054
Iteration 8325, loss = 0.07102980
Iteration 8326, loss = 0.07101878
Iteration 8327, loss = 0.07100800
Iteration 8328, loss = 0.07099718
Iteration 8329, loss = 0.07098634
Iteration 8330, loss = 0.07097549
Iteration 8331, loss = 0.07096464
Iteration 8332, loss = 0.07095383
Iteration 8333, loss = 0.07094295
Iteration 8334, loss = 0.07093216
Iteration 8335, loss = 0.07092137
Iteration 8336, loss = 0.07091060
Iteration 8337, loss = 0.07089977
Iteration 8338, loss = 0.07088899
Iteration 8339, loss = 0.07087815
Iteration 8340, loss = 0.07086717
Iteration 8341, loss = 0.07085634
Iteration 8342, loss = 0.07084560
Iteration 8343, loss = 0.07083482
Iteration 8344, loss = 0.07082403
Iteration 8345, loss = 0.07081321
Iteration 8346, loss = 0.07080233
Iteration 8347, loss = 0.07079161
Iteration 8348, loss = 0.07078072
Iteration 8349, loss = 0.07076999
Iteration 8350, loss = 0.07075916
Iteration 8351, loss = 0.07074850
Iteration 8352, loss = 0.07073757
Iteration 8353, loss = 0.07072680
Iteration 8354, loss = 0.07071608
Iteration 8355, loss = 0.07070530
Iteration 8356, loss = 0.07069470
Iteration 8357, loss = 0.07068372
Iteration 8358, loss = 0.07067307
Iteration 8359, loss = 0.07066230
Iteration 8360, loss = 0.07065152
Iteration 8361, loss = 0.07064064
Iteration 8362, loss = 0.07062996
Iteration 8363, loss = 0.07061921
Iteration 8364, loss = 0.07060846
Iteration 8365, loss = 0.07059766
Iteration 8366, loss = 0.07058705
Iteration 8367, loss = 0.07057630
Iteration 8368, loss = 0.07056566
Iteration 8369, loss = 0.07055486
Iteration 8370, loss = 0.07054407
Iteration 8371, loss = 0.07053327
Iteration 8372, loss = 0.07052247
Iteration 8373, loss = 0.07051188
Iteration 8374, loss = 0.07050098
Iteration 8375, loss = 0.07049040
Iteration 8376, loss = 0.07047954
Iteration 8377, loss = 0.07046881
Iteration 8378, loss = 0.07045831
Iteration 8379, loss = 0.07044724
Iteration 8380, loss = 0.07043667
Iteration 8381, loss = 0.07042600
Iteration 8382, loss = 0.07041532
Iteration 8383, loss = 0.07040457
Iteration 8384, loss = 0.07039384
Iteration 8385, loss = 0.07038304
Iteration 8386, loss = 0.07037247
Iteration 8387, loss = 0.07036181
Iteration 8388, loss = 0.07035106
Iteration 8389, loss = 0.07034040
Iteration 8390, loss = 0.07032967
Iteration 8391, loss = 0.07031905
Iteration 8392, loss = 0.07030827
Iteration 8393, loss = 0.07029762
Iteration 8394, loss = 0.07028703
Iteration 8395, loss = 0.07027644
Iteration 8396, loss = 0.07026570
Iteration 8397, loss = 0.07025495
Iteration 8398, loss = 0.07024421
Iteration 8399, loss = 0.07023377
Iteration 8400, loss = 0.07022318
Iteration 8401, loss = 0.07021240
Iteration 8402, loss = 0.07020183
Iteration 8403, loss = 0.07019122
Iteration 8404, loss = 0.07018063
Iteration 8405, loss = 0.07016975
Iteration 8406, loss = 0.07015920
Iteration 8407, loss = 0.07014871
Iteration 8408, loss = 0.07013816
Iteration 8409, loss = 0.07012739
Iteration 8410, loss = 0.07011673
Iteration 8411, loss = 0.07010618
Iteration 8412, loss = 0.07009566
Iteration 8413, loss = 0.07008498
Iteration 8414, loss = 0.07007444
Iteration 8415, loss = 0.07006400
Iteration 8416, loss = 0.07005332
Iteration 8417, loss = 0.07004266
Iteration 8418, loss = 0.07003226
Iteration 8419, loss = 0.07002164
Iteration 8420, loss = 0.07001100
Iteration 8421, loss = 0.07000048
Iteration 8422, loss = 0.06998991
Iteration 8423, loss = 0.06997939
Iteration 8424, loss = 0.06996882
Iteration 8425, loss = 0.06995833
Iteration 8426, loss = 0.06994772
Iteration 8427, loss = 0.06993717
Iteration 8428, loss = 0.06992661
Iteration 8429, loss = 0.06991610
Iteration 8430, loss = 0.06990557
Iteration 8431, loss = 0.06989508
Iteration 8432, loss = 0.06988462
Iteration 8433, loss = 0.06987396
Iteration 8434, loss = 0.06986347
Iteration 8435, loss = 0.06985292
Iteration 8436, loss = 0.06984237
Iteration 8437, loss = 0.06983183
Iteration 8438, loss = 0.06982125
Iteration 8439, loss = 0.06981070
Iteration 8440, loss = 0.06980025
Iteration 8441, loss = 0.06978957
Iteration 8442, loss = 0.06977916
Iteration 8443, loss = 0.06976861
Iteration 8444, loss = 0.06975798
Iteration 8445, loss = 0.06974750
Iteration 8446, loss = 0.06973702
Iteration 8447, loss = 0.06972662
Iteration 8448, loss = 0.06971611
Iteration 8449, loss = 0.06970560
Iteration 8450, loss = 0.06969499
Iteration 8451, loss = 0.06968449
Iteration 8452, loss = 0.06967405
Iteration 8453, loss = 0.06966364
Iteration 8454, loss = 0.06965298
Iteration 8455, loss = 0.06964261
Iteration 8456, loss = 0.06963211
Iteration 8457, loss = 0.06962156
Iteration 8458, loss = 0.06961113
Iteration 8459, loss = 0.06960063
Iteration 8460, loss = 0.06959017
Iteration 8461, loss = 0.06957970
Iteration 8462, loss = 0.06956922
Iteration 8463, loss = 0.06955871
Iteration 8464, loss = 0.06954819
Iteration 8465, loss = 0.06953775
Iteration 8466, loss = 0.06952737
Iteration 8467, loss = 0.06951696
Iteration 8468, loss = 0.06950633
Iteration 8469, loss = 0.06949581
Iteration 8470, loss = 0.06948546
Iteration 8471, loss = 0.06947500
Iteration 8472, loss = 0.06946443
Iteration 8473, loss = 0.06945408
Iteration 8474, loss = 0.06944359
Iteration 8475, loss = 0.06943323
Iteration 8476, loss = 0.06942282
Iteration 8477, loss = 0.06941232
Iteration 8478, loss = 0.06940207
Iteration 8479, loss = 0.06939152
Iteration 8480, loss = 0.06938112
Iteration 8481, loss = 0.06937073
Iteration 8482, loss = 0.06936028
Iteration 8483, loss = 0.06934997
Iteration 8484, loss = 0.06933941
Iteration 8485, loss = 0.06932903
Iteration 8486, loss = 0.06931864
Iteration 8487, loss = 0.06930817
Iteration 8488, loss = 0.06929772
Iteration 8489, loss = 0.06928752
Iteration 8490, loss = 0.06927719
Iteration 8491, loss = 0.06926666
Iteration 8492, loss = 0.06925618
Iteration 8493, loss = 0.06924592
Iteration 8494, loss = 0.06923555
Iteration 8495, loss = 0.06922521
Iteration 8496, loss = 0.06921489
Iteration 8497, loss = 0.06920453
Iteration 8498, loss = 0.06919418
Iteration 8499, loss = 0.06918378
Iteration 8500, loss = 0.06917349
Iteration 8501, loss = 0.06916321
Iteration 8502, loss = 0.06915288
Iteration 8503, loss = 0.06914245
Iteration 8504, loss = 0.06913229
Iteration 8505, loss = 0.06912191
Iteration 8506, loss = 0.06911154
Iteration 8507, loss = 0.06910143
Iteration 8508, loss = 0.06909094
Iteration 8509, loss = 0.06908068
Iteration 8510, loss = 0.06907011
Iteration 8511, loss = 0.06906007
Iteration 8512, loss = 0.06904969
Iteration 8513, loss = 0.06903939
Iteration 8514, loss = 0.06902895
Iteration 8515, loss = 0.06901873
Iteration 8516, loss = 0.06900844
Iteration 8517, loss = 0.06899819
Iteration 8518, loss = 0.06898780
Iteration 8519, loss = 0.06897739
Iteration 8520, loss = 0.06896709
Iteration 8521, loss = 0.06895682
Iteration 8522, loss = 0.06894672
Iteration 8523, loss = 0.06893621
Iteration 8524, loss = 0.06892584
Iteration 8525, loss = 0.06891575
Iteration 8526, loss = 0.06890531
Iteration 8527, loss = 0.06889493
Iteration 8528, loss = 0.06888476
Iteration 8529, loss = 0.06887452
Iteration 8530, loss = 0.06886415
Iteration 8531, loss = 0.06885393
Iteration 8532, loss = 0.06884360
Iteration 8533, loss = 0.06883337
Iteration 8534, loss = 0.06882319
Iteration 8535, loss = 0.06881310
Iteration 8536, loss = 0.06880261
Iteration 8537, loss = 0.06879245
Iteration 8538, loss = 0.06878224
Iteration 8539, loss = 0.06877196
Iteration 8540, loss = 0.06876180
Iteration 8541, loss = 0.06875160
Iteration 8542, loss = 0.06874132
Iteration 8543, loss = 0.06873119
Iteration 8544, loss = 0.06872096
Iteration 8545, loss = 0.06871082
Iteration 8546, loss = 0.06870058
Iteration 8547, loss = 0.06869032
Iteration 8548, loss = 0.06868001
Iteration 8549, loss = 0.06866995
Iteration 8550, loss = 0.06865975
Iteration 8551, loss = 0.06864950
Iteration 8552, loss = 0.06863950
Iteration 8553, loss = 0.06862934
Iteration 8554, loss = 0.06861906
Iteration 8555, loss = 0.06860887
Iteration 8556, loss = 0.06859873
Iteration 8557, loss = 0.06858857
Iteration 8558, loss = 0.06857845
Iteration 8559, loss = 0.06856826
Iteration 8560, loss = 0.06855805
Iteration 8561, loss = 0.06854779
Iteration 8562, loss = 0.06853769
Iteration 8563, loss = 0.06852752
Iteration 8564, loss = 0.06851745
Iteration 8565, loss = 0.06850714
Iteration 8566, loss = 0.06849701
Iteration 8567, loss = 0.06848688
Iteration 8568, loss = 0.06847681
Iteration 8569, loss = 0.06846653
Iteration 8570, loss = 0.06845642
Iteration 8571, loss = 0.06844624
Iteration 8572, loss = 0.06843608
Iteration 8573, loss = 0.06842603
Iteration 8574, loss = 0.06841588
Iteration 8575, loss = 0.06840560
Iteration 8576, loss = 0.06839559
Iteration 8577, loss = 0.06838552
Iteration 8578, loss = 0.06837535
Iteration 8579, loss = 0.06836529
Iteration 8580, loss = 0.06835510
Iteration 8581, loss = 0.06834498
Iteration 8582, loss = 0.06833489
Iteration 8583, loss = 0.06832484
Iteration 8584, loss = 0.06831469
Iteration 8585, loss = 0.06830453
Iteration 8586, loss = 0.06829445
Iteration 8587, loss = 0.06828436
Iteration 8588, loss = 0.06827440
Iteration 8589, loss = 0.06826427
Iteration 8590, loss = 0.06825420
Iteration 8591, loss = 0.06824416
Iteration 8592, loss = 0.06823389
Iteration 8593, loss = 0.06822392
Iteration 8594, loss = 0.06821388
Iteration 8595, loss = 0.06820369
Iteration 8596, loss = 0.06819366
Iteration 8597, loss = 0.06818350
Iteration 8598, loss = 0.06817343
Iteration 8599, loss = 0.06816347
Iteration 8600, loss = 0.06815343
Iteration 8601, loss = 0.06814330
Iteration 8602, loss = 0.06813333
Iteration 8603, loss = 0.06812317
Iteration 8604, loss = 0.06811308
Iteration 8605, loss = 0.06810302
Iteration 8606, loss = 0.06809297
Iteration 8607, loss = 0.06808294
Iteration 8608, loss = 0.06807294
Iteration 8609, loss = 0.06806278
Iteration 8610, loss = 0.06805294
Iteration 8611, loss = 0.06804277
Iteration 8612, loss = 0.06803301
Iteration 8613, loss = 0.06802279
Iteration 8614, loss = 0.06801283
Iteration 8615, loss = 0.06800273
Iteration 8616, loss = 0.06799274
Iteration 8617, loss = 0.06798275
Iteration 8618, loss = 0.06797271
Iteration 8619, loss = 0.06796277
Iteration 8620, loss = 0.06795275
Iteration 8621, loss = 0.06794273
Iteration 8622, loss = 0.06793274
Iteration 8623, loss = 0.06792276
Iteration 8624, loss = 0.06791275
Iteration 8625, loss = 0.06790296
Iteration 8626, loss = 0.06789283
Iteration 8627, loss = 0.06788286
Iteration 8628, loss = 0.06787284
Iteration 8629, loss = 0.06786292
Iteration 8630, loss = 0.06785291
Iteration 8631, loss = 0.06784295
Iteration 8632, loss = 0.06783302
Iteration 8633, loss = 0.06782301
Iteration 8634, loss = 0.06781304
Iteration 8635, loss = 0.06780312
Iteration 8636, loss = 0.06779322
Iteration 8637, loss = 0.06778316
Iteration 8638, loss = 0.06777317
Iteration 8639, loss = 0.06776332
Iteration 8640, loss = 0.06775347
Iteration 8641, loss = 0.06774338
Iteration 8642, loss = 0.06773334
Iteration 8643, loss = 0.06772350
Iteration 8644, loss = 0.06771352
Iteration 8645, loss = 0.06770358
Iteration 8646, loss = 0.06769366
Iteration 8647, loss = 0.06768366
Iteration 8648, loss = 0.06767379
Iteration 8649, loss = 0.06766393
Iteration 8650, loss = 0.06765400
Iteration 8651, loss = 0.06764405
Iteration 8652, loss = 0.06763412
Iteration 8653, loss = 0.06762422
Iteration 8654, loss = 0.06761429
Iteration 8655, loss = 0.06760437
Iteration 8656, loss = 0.06759442
Iteration 8657, loss = 0.06758447
Iteration 8658, loss = 0.06757473
Iteration 8659, loss = 0.06756466
Iteration 8660, loss = 0.06755485
Iteration 8661, loss = 0.06754506
Iteration 8662, loss = 0.06753489
Iteration 8663, loss = 0.06752501
Iteration 8664, loss = 0.06751519
Iteration 8665, loss = 0.06750521
Iteration 8666, loss = 0.06749534
Iteration 8667, loss = 0.06748545
Iteration 8668, loss = 0.06747567
Iteration 8669, loss = 0.06746558
Iteration 8670, loss = 0.06745588
Iteration 8671, loss = 0.06744585
Iteration 8672, loss = 0.06743606
Iteration 8673, loss = 0.06742626
Iteration 8674, loss = 0.06741623
Iteration 8675, loss = 0.06740638
Iteration 8676, loss = 0.06739664
Iteration 8677, loss = 0.06738675
Iteration 8678, loss = 0.06737678
Iteration 8679, loss = 0.06736704
Iteration 8680, loss = 0.06735710
Iteration 8681, loss = 0.06734718
Iteration 8682, loss = 0.06733744
Iteration 8683, loss = 0.06732759
Iteration 8684, loss = 0.06731772
Iteration 8685, loss = 0.06730796
Iteration 8686, loss = 0.06729797
Iteration 8687, loss = 0.06728819
Iteration 8688, loss = 0.06727839
Iteration 8689, loss = 0.06726848
Iteration 8690, loss = 0.06725868
Iteration 8691, loss = 0.06724883
Iteration 8692, loss = 0.06723908
Iteration 8693, loss = 0.06722930
Iteration 8694, loss = 0.06721947
Iteration 8695, loss = 0.06720962
Iteration 8696, loss = 0.06719985
Iteration 8697, loss = 0.06719004
Iteration 8698, loss = 0.06718031
Iteration 8699, loss = 0.06717059
Iteration 8700, loss = 0.06716084
Iteration 8701, loss = 0.06715094
Iteration 8702, loss = 0.06714123
Iteration 8703, loss = 0.06713149
Iteration 8704, loss = 0.06712163
Iteration 8705, loss = 0.06711189
Iteration 8706, loss = 0.06710211
Iteration 8707, loss = 0.06709223
Iteration 8708, loss = 0.06708240
Iteration 8709, loss = 0.06707273
Iteration 8710, loss = 0.06706294
Iteration 8711, loss = 0.06705312
Iteration 8712, loss = 0.06704338
Iteration 8713, loss = 0.06703343
Iteration 8714, loss = 0.06702391
Iteration 8715, loss = 0.06701404
Iteration 8716, loss = 0.06700429
Iteration 8717, loss = 0.06699454
Iteration 8718, loss = 0.06698480
Iteration 8719, loss = 0.06697503
Iteration 8720, loss = 0.06696529
Iteration 8721, loss = 0.06695556
Iteration 8722, loss = 0.06694584
Iteration 8723, loss = 0.06693609
Iteration 8724, loss = 0.06692626
Iteration 8725, loss = 0.06691681
Iteration 8726, loss = 0.06690692
Iteration 8727, loss = 0.06689716
Iteration 8728, loss = 0.06688738
Iteration 8729, loss = 0.06687759
Iteration 8730, loss = 0.06686803
Iteration 8731, loss = 0.06685828
Iteration 8732, loss = 0.06684856
Iteration 8733, loss = 0.06683880
Iteration 8734, loss = 0.06682925
Iteration 8735, loss = 0.06681948
Iteration 8736, loss = 0.06680971
Iteration 8737, loss = 0.06680004
Iteration 8738, loss = 0.06679045
Iteration 8739, loss = 0.06678066
Iteration 8740, loss = 0.06677102
Iteration 8741, loss = 0.06676133
Iteration 8742, loss = 0.06675153
Iteration 8743, loss = 0.06674193
Iteration 8744, loss = 0.06673221
Iteration 8745, loss = 0.06672254
Iteration 8746, loss = 0.06671282
Iteration 8747, loss = 0.06670307
Iteration 8748, loss = 0.06669360
Iteration 8749, loss = 0.06668383
Iteration 8750, loss = 0.06667410
Iteration 8751, loss = 0.06666451
Iteration 8752, loss = 0.06665492
Iteration 8753, loss = 0.06664517
Iteration 8754, loss = 0.06663561
Iteration 8755, loss = 0.06662598
Iteration 8756, loss = 0.06661634
Iteration 8757, loss = 0.06660675
Iteration 8758, loss = 0.06659708
Iteration 8759, loss = 0.06658732
Iteration 8760, loss = 0.06657791
Iteration 8761, loss = 0.06656822
Iteration 8762, loss = 0.06655866
Iteration 8763, loss = 0.06654907
Iteration 8764, loss = 0.06653939
Iteration 8765, loss = 0.06652988
Iteration 8766, loss = 0.06652023
Iteration 8767, loss = 0.06651065
Iteration 8768, loss = 0.06650106
Iteration 8769, loss = 0.06649149
Iteration 8770, loss = 0.06648192
Iteration 8771, loss = 0.06647226
Iteration 8772, loss = 0.06646276
Iteration 8773, loss = 0.06645311
Iteration 8774, loss = 0.06644336
Iteration 8775, loss = 0.06643398
Iteration 8776, loss = 0.06642430
Iteration 8777, loss = 0.06641471
Iteration 8778, loss = 0.06640524
Iteration 8779, loss = 0.06639561
Iteration 8780, loss = 0.06638607
Iteration 8781, loss = 0.06637651
Iteration 8782, loss = 0.06636685
Iteration 8783, loss = 0.06635726
Iteration 8784, loss = 0.06634764
Iteration 8785, loss = 0.06633815
Iteration 8786, loss = 0.06632853
Iteration 8787, loss = 0.06631900
Iteration 8788, loss = 0.06630942
Iteration 8789, loss = 0.06630002
Iteration 8790, loss = 0.06629025
Iteration 8791, loss = 0.06628079
Iteration 8792, loss = 0.06627124
Iteration 8793, loss = 0.06626171
Iteration 8794, loss = 0.06625215
Iteration 8795, loss = 0.06624268
Iteration 8796, loss = 0.06623308
Iteration 8797, loss = 0.06622358
Iteration 8798, loss = 0.06621396
Iteration 8799, loss = 0.06620446
Iteration 8800, loss = 0.06619509
Iteration 8801, loss = 0.06618541
Iteration 8802, loss = 0.06617595
Iteration 8803, loss = 0.06616632
Iteration 8804, loss = 0.06615678
Iteration 8805, loss = 0.06614734
Iteration 8806, loss = 0.06613779
Iteration 8807, loss = 0.06612823
Iteration 8808, loss = 0.06611868
Iteration 8809, loss = 0.06610930
Iteration 8810, loss = 0.06609983
Iteration 8811, loss = 0.06609020
Iteration 8812, loss = 0.06608073
Iteration 8813, loss = 0.06607131
Iteration 8814, loss = 0.06606179
Iteration 8815, loss = 0.06605247
Iteration 8816, loss = 0.06604283
Iteration 8817, loss = 0.06603333
Iteration 8818, loss = 0.06602394
Iteration 8819, loss = 0.06601435
Iteration 8820, loss = 0.06600490
Iteration 8821, loss = 0.06599542
Iteration 8822, loss = 0.06598588
Iteration 8823, loss = 0.06597646
Iteration 8824, loss = 0.06596702
Iteration 8825, loss = 0.06595759
Iteration 8826, loss = 0.06594798
Iteration 8827, loss = 0.06593866
Iteration 8828, loss = 0.06592913
Iteration 8829, loss = 0.06591961
Iteration 8830, loss = 0.06591014
Iteration 8831, loss = 0.06590075
Iteration 8832, loss = 0.06589129
Iteration 8833, loss = 0.06588170
Iteration 8834, loss = 0.06587244
Iteration 8835, loss = 0.06586297
Iteration 8836, loss = 0.06585354
Iteration 8837, loss = 0.06584401
Iteration 8838, loss = 0.06583454
Iteration 8839, loss = 0.06582519
Iteration 8840, loss = 0.06581583
Iteration 8841, loss = 0.06580643
Iteration 8842, loss = 0.06579701
Iteration 8843, loss = 0.06578746
Iteration 8844, loss = 0.06577808
Iteration 8845, loss = 0.06576876
Iteration 8846, loss = 0.06575942
Iteration 8847, loss = 0.06574990
Iteration 8848, loss = 0.06574046
Iteration 8849, loss = 0.06573115
Iteration 8850, loss = 0.06572178
Iteration 8851, loss = 0.06571240
Iteration 8852, loss = 0.06570296
Iteration 8853, loss = 0.06569368
Iteration 8854, loss = 0.06568424
Iteration 8855, loss = 0.06567496
Iteration 8856, loss = 0.06566557
Iteration 8857, loss = 0.06565628
Iteration 8858, loss = 0.06564691
Iteration 8859, loss = 0.06563748
Iteration 8860, loss = 0.06562821
Iteration 8861, loss = 0.06561879
Iteration 8862, loss = 0.06560953
Iteration 8863, loss = 0.06560016
Iteration 8864, loss = 0.06559084
Iteration 8865, loss = 0.06558151
Iteration 8866, loss = 0.06557209
Iteration 8867, loss = 0.06556294
Iteration 8868, loss = 0.06555353
Iteration 8869, loss = 0.06554418
Iteration 8870, loss = 0.06553474
Iteration 8871, loss = 0.06552551
Iteration 8872, loss = 0.06551618
Iteration 8873, loss = 0.06550676
Iteration 8874, loss = 0.06549742
Iteration 8875, loss = 0.06548818
Iteration 8876, loss = 0.06547866
Iteration 8877, loss = 0.06546934
Iteration 8878, loss = 0.06546002
Iteration 8879, loss = 0.06545087
Iteration 8880, loss = 0.06544132
Iteration 8881, loss = 0.06543197
Iteration 8882, loss = 0.06542275
Iteration 8883, loss = 0.06541336
Iteration 8884, loss = 0.06540398
Iteration 8885, loss = 0.06539474
Iteration 8886, loss = 0.06538547
Iteration 8887, loss = 0.06537603
Iteration 8888, loss = 0.06536685
Iteration 8889, loss = 0.06535744
Iteration 8890, loss = 0.06534824
Iteration 8891, loss = 0.06533891
Iteration 8892, loss = 0.06532957
Iteration 8893, loss = 0.06532020
Iteration 8894, loss = 0.06531083
Iteration 8895, loss = 0.06530160
Iteration 8896, loss = 0.06529239
Iteration 8897, loss = 0.06528291
Iteration 8898, loss = 0.06527376
Iteration 8899, loss = 0.06526448
Iteration 8900, loss = 0.06525494
Iteration 8901, loss = 0.06524571
Iteration 8902, loss = 0.06523661
Iteration 8903, loss = 0.06522714
Iteration 8904, loss = 0.06521774
Iteration 8905, loss = 0.06520855
Iteration 8906, loss = 0.06519936
Iteration 8907, loss = 0.06519010
Iteration 8908, loss = 0.06518083
Iteration 8909, loss = 0.06517149
Iteration 8910, loss = 0.06516223
Iteration 8911, loss = 0.06515281
Iteration 8912, loss = 0.06514384
Iteration 8913, loss = 0.06513446
Iteration 8914, loss = 0.06512532
Iteration 8915, loss = 0.06511591
Iteration 8916, loss = 0.06510679
Iteration 8917, loss = 0.06509761
Iteration 8918, loss = 0.06508823
Iteration 8919, loss = 0.06507919
Iteration 8920, loss = 0.06506985
Iteration 8921, loss = 0.06506062
Iteration 8922, loss = 0.06505143
Iteration 8923, loss = 0.06504229
Iteration 8924, loss = 0.06503320
Iteration 8925, loss = 0.06502392
Iteration 8926, loss = 0.06501466
Iteration 8927, loss = 0.06500561
Iteration 8928, loss = 0.06499632
Iteration 8929, loss = 0.06498717
Iteration 8930, loss = 0.06497799
Iteration 8931, loss = 0.06496874
Iteration 8932, loss = 0.06495963
Iteration 8933, loss = 0.06495065
Iteration 8934, loss = 0.06494131
Iteration 8935, loss = 0.06493224
Iteration 8936, loss = 0.06492309
Iteration 8937, loss = 0.06491391
Iteration 8938, loss = 0.06490481
Iteration 8939, loss = 0.06489560
Iteration 8940, loss = 0.06488656
Iteration 8941, loss = 0.06487752
Iteration 8942, loss = 0.06486821
Iteration 8943, loss = 0.06485905
Iteration 8944, loss = 0.06484991
Iteration 8945, loss = 0.06484072
Iteration 8946, loss = 0.06483159
Iteration 8947, loss = 0.06482244
Iteration 8948, loss = 0.06481332
Iteration 8949, loss = 0.06480413
Iteration 8950, loss = 0.06479493
Iteration 8951, loss = 0.06478579
Iteration 8952, loss = 0.06477669
Iteration 8953, loss = 0.06476748
Iteration 8954, loss = 0.06475845
Iteration 8955, loss = 0.06474932
Iteration 8956, loss = 0.06474009
Iteration 8957, loss = 0.06473096
Iteration 8958, loss = 0.06472180
Iteration 8959, loss = 0.06471266
Iteration 8960, loss = 0.06470375
Iteration 8961, loss = 0.06469449
Iteration 8962, loss = 0.06468541
Iteration 8963, loss = 0.06467637
Iteration 8964, loss = 0.06466725
Iteration 8965, loss = 0.06465816
Iteration 8966, loss = 0.06464898
Iteration 8967, loss = 0.06463988
Iteration 8968, loss = 0.06463097
Iteration 8969, loss = 0.06462173
Iteration 8970, loss = 0.06461268
Iteration 8971, loss = 0.06460359
Iteration 8972, loss = 0.06459439
Iteration 8973, loss = 0.06458532
Iteration 8974, loss = 0.06457622
Iteration 8975, loss = 0.06456719
Iteration 8976, loss = 0.06455810
Iteration 8977, loss = 0.06454913
Iteration 8978, loss = 0.06453991
Iteration 8979, loss = 0.06453079
Iteration 8980, loss = 0.06452182
Iteration 8981, loss = 0.06451282
Iteration 8982, loss = 0.06450365
Iteration 8983, loss = 0.06449472
Iteration 8984, loss = 0.06448555
Iteration 8985, loss = 0.06447649
Iteration 8986, loss = 0.06446755
Iteration 8987, loss = 0.06445842
Iteration 8988, loss = 0.06444941
Iteration 8989, loss = 0.06444037
Iteration 8990, loss = 0.06443132
Iteration 8991, loss = 0.06442236
Iteration 8992, loss = 0.06441335
Iteration 8993, loss = 0.06440441
Iteration 8994, loss = 0.06439531
Iteration 8995, loss = 0.06438621
Iteration 8996, loss = 0.06437715
Iteration 8997, loss = 0.06436827
Iteration 8998, loss = 0.06435910
Iteration 8999, loss = 0.06435018
Iteration 9000, loss = 0.06434107
Iteration 9001, loss = 0.06433214
Iteration 9002, loss = 0.06432315
Iteration 9003, loss = 0.06431400
Iteration 9004, loss = 0.06430502
Iteration 9005, loss = 0.06429617
Iteration 9006, loss = 0.06428702
Iteration 9007, loss = 0.06427803
Iteration 9008, loss = 0.06426917
Iteration 9009, loss = 0.06426001
Iteration 9010, loss = 0.06425102
Iteration 9011, loss = 0.06424197
Iteration 9012, loss = 0.06423312
Iteration 9013, loss = 0.06422399
Iteration 9014, loss = 0.06421509
Iteration 9015, loss = 0.06420624
Iteration 9016, loss = 0.06419710
Iteration 9017, loss = 0.06418807
Iteration 9018, loss = 0.06417921
Iteration 9019, loss = 0.06417014
Iteration 9020, loss = 0.06416123
Iteration 9021, loss = 0.06415224
Iteration 9022, loss = 0.06414329
Iteration 9023, loss = 0.06413432
Iteration 9024, loss = 0.06412529
Iteration 9025, loss = 0.06411639
Iteration 9026, loss = 0.06410755
Iteration 9027, loss = 0.06409842
Iteration 9028, loss = 0.06408955
Iteration 9029, loss = 0.06408067
Iteration 9030, loss = 0.06407165
Iteration 9031, loss = 0.06406273
Iteration 9032, loss = 0.06405371
Iteration 9033, loss = 0.06404487
Iteration 9034, loss = 0.06403590
Iteration 9035, loss = 0.06402691
Iteration 9036, loss = 0.06401803
Iteration 9037, loss = 0.06400903
Iteration 9038, loss = 0.06400025
Iteration 9039, loss = 0.06399126
Iteration 9040, loss = 0.06398234
Iteration 9041, loss = 0.06397336
Iteration 9042, loss = 0.06396443
Iteration 9043, loss = 0.06395547
Iteration 9044, loss = 0.06394666
Iteration 9045, loss = 0.06393772
Iteration 9046, loss = 0.06392874
Iteration 9047, loss = 0.06391976
Iteration 9048, loss = 0.06391097
Iteration 9049, loss = 0.06390222
Iteration 9050, loss = 0.06389314
Iteration 9051, loss = 0.06388418
Iteration 9052, loss = 0.06387536
Iteration 9053, loss = 0.06386639
Iteration 9054, loss = 0.06385755
Iteration 9055, loss = 0.06384861
Iteration 9056, loss = 0.06383972
Iteration 9057, loss = 0.06383079
Iteration 9058, loss = 0.06382195
Iteration 9059, loss = 0.06381316
Iteration 9060, loss = 0.06380432
Iteration 9061, loss = 0.06379532
Iteration 9062, loss = 0.06378656
Iteration 9063, loss = 0.06377751
Iteration 9064, loss = 0.06376866
Iteration 9065, loss = 0.06375986
Iteration 9066, loss = 0.06375108
Iteration 9067, loss = 0.06374211
Iteration 9068, loss = 0.06373326
Iteration 9069, loss = 0.06372437
Iteration 9070, loss = 0.06371550
Iteration 9071, loss = 0.06370671
Iteration 9072, loss = 0.06369791
Iteration 9073, loss = 0.06368911
Iteration 9074, loss = 0.06368017
Iteration 9075, loss = 0.06367136
Iteration 9076, loss = 0.06366258
Iteration 9077, loss = 0.06365382
Iteration 9078, loss = 0.06364504
Iteration 9079, loss = 0.06363620
Iteration 9080, loss = 0.06362734
Iteration 9081, loss = 0.06361863
Iteration 9082, loss = 0.06360979
Iteration 9083, loss = 0.06360090
Iteration 9084, loss = 0.06359215
Iteration 9085, loss = 0.06358334
Iteration 9086, loss = 0.06357460
Iteration 9087, loss = 0.06356580
Iteration 9088, loss = 0.06355712
Iteration 9089, loss = 0.06354824
Iteration 9090, loss = 0.06353942
Iteration 9091, loss = 0.06353059
Iteration 9092, loss = 0.06352182
Iteration 9093, loss = 0.06351304
Iteration 9094, loss = 0.06350421
Iteration 9095, loss = 0.06349549
Iteration 9096, loss = 0.06348672
Iteration 9097, loss = 0.06347792
Iteration 9098, loss = 0.06346918
Iteration 9099, loss = 0.06346034
Iteration 9100, loss = 0.06345160
Iteration 9101, loss = 0.06344295
Iteration 9102, loss = 0.06343405
Iteration 9103, loss = 0.06342537
Iteration 9104, loss = 0.06341662
Iteration 9105, loss = 0.06340783
Iteration 9106, loss = 0.06339897
Iteration 9107, loss = 0.06339049
Iteration 9108, loss = 0.06338161
Iteration 9109, loss = 0.06337302
Iteration 9110, loss = 0.06336415
Iteration 9111, loss = 0.06335543
Iteration 9112, loss = 0.06334671
Iteration 9113, loss = 0.06333801
Iteration 9114, loss = 0.06332938
Iteration 9115, loss = 0.06332064
Iteration 9116, loss = 0.06331186
Iteration 9117, loss = 0.06330313
Iteration 9118, loss = 0.06329461
Iteration 9119, loss = 0.06328581
Iteration 9120, loss = 0.06327699
Iteration 9121, loss = 0.06326834
Iteration 9122, loss = 0.06325968
Iteration 9123, loss = 0.06325101
Iteration 9124, loss = 0.06324239
Iteration 9125, loss = 0.06323361
Iteration 9126, loss = 0.06322481
Iteration 9127, loss = 0.06321616
Iteration 9128, loss = 0.06320745
Iteration 9129, loss = 0.06319882
Iteration 9130, loss = 0.06319004
Iteration 9131, loss = 0.06318154
Iteration 9132, loss = 0.06317273
Iteration 9133, loss = 0.06316402
Iteration 9134, loss = 0.06315524
Iteration 9135, loss = 0.06314660
Iteration 9136, loss = 0.06313791
Iteration 9137, loss = 0.06312931
Iteration 9138, loss = 0.06312055
Iteration 9139, loss = 0.06311200
Iteration 9140, loss = 0.06310326
Iteration 9141, loss = 0.06309464
Iteration 9142, loss = 0.06308597
Iteration 9143, loss = 0.06307731
Iteration 9144, loss = 0.06306865
Iteration 9145, loss = 0.06305989
Iteration 9146, loss = 0.06305137
Iteration 9147, loss = 0.06304263
Iteration 9148, loss = 0.06303403
Iteration 9149, loss = 0.06302538
Iteration 9150, loss = 0.06301666
Iteration 9151, loss = 0.06300812
Iteration 9152, loss = 0.06299946
Iteration 9153, loss = 0.06299089
Iteration 9154, loss = 0.06298220
Iteration 9155, loss = 0.06297374
Iteration 9156, loss = 0.06296491
Iteration 9157, loss = 0.06295637
Iteration 9158, loss = 0.06294765
Iteration 9159, loss = 0.06293899
Iteration 9160, loss = 0.06293052
Iteration 9161, loss = 0.06292191
Iteration 9162, loss = 0.06291319
Iteration 9163, loss = 0.06290458
Iteration 9164, loss = 0.06289595
Iteration 9165, loss = 0.06288734
Iteration 9166, loss = 0.06287878
Iteration 9167, loss = 0.06287015
Iteration 9168, loss = 0.06286142
Iteration 9169, loss = 0.06285300
Iteration 9170, loss = 0.06284420
Iteration 9171, loss = 0.06283573
Iteration 9172, loss = 0.06282713
Iteration 9173, loss = 0.06281843
Iteration 9174, loss = 0.06280997
Iteration 9175, loss = 0.06280118
Iteration 9176, loss = 0.06279279
Iteration 9177, loss = 0.06278415
Iteration 9178, loss = 0.06277553
Iteration 9179, loss = 0.06276694
Iteration 9180, loss = 0.06275827
Iteration 9181, loss = 0.06274967
Iteration 9182, loss = 0.06274105
Iteration 9183, loss = 0.06273258
Iteration 9184, loss = 0.06272385
Iteration 9185, loss = 0.06271530
Iteration 9186, loss = 0.06270679
Iteration 9187, loss = 0.06269815
Iteration 9188, loss = 0.06268955
Iteration 9189, loss = 0.06268108
Iteration 9190, loss = 0.06267243
Iteration 9191, loss = 0.06266385
Iteration 9192, loss = 0.06265532
Iteration 9193, loss = 0.06264673
Iteration 9194, loss = 0.06263825
Iteration 9195, loss = 0.06262971
Iteration 9196, loss = 0.06262115
Iteration 9197, loss = 0.06261265
Iteration 9198, loss = 0.06260421
Iteration 9199, loss = 0.06259559
Iteration 9200, loss = 0.06258709
Iteration 9201, loss = 0.06257873
Iteration 9202, loss = 0.06257002
Iteration 9203, loss = 0.06256151
Iteration 9204, loss = 0.06255301
Iteration 9205, loss = 0.06254449
Iteration 9206, loss = 0.06253591
Iteration 9207, loss = 0.06252742
Iteration 9208, loss = 0.06251887
Iteration 9209, loss = 0.06251033
Iteration 9210, loss = 0.06250193
Iteration 9211, loss = 0.06249339
Iteration 9212, loss = 0.06248484
Iteration 9213, loss = 0.06247634
Iteration 9214, loss = 0.06246773
Iteration 9215, loss = 0.06245928
Iteration 9216, loss = 0.06245090
Iteration 9217, loss = 0.06244237
Iteration 9218, loss = 0.06243378
Iteration 9219, loss = 0.06242525
Iteration 9220, loss = 0.06241689
Iteration 9221, loss = 0.06240823
Iteration 9222, loss = 0.06239982
Iteration 9223, loss = 0.06239144
Iteration 9224, loss = 0.06238282
Iteration 9225, loss = 0.06237434
Iteration 9226, loss = 0.06236579
Iteration 9227, loss = 0.06235739
Iteration 9228, loss = 0.06234894
Iteration 9229, loss = 0.06234043
Iteration 9230, loss = 0.06233197
Iteration 9231, loss = 0.06232350
Iteration 9232, loss = 0.06231504
Iteration 9233, loss = 0.06230666
Iteration 9234, loss = 0.06229813
Iteration 9235, loss = 0.06228971
Iteration 9236, loss = 0.06228132
Iteration 9237, loss = 0.06227284
Iteration 9238, loss = 0.06226441
Iteration 9239, loss = 0.06225606
Iteration 9240, loss = 0.06224751
Iteration 9241, loss = 0.06223920
Iteration 9242, loss = 0.06223067
Iteration 9243, loss = 0.06222229
Iteration 9244, loss = 0.06221388
Iteration 9245, loss = 0.06220540
Iteration 9246, loss = 0.06219700
Iteration 9247, loss = 0.06218879
Iteration 9248, loss = 0.06218020
Iteration 9249, loss = 0.06217178
Iteration 9250, loss = 0.06216341
Iteration 9251, loss = 0.06215496
Iteration 9252, loss = 0.06214654
Iteration 9253, loss = 0.06213813
Iteration 9254, loss = 0.06212971
Iteration 9255, loss = 0.06212134
Iteration 9256, loss = 0.06211295
Iteration 9257, loss = 0.06210445
Iteration 9258, loss = 0.06209610
Iteration 9259, loss = 0.06208765
Iteration 9260, loss = 0.06207915
Iteration 9261, loss = 0.06207070
Iteration 9262, loss = 0.06206226
Iteration 9263, loss = 0.06205389
Iteration 9264, loss = 0.06204549
Iteration 9265, loss = 0.06203704
Iteration 9266, loss = 0.06202858
Iteration 9267, loss = 0.06202035
Iteration 9268, loss = 0.06201180
Iteration 9269, loss = 0.06200346
Iteration 9270, loss = 0.06199513
Iteration 9271, loss = 0.06198673
Iteration 9272, loss = 0.06197840
Iteration 9273, loss = 0.06197001
Iteration 9274, loss = 0.06196162
Iteration 9275, loss = 0.06195322
Iteration 9276, loss = 0.06194489
Iteration 9277, loss = 0.06193650
Iteration 9278, loss = 0.06192816
Iteration 9279, loss = 0.06191983
Iteration 9280, loss = 0.06191146
Iteration 9281, loss = 0.06190316
Iteration 9282, loss = 0.06189480
Iteration 9283, loss = 0.06188635
Iteration 9284, loss = 0.06187812
Iteration 9285, loss = 0.06186964
Iteration 9286, loss = 0.06186138
Iteration 9287, loss = 0.06185304
Iteration 9288, loss = 0.06184465
Iteration 9289, loss = 0.06183647
Iteration 9290, loss = 0.06182794
Iteration 9291, loss = 0.06181964
Iteration 9292, loss = 0.06181140
Iteration 9293, loss = 0.06180302
Iteration 9294, loss = 0.06179470
Iteration 9295, loss = 0.06178640
Iteration 9296, loss = 0.06177815
Iteration 9297, loss = 0.06176971
Iteration 9298, loss = 0.06176139
Iteration 9299, loss = 0.06175303
Iteration 9300, loss = 0.06174472
Iteration 9301, loss = 0.06173646
Iteration 9302, loss = 0.06172814
Iteration 9303, loss = 0.06171979
Iteration 9304, loss = 0.06171150
Iteration 9305, loss = 0.06170322
Iteration 9306, loss = 0.06169486
Iteration 9307, loss = 0.06168661
Iteration 9308, loss = 0.06167820
Iteration 9309, loss = 0.06167002
Iteration 9310, loss = 0.06166174
Iteration 9311, loss = 0.06165342
Iteration 9312, loss = 0.06164527
Iteration 9313, loss = 0.06163697
Iteration 9314, loss = 0.06162860
Iteration 9315, loss = 0.06162035
Iteration 9316, loss = 0.06161202
Iteration 9317, loss = 0.06160396
Iteration 9318, loss = 0.06159563
Iteration 9319, loss = 0.06158744
Iteration 9320, loss = 0.06157911
Iteration 9321, loss = 0.06157086
Iteration 9322, loss = 0.06156274
Iteration 9323, loss = 0.06155438
Iteration 9324, loss = 0.06154608
Iteration 9325, loss = 0.06153789
Iteration 9326, loss = 0.06152964
Iteration 9327, loss = 0.06152136
Iteration 9328, loss = 0.06151310
Iteration 9329, loss = 0.06150499
Iteration 9330, loss = 0.06149658
Iteration 9331, loss = 0.06148841
Iteration 9332, loss = 0.06148018
Iteration 9333, loss = 0.06147201
Iteration 9334, loss = 0.06146371
Iteration 9335, loss = 0.06145546
Iteration 9336, loss = 0.06144728
Iteration 9337, loss = 0.06143900
Iteration 9338, loss = 0.06143069
Iteration 9339, loss = 0.06142247
Iteration 9340, loss = 0.06141421
Iteration 9341, loss = 0.06140599
Iteration 9342, loss = 0.06139772
Iteration 9343, loss = 0.06138939
Iteration 9344, loss = 0.06138117
Iteration 9345, loss = 0.06137290
Iteration 9346, loss = 0.06136462
Iteration 9347, loss = 0.06135651
Iteration 9348, loss = 0.06134833
Iteration 9349, loss = 0.06134007
Iteration 9350, loss = 0.06133177
Iteration 9351, loss = 0.06132364
Iteration 9352, loss = 0.06131557
Iteration 9353, loss = 0.06130713
Iteration 9354, loss = 0.06129903
Iteration 9355, loss = 0.06129081
Iteration 9356, loss = 0.06128258
Iteration 9357, loss = 0.06127446
Iteration 9358, loss = 0.06126618
Iteration 9359, loss = 0.06125811
Iteration 9360, loss = 0.06124981
Iteration 9361, loss = 0.06124173
Iteration 9362, loss = 0.06123347
Iteration 9363, loss = 0.06122529
Iteration 9364, loss = 0.06121710
Iteration 9365, loss = 0.06120886
Iteration 9366, loss = 0.06120066
Iteration 9367, loss = 0.06119249
Iteration 9368, loss = 0.06118435
Iteration 9369, loss = 0.06117615
Iteration 9370, loss = 0.06116802
Iteration 9371, loss = 0.06115988
Iteration 9372, loss = 0.06115164
Iteration 9373, loss = 0.06114345
Iteration 9374, loss = 0.06113527
Iteration 9375, loss = 0.06112711
Iteration 9376, loss = 0.06111896
Iteration 9377, loss = 0.06111081
Iteration 9378, loss = 0.06110267
Iteration 9379, loss = 0.06109443
Iteration 9380, loss = 0.06108634
Iteration 9381, loss = 0.06107824
Iteration 9382, loss = 0.06107001
Iteration 9383, loss = 0.06106207
Iteration 9384, loss = 0.06105386
Iteration 9385, loss = 0.06104569
Iteration 9386, loss = 0.06103750
Iteration 9387, loss = 0.06102951
Iteration 9388, loss = 0.06102126
Iteration 9389, loss = 0.06101317
Iteration 9390, loss = 0.06100502
Iteration 9391, loss = 0.06099697
Iteration 9392, loss = 0.06098881
Iteration 9393, loss = 0.06098072
Iteration 9394, loss = 0.06097262
Iteration 9395, loss = 0.06096451
Iteration 9396, loss = 0.06095640
Iteration 9397, loss = 0.06094824
Iteration 9398, loss = 0.06094027
Iteration 9399, loss = 0.06093215
Iteration 9400, loss = 0.06092410
Iteration 9401, loss = 0.06091599
Iteration 9402, loss = 0.06090779
Iteration 9403, loss = 0.06089984
Iteration 9404, loss = 0.06089171
Iteration 9405, loss = 0.06088354
Iteration 9406, loss = 0.06087555
Iteration 9407, loss = 0.06086752
Iteration 9408, loss = 0.06085942
Iteration 9409, loss = 0.06085141
Iteration 9410, loss = 0.06084328
Iteration 9411, loss = 0.06083522
Iteration 9412, loss = 0.06082728
Iteration 9413, loss = 0.06081900
Iteration 9414, loss = 0.06081105
Iteration 9415, loss = 0.06080295
Iteration 9416, loss = 0.06079488
Iteration 9417, loss = 0.06078699
Iteration 9418, loss = 0.06077891
Iteration 9419, loss = 0.06077076
Iteration 9420, loss = 0.06076283
Iteration 9421, loss = 0.06075466
Iteration 9422, loss = 0.06074674
Iteration 9423, loss = 0.06073875
Iteration 9424, loss = 0.06073066
Iteration 9425, loss = 0.06072257
Iteration 9426, loss = 0.06071456
Iteration 9427, loss = 0.06070646
Iteration 9428, loss = 0.06069850
Iteration 9429, loss = 0.06069037
Iteration 9430, loss = 0.06068240
Iteration 9431, loss = 0.06067439
Iteration 9432, loss = 0.06066625
Iteration 9433, loss = 0.06065826
Iteration 9434, loss = 0.06065025
Iteration 9435, loss = 0.06064217
Iteration 9436, loss = 0.06063422
Iteration 9437, loss = 0.06062617
Iteration 9438, loss = 0.06061815
Iteration 9439, loss = 0.06061005
Iteration 9440, loss = 0.06060204
Iteration 9441, loss = 0.06059407
Iteration 9442, loss = 0.06058612
Iteration 9443, loss = 0.06057809
Iteration 9444, loss = 0.06057010
Iteration 9445, loss = 0.06056199
Iteration 9446, loss = 0.06055412
Iteration 9447, loss = 0.06054594
Iteration 9448, loss = 0.06053795
Iteration 9449, loss = 0.06053003
Iteration 9450, loss = 0.06052205
Iteration 9451, loss = 0.06051400
Iteration 9452, loss = 0.06050598
Iteration 9453, loss = 0.06049797
Iteration 9454, loss = 0.06048996
Iteration 9455, loss = 0.06048203
Iteration 9456, loss = 0.06047404
Iteration 9457, loss = 0.06046613
Iteration 9458, loss = 0.06045803
Iteration 9459, loss = 0.06045010
Iteration 9460, loss = 0.06044206
Iteration 9461, loss = 0.06043399
Iteration 9462, loss = 0.06042609
Iteration 9463, loss = 0.06041810
Iteration 9464, loss = 0.06041014
Iteration 9465, loss = 0.06040211
Iteration 9466, loss = 0.06039418
Iteration 9467, loss = 0.06038619
Iteration 9468, loss = 0.06037818
Iteration 9469, loss = 0.06037024
Iteration 9470, loss = 0.06036232
Iteration 9471, loss = 0.06035428
Iteration 9472, loss = 0.06034629
Iteration 9473, loss = 0.06033845
Iteration 9474, loss = 0.06033039
Iteration 9475, loss = 0.06032258
Iteration 9476, loss = 0.06031462
Iteration 9477, loss = 0.06030653
Iteration 9478, loss = 0.06029881
Iteration 9479, loss = 0.06029080
Iteration 9480, loss = 0.06028294
Iteration 9481, loss = 0.06027495
Iteration 9482, loss = 0.06026703
Iteration 9483, loss = 0.06025921
Iteration 9484, loss = 0.06025116
Iteration 9485, loss = 0.06024344
Iteration 9486, loss = 0.06023555
Iteration 9487, loss = 0.06022751
Iteration 9488, loss = 0.06021971
Iteration 9489, loss = 0.06021175
Iteration 9490, loss = 0.06020380
Iteration 9491, loss = 0.06019583
Iteration 9492, loss = 0.06018788
Iteration 9493, loss = 0.06018006
Iteration 9494, loss = 0.06017203
Iteration 9495, loss = 0.06016418
Iteration 9496, loss = 0.06015618
Iteration 9497, loss = 0.06014838
Iteration 9498, loss = 0.06014049
Iteration 9499, loss = 0.06013262
Iteration 9500, loss = 0.06012470
Iteration 9501, loss = 0.06011679
Iteration 9502, loss = 0.06010887
Iteration 9503, loss = 0.06010101
Iteration 9504, loss = 0.06009312
Iteration 9505, loss = 0.06008537
Iteration 9506, loss = 0.06007734
Iteration 9507, loss = 0.06006952
Iteration 9508, loss = 0.06006164
Iteration 9509, loss = 0.06005373
Iteration 9510, loss = 0.06004583
Iteration 9511, loss = 0.06003812
Iteration 9512, loss = 0.06003016
Iteration 9513, loss = 0.06002239
Iteration 9514, loss = 0.06001441
Iteration 9515, loss = 0.06000657
Iteration 9516, loss = 0.05999882
Iteration 9517, loss = 0.05999091
Iteration 9518, loss = 0.05998302
Iteration 9519, loss = 0.05997517
Iteration 9520, loss = 0.05996727
Iteration 9521, loss = 0.05995951
Iteration 9522, loss = 0.05995168
Iteration 9523, loss = 0.05994381
Iteration 9524, loss = 0.05993593
Iteration 9525, loss = 0.05992808
Iteration 9526, loss = 0.05992024
Iteration 9527, loss = 0.05991247
Iteration 9528, loss = 0.05990457
Iteration 9529, loss = 0.05989684
Iteration 9530, loss = 0.05988905
Iteration 9531, loss = 0.05988113
Iteration 9532, loss = 0.05987335
Iteration 9533, loss = 0.05986556
Iteration 9534, loss = 0.05985777
Iteration 9535, loss = 0.05984988
Iteration 9536, loss = 0.05984200
Iteration 9537, loss = 0.05983429
Iteration 9538, loss = 0.05982654
Iteration 9539, loss = 0.05981872
Iteration 9540, loss = 0.05981080
Iteration 9541, loss = 0.05980301
Iteration 9542, loss = 0.05979520
Iteration 9543, loss = 0.05978748
Iteration 9544, loss = 0.05977959
Iteration 9545, loss = 0.05977182
Iteration 9546, loss = 0.05976400
Iteration 9547, loss = 0.05975622
Iteration 9548, loss = 0.05974846
Iteration 9549, loss = 0.05974061
Iteration 9550, loss = 0.05973287
Iteration 9551, loss = 0.05972508
Iteration 9552, loss = 0.05971727
Iteration 9553, loss = 0.05970946
Iteration 9554, loss = 0.05970175
Iteration 9555, loss = 0.05969398
Iteration 9556, loss = 0.05968609
Iteration 9557, loss = 0.05967842
Iteration 9558, loss = 0.05967054
Iteration 9559, loss = 0.05966284
Iteration 9560, loss = 0.05965500
Iteration 9561, loss = 0.05964725
Iteration 9562, loss = 0.05963956
Iteration 9563, loss = 0.05963175
Iteration 9564, loss = 0.05962390
Iteration 9565, loss = 0.05961623
Iteration 9566, loss = 0.05960839
Iteration 9567, loss = 0.05960067
Iteration 9568, loss = 0.05959298
Iteration 9569, loss = 0.05958513
Iteration 9570, loss = 0.05957745
Iteration 9571, loss = 0.05956961
Iteration 9572, loss = 0.05956191
Iteration 9573, loss = 0.05955421
Iteration 9574, loss = 0.05954636
Iteration 9575, loss = 0.05953866
Iteration 9576, loss = 0.05953081
Iteration 9577, loss = 0.05952321
Iteration 9578, loss = 0.05951553
Iteration 9579, loss = 0.05950770
Iteration 9580, loss = 0.05949996
Iteration 9581, loss = 0.05949227
Iteration 9582, loss = 0.05948456
Iteration 9583, loss = 0.05947682
Iteration 9584, loss = 0.05946914
Iteration 9585, loss = 0.05946135
Iteration 9586, loss = 0.05945360
Iteration 9587, loss = 0.05944593
Iteration 9588, loss = 0.05943826
Iteration 9589, loss = 0.05943049
Iteration 9590, loss = 0.05942283
Iteration 9591, loss = 0.05941517
Iteration 9592, loss = 0.05940744
Iteration 9593, loss = 0.05939982
Iteration 9594, loss = 0.05939207
Iteration 9595, loss = 0.05938429
Iteration 9596, loss = 0.05937663
Iteration 9597, loss = 0.05936894
Iteration 9598, loss = 0.05936139
Iteration 9599, loss = 0.05935368
Iteration 9600, loss = 0.05934601
Iteration 9601, loss = 0.05933821
Iteration 9602, loss = 0.05933056
Iteration 9603, loss = 0.05932286
Iteration 9604, loss = 0.05931539
Iteration 9605, loss = 0.05930755
Iteration 9606, loss = 0.05929999
Iteration 9607, loss = 0.05929226
Iteration 9608, loss = 0.05928458
Iteration 9609, loss = 0.05927686
Iteration 9610, loss = 0.05926922
Iteration 9611, loss = 0.05926155
Iteration 9612, loss = 0.05925396
Iteration 9613, loss = 0.05924625
Iteration 9614, loss = 0.05923857
Iteration 9615, loss = 0.05923090
Iteration 9616, loss = 0.05922332
Iteration 9617, loss = 0.05921566
Iteration 9618, loss = 0.05920815
Iteration 9619, loss = 0.05920043
Iteration 9620, loss = 0.05919273
Iteration 9621, loss = 0.05918512
Iteration 9622, loss = 0.05917740
Iteration 9623, loss = 0.05916974
Iteration 9624, loss = 0.05916211
Iteration 9625, loss = 0.05915447
Iteration 9626, loss = 0.05914681
Iteration 9627, loss = 0.05913922
Iteration 9628, loss = 0.05913153
Iteration 9629, loss = 0.05912405
Iteration 9630, loss = 0.05911627
Iteration 9631, loss = 0.05910871
Iteration 9632, loss = 0.05910115
Iteration 9633, loss = 0.05909336
Iteration 9634, loss = 0.05908583
Iteration 9635, loss = 0.05907824
Iteration 9636, loss = 0.05907056
Iteration 9637, loss = 0.05906287
Iteration 9638, loss = 0.05905531
Iteration 9639, loss = 0.05904771
Iteration 9640, loss = 0.05904019
Iteration 9641, loss = 0.05903252
Iteration 9642, loss = 0.05902495
Iteration 9643, loss = 0.05901737
Iteration 9644, loss = 0.05900975
Iteration 9645, loss = 0.05900214
Iteration 9646, loss = 0.05899462
Iteration 9647, loss = 0.05898693
Iteration 9648, loss = 0.05897940
Iteration 9649, loss = 0.05897179
Iteration 9650, loss = 0.05896416
Iteration 9651, loss = 0.05895663
Iteration 9652, loss = 0.05894900
Iteration 9653, loss = 0.05894142
Iteration 9654, loss = 0.05893375
Iteration 9655, loss = 0.05892610
Iteration 9656, loss = 0.05891865
Iteration 9657, loss = 0.05891101
Iteration 9658, loss = 0.05890346
Iteration 9659, loss = 0.05889597
Iteration 9660, loss = 0.05888838
Iteration 9661, loss = 0.05888070
Iteration 9662, loss = 0.05887314
Iteration 9663, loss = 0.05886570
Iteration 9664, loss = 0.05885807
Iteration 9665, loss = 0.05885041
Iteration 9666, loss = 0.05884296
Iteration 9667, loss = 0.05883533
Iteration 9668, loss = 0.05882786
Iteration 9669, loss = 0.05882013
Iteration 9670, loss = 0.05881275
Iteration 9671, loss = 0.05880512
Iteration 9672, loss = 0.05879752
Iteration 9673, loss = 0.05878993
Iteration 9674, loss = 0.05878246
Iteration 9675, loss = 0.05877492
Iteration 9676, loss = 0.05876727
Iteration 9677, loss = 0.05875980
Iteration 9678, loss = 0.05875224
Iteration 9679, loss = 0.05874472
Iteration 9680, loss = 0.05873719
Iteration 9681, loss = 0.05872966
Iteration 9682, loss = 0.05872214
Iteration 9683, loss = 0.05871460
Iteration 9684, loss = 0.05870707
Iteration 9685, loss = 0.05869947
Iteration 9686, loss = 0.05869201
Iteration 9687, loss = 0.05868444
Iteration 9688, loss = 0.05867687
Iteration 9689, loss = 0.05866942
Iteration 9690, loss = 0.05866186
Iteration 9691, loss = 0.05865424
Iteration 9692, loss = 0.05864680
Iteration 9693, loss = 0.05863923
Iteration 9694, loss = 0.05863173
Iteration 9695, loss = 0.05862415
Iteration 9696, loss = 0.05861664
Iteration 9697, loss = 0.05860913
Iteration 9698, loss = 0.05860162
Iteration 9699, loss = 0.05859415
Iteration 9700, loss = 0.05858659
Iteration 9701, loss = 0.05857920
Iteration 9702, loss = 0.05857166
Iteration 9703, loss = 0.05856412
Iteration 9704, loss = 0.05855662
Iteration 9705, loss = 0.05854911
Iteration 9706, loss = 0.05854164
Iteration 9707, loss = 0.05853412
Iteration 9708, loss = 0.05852670
Iteration 9709, loss = 0.05851918
Iteration 9710, loss = 0.05851166
Iteration 9711, loss = 0.05850419
Iteration 9712, loss = 0.05849672
Iteration 9713, loss = 0.05848931
Iteration 9714, loss = 0.05848179
Iteration 9715, loss = 0.05847425
Iteration 9716, loss = 0.05846691
Iteration 9717, loss = 0.05845931
Iteration 9718, loss = 0.05845194
Iteration 9719, loss = 0.05844441
Iteration 9720, loss = 0.05843695
Iteration 9721, loss = 0.05842955
Iteration 9722, loss = 0.05842200
Iteration 9723, loss = 0.05841463
Iteration 9724, loss = 0.05840702
Iteration 9725, loss = 0.05839964
Iteration 9726, loss = 0.05839222
Iteration 9727, loss = 0.05838483
Iteration 9728, loss = 0.05837735
Iteration 9729, loss = 0.05836985
Iteration 9730, loss = 0.05836242
Iteration 9731, loss = 0.05835495
Iteration 9732, loss = 0.05834758
Iteration 9733, loss = 0.05834021
Iteration 9734, loss = 0.05833270
Iteration 9735, loss = 0.05832545
Iteration 9736, loss = 0.05831800
Iteration 9737, loss = 0.05831049
Iteration 9738, loss = 0.05830310
Iteration 9739, loss = 0.05829577
Iteration 9740, loss = 0.05828840
Iteration 9741, loss = 0.05828093
Iteration 9742, loss = 0.05827352
Iteration 9743, loss = 0.05826618
Iteration 9744, loss = 0.05825882
Iteration 9745, loss = 0.05825145
Iteration 9746, loss = 0.05824401
Iteration 9747, loss = 0.05823668
Iteration 9748, loss = 0.05822928
Iteration 9749, loss = 0.05822178
Iteration 9750, loss = 0.05821443
Iteration 9751, loss = 0.05820704
Iteration 9752, loss = 0.05819968
Iteration 9753, loss = 0.05819225
Iteration 9754, loss = 0.05818487
Iteration 9755, loss = 0.05817755
Iteration 9756, loss = 0.05817002
Iteration 9757, loss = 0.05816275
Iteration 9758, loss = 0.05815537
Iteration 9759, loss = 0.05814791
Iteration 9760, loss = 0.05814050
Iteration 9761, loss = 0.05813311
Iteration 9762, loss = 0.05812576
Iteration 9763, loss = 0.05811847
Iteration 9764, loss = 0.05811112
Iteration 9765, loss = 0.05810367
Iteration 9766, loss = 0.05809627
Iteration 9767, loss = 0.05808895
Iteration 9768, loss = 0.05808155
Iteration 9769, loss = 0.05807416
Iteration 9770, loss = 0.05806683
Iteration 9771, loss = 0.05805938
Iteration 9772, loss = 0.05805197
Iteration 9773, loss = 0.05804466
Iteration 9774, loss = 0.05803722
Iteration 9775, loss = 0.05802994
Iteration 9776, loss = 0.05802269
Iteration 9777, loss = 0.05801518
Iteration 9778, loss = 0.05800788
Iteration 9779, loss = 0.05800047
Iteration 9780, loss = 0.05799316
Iteration 9781, loss = 0.05798583
Iteration 9782, loss = 0.05797851
Iteration 9783, loss = 0.05797124
Iteration 9784, loss = 0.05796378
Iteration 9785, loss = 0.05795642
Iteration 9786, loss = 0.05794911
Iteration 9787, loss = 0.05794186
Iteration 9788, loss = 0.05793443
Iteration 9789, loss = 0.05792709
Iteration 9790, loss = 0.05791971
Iteration 9791, loss = 0.05791247
Iteration 9792, loss = 0.05790505
Iteration 9793, loss = 0.05789760
Iteration 9794, loss = 0.05789036
Iteration 9795, loss = 0.05788293
Iteration 9796, loss = 0.05787567
Iteration 9797, loss = 0.05786831
Iteration 9798, loss = 0.05786095
Iteration 9799, loss = 0.05785365
Iteration 9800, loss = 0.05784628
Iteration 9801, loss = 0.05783901
Iteration 9802, loss = 0.05783162
Iteration 9803, loss = 0.05782435
Iteration 9804, loss = 0.05781699
Iteration 9805, loss = 0.05780966
Iteration 9806, loss = 0.05780230
Iteration 9807, loss = 0.05779505
Iteration 9808, loss = 0.05778777
Iteration 9809, loss = 0.05778047
Iteration 9810, loss = 0.05777314
Iteration 9811, loss = 0.05776589
Iteration 9812, loss = 0.05775855
Iteration 9813, loss = 0.05775124
Iteration 9814, loss = 0.05774409
Iteration 9815, loss = 0.05773669
Iteration 9816, loss = 0.05772950
Iteration 9817, loss = 0.05772202
Iteration 9818, loss = 0.05771477
Iteration 9819, loss = 0.05770750
Iteration 9820, loss = 0.05770027
Iteration 9821, loss = 0.05769293
Iteration 9822, loss = 0.05768565
Iteration 9823, loss = 0.05767833
Iteration 9824, loss = 0.05767117
Iteration 9825, loss = 0.05766382
Iteration 9826, loss = 0.05765659
Iteration 9827, loss = 0.05764932
Iteration 9828, loss = 0.05764198
Iteration 9829, loss = 0.05763468
Iteration 9830, loss = 0.05762748
Iteration 9831, loss = 0.05762013
Iteration 9832, loss = 0.05761296
Iteration 9833, loss = 0.05760563
Iteration 9834, loss = 0.05759841
Iteration 9835, loss = 0.05759112
Iteration 9836, loss = 0.05758392
Iteration 9837, loss = 0.05757676
Iteration 9838, loss = 0.05756939
Iteration 9839, loss = 0.05756217
Iteration 9840, loss = 0.05755490
Iteration 9841, loss = 0.05754766
Iteration 9842, loss = 0.05754047
Iteration 9843, loss = 0.05753343
Iteration 9844, loss = 0.05752604
Iteration 9845, loss = 0.05751877
Iteration 9846, loss = 0.05751160
Iteration 9847, loss = 0.05750439
Iteration 9848, loss = 0.05749719
Iteration 9849, loss = 0.05748997
Iteration 9850, loss = 0.05748269
Iteration 9851, loss = 0.05747560
Iteration 9852, loss = 0.05746829
Iteration 9853, loss = 0.05746108
Iteration 9854, loss = 0.05745386
Iteration 9855, loss = 0.05744656
Iteration 9856, loss = 0.05743941
Iteration 9857, loss = 0.05743213
Iteration 9858, loss = 0.05742503
Iteration 9859, loss = 0.05741774
Iteration 9860, loss = 0.05741061
Iteration 9861, loss = 0.05740338
Iteration 9862, loss = 0.05739616
Iteration 9863, loss = 0.05738897
Iteration 9864, loss = 0.05738167
Iteration 9865, loss = 0.05737452
Iteration 9866, loss = 0.05736727
Iteration 9867, loss = 0.05736005
Iteration 9868, loss = 0.05735302
Iteration 9869, loss = 0.05734565
Iteration 9870, loss = 0.05733848
Iteration 9871, loss = 0.05733126
Iteration 9872, loss = 0.05732411
Iteration 9873, loss = 0.05731696
Iteration 9874, loss = 0.05730973
Iteration 9875, loss = 0.05730256
Iteration 9876, loss = 0.05729539
Iteration 9877, loss = 0.05728814
Iteration 9878, loss = 0.05728102
Iteration 9879, loss = 0.05727387
Iteration 9880, loss = 0.05726664
Iteration 9881, loss = 0.05725959
Iteration 9882, loss = 0.05725226
Iteration 9883, loss = 0.05724523
Iteration 9884, loss = 0.05723802
Iteration 9885, loss = 0.05723088
Iteration 9886, loss = 0.05722375
Iteration 9887, loss = 0.05721647
Iteration 9888, loss = 0.05720936
Iteration 9889, loss = 0.05720219
Iteration 9890, loss = 0.05719501
Iteration 9891, loss = 0.05718784
Iteration 9892, loss = 0.05718058
Iteration 9893, loss = 0.05717349
Iteration 9894, loss = 0.05716628
Iteration 9895, loss = 0.05715906
Iteration 9896, loss = 0.05715191
Iteration 9897, loss = 0.05714474
Iteration 9898, loss = 0.05713760
Iteration 9899, loss = 0.05713040
Iteration 9900, loss = 0.05712328
Iteration 9901, loss = 0.05711612
Iteration 9902, loss = 0.05710896
Iteration 9903, loss = 0.05710177
Iteration 9904, loss = 0.05709480
Iteration 9905, loss = 0.05708754
Iteration 9906, loss = 0.05708041
Iteration 9907, loss = 0.05707336
Iteration 9908, loss = 0.05706614
Iteration 9909, loss = 0.05705905
Iteration 9910, loss = 0.05705184
Iteration 9911, loss = 0.05704473
Iteration 9912, loss = 0.05703763
Iteration 9913, loss = 0.05703055
Iteration 9914, loss = 0.05702335
Iteration 9915, loss = 0.05701635
Iteration 9916, loss = 0.05700914
Iteration 9917, loss = 0.05700201
Iteration 9918, loss = 0.05699491
Iteration 9919, loss = 0.05698786
Iteration 9920, loss = 0.05698071
Iteration 9921, loss = 0.05697360
Iteration 9922, loss = 0.05696661
Iteration 9923, loss = 0.05695946
Iteration 9924, loss = 0.05695235
Iteration 9925, loss = 0.05694527
Iteration 9926, loss = 0.05693817
Iteration 9927, loss = 0.05693111
Iteration 9928, loss = 0.05692413
Iteration 9929, loss = 0.05691703
Iteration 9930, loss = 0.05690988
Iteration 9931, loss = 0.05690287
Iteration 9932, loss = 0.05689576
Iteration 9933, loss = 0.05688871
Iteration 9934, loss = 0.05688162
Iteration 9935, loss = 0.05687451
Iteration 9936, loss = 0.05686750
Iteration 9937, loss = 0.05686036
Iteration 9938, loss = 0.05685337
Iteration 9939, loss = 0.05684634
Iteration 9940, loss = 0.05683925
Iteration 9941, loss = 0.05683210
Iteration 9942, loss = 0.05682497
Iteration 9943, loss = 0.05681799
Iteration 9944, loss = 0.05681088
Iteration 9945, loss = 0.05680380
Iteration 9946, loss = 0.05679680
Iteration 9947, loss = 0.05678968
Iteration 9948, loss = 0.05678258
Iteration 9949, loss = 0.05677566
Iteration 9950, loss = 0.05676843
Iteration 9951, loss = 0.05676146
Iteration 9952, loss = 0.05675442
Iteration 9953, loss = 0.05674731
Iteration 9954, loss = 0.05674024
Iteration 9955, loss = 0.05673319
Iteration 9956, loss = 0.05672611
Iteration 9957, loss = 0.05671910
Iteration 9958, loss = 0.05671206
Iteration 9959, loss = 0.05670495
Iteration 9960, loss = 0.05669796
Iteration 9961, loss = 0.05669100
Iteration 9962, loss = 0.05668390
Iteration 9963, loss = 0.05667677
Iteration 9964, loss = 0.05666989
Iteration 9965, loss = 0.05666275
Iteration 9966, loss = 0.05665577
Iteration 9967, loss = 0.05664884
Iteration 9968, loss = 0.05664177
Iteration 9969, loss = 0.05663485
Iteration 9970, loss = 0.05662769
Iteration 9971, loss = 0.05662064
Iteration 9972, loss = 0.05661360
Iteration 9973, loss = 0.05660655
Iteration 9974, loss = 0.05659952
Iteration 9975, loss = 0.05659258
Iteration 9976, loss = 0.05658549
Iteration 9977, loss = 0.05657855
Iteration 9978, loss = 0.05657143
Iteration 9979, loss = 0.05656440
Iteration 9980, loss = 0.05655745
Iteration 9981, loss = 0.05655042
Iteration 9982, loss = 0.05654334
Iteration 9983, loss = 0.05653647
Iteration 9984, loss = 0.05652940
Iteration 9985, loss = 0.05652240
Iteration 9986, loss = 0.05651542
Iteration 9987, loss = 0.05650846
Iteration 9988, loss = 0.05650142
Iteration 9989, loss = 0.05649452
Iteration 9990, loss = 0.05648757
Iteration 9991, loss = 0.05648052
Iteration 9992, loss = 0.05647349
Iteration 9993, loss = 0.05646652
Iteration 9994, loss = 0.05645956
Iteration 9995, loss = 0.05645259
Iteration 9996, loss = 0.05644555
Iteration 9997, loss = 0.05643865
Iteration 9998, loss = 0.05643165
Iteration 9999, loss = 0.05642458
Iteration 10000, loss = 0.05641771
Iteration 10001, loss = 0.05641068
Iteration 10002, loss = 0.05640365
Iteration 10003, loss = 0.05639683
Iteration 10004, loss = 0.05638975
Iteration 10005, loss = 0.05638277
Iteration 10006, loss = 0.05637581
Iteration 10007, loss = 0.05636892
Iteration 10008, loss = 0.05636191
Iteration 10009, loss = 0.05635499
Iteration 10010, loss = 0.05634807
Iteration 10011, loss = 0.05634104
Iteration 10012, loss = 0.05633415
Iteration 10013, loss = 0.05632723
Iteration 10014, loss = 0.05632029
Iteration 10015, loss = 0.05631334
Iteration 10016, loss = 0.05630649
Iteration 10017, loss = 0.05629947
Iteration 10018, loss = 0.05629257
Iteration 10019, loss = 0.05628558
Iteration 10020, loss = 0.05627868
Iteration 10021, loss = 0.05627168
Iteration 10022, loss = 0.05626472
Iteration 10023, loss = 0.05625779
Iteration 10024, loss = 0.05625084
Iteration 10025, loss = 0.05624389
Iteration 10026, loss = 0.05623694
Iteration 10027, loss = 0.05623004
Iteration 10028, loss = 0.05622310
Iteration 10029, loss = 0.05621614
Iteration 10030, loss = 0.05620920
Iteration 10031, loss = 0.05620238
Iteration 10032, loss = 0.05619534
Iteration 10033, loss = 0.05618847
Iteration 10034, loss = 0.05618161
Iteration 10035, loss = 0.05617461
Iteration 10036, loss = 0.05616782
Iteration 10037, loss = 0.05616079
Iteration 10038, loss = 0.05615388
Iteration 10039, loss = 0.05614698
Iteration 10040, loss = 0.05614009
Iteration 10041, loss = 0.05613322
Iteration 10042, loss = 0.05612626
Iteration 10043, loss = 0.05611949
Iteration 10044, loss = 0.05611245
Iteration 10045, loss = 0.05610556
Iteration 10046, loss = 0.05609863
Iteration 10047, loss = 0.05609180
Iteration 10048, loss = 0.05608489
Iteration 10049, loss = 0.05607800
Iteration 10050, loss = 0.05607113
Iteration 10051, loss = 0.05606424
Iteration 10052, loss = 0.05605735
Iteration 10053, loss = 0.05605040
Iteration 10054, loss = 0.05604352
Iteration 10055, loss = 0.05603662
Iteration 10056, loss = 0.05602973
Iteration 10057, loss = 0.05602299
Iteration 10058, loss = 0.05601599
Iteration 10059, loss = 0.05600911
Iteration 10060, loss = 0.05600225
Iteration 10061, loss = 0.05599533
Iteration 10062, loss = 0.05598842
Iteration 10063, loss = 0.05598160
Iteration 10064, loss = 0.05597480
Iteration 10065, loss = 0.05596790
Iteration 10066, loss = 0.05596096
Iteration 10067, loss = 0.05595420
Iteration 10068, loss = 0.05594729
Iteration 10069, loss = 0.05594040
Iteration 10070, loss = 0.05593356
Iteration 10071, loss = 0.05592670
Iteration 10072, loss = 0.05591982
Iteration 10073, loss = 0.05591305
Iteration 10074, loss = 0.05590611
Iteration 10075, loss = 0.05589935
Iteration 10076, loss = 0.05589235
Iteration 10077, loss = 0.05588552
Iteration 10078, loss = 0.05587875
Iteration 10079, loss = 0.05587184
Iteration 10080, loss = 0.05586504
Iteration 10081, loss = 0.05585823
Iteration 10082, loss = 0.05585124
Iteration 10083, loss = 0.05584454
Iteration 10084, loss = 0.05583750
Iteration 10085, loss = 0.05583069
Iteration 10086, loss = 0.05582389
Iteration 10087, loss = 0.05581706
Iteration 10088, loss = 0.05581023
Iteration 10089, loss = 0.05580356
Iteration 10090, loss = 0.05579663
Iteration 10091, loss = 0.05578982
Iteration 10092, loss = 0.05578309
Iteration 10093, loss = 0.05577618
Iteration 10094, loss = 0.05576943
Iteration 10095, loss = 0.05576257
Iteration 10096, loss = 0.05575590
Iteration 10097, loss = 0.05574906
Iteration 10098, loss = 0.05574219
Iteration 10099, loss = 0.05573548
Iteration 10100, loss = 0.05572861
Iteration 10101, loss = 0.05572179
Iteration 10102, loss = 0.05571509
Iteration 10103, loss = 0.05570830
Iteration 10104, loss = 0.05570149
Iteration 10105, loss = 0.05569464
Iteration 10106, loss = 0.05568792
Iteration 10107, loss = 0.05568105
Iteration 10108, loss = 0.05567432
Iteration 10109, loss = 0.05566743
Iteration 10110, loss = 0.05566071
Iteration 10111, loss = 0.05565391
Iteration 10112, loss = 0.05564715
Iteration 10113, loss = 0.05564034
Iteration 10114, loss = 0.05563348
Iteration 10115, loss = 0.05562676
Iteration 10116, loss = 0.05561996
Iteration 10117, loss = 0.05561318
Iteration 10118, loss = 0.05560637
Iteration 10119, loss = 0.05559970
Iteration 10120, loss = 0.05559294
Iteration 10121, loss = 0.05558618
Iteration 10122, loss = 0.05557941
Iteration 10123, loss = 0.05557272
Iteration 10124, loss = 0.05556593
Iteration 10125, loss = 0.05555916
Iteration 10126, loss = 0.05555242
Iteration 10127, loss = 0.05554571
Iteration 10128, loss = 0.05553901
Iteration 10129, loss = 0.05553213
Iteration 10130, loss = 0.05552545
Iteration 10131, loss = 0.05551859
Iteration 10132, loss = 0.05551189
Iteration 10133, loss = 0.05550512
Iteration 10134, loss = 0.05549839
Iteration 10135, loss = 0.05549163
Iteration 10136, loss = 0.05548490
Iteration 10137, loss = 0.05547816
Iteration 10138, loss = 0.05547142
Iteration 10139, loss = 0.05546459
Iteration 10140, loss = 0.05545797
Iteration 10141, loss = 0.05545123
Iteration 10142, loss = 0.05544440
Iteration 10143, loss = 0.05543773
Iteration 10144, loss = 0.05543100
Iteration 10145, loss = 0.05542433
Iteration 10146, loss = 0.05541753
Iteration 10147, loss = 0.05541081
Iteration 10148, loss = 0.05540403
Iteration 10149, loss = 0.05539737
Iteration 10150, loss = 0.05539063
Iteration 10151, loss = 0.05538386
Iteration 10152, loss = 0.05537722
Iteration 10153, loss = 0.05537039
Iteration 10154, loss = 0.05536368
Iteration 10155, loss = 0.05535699
Iteration 10156, loss = 0.05535020
Iteration 10157, loss = 0.05534369
Iteration 10158, loss = 0.05533674
Iteration 10159, loss = 0.05533014
Iteration 10160, loss = 0.05532338
Iteration 10161, loss = 0.05531668
Iteration 10162, loss = 0.05530998
Iteration 10163, loss = 0.05530318
Iteration 10164, loss = 0.05529660
Iteration 10165, loss = 0.05528991
Iteration 10166, loss = 0.05528314
Iteration 10167, loss = 0.05527638
Iteration 10168, loss = 0.05526975
Iteration 10169, loss = 0.05526309
Iteration 10170, loss = 0.05525632
Iteration 10171, loss = 0.05524962
Iteration 10172, loss = 0.05524296
Iteration 10173, loss = 0.05523635
Iteration 10174, loss = 0.05522964
Iteration 10175, loss = 0.05522286
Iteration 10176, loss = 0.05521625
Iteration 10177, loss = 0.05520958
Iteration 10178, loss = 0.05520289
Iteration 10179, loss = 0.05519620
Iteration 10180, loss = 0.05518953
Iteration 10181, loss = 0.05518284
Iteration 10182, loss = 0.05517619
Iteration 10183, loss = 0.05516962
Iteration 10184, loss = 0.05516290
Iteration 10185, loss = 0.05515628
Iteration 10186, loss = 0.05514961
Iteration 10187, loss = 0.05514286
Iteration 10188, loss = 0.05513623
Iteration 10189, loss = 0.05512962
Iteration 10190, loss = 0.05512293
Iteration 10191, loss = 0.05511626
Iteration 10192, loss = 0.05510957
Iteration 10193, loss = 0.05510293
Iteration 10194, loss = 0.05509630
Iteration 10195, loss = 0.05508963
Iteration 10196, loss = 0.05508302
Iteration 10197, loss = 0.05507631
Iteration 10198, loss = 0.05506966
Iteration 10199, loss = 0.05506301
Iteration 10200, loss = 0.05505633
Iteration 10201, loss = 0.05504968
Iteration 10202, loss = 0.05504306
Iteration 10203, loss = 0.05503633
Iteration 10204, loss = 0.05502967
Iteration 10205, loss = 0.05502304
Iteration 10206, loss = 0.05501638
Iteration 10207, loss = 0.05500979
Iteration 10208, loss = 0.05500304
Iteration 10209, loss = 0.05499645
Iteration 10210, loss = 0.05498977
Iteration 10211, loss = 0.05498322
Iteration 10212, loss = 0.05497656
Iteration 10213, loss = 0.05496985
Iteration 10214, loss = 0.05496330
Iteration 10215, loss = 0.05495673
Iteration 10216, loss = 0.05495004
Iteration 10217, loss = 0.05494339
Iteration 10218, loss = 0.05493679
Iteration 10219, loss = 0.05493016
Iteration 10220, loss = 0.05492364
Iteration 10221, loss = 0.05491691
Iteration 10222, loss = 0.05491033
Iteration 10223, loss = 0.05490376
Iteration 10224, loss = 0.05489716
Iteration 10225, loss = 0.05489049
Iteration 10226, loss = 0.05488387
Iteration 10227, loss = 0.05487723
Iteration 10228, loss = 0.05487071
Iteration 10229, loss = 0.05486409
Iteration 10230, loss = 0.05485747
Iteration 10231, loss = 0.05485076
Iteration 10232, loss = 0.05484417
Iteration 10233, loss = 0.05483762
Iteration 10234, loss = 0.05483099
Iteration 10235, loss = 0.05482439
Iteration 10236, loss = 0.05481781
Iteration 10237, loss = 0.05481120
Iteration 10238, loss = 0.05480466
Iteration 10239, loss = 0.05479795
Iteration 10240, loss = 0.05479149
Iteration 10241, loss = 0.05478490
Iteration 10242, loss = 0.05477832
Iteration 10243, loss = 0.05477166
Iteration 10244, loss = 0.05476508
Iteration 10245, loss = 0.05475845
Iteration 10246, loss = 0.05475204
Iteration 10247, loss = 0.05474546
Iteration 10248, loss = 0.05473889
Iteration 10249, loss = 0.05473225
Iteration 10250, loss = 0.05472563
Iteration 10251, loss = 0.05471916
Iteration 10252, loss = 0.05471254
Iteration 10253, loss = 0.05470603
Iteration 10254, loss = 0.05469946
Iteration 10255, loss = 0.05469291
Iteration 10256, loss = 0.05468632
Iteration 10257, loss = 0.05467978
Iteration 10258, loss = 0.05467318
Iteration 10259, loss = 0.05466668
Iteration 10260, loss = 0.05466010
Iteration 10261, loss = 0.05465357
Iteration 10262, loss = 0.05464697
Iteration 10263, loss = 0.05464036
Iteration 10264, loss = 0.05463387
Iteration 10265, loss = 0.05462733
Iteration 10266, loss = 0.05462066
Iteration 10267, loss = 0.05461418
Iteration 10268, loss = 0.05460772
Iteration 10269, loss = 0.05460105
Iteration 10270, loss = 0.05459449
Iteration 10271, loss = 0.05458802
Iteration 10272, loss = 0.05458146
Iteration 10273, loss = 0.05457491
Iteration 10274, loss = 0.05456835
Iteration 10275, loss = 0.05456175
Iteration 10276, loss = 0.05455527
Iteration 10277, loss = 0.05454882
Iteration 10278, loss = 0.05454219
Iteration 10279, loss = 0.05453571
Iteration 10280, loss = 0.05452919
Iteration 10281, loss = 0.05452274
Iteration 10282, loss = 0.05451612
Iteration 10283, loss = 0.05450964
Iteration 10284, loss = 0.05450321
Iteration 10285, loss = 0.05449665
Iteration 10286, loss = 0.05449010
Iteration 10287, loss = 0.05448365
Iteration 10288, loss = 0.05447717
Iteration 10289, loss = 0.05447065
Iteration 10290, loss = 0.05446409
Iteration 10291, loss = 0.05445760
Iteration 10292, loss = 0.05445120
Iteration 10293, loss = 0.05444457
Iteration 10294, loss = 0.05443809
Iteration 10295, loss = 0.05443157
Iteration 10296, loss = 0.05442514
Iteration 10297, loss = 0.05441854
Iteration 10298, loss = 0.05441201
Iteration 10299, loss = 0.05440555
Iteration 10300, loss = 0.05439899
Iteration 10301, loss = 0.05439258
Iteration 10302, loss = 0.05438602
Iteration 10303, loss = 0.05437962
Iteration 10304, loss = 0.05437307
Iteration 10305, loss = 0.05436662
Iteration 10306, loss = 0.05436013
Iteration 10307, loss = 0.05435367
Iteration 10308, loss = 0.05434711
Iteration 10309, loss = 0.05434064
Iteration 10310, loss = 0.05433416
Iteration 10311, loss = 0.05432777
Iteration 10312, loss = 0.05432127
Iteration 10313, loss = 0.05431484
Iteration 10314, loss = 0.05430833
Iteration 10315, loss = 0.05430195
Iteration 10316, loss = 0.05429537
Iteration 10317, loss = 0.05428894
Iteration 10318, loss = 0.05428238
Iteration 10319, loss = 0.05427599
Iteration 10320, loss = 0.05426950
Iteration 10321, loss = 0.05426302
Iteration 10322, loss = 0.05425654
Iteration 10323, loss = 0.05425019
Iteration 10324, loss = 0.05424366
Iteration 10325, loss = 0.05423722
Iteration 10326, loss = 0.05423069
Iteration 10327, loss = 0.05422430
Iteration 10328, loss = 0.05421790
Iteration 10329, loss = 0.05421140
Iteration 10330, loss = 0.05420492
Iteration 10331, loss = 0.05419848
Iteration 10332, loss = 0.05419206
Iteration 10333, loss = 0.05418565
Iteration 10334, loss = 0.05417912
Iteration 10335, loss = 0.05417270
Iteration 10336, loss = 0.05416631
Iteration 10337, loss = 0.05415979
Iteration 10338, loss = 0.05415331
Iteration 10339, loss = 0.05414698
Iteration 10340, loss = 0.05414046
Iteration 10341, loss = 0.05413402
Iteration 10342, loss = 0.05412764
Iteration 10343, loss = 0.05412114
Iteration 10344, loss = 0.05411472
Iteration 10345, loss = 0.05410836
Iteration 10346, loss = 0.05410188
Iteration 10347, loss = 0.05409551
Iteration 10348, loss = 0.05408907
Iteration 10349, loss = 0.05408265
Iteration 10350, loss = 0.05407625
Iteration 10351, loss = 0.05406986
Iteration 10352, loss = 0.05406344
Iteration 10353, loss = 0.05405699
Iteration 10354, loss = 0.05405060
Iteration 10355, loss = 0.05404423
Iteration 10356, loss = 0.05403770
Iteration 10357, loss = 0.05403135
Iteration 10358, loss = 0.05402493
Iteration 10359, loss = 0.05401848
Iteration 10360, loss = 0.05401210
Iteration 10361, loss = 0.05400573
Iteration 10362, loss = 0.05399925
Iteration 10363, loss = 0.05399287
Iteration 10364, loss = 0.05398648
Iteration 10365, loss = 0.05398001
Iteration 10366, loss = 0.05397364
Iteration 10367, loss = 0.05396732
Iteration 10368, loss = 0.05396092
Iteration 10369, loss = 0.05395441
Iteration 10370, loss = 0.05394808
Iteration 10371, loss = 0.05394176
Iteration 10372, loss = 0.05393538
Iteration 10373, loss = 0.05392889
Iteration 10374, loss = 0.05392253
Iteration 10375, loss = 0.05391607
Iteration 10376, loss = 0.05390986
Iteration 10377, loss = 0.05390332
Iteration 10378, loss = 0.05389700
Iteration 10379, loss = 0.05389057
Iteration 10380, loss = 0.05388422
Iteration 10381, loss = 0.05387787
Iteration 10382, loss = 0.05387155
Iteration 10383, loss = 0.05386517
Iteration 10384, loss = 0.05385877
Iteration 10385, loss = 0.05385238
Iteration 10386, loss = 0.05384601
Iteration 10387, loss = 0.05383968
Iteration 10388, loss = 0.05383339
Iteration 10389, loss = 0.05382691
Iteration 10390, loss = 0.05382063
Iteration 10391, loss = 0.05381437
Iteration 10392, loss = 0.05380795
Iteration 10393, loss = 0.05380158
Iteration 10394, loss = 0.05379522
Iteration 10395, loss = 0.05378881
Iteration 10396, loss = 0.05378254
Iteration 10397, loss = 0.05377611
Iteration 10398, loss = 0.05376982
Iteration 10399, loss = 0.05376346
Iteration 10400, loss = 0.05375720
Iteration 10401, loss = 0.05375075
Iteration 10402, loss = 0.05374444
Iteration 10403, loss = 0.05373803
Iteration 10404, loss = 0.05373180
Iteration 10405, loss = 0.05372551
Iteration 10406, loss = 0.05371905
Iteration 10407, loss = 0.05371273
Iteration 10408, loss = 0.05370643
Iteration 10409, loss = 0.05370002
Iteration 10410, loss = 0.05369368
Iteration 10411, loss = 0.05368745
Iteration 10412, loss = 0.05368102
Iteration 10413, loss = 0.05367474
Iteration 10414, loss = 0.05366835
Iteration 10415, loss = 0.05366196
Iteration 10416, loss = 0.05365573
Iteration 10417, loss = 0.05364946
Iteration 10418, loss = 0.05364307
Iteration 10419, loss = 0.05363677
Iteration 10420, loss = 0.05363047
Iteration 10421, loss = 0.05362410
Iteration 10422, loss = 0.05361783
Iteration 10423, loss = 0.05361152
Iteration 10424, loss = 0.05360517
Iteration 10425, loss = 0.05359874
Iteration 10426, loss = 0.05359259
Iteration 10427, loss = 0.05358626
Iteration 10428, loss = 0.05357988
Iteration 10429, loss = 0.05357360
Iteration 10430, loss = 0.05356733
Iteration 10431, loss = 0.05356098
Iteration 10432, loss = 0.05355474
Iteration 10433, loss = 0.05354839
Iteration 10434, loss = 0.05354214
Iteration 10435, loss = 0.05353581
Iteration 10436, loss = 0.05352951
Iteration 10437, loss = 0.05352323
Iteration 10438, loss = 0.05351701
Iteration 10439, loss = 0.05351071
Iteration 10440, loss = 0.05350441
Iteration 10441, loss = 0.05349814
Iteration 10442, loss = 0.05349181
Iteration 10443, loss = 0.05348559
Iteration 10444, loss = 0.05347935
Iteration 10445, loss = 0.05347303
Iteration 10446, loss = 0.05346683
Iteration 10447, loss = 0.05346050
Iteration 10448, loss = 0.05345431
Iteration 10449, loss = 0.05344805
Iteration 10450, loss = 0.05344173
Iteration 10451, loss = 0.05343546
Iteration 10452, loss = 0.05342917
Iteration 10453, loss = 0.05342301
Iteration 10454, loss = 0.05341673
Iteration 10455, loss = 0.05341042
Iteration 10456, loss = 0.05340422
Iteration 10457, loss = 0.05339798
Iteration 10458, loss = 0.05339170
Iteration 10459, loss = 0.05338549
Iteration 10460, loss = 0.05337928
Iteration 10461, loss = 0.05337306
Iteration 10462, loss = 0.05336675
Iteration 10463, loss = 0.05336055
Iteration 10464, loss = 0.05335425
Iteration 10465, loss = 0.05334806
Iteration 10466, loss = 0.05334181
Iteration 10467, loss = 0.05333558
Iteration 10468, loss = 0.05332930
Iteration 10469, loss = 0.05332306
Iteration 10470, loss = 0.05331687
Iteration 10471, loss = 0.05331060
Iteration 10472, loss = 0.05330444
Iteration 10473, loss = 0.05329815
Iteration 10474, loss = 0.05329201
Iteration 10475, loss = 0.05328570
Iteration 10476, loss = 0.05327950
Iteration 10477, loss = 0.05327327
Iteration 10478, loss = 0.05326695
Iteration 10479, loss = 0.05326078
Iteration 10480, loss = 0.05325455
Iteration 10481, loss = 0.05324841
Iteration 10482, loss = 0.05324208
Iteration 10483, loss = 0.05323587
Iteration 10484, loss = 0.05322967
Iteration 10485, loss = 0.05322350
Iteration 10486, loss = 0.05321733
Iteration 10487, loss = 0.05321108
Iteration 10488, loss = 0.05320489
Iteration 10489, loss = 0.05319865
Iteration 10490, loss = 0.05319248
Iteration 10491, loss = 0.05318630
Iteration 10492, loss = 0.05318009
Iteration 10493, loss = 0.05317388
Iteration 10494, loss = 0.05316764
Iteration 10495, loss = 0.05316148
Iteration 10496, loss = 0.05315528
Iteration 10497, loss = 0.05314910
Iteration 10498, loss = 0.05314280
Iteration 10499, loss = 0.05313668
Iteration 10500, loss = 0.05313049
Iteration 10501, loss = 0.05312417
Iteration 10502, loss = 0.05311803
Iteration 10503, loss = 0.05311193
Iteration 10504, loss = 0.05310561
Iteration 10505, loss = 0.05309942
Iteration 10506, loss = 0.05309321
Iteration 10507, loss = 0.05308710
Iteration 10508, loss = 0.05308083
Iteration 10509, loss = 0.05307475
Iteration 10510, loss = 0.05306850
Iteration 10511, loss = 0.05306237
Iteration 10512, loss = 0.05305621
Iteration 10513, loss = 0.05304997
Iteration 10514, loss = 0.05304398
Iteration 10515, loss = 0.05303766
Iteration 10516, loss = 0.05303149
Iteration 10517, loss = 0.05302528
Iteration 10518, loss = 0.05301929
Iteration 10519, loss = 0.05301302
Iteration 10520, loss = 0.05300692
Iteration 10521, loss = 0.05300077
Iteration 10522, loss = 0.05299460
Iteration 10523, loss = 0.05298846
Iteration 10524, loss = 0.05298239
Iteration 10525, loss = 0.05297618
Iteration 10526, loss = 0.05297003
Iteration 10527, loss = 0.05296387
Iteration 10528, loss = 0.05295772
Iteration 10529, loss = 0.05295155
Iteration 10530, loss = 0.05294552
Iteration 10531, loss = 0.05293925
Iteration 10532, loss = 0.05293311
Iteration 10533, loss = 0.05292694
Iteration 10534, loss = 0.05292075
Iteration 10535, loss = 0.05291459
Iteration 10536, loss = 0.05290846
Iteration 10537, loss = 0.05290224
Iteration 10538, loss = 0.05289616
Iteration 10539, loss = 0.05289002
Iteration 10540, loss = 0.05288391
Iteration 10541, loss = 0.05287772
Iteration 10542, loss = 0.05287159
Iteration 10543, loss = 0.05286546
Iteration 10544, loss = 0.05285926
Iteration 10545, loss = 0.05285317
Iteration 10546, loss = 0.05284706
Iteration 10547, loss = 0.05284088
Iteration 10548, loss = 0.05283485
Iteration 10549, loss = 0.05282878
Iteration 10550, loss = 0.05282257
Iteration 10551, loss = 0.05281644
Iteration 10552, loss = 0.05281030
Iteration 10553, loss = 0.05280417
Iteration 10554, loss = 0.05279809
Iteration 10555, loss = 0.05279192
Iteration 10556, loss = 0.05278589
Iteration 10557, loss = 0.05277979
Iteration 10558, loss = 0.05277359
Iteration 10559, loss = 0.05276754
Iteration 10560, loss = 0.05276142
Iteration 10561, loss = 0.05275525
Iteration 10562, loss = 0.05274920
Iteration 10563, loss = 0.05274307
Iteration 10564, loss = 0.05273699
Iteration 10565, loss = 0.05273088
Iteration 10566, loss = 0.05272476
Iteration 10567, loss = 0.05271878
Iteration 10568, loss = 0.05271261
Iteration 10569, loss = 0.05270650
Iteration 10570, loss = 0.05270047
Iteration 10571, loss = 0.05269437
Iteration 10572, loss = 0.05268818
Iteration 10573, loss = 0.05268217
Iteration 10574, loss = 0.05267612
Iteration 10575, loss = 0.05267004
Iteration 10576, loss = 0.05266399
Iteration 10577, loss = 0.05265788
Iteration 10578, loss = 0.05265182
Iteration 10579, loss = 0.05264568
Iteration 10580, loss = 0.05263961
Iteration 10581, loss = 0.05263364
Iteration 10582, loss = 0.05262755
Iteration 10583, loss = 0.05262146
Iteration 10584, loss = 0.05261531
Iteration 10585, loss = 0.05260931
Iteration 10586, loss = 0.05260323
Iteration 10587, loss = 0.05259715
Iteration 10588, loss = 0.05259114
Iteration 10589, loss = 0.05258516
Iteration 10590, loss = 0.05257903
Iteration 10591, loss = 0.05257294
Iteration 10592, loss = 0.05256691
Iteration 10593, loss = 0.05256081
Iteration 10594, loss = 0.05255471
Iteration 10595, loss = 0.05254868
Iteration 10596, loss = 0.05254269
Iteration 10597, loss = 0.05253660
Iteration 10598, loss = 0.05253054
Iteration 10599, loss = 0.05252451
Iteration 10600, loss = 0.05251844
Iteration 10601, loss = 0.05251234
Iteration 10602, loss = 0.05250632
Iteration 10603, loss = 0.05250030
Iteration 10604, loss = 0.05249419
Iteration 10605, loss = 0.05248817
Iteration 10606, loss = 0.05248218
Iteration 10607, loss = 0.05247613
Iteration 10608, loss = 0.05247007
Iteration 10609, loss = 0.05246402
Iteration 10610, loss = 0.05245804
Iteration 10611, loss = 0.05245194
Iteration 10612, loss = 0.05244595
Iteration 10613, loss = 0.05243991
Iteration 10614, loss = 0.05243390
Iteration 10615, loss = 0.05242784
Iteration 10616, loss = 0.05242184
Iteration 10617, loss = 0.05241585
Iteration 10618, loss = 0.05240978
Iteration 10619, loss = 0.05240379
Iteration 10620, loss = 0.05239777
Iteration 10621, loss = 0.05239180
Iteration 10622, loss = 0.05238582
Iteration 10623, loss = 0.05237975
Iteration 10624, loss = 0.05237375
Iteration 10625, loss = 0.05236777
Iteration 10626, loss = 0.05236177
Iteration 10627, loss = 0.05235572
Iteration 10628, loss = 0.05234976
Iteration 10629, loss = 0.05234382
Iteration 10630, loss = 0.05233776
Iteration 10631, loss = 0.05233175
Iteration 10632, loss = 0.05232571
Iteration 10633, loss = 0.05231970
Iteration 10634, loss = 0.05231376
Iteration 10635, loss = 0.05230773
Iteration 10636, loss = 0.05230176
Iteration 10637, loss = 0.05229576
Iteration 10638, loss = 0.05228973
Iteration 10639, loss = 0.05228372
Iteration 10640, loss = 0.05227773
Iteration 10641, loss = 0.05227169
Iteration 10642, loss = 0.05226576
Iteration 10643, loss = 0.05225978
Iteration 10644, loss = 0.05225384
Iteration 10645, loss = 0.05224774
Iteration 10646, loss = 0.05224173
Iteration 10647, loss = 0.05223578
Iteration 10648, loss = 0.05222980
Iteration 10649, loss = 0.05222381
Iteration 10650, loss = 0.05221781
Iteration 10651, loss = 0.05221186
Iteration 10652, loss = 0.05220583
Iteration 10653, loss = 0.05219983
Iteration 10654, loss = 0.05219389
Iteration 10655, loss = 0.05218789
Iteration 10656, loss = 0.05218193
Iteration 10657, loss = 0.05217593
Iteration 10658, loss = 0.05216998
Iteration 10659, loss = 0.05216403
Iteration 10660, loss = 0.05215801
Iteration 10661, loss = 0.05215209
Iteration 10662, loss = 0.05214601
Iteration 10663, loss = 0.05214016
Iteration 10664, loss = 0.05213412
Iteration 10665, loss = 0.05212815
Iteration 10666, loss = 0.05212217
Iteration 10667, loss = 0.05211617
Iteration 10668, loss = 0.05211030
Iteration 10669, loss = 0.05210432
Iteration 10670, loss = 0.05209835
Iteration 10671, loss = 0.05209240
Iteration 10672, loss = 0.05208639
Iteration 10673, loss = 0.05208046
Iteration 10674, loss = 0.05207459
Iteration 10675, loss = 0.05206857
Iteration 10676, loss = 0.05206262
Iteration 10677, loss = 0.05205659
Iteration 10678, loss = 0.05205079
Iteration 10679, loss = 0.05204475
Iteration 10680, loss = 0.05203881
Iteration 10681, loss = 0.05203289
Iteration 10682, loss = 0.05202694
Iteration 10683, loss = 0.05202101
Iteration 10684, loss = 0.05201505
Iteration 10685, loss = 0.05200919
Iteration 10686, loss = 0.05200321
Iteration 10687, loss = 0.05199726
Iteration 10688, loss = 0.05199135
Iteration 10689, loss = 0.05198544
Iteration 10690, loss = 0.05197950
Iteration 10691, loss = 0.05197352
Iteration 10692, loss = 0.05196756
Iteration 10693, loss = 0.05196168
Iteration 10694, loss = 0.05195581
Iteration 10695, loss = 0.05194982
Iteration 10696, loss = 0.05194391
Iteration 10697, loss = 0.05193798
Iteration 10698, loss = 0.05193212
Iteration 10699, loss = 0.05192613
Iteration 10700, loss = 0.05192015
Iteration 10701, loss = 0.05191432
Iteration 10702, loss = 0.05190839
Iteration 10703, loss = 0.05190251
Iteration 10704, loss = 0.05189661
Iteration 10705, loss = 0.05189062
Iteration 10706, loss = 0.05188478
Iteration 10707, loss = 0.05187893
Iteration 10708, loss = 0.05187298
Iteration 10709, loss = 0.05186709
Iteration 10710, loss = 0.05186121
Iteration 10711, loss = 0.05185535
Iteration 10712, loss = 0.05184934
Iteration 10713, loss = 0.05184356
Iteration 10714, loss = 0.05183765
Iteration 10715, loss = 0.05183172
Iteration 10716, loss = 0.05182587
Iteration 10717, loss = 0.05182004
Iteration 10718, loss = 0.05181412
Iteration 10719, loss = 0.05180824
Iteration 10720, loss = 0.05180244
Iteration 10721, loss = 0.05179648
Iteration 10722, loss = 0.05179058
Iteration 10723, loss = 0.05178477
Iteration 10724, loss = 0.05177893
Iteration 10725, loss = 0.05177300
Iteration 10726, loss = 0.05176714
Iteration 10727, loss = 0.05176120
Iteration 10728, loss = 0.05175541
Iteration 10729, loss = 0.05174954
Iteration 10730, loss = 0.05174358
Iteration 10731, loss = 0.05173779
Iteration 10732, loss = 0.05173190
Iteration 10733, loss = 0.05172597
Iteration 10734, loss = 0.05172021
Iteration 10735, loss = 0.05171424
Iteration 10736, loss = 0.05170850
Iteration 10737, loss = 0.05170260
Iteration 10738, loss = 0.05169681
Iteration 10739, loss = 0.05169087
Iteration 10740, loss = 0.05168500
Iteration 10741, loss = 0.05167912
Iteration 10742, loss = 0.05167334
Iteration 10743, loss = 0.05166748
Iteration 10744, loss = 0.05166168
Iteration 10745, loss = 0.05165579
Iteration 10746, loss = 0.05164997
Iteration 10747, loss = 0.05164413
Iteration 10748, loss = 0.05163827
Iteration 10749, loss = 0.05163247
Iteration 10750, loss = 0.05162659
Iteration 10751, loss = 0.05162074
Iteration 10752, loss = 0.05161490
Iteration 10753, loss = 0.05160909
Iteration 10754, loss = 0.05160322
Iteration 10755, loss = 0.05159743
Iteration 10756, loss = 0.05159155
Iteration 10757, loss = 0.05158571
Iteration 10758, loss = 0.05157990
Iteration 10759, loss = 0.05157400
Iteration 10760, loss = 0.05156816
Iteration 10761, loss = 0.05156231
Iteration 10762, loss = 0.05155655
Iteration 10763, loss = 0.05155064
Iteration 10764, loss = 0.05154479
Iteration 10765, loss = 0.05153907
Iteration 10766, loss = 0.05153317
Iteration 10767, loss = 0.05152738
Iteration 10768, loss = 0.05152148
Iteration 10769, loss = 0.05151568
Iteration 10770, loss = 0.05150983
Iteration 10771, loss = 0.05150399
Iteration 10772, loss = 0.05149815
Iteration 10773, loss = 0.05149234
Iteration 10774, loss = 0.05148664
Iteration 10775, loss = 0.05148077
Iteration 10776, loss = 0.05147494
Iteration 10777, loss = 0.05146923
Iteration 10778, loss = 0.05146349
Iteration 10779, loss = 0.05145758
Iteration 10780, loss = 0.05145178
Iteration 10781, loss = 0.05144598
Iteration 10782, loss = 0.05144023
Iteration 10783, loss = 0.05143438
Iteration 10784, loss = 0.05142865
Iteration 10785, loss = 0.05142282
Iteration 10786, loss = 0.05141706
Iteration 10787, loss = 0.05141122
Iteration 10788, loss = 0.05140543
Iteration 10789, loss = 0.05139973
Iteration 10790, loss = 0.05139393
Iteration 10791, loss = 0.05138810
Iteration 10792, loss = 0.05138232
Iteration 10793, loss = 0.05137652
Iteration 10794, loss = 0.05137076
Iteration 10795, loss = 0.05136495
Iteration 10796, loss = 0.05135915
Iteration 10797, loss = 0.05135344
Iteration 10798, loss = 0.05134758
Iteration 10799, loss = 0.05134185
Iteration 10800, loss = 0.05133602
Iteration 10801, loss = 0.05133025
Iteration 10802, loss = 0.05132446
Iteration 10803, loss = 0.05131862
Iteration 10804, loss = 0.05131288
Iteration 10805, loss = 0.05130720
Iteration 10806, loss = 0.05130131
Iteration 10807, loss = 0.05129556
Iteration 10808, loss = 0.05128968
Iteration 10809, loss = 0.05128389
Iteration 10810, loss = 0.05127810
Iteration 10811, loss = 0.05127242
Iteration 10812, loss = 0.05126650
Iteration 10813, loss = 0.05126079
Iteration 10814, loss = 0.05125499
Iteration 10815, loss = 0.05124925
Iteration 10816, loss = 0.05124340
Iteration 10817, loss = 0.05123771
Iteration 10818, loss = 0.05123196
Iteration 10819, loss = 0.05122620
Iteration 10820, loss = 0.05122034
Iteration 10821, loss = 0.05121459
Iteration 10822, loss = 0.05120893
Iteration 10823, loss = 0.05120311
Iteration 10824, loss = 0.05119741
Iteration 10825, loss = 0.05119163
Iteration 10826, loss = 0.05118588
Iteration 10827, loss = 0.05118012
Iteration 10828, loss = 0.05117430
Iteration 10829, loss = 0.05116865
Iteration 10830, loss = 0.05116301
Iteration 10831, loss = 0.05115717
Iteration 10832, loss = 0.05115148
Iteration 10833, loss = 0.05114577
Iteration 10834, loss = 0.05114006
Iteration 10835, loss = 0.05113435
Iteration 10836, loss = 0.05112859
Iteration 10837, loss = 0.05112290
Iteration 10838, loss = 0.05111718
Iteration 10839, loss = 0.05111153
Iteration 10840, loss = 0.05110578
Iteration 10841, loss = 0.05110001
Iteration 10842, loss = 0.05109441
Iteration 10843, loss = 0.05108861
Iteration 10844, loss = 0.05108294
Iteration 10845, loss = 0.05107720
Iteration 10846, loss = 0.05107142
Iteration 10847, loss = 0.05106582
Iteration 10848, loss = 0.05106005
Iteration 10849, loss = 0.05105444
Iteration 10850, loss = 0.05104866
Iteration 10851, loss = 0.05104297
Iteration 10852, loss = 0.05103728
Iteration 10853, loss = 0.05103154
Iteration 10854, loss = 0.05102585
Iteration 10855, loss = 0.05102014
Iteration 10856, loss = 0.05101446
Iteration 10857, loss = 0.05100873
Iteration 10858, loss = 0.05100310
Iteration 10859, loss = 0.05099732
Iteration 10860, loss = 0.05099168
Iteration 10861, loss = 0.05098588
Iteration 10862, loss = 0.05098028
Iteration 10863, loss = 0.05097454
Iteration 10864, loss = 0.05096891
Iteration 10865, loss = 0.05096316
Iteration 10866, loss = 0.05095749
Iteration 10867, loss = 0.05095177
Iteration 10868, loss = 0.05094612
Iteration 10869, loss = 0.05094042
Iteration 10870, loss = 0.05093471
Iteration 10871, loss = 0.05092911
Iteration 10872, loss = 0.05092335
Iteration 10873, loss = 0.05091777
Iteration 10874, loss = 0.05091201
Iteration 10875, loss = 0.05090633
Iteration 10876, loss = 0.05090064
Iteration 10877, loss = 0.05089500
Iteration 10878, loss = 0.05088927
Iteration 10879, loss = 0.05088360
Iteration 10880, loss = 0.05087794
Iteration 10881, loss = 0.05087225
Iteration 10882, loss = 0.05086660
Iteration 10883, loss = 0.05086091
Iteration 10884, loss = 0.05085523
Iteration 10885, loss = 0.05084953
Iteration 10886, loss = 0.05084387
Iteration 10887, loss = 0.05083821
Iteration 10888, loss = 0.05083253
Iteration 10889, loss = 0.05082689
Iteration 10890, loss = 0.05082112
Iteration 10891, loss = 0.05081547
Iteration 10892, loss = 0.05080994
Iteration 10893, loss = 0.05080427
Iteration 10894, loss = 0.05079849
Iteration 10895, loss = 0.05079288
Iteration 10896, loss = 0.05078721
Iteration 10897, loss = 0.05078156
Iteration 10898, loss = 0.05077596
Iteration 10899, loss = 0.05077021
Iteration 10900, loss = 0.05076463
Iteration 10901, loss = 0.05075889
Iteration 10902, loss = 0.05075336
Iteration 10903, loss = 0.05074761
Iteration 10904, loss = 0.05074196
Iteration 10905, loss = 0.05073635
Iteration 10906, loss = 0.05073072
Iteration 10907, loss = 0.05072502
Iteration 10908, loss = 0.05071945
Iteration 10909, loss = 0.05071374
Iteration 10910, loss = 0.05070819
Iteration 10911, loss = 0.05070243
Iteration 10912, loss = 0.05069686
Iteration 10913, loss = 0.05069127
Iteration 10914, loss = 0.05068561
Iteration 10915, loss = 0.05067991
Iteration 10916, loss = 0.05067435
Iteration 10917, loss = 0.05066864
Iteration 10918, loss = 0.05066315
Iteration 10919, loss = 0.05065744
Iteration 10920, loss = 0.05065183
Iteration 10921, loss = 0.05064616
Iteration 10922, loss = 0.05064054
Iteration 10923, loss = 0.05063485
Iteration 10924, loss = 0.05062935
Iteration 10925, loss = 0.05062371
Iteration 10926, loss = 0.05061806
Iteration 10927, loss = 0.05061249
Iteration 10928, loss = 0.05060691
Iteration 10929, loss = 0.05060134
Iteration 10930, loss = 0.05059565
Iteration 10931, loss = 0.05059002
Iteration 10932, loss = 0.05058447
Iteration 10933, loss = 0.05057891
Iteration 10934, loss = 0.05057322
Iteration 10935, loss = 0.05056767
Iteration 10936, loss = 0.05056205
Iteration 10937, loss = 0.05055652
Iteration 10938, loss = 0.05055092
Iteration 10939, loss = 0.05054526
Iteration 10940, loss = 0.05053964
Iteration 10941, loss = 0.05053409
Iteration 10942, loss = 0.05052849
Iteration 10943, loss = 0.05052293
Iteration 10944, loss = 0.05051734
Iteration 10945, loss = 0.05051175
Iteration 10946, loss = 0.05050620
Iteration 10947, loss = 0.05050060
Iteration 10948, loss = 0.05049500
Iteration 10949, loss = 0.05048940
Iteration 10950, loss = 0.05048385
Iteration 10951, loss = 0.05047823
Iteration 10952, loss = 0.05047268
Iteration 10953, loss = 0.05046708
Iteration 10954, loss = 0.05046149
Iteration 10955, loss = 0.05045590
Iteration 10956, loss = 0.05045035
Iteration 10957, loss = 0.05044472
Iteration 10958, loss = 0.05043918
Iteration 10959, loss = 0.05043368
Iteration 10960, loss = 0.05042808
Iteration 10961, loss = 0.05042253
Iteration 10962, loss = 0.05041692
Iteration 10963, loss = 0.05041136
Iteration 10964, loss = 0.05040583
Iteration 10965, loss = 0.05040028
Iteration 10966, loss = 0.05039472
Iteration 10967, loss = 0.05038913
Iteration 10968, loss = 0.05038359
Iteration 10969, loss = 0.05037801
Iteration 10970, loss = 0.05037257
Iteration 10971, loss = 0.05036688
Iteration 10972, loss = 0.05036140
Iteration 10973, loss = 0.05035580
Iteration 10974, loss = 0.05035022
Iteration 10975, loss = 0.05034471
Iteration 10976, loss = 0.05033908
Iteration 10977, loss = 0.05033353
Iteration 10978, loss = 0.05032804
Iteration 10979, loss = 0.05032246
Iteration 10980, loss = 0.05031694
Iteration 10981, loss = 0.05031134
Iteration 10982, loss = 0.05030581
Iteration 10983, loss = 0.05030020
Iteration 10984, loss = 0.05029465
Iteration 10985, loss = 0.05028914
Iteration 10986, loss = 0.05028354
Iteration 10987, loss = 0.05027808
Iteration 10988, loss = 0.05027255
Iteration 10989, loss = 0.05026698
Iteration 10990, loss = 0.05026143
Iteration 10991, loss = 0.05025584
Iteration 10992, loss = 0.05025032
Iteration 10993, loss = 0.05024482
Iteration 10994, loss = 0.05023926
Iteration 10995, loss = 0.05023375
Iteration 10996, loss = 0.05022821
Iteration 10997, loss = 0.05022273
Iteration 10998, loss = 0.05021717
Iteration 10999, loss = 0.05021161
Iteration 11000, loss = 0.05020611
Iteration 11001, loss = 0.05020063
Iteration 11002, loss = 0.05019501
Iteration 11003, loss = 0.05018951
Iteration 11004, loss = 0.05018399
Iteration 11005, loss = 0.05017847
Iteration 11006, loss = 0.05017295
Iteration 11007, loss = 0.05016747
Iteration 11008, loss = 0.05016197
Iteration 11009, loss = 0.05015637
Iteration 11010, loss = 0.05015079
Iteration 11011, loss = 0.05014531
Iteration 11012, loss = 0.05013974
Iteration 11013, loss = 0.05013425
Iteration 11014, loss = 0.05012872
Iteration 11015, loss = 0.05012323
Iteration 11016, loss = 0.05011771
Iteration 11017, loss = 0.05011221
Iteration 11018, loss = 0.05010664
Iteration 11019, loss = 0.05010116
Iteration 11020, loss = 0.05009564
Iteration 11021, loss = 0.05009022
Iteration 11022, loss = 0.05008473
Iteration 11023, loss = 0.05007912
Iteration 11024, loss = 0.05007368
Iteration 11025, loss = 0.05006810
Iteration 11026, loss = 0.05006273
Iteration 11027, loss = 0.05005714
Iteration 11028, loss = 0.05005171
Iteration 11029, loss = 0.05004619
Iteration 11030, loss = 0.05004069
Iteration 11031, loss = 0.05003519
Iteration 11032, loss = 0.05002972
Iteration 11033, loss = 0.05002425
Iteration 11034, loss = 0.05001876
Iteration 11035, loss = 0.05001324
Iteration 11036, loss = 0.05000775
Iteration 11037, loss = 0.05000233
Iteration 11038, loss = 0.04999677
Iteration 11039, loss = 0.04999127
Iteration 11040, loss = 0.04998592
Iteration 11041, loss = 0.04998037
Iteration 11042, loss = 0.04997490
Iteration 11043, loss = 0.04996931
Iteration 11044, loss = 0.04996394
Iteration 11045, loss = 0.04995851
Iteration 11046, loss = 0.04995293
Iteration 11047, loss = 0.04994750
Iteration 11048, loss = 0.04994200
Iteration 11049, loss = 0.04993650
Iteration 11050, loss = 0.04993112
Iteration 11051, loss = 0.04992560
Iteration 11052, loss = 0.04992016
Iteration 11053, loss = 0.04991461
Iteration 11054, loss = 0.04990920
Iteration 11055, loss = 0.04990371
Iteration 11056, loss = 0.04989831
Iteration 11057, loss = 0.04989282
Iteration 11058, loss = 0.04988731
Iteration 11059, loss = 0.04988192
Iteration 11060, loss = 0.04987645
Iteration 11061, loss = 0.04987106
Iteration 11062, loss = 0.04986547
Iteration 11063, loss = 0.04986012
Iteration 11064, loss = 0.04985468
Iteration 11065, loss = 0.04984924
Iteration 11066, loss = 0.04984380
Iteration 11067, loss = 0.04983831
Iteration 11068, loss = 0.04983292
Iteration 11069, loss = 0.04982746
Iteration 11070, loss = 0.04982208
Iteration 11071, loss = 0.04981659
Iteration 11072, loss = 0.04981114
Iteration 11073, loss = 0.04980568
Iteration 11074, loss = 0.04980026
Iteration 11075, loss = 0.04979487
Iteration 11076, loss = 0.04978948
Iteration 11077, loss = 0.04978398
Iteration 11078, loss = 0.04977854
Iteration 11079, loss = 0.04977313
Iteration 11080, loss = 0.04976768
Iteration 11081, loss = 0.04976230
Iteration 11082, loss = 0.04975688
Iteration 11083, loss = 0.04975135
Iteration 11084, loss = 0.04974599
Iteration 11085, loss = 0.04974050
Iteration 11086, loss = 0.04973512
Iteration 11087, loss = 0.04972966
Iteration 11088, loss = 0.04972428
Iteration 11089, loss = 0.04971887
Iteration 11090, loss = 0.04971339
Iteration 11091, loss = 0.04970797
Iteration 11092, loss = 0.04970263
Iteration 11093, loss = 0.04969717
Iteration 11094, loss = 0.04969185
Iteration 11095, loss = 0.04968643
Iteration 11096, loss = 0.04968103
Iteration 11097, loss = 0.04967556
Iteration 11098, loss = 0.04967020
Iteration 11099, loss = 0.04966479
Iteration 11100, loss = 0.04965944
Iteration 11101, loss = 0.04965400
Iteration 11102, loss = 0.04964861
Iteration 11103, loss = 0.04964322
Iteration 11104, loss = 0.04963780
Iteration 11105, loss = 0.04963240
Iteration 11106, loss = 0.04962700
Iteration 11107, loss = 0.04962154
Iteration 11108, loss = 0.04961614
Iteration 11109, loss = 0.04961079
Iteration 11110, loss = 0.04960540
Iteration 11111, loss = 0.04959998
Iteration 11112, loss = 0.04959462
Iteration 11113, loss = 0.04958924
Iteration 11114, loss = 0.04958380
Iteration 11115, loss = 0.04957847
Iteration 11116, loss = 0.04957311
Iteration 11117, loss = 0.04956767
Iteration 11118, loss = 0.04956228
Iteration 11119, loss = 0.04955692
Iteration 11120, loss = 0.04955156
Iteration 11121, loss = 0.04954624
Iteration 11122, loss = 0.04954076
Iteration 11123, loss = 0.04953544
Iteration 11124, loss = 0.04953010
Iteration 11125, loss = 0.04952473
Iteration 11126, loss = 0.04951927
Iteration 11127, loss = 0.04951403
Iteration 11128, loss = 0.04950854
Iteration 11129, loss = 0.04950319
Iteration 11130, loss = 0.04949781
Iteration 11131, loss = 0.04949246
Iteration 11132, loss = 0.04948713
Iteration 11133, loss = 0.04948176
Iteration 11134, loss = 0.04947646
Iteration 11135, loss = 0.04947110
Iteration 11136, loss = 0.04946568
Iteration 11137, loss = 0.04946036
Iteration 11138, loss = 0.04945501
Iteration 11139, loss = 0.04944963
Iteration 11140, loss = 0.04944429
Iteration 11141, loss = 0.04943895
Iteration 11142, loss = 0.04943366
Iteration 11143, loss = 0.04942824
Iteration 11144, loss = 0.04942291
Iteration 11145, loss = 0.04941750
Iteration 11146, loss = 0.04941218
Iteration 11147, loss = 0.04940682
Iteration 11148, loss = 0.04940150
Iteration 11149, loss = 0.04939617
Iteration 11150, loss = 0.04939082
Iteration 11151, loss = 0.04938544
Iteration 11152, loss = 0.04938015
Iteration 11153, loss = 0.04937477
Iteration 11154, loss = 0.04936940
Iteration 11155, loss = 0.04936407
Iteration 11156, loss = 0.04935876
Iteration 11157, loss = 0.04935344
Iteration 11158, loss = 0.04934809
Iteration 11159, loss = 0.04934279
Iteration 11160, loss = 0.04933736
Iteration 11161, loss = 0.04933205
Iteration 11162, loss = 0.04932677
Iteration 11163, loss = 0.04932137
Iteration 11164, loss = 0.04931614
Iteration 11165, loss = 0.04931078
Iteration 11166, loss = 0.04930542
Iteration 11167, loss = 0.04930018
Iteration 11168, loss = 0.04929479
Iteration 11169, loss = 0.04928943
Iteration 11170, loss = 0.04928420
Iteration 11171, loss = 0.04927889
Iteration 11172, loss = 0.04927362
Iteration 11173, loss = 0.04926828
Iteration 11174, loss = 0.04926297
Iteration 11175, loss = 0.04925768
Iteration 11176, loss = 0.04925234
Iteration 11177, loss = 0.04924704
Iteration 11178, loss = 0.04924183
Iteration 11179, loss = 0.04923653
Iteration 11180, loss = 0.04923112
Iteration 11181, loss = 0.04922580
Iteration 11182, loss = 0.04922059
Iteration 11183, loss = 0.04921529
Iteration 11184, loss = 0.04920994
Iteration 11185, loss = 0.04920459
Iteration 11186, loss = 0.04919934
Iteration 11187, loss = 0.04919394
Iteration 11188, loss = 0.04918875
Iteration 11189, loss = 0.04918337
Iteration 11190, loss = 0.04917806
Iteration 11191, loss = 0.04917273
Iteration 11192, loss = 0.04916743
Iteration 11193, loss = 0.04916218
Iteration 11194, loss = 0.04915692
Iteration 11195, loss = 0.04915159
Iteration 11196, loss = 0.04914629
Iteration 11197, loss = 0.04914093
Iteration 11198, loss = 0.04913570
Iteration 11199, loss = 0.04913051
Iteration 11200, loss = 0.04912514
Iteration 11201, loss = 0.04911990
Iteration 11202, loss = 0.04911455
Iteration 11203, loss = 0.04910927
Iteration 11204, loss = 0.04910404
Iteration 11205, loss = 0.04909873
Iteration 11206, loss = 0.04909344
Iteration 11207, loss = 0.04908814
Iteration 11208, loss = 0.04908287
Iteration 11209, loss = 0.04907757
Iteration 11210, loss = 0.04907227
Iteration 11211, loss = 0.04906698
Iteration 11212, loss = 0.04906178
Iteration 11213, loss = 0.04905645
Iteration 11214, loss = 0.04905109
Iteration 11215, loss = 0.04904585
Iteration 11216, loss = 0.04904070
Iteration 11217, loss = 0.04903536
Iteration 11218, loss = 0.04903001
Iteration 11219, loss = 0.04902479
Iteration 11220, loss = 0.04901950
Iteration 11221, loss = 0.04901423
Iteration 11222, loss = 0.04900892
Iteration 11223, loss = 0.04900374
Iteration 11224, loss = 0.04899843
Iteration 11225, loss = 0.04899318
Iteration 11226, loss = 0.04898798
Iteration 11227, loss = 0.04898264
Iteration 11228, loss = 0.04897737
Iteration 11229, loss = 0.04897217
Iteration 11230, loss = 0.04896688
Iteration 11231, loss = 0.04896159
Iteration 11232, loss = 0.04895632
Iteration 11233, loss = 0.04895105
Iteration 11234, loss = 0.04894584
Iteration 11235, loss = 0.04894054
Iteration 11236, loss = 0.04893531
Iteration 11237, loss = 0.04893006
Iteration 11238, loss = 0.04892482
Iteration 11239, loss = 0.04891960
Iteration 11240, loss = 0.04891427
Iteration 11241, loss = 0.04890908
Iteration 11242, loss = 0.04890376
Iteration 11243, loss = 0.04889852
Iteration 11244, loss = 0.04889339
Iteration 11245, loss = 0.04888806
Iteration 11246, loss = 0.04888283
Iteration 11247, loss = 0.04887767
Iteration 11248, loss = 0.04887237
Iteration 11249, loss = 0.04886712
Iteration 11250, loss = 0.04886194
Iteration 11251, loss = 0.04885663
Iteration 11252, loss = 0.04885145
Iteration 11253, loss = 0.04884622
Iteration 11254, loss = 0.04884102
Iteration 11255, loss = 0.04883580
Iteration 11256, loss = 0.04883056
Iteration 11257, loss = 0.04882530
Iteration 11258, loss = 0.04882020
Iteration 11259, loss = 0.04881490
Iteration 11260, loss = 0.04880970
Iteration 11261, loss = 0.04880440
Iteration 11262, loss = 0.04879918
Iteration 11263, loss = 0.04879404
Iteration 11264, loss = 0.04878876
Iteration 11265, loss = 0.04878353
Iteration 11266, loss = 0.04877833
Iteration 11267, loss = 0.04877311
Iteration 11268, loss = 0.04876788
Iteration 11269, loss = 0.04876269
Iteration 11270, loss = 0.04875744
Iteration 11271, loss = 0.04875223
Iteration 11272, loss = 0.04874702
Iteration 11273, loss = 0.04874185
Iteration 11274, loss = 0.04873661
Iteration 11275, loss = 0.04873138
Iteration 11276, loss = 0.04872616
Iteration 11277, loss = 0.04872095
Iteration 11278, loss = 0.04871575
Iteration 11279, loss = 0.04871056
Iteration 11280, loss = 0.04870531
Iteration 11281, loss = 0.04870019
Iteration 11282, loss = 0.04869489
Iteration 11283, loss = 0.04868973
Iteration 11284, loss = 0.04868459
Iteration 11285, loss = 0.04867929
Iteration 11286, loss = 0.04867419
Iteration 11287, loss = 0.04866895
Iteration 11288, loss = 0.04866369
Iteration 11289, loss = 0.04865858
Iteration 11290, loss = 0.04865338
Iteration 11291, loss = 0.04864821
Iteration 11292, loss = 0.04864302
Iteration 11293, loss = 0.04863786
Iteration 11294, loss = 0.04863267
Iteration 11295, loss = 0.04862752
Iteration 11296, loss = 0.04862240
Iteration 11297, loss = 0.04861720
Iteration 11298, loss = 0.04861201
Iteration 11299, loss = 0.04860687
Iteration 11300, loss = 0.04860176
Iteration 11301, loss = 0.04859659
Iteration 11302, loss = 0.04859144
Iteration 11303, loss = 0.04858630
Iteration 11304, loss = 0.04858114
Iteration 11305, loss = 0.04857596
Iteration 11306, loss = 0.04857079
Iteration 11307, loss = 0.04856558
Iteration 11308, loss = 0.04856046
Iteration 11309, loss = 0.04855529
Iteration 11310, loss = 0.04855019
Iteration 11311, loss = 0.04854501
Iteration 11312, loss = 0.04853984
Iteration 11313, loss = 0.04853470
Iteration 11314, loss = 0.04852952
Iteration 11315, loss = 0.04852435
Iteration 11316, loss = 0.04851917
Iteration 11317, loss = 0.04851399
Iteration 11318, loss = 0.04850892
Iteration 11319, loss = 0.04850369
Iteration 11320, loss = 0.04849855
Iteration 11321, loss = 0.04849337
Iteration 11322, loss = 0.04848816
Iteration 11323, loss = 0.04848304
Iteration 11324, loss = 0.04847785
Iteration 11325, loss = 0.04847278
Iteration 11326, loss = 0.04846765
Iteration 11327, loss = 0.04846243
Iteration 11328, loss = 0.04845732
Iteration 11329, loss = 0.04845217
Iteration 11330, loss = 0.04844698
Iteration 11331, loss = 0.04844188
Iteration 11332, loss = 0.04843680
Iteration 11333, loss = 0.04843162
Iteration 11334, loss = 0.04842648
Iteration 11335, loss = 0.04842132
Iteration 11336, loss = 0.04841622
Iteration 11337, loss = 0.04841104
Iteration 11338, loss = 0.04840594
Iteration 11339, loss = 0.04840082
Iteration 11340, loss = 0.04839568
Iteration 11341, loss = 0.04839051
Iteration 11342, loss = 0.04838537
Iteration 11343, loss = 0.04838027
Iteration 11344, loss = 0.04837512
Iteration 11345, loss = 0.04837004
Iteration 11346, loss = 0.04836493
Iteration 11347, loss = 0.04835969
Iteration 11348, loss = 0.04835468
Iteration 11349, loss = 0.04834953
Iteration 11350, loss = 0.04834448
Iteration 11351, loss = 0.04833924
Iteration 11352, loss = 0.04833426
Iteration 11353, loss = 0.04832910
Iteration 11354, loss = 0.04832395
Iteration 11355, loss = 0.04831878
Iteration 11356, loss = 0.04831374
Iteration 11357, loss = 0.04830869
Iteration 11358, loss = 0.04830355
Iteration 11359, loss = 0.04829842
Iteration 11360, loss = 0.04829331
Iteration 11361, loss = 0.04828823
Iteration 11362, loss = 0.04828313
Iteration 11363, loss = 0.04827800
Iteration 11364, loss = 0.04827288
Iteration 11365, loss = 0.04826780
Iteration 11366, loss = 0.04826261
Iteration 11367, loss = 0.04825751
Iteration 11368, loss = 0.04825240
Iteration 11369, loss = 0.04824737
Iteration 11370, loss = 0.04824221
Iteration 11371, loss = 0.04823718
Iteration 11372, loss = 0.04823201
Iteration 11373, loss = 0.04822686
Iteration 11374, loss = 0.04822182
Iteration 11375, loss = 0.04821681
Iteration 11376, loss = 0.04821163
Iteration 11377, loss = 0.04820651
Iteration 11378, loss = 0.04820149
Iteration 11379, loss = 0.04819641
Iteration 11380, loss = 0.04819134
Iteration 11381, loss = 0.04818619
Iteration 11382, loss = 0.04818114
Iteration 11383, loss = 0.04817607
Iteration 11384, loss = 0.04817096
Iteration 11385, loss = 0.04816590
Iteration 11386, loss = 0.04816083
Iteration 11387, loss = 0.04815571
Iteration 11388, loss = 0.04815065
Iteration 11389, loss = 0.04814555
Iteration 11390, loss = 0.04814054
Iteration 11391, loss = 0.04813544
Iteration 11392, loss = 0.04813040
Iteration 11393, loss = 0.04812528
Iteration 11394, loss = 0.04812016
Iteration 11395, loss = 0.04811517
Iteration 11396, loss = 0.04811008
Iteration 11397, loss = 0.04810504
Iteration 11398, loss = 0.04809991
Iteration 11399, loss = 0.04809483
Iteration 11400, loss = 0.04808976
Iteration 11401, loss = 0.04808476
Iteration 11402, loss = 0.04807967
Iteration 11403, loss = 0.04807464
Iteration 11404, loss = 0.04806955
Iteration 11405, loss = 0.04806452
Iteration 11406, loss = 0.04805944
Iteration 11407, loss = 0.04805436
Iteration 11408, loss = 0.04804936
Iteration 11409, loss = 0.04804428
Iteration 11410, loss = 0.04803932
Iteration 11411, loss = 0.04803416
Iteration 11412, loss = 0.04802914
Iteration 11413, loss = 0.04802409
Iteration 11414, loss = 0.04801900
Iteration 11415, loss = 0.04801401
Iteration 11416, loss = 0.04800899
Iteration 11417, loss = 0.04800394
Iteration 11418, loss = 0.04799889
Iteration 11419, loss = 0.04799382
Iteration 11420, loss = 0.04798873
Iteration 11421, loss = 0.04798367
Iteration 11422, loss = 0.04797864
Iteration 11423, loss = 0.04797360
Iteration 11424, loss = 0.04796853
Iteration 11425, loss = 0.04796354
Iteration 11426, loss = 0.04795845
Iteration 11427, loss = 0.04795342
Iteration 11428, loss = 0.04794838
Iteration 11429, loss = 0.04794337
Iteration 11430, loss = 0.04793837
Iteration 11431, loss = 0.04793324
Iteration 11432, loss = 0.04792829
Iteration 11433, loss = 0.04792331
Iteration 11434, loss = 0.04791821
Iteration 11435, loss = 0.04791320
Iteration 11436, loss = 0.04790819
Iteration 11437, loss = 0.04790313
Iteration 11438, loss = 0.04789812
Iteration 11439, loss = 0.04789308
Iteration 11440, loss = 0.04788803
Iteration 11441, loss = 0.04788316
Iteration 11442, loss = 0.04787808
Iteration 11443, loss = 0.04787299
Iteration 11444, loss = 0.04786806
Iteration 11445, loss = 0.04786303
Iteration 11446, loss = 0.04785802
Iteration 11447, loss = 0.04785304
Iteration 11448, loss = 0.04784796
Iteration 11449, loss = 0.04784297
Iteration 11450, loss = 0.04783792
Iteration 11451, loss = 0.04783295
Iteration 11452, loss = 0.04782791
Iteration 11453, loss = 0.04782297
Iteration 11454, loss = 0.04781796
Iteration 11455, loss = 0.04781292
Iteration 11456, loss = 0.04780790
Iteration 11457, loss = 0.04780293
Iteration 11458, loss = 0.04779796
Iteration 11459, loss = 0.04779291
Iteration 11460, loss = 0.04778794
Iteration 11461, loss = 0.04778292
Iteration 11462, loss = 0.04777789
Iteration 11463, loss = 0.04777291
Iteration 11464, loss = 0.04776795
Iteration 11465, loss = 0.04776291
Iteration 11466, loss = 0.04775791
Iteration 11467, loss = 0.04775298
Iteration 11468, loss = 0.04774797
Iteration 11469, loss = 0.04774293
Iteration 11470, loss = 0.04773795
Iteration 11471, loss = 0.04773296
Iteration 11472, loss = 0.04772790
Iteration 11473, loss = 0.04772293
Iteration 11474, loss = 0.04771795
Iteration 11475, loss = 0.04771293
Iteration 11476, loss = 0.04770798
Iteration 11477, loss = 0.04770300
Iteration 11478, loss = 0.04769798
Iteration 11479, loss = 0.04769303
Iteration 11480, loss = 0.04768800
Iteration 11481, loss = 0.04768303
Iteration 11482, loss = 0.04767801
Iteration 11483, loss = 0.04767315
Iteration 11484, loss = 0.04766812
Iteration 11485, loss = 0.04766319
Iteration 11486, loss = 0.04765819
Iteration 11487, loss = 0.04765320
Iteration 11488, loss = 0.04764817
Iteration 11489, loss = 0.04764318
Iteration 11490, loss = 0.04763834
Iteration 11491, loss = 0.04763328
Iteration 11492, loss = 0.04762830
Iteration 11493, loss = 0.04762330
Iteration 11494, loss = 0.04761833
Iteration 11495, loss = 0.04761339
Iteration 11496, loss = 0.04760850
Iteration 11497, loss = 0.04760346
Iteration 11498, loss = 0.04759848
Iteration 11499, loss = 0.04759349
Iteration 11500, loss = 0.04758861
Iteration 11501, loss = 0.04758364
Iteration 11502, loss = 0.04757863
Iteration 11503, loss = 0.04757369
Iteration 11504, loss = 0.04756880
Iteration 11505, loss = 0.04756383
Iteration 11506, loss = 0.04755882
Iteration 11507, loss = 0.04755383
Iteration 11508, loss = 0.04754891
Iteration 11509, loss = 0.04754401
Iteration 11510, loss = 0.04753905
Iteration 11511, loss = 0.04753401
Iteration 11512, loss = 0.04752905
Iteration 11513, loss = 0.04752414
Iteration 11514, loss = 0.04751918
Iteration 11515, loss = 0.04751425
Iteration 11516, loss = 0.04750930
Iteration 11517, loss = 0.04750434
Iteration 11518, loss = 0.04749947
Iteration 11519, loss = 0.04749442
Iteration 11520, loss = 0.04748951
Iteration 11521, loss = 0.04748462
Iteration 11522, loss = 0.04747964
Iteration 11523, loss = 0.04747473
Iteration 11524, loss = 0.04746985
Iteration 11525, loss = 0.04746486
Iteration 11526, loss = 0.04745993
Iteration 11527, loss = 0.04745493
Iteration 11528, loss = 0.04745009
Iteration 11529, loss = 0.04744509
Iteration 11530, loss = 0.04744026
Iteration 11531, loss = 0.04743525
Iteration 11532, loss = 0.04743037
Iteration 11533, loss = 0.04742546
Iteration 11534, loss = 0.04742052
Iteration 11535, loss = 0.04741563
Iteration 11536, loss = 0.04741066
Iteration 11537, loss = 0.04740579
Iteration 11538, loss = 0.04740082
Iteration 11539, loss = 0.04739594
Iteration 11540, loss = 0.04739101
Iteration 11541, loss = 0.04738611
Iteration 11542, loss = 0.04738118
Iteration 11543, loss = 0.04737625
Iteration 11544, loss = 0.04737138
Iteration 11545, loss = 0.04736643
Iteration 11546, loss = 0.04736154
Iteration 11547, loss = 0.04735670
Iteration 11548, loss = 0.04735169
Iteration 11549, loss = 0.04734684
Iteration 11550, loss = 0.04734196
Iteration 11551, loss = 0.04733706
Iteration 11552, loss = 0.04733214
Iteration 11553, loss = 0.04732721
Iteration 11554, loss = 0.04732235
Iteration 11555, loss = 0.04731746
Iteration 11556, loss = 0.04731256
Iteration 11557, loss = 0.04730764
Iteration 11558, loss = 0.04730277
Iteration 11559, loss = 0.04729789
Iteration 11560, loss = 0.04729299
Iteration 11561, loss = 0.04728805
Iteration 11562, loss = 0.04728314
Iteration 11563, loss = 0.04727827
Iteration 11564, loss = 0.04727339
Iteration 11565, loss = 0.04726839
Iteration 11566, loss = 0.04726354
Iteration 11567, loss = 0.04725864
Iteration 11568, loss = 0.04725374
Iteration 11569, loss = 0.04724888
Iteration 11570, loss = 0.04724397
Iteration 11571, loss = 0.04723907
Iteration 11572, loss = 0.04723418
Iteration 11573, loss = 0.04722927
Iteration 11574, loss = 0.04722440
Iteration 11575, loss = 0.04721955
Iteration 11576, loss = 0.04721456
Iteration 11577, loss = 0.04720974
Iteration 11578, loss = 0.04720491
Iteration 11579, loss = 0.04720001
Iteration 11580, loss = 0.04719505
Iteration 11581, loss = 0.04719021
Iteration 11582, loss = 0.04718528
Iteration 11583, loss = 0.04718045
Iteration 11584, loss = 0.04717558
Iteration 11585, loss = 0.04717059
Iteration 11586, loss = 0.04716572
Iteration 11587, loss = 0.04716080
Iteration 11588, loss = 0.04715593
Iteration 11589, loss = 0.04715114
Iteration 11590, loss = 0.04714620
Iteration 11591, loss = 0.04714137
Iteration 11592, loss = 0.04713639
Iteration 11593, loss = 0.04713160
Iteration 11594, loss = 0.04712668
Iteration 11595, loss = 0.04712175
Iteration 11596, loss = 0.04711696
Iteration 11597, loss = 0.04711208
Iteration 11598, loss = 0.04710722
Iteration 11599, loss = 0.04710235
Iteration 11600, loss = 0.04709747
Iteration 11601, loss = 0.04709258
Iteration 11602, loss = 0.04708782
Iteration 11603, loss = 0.04708283
Iteration 11604, loss = 0.04707800
Iteration 11605, loss = 0.04707326
Iteration 11606, loss = 0.04706823
Iteration 11607, loss = 0.04706344
Iteration 11608, loss = 0.04705860
Iteration 11609, loss = 0.04705376
Iteration 11610, loss = 0.04704897
Iteration 11611, loss = 0.04704411
Iteration 11612, loss = 0.04703925
Iteration 11613, loss = 0.04703440
Iteration 11614, loss = 0.04702953
Iteration 11615, loss = 0.04702472
Iteration 11616, loss = 0.04701983
Iteration 11617, loss = 0.04701506
Iteration 11618, loss = 0.04701019
Iteration 11619, loss = 0.04700539
Iteration 11620, loss = 0.04700051
Iteration 11621, loss = 0.04699573
Iteration 11622, loss = 0.04699085
Iteration 11623, loss = 0.04698603
Iteration 11624, loss = 0.04698122
Iteration 11625, loss = 0.04697640
Iteration 11626, loss = 0.04697159
Iteration 11627, loss = 0.04696677
Iteration 11628, loss = 0.04696193
Iteration 11629, loss = 0.04695708
Iteration 11630, loss = 0.04695224
Iteration 11631, loss = 0.04694743
Iteration 11632, loss = 0.04694261
Iteration 11633, loss = 0.04693776
Iteration 11634, loss = 0.04693286
Iteration 11635, loss = 0.04692806
Iteration 11636, loss = 0.04692324
Iteration 11637, loss = 0.04691838
Iteration 11638, loss = 0.04691360
Iteration 11639, loss = 0.04690878
Iteration 11640, loss = 0.04690403
Iteration 11641, loss = 0.04689917
Iteration 11642, loss = 0.04689432
Iteration 11643, loss = 0.04688953
Iteration 11644, loss = 0.04688471
Iteration 11645, loss = 0.04687995
Iteration 11646, loss = 0.04687509
Iteration 11647, loss = 0.04687034
Iteration 11648, loss = 0.04686545
Iteration 11649, loss = 0.04686071
Iteration 11650, loss = 0.04685581
Iteration 11651, loss = 0.04685097
Iteration 11652, loss = 0.04684623
Iteration 11653, loss = 0.04684135
Iteration 11654, loss = 0.04683661
Iteration 11655, loss = 0.04683175
Iteration 11656, loss = 0.04682694
Iteration 11657, loss = 0.04682212
Iteration 11658, loss = 0.04681734
Iteration 11659, loss = 0.04681252
Iteration 11660, loss = 0.04680779
Iteration 11661, loss = 0.04680294
Iteration 11662, loss = 0.04679809
Iteration 11663, loss = 0.04679332
Iteration 11664, loss = 0.04678852
Iteration 11665, loss = 0.04678373
Iteration 11666, loss = 0.04677892
Iteration 11667, loss = 0.04677415
Iteration 11668, loss = 0.04676940
Iteration 11669, loss = 0.04676462
Iteration 11670, loss = 0.04675984
Iteration 11671, loss = 0.04675505
Iteration 11672, loss = 0.04675028
Iteration 11673, loss = 0.04674546
Iteration 11674, loss = 0.04674073
Iteration 11675, loss = 0.04673599
Iteration 11676, loss = 0.04673116
Iteration 11677, loss = 0.04672640
Iteration 11678, loss = 0.04672166
Iteration 11679, loss = 0.04671688
Iteration 11680, loss = 0.04671208
Iteration 11681, loss = 0.04670737
Iteration 11682, loss = 0.04670255
Iteration 11683, loss = 0.04669773
Iteration 11684, loss = 0.04669300
Iteration 11685, loss = 0.04668819
Iteration 11686, loss = 0.04668341
Iteration 11687, loss = 0.04667872
Iteration 11688, loss = 0.04667392
Iteration 11689, loss = 0.04666915
Iteration 11690, loss = 0.04666441
Iteration 11691, loss = 0.04665956
Iteration 11692, loss = 0.04665486
Iteration 11693, loss = 0.04665009
Iteration 11694, loss = 0.04664534
Iteration 11695, loss = 0.04664051
Iteration 11696, loss = 0.04663575
Iteration 11697, loss = 0.04663104
Iteration 11698, loss = 0.04662623
Iteration 11699, loss = 0.04662153
Iteration 11700, loss = 0.04661670
Iteration 11701, loss = 0.04661197
Iteration 11702, loss = 0.04660721
Iteration 11703, loss = 0.04660247
Iteration 11704, loss = 0.04659764
Iteration 11705, loss = 0.04659294
Iteration 11706, loss = 0.04658814
Iteration 11707, loss = 0.04658340
Iteration 11708, loss = 0.04657863
Iteration 11709, loss = 0.04657385
Iteration 11710, loss = 0.04656911
Iteration 11711, loss = 0.04656442
Iteration 11712, loss = 0.04655961
Iteration 11713, loss = 0.04655491
Iteration 11714, loss = 0.04655018
Iteration 11715, loss = 0.04654533
Iteration 11716, loss = 0.04654075
Iteration 11717, loss = 0.04653600
Iteration 11718, loss = 0.04653128
Iteration 11719, loss = 0.04652655
Iteration 11720, loss = 0.04652180
Iteration 11721, loss = 0.04651704
Iteration 11722, loss = 0.04651235
Iteration 11723, loss = 0.04650767
Iteration 11724, loss = 0.04650294
Iteration 11725, loss = 0.04649821
Iteration 11726, loss = 0.04649344
Iteration 11727, loss = 0.04648878
Iteration 11728, loss = 0.04648405
Iteration 11729, loss = 0.04647926
Iteration 11730, loss = 0.04647457
Iteration 11731, loss = 0.04646983
Iteration 11732, loss = 0.04646510
Iteration 11733, loss = 0.04646042
Iteration 11734, loss = 0.04645561
Iteration 11735, loss = 0.04645094
Iteration 11736, loss = 0.04644627
Iteration 11737, loss = 0.04644149
Iteration 11738, loss = 0.04643679
Iteration 11739, loss = 0.04643209
Iteration 11740, loss = 0.04642733
Iteration 11741, loss = 0.04642264
Iteration 11742, loss = 0.04641789
Iteration 11743, loss = 0.04641319
Iteration 11744, loss = 0.04640842
Iteration 11745, loss = 0.04640375
Iteration 11746, loss = 0.04639904
Iteration 11747, loss = 0.04639436
Iteration 11748, loss = 0.04638954
Iteration 11749, loss = 0.04638490
Iteration 11750, loss = 0.04638019
Iteration 11751, loss = 0.04637547
Iteration 11752, loss = 0.04637075
Iteration 11753, loss = 0.04636601
Iteration 11754, loss = 0.04636133
Iteration 11755, loss = 0.04635671
Iteration 11756, loss = 0.04635189
Iteration 11757, loss = 0.04634724
Iteration 11758, loss = 0.04634253
Iteration 11759, loss = 0.04633780
Iteration 11760, loss = 0.04633317
Iteration 11761, loss = 0.04632843
Iteration 11762, loss = 0.04632375
Iteration 11763, loss = 0.04631909
Iteration 11764, loss = 0.04631435
Iteration 11765, loss = 0.04630976
Iteration 11766, loss = 0.04630505
Iteration 11767, loss = 0.04630033
Iteration 11768, loss = 0.04629565
Iteration 11769, loss = 0.04629091
Iteration 11770, loss = 0.04628630
Iteration 11771, loss = 0.04628163
Iteration 11772, loss = 0.04627695
Iteration 11773, loss = 0.04627227
Iteration 11774, loss = 0.04626758
Iteration 11775, loss = 0.04626286
Iteration 11776, loss = 0.04625821
Iteration 11777, loss = 0.04625352
Iteration 11778, loss = 0.04624882
Iteration 11779, loss = 0.04624409
Iteration 11780, loss = 0.04623947
Iteration 11781, loss = 0.04623478
Iteration 11782, loss = 0.04623007
Iteration 11783, loss = 0.04622539
Iteration 11784, loss = 0.04622074
Iteration 11785, loss = 0.04621603
Iteration 11786, loss = 0.04621139
Iteration 11787, loss = 0.04620667
Iteration 11788, loss = 0.04620203
Iteration 11789, loss = 0.04619735
Iteration 11790, loss = 0.04619265
Iteration 11791, loss = 0.04618798
Iteration 11792, loss = 0.04618334
Iteration 11793, loss = 0.04617865
Iteration 11794, loss = 0.04617398
Iteration 11795, loss = 0.04616942
Iteration 11796, loss = 0.04616468
Iteration 11797, loss = 0.04615998
Iteration 11798, loss = 0.04615530
Iteration 11799, loss = 0.04615066
Iteration 11800, loss = 0.04614601
Iteration 11801, loss = 0.04614137
Iteration 11802, loss = 0.04613670
Iteration 11803, loss = 0.04613206
Iteration 11804, loss = 0.04612735
Iteration 11805, loss = 0.04612280
Iteration 11806, loss = 0.04611811
Iteration 11807, loss = 0.04611344
Iteration 11808, loss = 0.04610874
Iteration 11809, loss = 0.04610417
Iteration 11810, loss = 0.04609949
Iteration 11811, loss = 0.04609488
Iteration 11812, loss = 0.04609022
Iteration 11813, loss = 0.04608552
Iteration 11814, loss = 0.04608090
Iteration 11815, loss = 0.04607629
Iteration 11816, loss = 0.04607158
Iteration 11817, loss = 0.04606696
Iteration 11818, loss = 0.04606231
Iteration 11819, loss = 0.04605766
Iteration 11820, loss = 0.04605306
Iteration 11821, loss = 0.04604842
Iteration 11822, loss = 0.04604374
Iteration 11823, loss = 0.04603916
Iteration 11824, loss = 0.04603448
Iteration 11825, loss = 0.04602990
Iteration 11826, loss = 0.04602526
Iteration 11827, loss = 0.04602063
Iteration 11828, loss = 0.04601592
Iteration 11829, loss = 0.04601130
Iteration 11830, loss = 0.04600672
Iteration 11831, loss = 0.04600207
Iteration 11832, loss = 0.04599746
Iteration 11833, loss = 0.04599285
Iteration 11834, loss = 0.04598816
Iteration 11835, loss = 0.04598354
Iteration 11836, loss = 0.04597895
Iteration 11837, loss = 0.04597430
Iteration 11838, loss = 0.04596966
Iteration 11839, loss = 0.04596502
Iteration 11840, loss = 0.04596041
Iteration 11841, loss = 0.04595580
Iteration 11842, loss = 0.04595117
Iteration 11843, loss = 0.04594652
Iteration 11844, loss = 0.04594190
Iteration 11845, loss = 0.04593731
Iteration 11846, loss = 0.04593269
Iteration 11847, loss = 0.04592809
Iteration 11848, loss = 0.04592348
Iteration 11849, loss = 0.04591888
Iteration 11850, loss = 0.04591420
Iteration 11851, loss = 0.04590961
Iteration 11852, loss = 0.04590504
Iteration 11853, loss = 0.04590042
Iteration 11854, loss = 0.04589580
Iteration 11855, loss = 0.04589115
Iteration 11856, loss = 0.04588661
Iteration 11857, loss = 0.04588197
Iteration 11858, loss = 0.04587737
Iteration 11859, loss = 0.04587278
Iteration 11860, loss = 0.04586816
Iteration 11861, loss = 0.04586357
Iteration 11862, loss = 0.04585896
Iteration 11863, loss = 0.04585439
Iteration 11864, loss = 0.04584979
Iteration 11865, loss = 0.04584518
Iteration 11866, loss = 0.04584056
Iteration 11867, loss = 0.04583599
Iteration 11868, loss = 0.04583140
Iteration 11869, loss = 0.04582679
Iteration 11870, loss = 0.04582219
Iteration 11871, loss = 0.04581760
Iteration 11872, loss = 0.04581297
Iteration 11873, loss = 0.04580842
Iteration 11874, loss = 0.04580381
Iteration 11875, loss = 0.04579923
Iteration 11876, loss = 0.04579469
Iteration 11877, loss = 0.04579005
Iteration 11878, loss = 0.04578542
Iteration 11879, loss = 0.04578088
Iteration 11880, loss = 0.04577632
Iteration 11881, loss = 0.04577168
Iteration 11882, loss = 0.04576721
Iteration 11883, loss = 0.04576250
Iteration 11884, loss = 0.04575797
Iteration 11885, loss = 0.04575337
Iteration 11886, loss = 0.04574879
Iteration 11887, loss = 0.04574423
Iteration 11888, loss = 0.04573973
Iteration 11889, loss = 0.04573508
Iteration 11890, loss = 0.04573053
Iteration 11891, loss = 0.04572596
Iteration 11892, loss = 0.04572134
Iteration 11893, loss = 0.04571680
Iteration 11894, loss = 0.04571213
Iteration 11895, loss = 0.04570765
Iteration 11896, loss = 0.04570313
Iteration 11897, loss = 0.04569854
Iteration 11898, loss = 0.04569386
Iteration 11899, loss = 0.04568936
Iteration 11900, loss = 0.04568475
Iteration 11901, loss = 0.04568017
Iteration 11902, loss = 0.04567559
Iteration 11903, loss = 0.04567103
Iteration 11904, loss = 0.04566648
Iteration 11905, loss = 0.04566189
Iteration 11906, loss = 0.04565733
Iteration 11907, loss = 0.04565273
Iteration 11908, loss = 0.04564822
Iteration 11909, loss = 0.04564367
Iteration 11910, loss = 0.04563905
Iteration 11911, loss = 0.04563452
Iteration 11912, loss = 0.04562994
Iteration 11913, loss = 0.04562547
Iteration 11914, loss = 0.04562080
Iteration 11915, loss = 0.04561632
Iteration 11916, loss = 0.04561173
Iteration 11917, loss = 0.04560715
Iteration 11918, loss = 0.04560259
Iteration 11919, loss = 0.04559808
Iteration 11920, loss = 0.04559353
Iteration 11921, loss = 0.04558905
Iteration 11922, loss = 0.04558443
Iteration 11923, loss = 0.04557989
Iteration 11924, loss = 0.04557538
Iteration 11925, loss = 0.04557081
Iteration 11926, loss = 0.04556633
Iteration 11927, loss = 0.04556178
Iteration 11928, loss = 0.04555733
Iteration 11929, loss = 0.04555274
Iteration 11930, loss = 0.04554818
Iteration 11931, loss = 0.04554365
Iteration 11932, loss = 0.04553912
Iteration 11933, loss = 0.04553461
Iteration 11934, loss = 0.04553009
Iteration 11935, loss = 0.04552551
Iteration 11936, loss = 0.04552104
Iteration 11937, loss = 0.04551647
Iteration 11938, loss = 0.04551198
Iteration 11939, loss = 0.04550740
Iteration 11940, loss = 0.04550282
Iteration 11941, loss = 0.04549835
Iteration 11942, loss = 0.04549377
Iteration 11943, loss = 0.04548922
Iteration 11944, loss = 0.04548472
Iteration 11945, loss = 0.04548017
Iteration 11946, loss = 0.04547565
Iteration 11947, loss = 0.04547115
Iteration 11948, loss = 0.04546658
Iteration 11949, loss = 0.04546208
Iteration 11950, loss = 0.04545752
Iteration 11951, loss = 0.04545301
Iteration 11952, loss = 0.04544843
Iteration 11953, loss = 0.04544393
Iteration 11954, loss = 0.04543938
Iteration 11955, loss = 0.04543485
Iteration 11956, loss = 0.04543030
Iteration 11957, loss = 0.04542582
Iteration 11958, loss = 0.04542125
Iteration 11959, loss = 0.04541678
Iteration 11960, loss = 0.04541224
Iteration 11961, loss = 0.04540766
Iteration 11962, loss = 0.04540317
Iteration 11963, loss = 0.04539863
Iteration 11964, loss = 0.04539416
Iteration 11965, loss = 0.04538956
Iteration 11966, loss = 0.04538509
Iteration 11967, loss = 0.04538056
Iteration 11968, loss = 0.04537606
Iteration 11969, loss = 0.04537156
Iteration 11970, loss = 0.04536699
Iteration 11971, loss = 0.04536251
Iteration 11972, loss = 0.04535800
Iteration 11973, loss = 0.04535349
Iteration 11974, loss = 0.04534895
Iteration 11975, loss = 0.04534447
Iteration 11976, loss = 0.04533996
Iteration 11977, loss = 0.04533540
Iteration 11978, loss = 0.04533093
Iteration 11979, loss = 0.04532638
Iteration 11980, loss = 0.04532190
Iteration 11981, loss = 0.04531738
Iteration 11982, loss = 0.04531285
Iteration 11983, loss = 0.04530837
Iteration 11984, loss = 0.04530388
Iteration 11985, loss = 0.04529936
Iteration 11986, loss = 0.04529485
Iteration 11987, loss = 0.04529045
Iteration 11988, loss = 0.04528587
Iteration 11989, loss = 0.04528143
Iteration 11990, loss = 0.04527690
Iteration 11991, loss = 0.04527248
Iteration 11992, loss = 0.04526798
Iteration 11993, loss = 0.04526346
Iteration 11994, loss = 0.04525897
Iteration 11995, loss = 0.04525448
Iteration 11996, loss = 0.04524998
Iteration 11997, loss = 0.04524554
Iteration 11998, loss = 0.04524107
Iteration 11999, loss = 0.04523647
Iteration 12000, loss = 0.04523206
Iteration 12001, loss = 0.04522758
Iteration 12002, loss = 0.04522310
Iteration 12003, loss = 0.04521860
Iteration 12004, loss = 0.04521412
Iteration 12005, loss = 0.04520962
Iteration 12006, loss = 0.04520516
Iteration 12007, loss = 0.04520068
Iteration 12008, loss = 0.04519617
Iteration 12009, loss = 0.04519169
Iteration 12010, loss = 0.04518722
Iteration 12011, loss = 0.04518275
Iteration 12012, loss = 0.04517828
Iteration 12013, loss = 0.04517378
Iteration 12014, loss = 0.04516936
Iteration 12015, loss = 0.04516494
Iteration 12016, loss = 0.04516041
Iteration 12017, loss = 0.04515600
Iteration 12018, loss = 0.04515144
Iteration 12019, loss = 0.04514696
Iteration 12020, loss = 0.04514246
Iteration 12021, loss = 0.04513800
Iteration 12022, loss = 0.04513355
Iteration 12023, loss = 0.04512908
Iteration 12024, loss = 0.04512461
Iteration 12025, loss = 0.04512020
Iteration 12026, loss = 0.04511570
Iteration 12027, loss = 0.04511120
Iteration 12028, loss = 0.04510676
Iteration 12029, loss = 0.04510234
Iteration 12030, loss = 0.04509785
Iteration 12031, loss = 0.04509338
Iteration 12032, loss = 0.04508894
Iteration 12033, loss = 0.04508451
Iteration 12034, loss = 0.04508002
Iteration 12035, loss = 0.04507561
Iteration 12036, loss = 0.04507116
Iteration 12037, loss = 0.04506671
Iteration 12038, loss = 0.04506224
Iteration 12039, loss = 0.04505775
Iteration 12040, loss = 0.04505329
Iteration 12041, loss = 0.04504882
Iteration 12042, loss = 0.04504440
Iteration 12043, loss = 0.04503996
Iteration 12044, loss = 0.04503553
Iteration 12045, loss = 0.04503107
Iteration 12046, loss = 0.04502660
Iteration 12047, loss = 0.04502218
Iteration 12048, loss = 0.04501774
Iteration 12049, loss = 0.04501331
Iteration 12050, loss = 0.04500882
Iteration 12051, loss = 0.04500439
Iteration 12052, loss = 0.04499991
Iteration 12053, loss = 0.04499547
Iteration 12054, loss = 0.04499105
Iteration 12055, loss = 0.04498658
Iteration 12056, loss = 0.04498215
Iteration 12057, loss = 0.04497768
Iteration 12058, loss = 0.04497329
Iteration 12059, loss = 0.04496881
Iteration 12060, loss = 0.04496441
Iteration 12061, loss = 0.04495995
Iteration 12062, loss = 0.04495556
Iteration 12063, loss = 0.04495106
Iteration 12064, loss = 0.04494668
Iteration 12065, loss = 0.04494222
Iteration 12066, loss = 0.04493778
Iteration 12067, loss = 0.04493334
Iteration 12068, loss = 0.04492896
Iteration 12069, loss = 0.04492448
Iteration 12070, loss = 0.04492008
Iteration 12071, loss = 0.04491571
Iteration 12072, loss = 0.04491121
Iteration 12073, loss = 0.04490678
Iteration 12074, loss = 0.04490241
Iteration 12075, loss = 0.04489794
Iteration 12076, loss = 0.04489359
Iteration 12077, loss = 0.04488912
Iteration 12078, loss = 0.04488468
Iteration 12079, loss = 0.04488033
Iteration 12080, loss = 0.04487582
Iteration 12081, loss = 0.04487144
Iteration 12082, loss = 0.04486700
Iteration 12083, loss = 0.04486265
Iteration 12084, loss = 0.04485824
Iteration 12085, loss = 0.04485379
Iteration 12086, loss = 0.04484936
Iteration 12087, loss = 0.04484496
Iteration 12088, loss = 0.04484050
Iteration 12089, loss = 0.04483611
Iteration 12090, loss = 0.04483167
Iteration 12091, loss = 0.04482730
Iteration 12092, loss = 0.04482290
Iteration 12093, loss = 0.04481844
Iteration 12094, loss = 0.04481406
Iteration 12095, loss = 0.04480964
Iteration 12096, loss = 0.04480523
Iteration 12097, loss = 0.04480091
Iteration 12098, loss = 0.04479639
Iteration 12099, loss = 0.04479202
Iteration 12100, loss = 0.04478762
Iteration 12101, loss = 0.04478325
Iteration 12102, loss = 0.04477882
Iteration 12103, loss = 0.04477441
Iteration 12104, loss = 0.04477005
Iteration 12105, loss = 0.04476563
Iteration 12106, loss = 0.04476126
Iteration 12107, loss = 0.04475685
Iteration 12108, loss = 0.04475252
Iteration 12109, loss = 0.04474805
Iteration 12110, loss = 0.04474369
Iteration 12111, loss = 0.04473931
Iteration 12112, loss = 0.04473489
Iteration 12113, loss = 0.04473054
Iteration 12114, loss = 0.04472617
Iteration 12115, loss = 0.04472179
Iteration 12116, loss = 0.04471737
Iteration 12117, loss = 0.04471300
Iteration 12118, loss = 0.04470859
Iteration 12119, loss = 0.04470424
Iteration 12120, loss = 0.04469982
Iteration 12121, loss = 0.04469549
Iteration 12122, loss = 0.04469112
Iteration 12123, loss = 0.04468679
Iteration 12124, loss = 0.04468234
Iteration 12125, loss = 0.04467799
Iteration 12126, loss = 0.04467367
Iteration 12127, loss = 0.04466923
Iteration 12128, loss = 0.04466494
Iteration 12129, loss = 0.04466047
Iteration 12130, loss = 0.04465615
Iteration 12131, loss = 0.04465176
Iteration 12132, loss = 0.04464739
Iteration 12133, loss = 0.04464306
Iteration 12134, loss = 0.04463869
Iteration 12135, loss = 0.04463436
Iteration 12136, loss = 0.04462997
Iteration 12137, loss = 0.04462559
Iteration 12138, loss = 0.04462129
Iteration 12139, loss = 0.04461689
Iteration 12140, loss = 0.04461257
Iteration 12141, loss = 0.04460818
Iteration 12142, loss = 0.04460386
Iteration 12143, loss = 0.04459944
Iteration 12144, loss = 0.04459510
Iteration 12145, loss = 0.04459073
Iteration 12146, loss = 0.04458637
Iteration 12147, loss = 0.04458204
Iteration 12148, loss = 0.04457766
Iteration 12149, loss = 0.04457333
Iteration 12150, loss = 0.04456897
Iteration 12151, loss = 0.04456464
Iteration 12152, loss = 0.04456030
Iteration 12153, loss = 0.04455584
Iteration 12154, loss = 0.04455150
Iteration 12155, loss = 0.04454720
Iteration 12156, loss = 0.04454286
Iteration 12157, loss = 0.04453847
Iteration 12158, loss = 0.04453416
Iteration 12159, loss = 0.04452980
Iteration 12160, loss = 0.04452548
Iteration 12161, loss = 0.04452112
Iteration 12162, loss = 0.04451685
Iteration 12163, loss = 0.04451246
Iteration 12164, loss = 0.04450813
Iteration 12165, loss = 0.04450381
Iteration 12166, loss = 0.04449945
Iteration 12167, loss = 0.04449518
Iteration 12168, loss = 0.04449081
Iteration 12169, loss = 0.04448647
Iteration 12170, loss = 0.04448215
Iteration 12171, loss = 0.04447778
Iteration 12172, loss = 0.04447343
Iteration 12173, loss = 0.04446916
Iteration 12174, loss = 0.04446483
Iteration 12175, loss = 0.04446053
Iteration 12176, loss = 0.04445612
Iteration 12177, loss = 0.04445182
Iteration 12178, loss = 0.04444751
Iteration 12179, loss = 0.04444314
Iteration 12180, loss = 0.04443881
Iteration 12181, loss = 0.04443452
Iteration 12182, loss = 0.04443015
Iteration 12183, loss = 0.04442586
Iteration 12184, loss = 0.04442151
Iteration 12185, loss = 0.04441719
Iteration 12186, loss = 0.04441287
Iteration 12187, loss = 0.04440854
Iteration 12188, loss = 0.04440421
Iteration 12189, loss = 0.04439988
Iteration 12190, loss = 0.04439556
Iteration 12191, loss = 0.04439120
Iteration 12192, loss = 0.04438692
Iteration 12193, loss = 0.04438259
Iteration 12194, loss = 0.04437825
Iteration 12195, loss = 0.04437397
Iteration 12196, loss = 0.04436963
Iteration 12197, loss = 0.04436530
Iteration 12198, loss = 0.04436116
Iteration 12199, loss = 0.04435671
Iteration 12200, loss = 0.04435239
Iteration 12201, loss = 0.04434804
Iteration 12202, loss = 0.04434381
Iteration 12203, loss = 0.04433948
Iteration 12204, loss = 0.04433514
Iteration 12205, loss = 0.04433085
Iteration 12206, loss = 0.04432660
Iteration 12207, loss = 0.04432228
Iteration 12208, loss = 0.04431796
Iteration 12209, loss = 0.04431364
Iteration 12210, loss = 0.04430939
Iteration 12211, loss = 0.04430503
Iteration 12212, loss = 0.04430079
Iteration 12213, loss = 0.04429651
Iteration 12214, loss = 0.04429222
Iteration 12215, loss = 0.04428789
Iteration 12216, loss = 0.04428367
Iteration 12217, loss = 0.04427931
Iteration 12218, loss = 0.04427502
Iteration 12219, loss = 0.04427076
Iteration 12220, loss = 0.04426644
Iteration 12221, loss = 0.04426211
Iteration 12222, loss = 0.04425788
Iteration 12223, loss = 0.04425351
Iteration 12224, loss = 0.04424922
Iteration 12225, loss = 0.04424496
Iteration 12226, loss = 0.04424066
Iteration 12227, loss = 0.04423639
Iteration 12228, loss = 0.04423202
Iteration 12229, loss = 0.04422776
Iteration 12230, loss = 0.04422350
Iteration 12231, loss = 0.04421920
Iteration 12232, loss = 0.04421492
Iteration 12233, loss = 0.04421062
Iteration 12234, loss = 0.04420633
Iteration 12235, loss = 0.04420204
Iteration 12236, loss = 0.04419778
Iteration 12237, loss = 0.04419342
Iteration 12238, loss = 0.04418927
Iteration 12239, loss = 0.04418490
Iteration 12240, loss = 0.04418058
Iteration 12241, loss = 0.04417634
Iteration 12242, loss = 0.04417207
Iteration 12243, loss = 0.04416779
Iteration 12244, loss = 0.04416351
Iteration 12245, loss = 0.04415922
Iteration 12246, loss = 0.04415500
Iteration 12247, loss = 0.04415070
Iteration 12248, loss = 0.04414649
Iteration 12249, loss = 0.04414219
Iteration 12250, loss = 0.04413789
Iteration 12251, loss = 0.04413368
Iteration 12252, loss = 0.04412940
Iteration 12253, loss = 0.04412517
Iteration 12254, loss = 0.04412093
Iteration 12255, loss = 0.04411666
Iteration 12256, loss = 0.04411238
Iteration 12257, loss = 0.04410815
Iteration 12258, loss = 0.04410393
Iteration 12259, loss = 0.04409960
Iteration 12260, loss = 0.04409538
Iteration 12261, loss = 0.04409113
Iteration 12262, loss = 0.04408688
Iteration 12263, loss = 0.04408258
Iteration 12264, loss = 0.04407836
Iteration 12265, loss = 0.04407409
Iteration 12266, loss = 0.04406983
Iteration 12267, loss = 0.04406560
Iteration 12268, loss = 0.04406134
Iteration 12269, loss = 0.04405705
Iteration 12270, loss = 0.04405282
Iteration 12271, loss = 0.04404861
Iteration 12272, loss = 0.04404435
Iteration 12273, loss = 0.04404007
Iteration 12274, loss = 0.04403587
Iteration 12275, loss = 0.04403160
Iteration 12276, loss = 0.04402742
Iteration 12277, loss = 0.04402314
Iteration 12278, loss = 0.04401895
Iteration 12279, loss = 0.04401469
Iteration 12280, loss = 0.04401052
Iteration 12281, loss = 0.04400633
Iteration 12282, loss = 0.04400207
Iteration 12283, loss = 0.04399781
Iteration 12284, loss = 0.04399358
Iteration 12285, loss = 0.04398941
Iteration 12286, loss = 0.04398515
Iteration 12287, loss = 0.04398092
Iteration 12288, loss = 0.04397672
Iteration 12289, loss = 0.04397255
Iteration 12290, loss = 0.04396827
Iteration 12291, loss = 0.04396406
Iteration 12292, loss = 0.04395983
Iteration 12293, loss = 0.04395558
Iteration 12294, loss = 0.04395132
Iteration 12295, loss = 0.04394715
Iteration 12296, loss = 0.04394284
Iteration 12297, loss = 0.04393869
Iteration 12298, loss = 0.04393439
Iteration 12299, loss = 0.04393019
Iteration 12300, loss = 0.04392591
Iteration 12301, loss = 0.04392175
Iteration 12302, loss = 0.04391754
Iteration 12303, loss = 0.04391327
Iteration 12304, loss = 0.04390902
Iteration 12305, loss = 0.04390487
Iteration 12306, loss = 0.04390062
Iteration 12307, loss = 0.04389630
Iteration 12308, loss = 0.04389213
Iteration 12309, loss = 0.04388787
Iteration 12310, loss = 0.04388366
Iteration 12311, loss = 0.04387942
Iteration 12312, loss = 0.04387523
Iteration 12313, loss = 0.04387097
Iteration 12314, loss = 0.04386679
Iteration 12315, loss = 0.04386255
Iteration 12316, loss = 0.04385833
Iteration 12317, loss = 0.04385408
Iteration 12318, loss = 0.04384985
Iteration 12319, loss = 0.04384568
Iteration 12320, loss = 0.04384145
Iteration 12321, loss = 0.04383729
Iteration 12322, loss = 0.04383300
Iteration 12323, loss = 0.04382887
Iteration 12324, loss = 0.04382461
Iteration 12325, loss = 0.04382039
Iteration 12326, loss = 0.04381620
Iteration 12327, loss = 0.04381205
Iteration 12328, loss = 0.04380782
Iteration 12329, loss = 0.04380365
Iteration 12330, loss = 0.04379945
Iteration 12331, loss = 0.04379528
Iteration 12332, loss = 0.04379105
Iteration 12333, loss = 0.04378684
Iteration 12334, loss = 0.04378270
Iteration 12335, loss = 0.04377847
Iteration 12336, loss = 0.04377427
Iteration 12337, loss = 0.04377008
Iteration 12338, loss = 0.04376588
Iteration 12339, loss = 0.04376167
Iteration 12340, loss = 0.04375750
Iteration 12341, loss = 0.04375336
Iteration 12342, loss = 0.04374911
Iteration 12343, loss = 0.04374490
Iteration 12344, loss = 0.04374068
Iteration 12345, loss = 0.04373646
Iteration 12346, loss = 0.04373233
Iteration 12347, loss = 0.04372807
Iteration 12348, loss = 0.04372391
Iteration 12349, loss = 0.04371964
Iteration 12350, loss = 0.04371546
Iteration 12351, loss = 0.04371129
Iteration 12352, loss = 0.04370713
Iteration 12353, loss = 0.04370285
Iteration 12354, loss = 0.04369868
Iteration 12355, loss = 0.04369452
Iteration 12356, loss = 0.04369030
Iteration 12357, loss = 0.04368608
Iteration 12358, loss = 0.04368197
Iteration 12359, loss = 0.04367770
Iteration 12360, loss = 0.04367358
Iteration 12361, loss = 0.04366938
Iteration 12362, loss = 0.04366517
Iteration 12363, loss = 0.04366100
Iteration 12364, loss = 0.04365683
Iteration 12365, loss = 0.04365264
Iteration 12366, loss = 0.04364849
Iteration 12367, loss = 0.04364430
Iteration 12368, loss = 0.04364018
Iteration 12369, loss = 0.04363598
Iteration 12370, loss = 0.04363176
Iteration 12371, loss = 0.04362763
Iteration 12372, loss = 0.04362343
Iteration 12373, loss = 0.04361926
Iteration 12374, loss = 0.04361506
Iteration 12375, loss = 0.04361088
Iteration 12376, loss = 0.04360677
Iteration 12377, loss = 0.04360256
Iteration 12378, loss = 0.04359844
Iteration 12379, loss = 0.04359424
Iteration 12380, loss = 0.04359006
Iteration 12381, loss = 0.04358589
Iteration 12382, loss = 0.04358174
Iteration 12383, loss = 0.04357757
Iteration 12384, loss = 0.04357345
Iteration 12385, loss = 0.04356925
Iteration 12386, loss = 0.04356502
Iteration 12387, loss = 0.04356092
Iteration 12388, loss = 0.04355676
Iteration 12389, loss = 0.04355260
Iteration 12390, loss = 0.04354843
Iteration 12391, loss = 0.04354423
Iteration 12392, loss = 0.04354009
Iteration 12393, loss = 0.04353593
Iteration 12394, loss = 0.04353180
Iteration 12395, loss = 0.04352760
Iteration 12396, loss = 0.04352345
Iteration 12397, loss = 0.04351936
Iteration 12398, loss = 0.04351515
Iteration 12399, loss = 0.04351099
Iteration 12400, loss = 0.04350690
Iteration 12401, loss = 0.04350267
Iteration 12402, loss = 0.04349854
Iteration 12403, loss = 0.04349442
Iteration 12404, loss = 0.04349023
Iteration 12405, loss = 0.04348608
Iteration 12406, loss = 0.04348204
Iteration 12407, loss = 0.04347780
Iteration 12408, loss = 0.04347365
Iteration 12409, loss = 0.04346951
Iteration 12410, loss = 0.04346535
Iteration 12411, loss = 0.04346129
Iteration 12412, loss = 0.04345712
Iteration 12413, loss = 0.04345305
Iteration 12414, loss = 0.04344887
Iteration 12415, loss = 0.04344470
Iteration 12416, loss = 0.04344058
Iteration 12417, loss = 0.04343651
Iteration 12418, loss = 0.04343230
Iteration 12419, loss = 0.04342820
Iteration 12420, loss = 0.04342408
Iteration 12421, loss = 0.04341992
Iteration 12422, loss = 0.04341580
Iteration 12423, loss = 0.04341170
Iteration 12424, loss = 0.04340754
Iteration 12425, loss = 0.04340341
Iteration 12426, loss = 0.04339928
Iteration 12427, loss = 0.04339518
Iteration 12428, loss = 0.04339103
Iteration 12429, loss = 0.04338691
Iteration 12430, loss = 0.04338279
Iteration 12431, loss = 0.04337871
Iteration 12432, loss = 0.04337457
Iteration 12433, loss = 0.04337042
Iteration 12434, loss = 0.04336635
Iteration 12435, loss = 0.04336221
Iteration 12436, loss = 0.04335810
Iteration 12437, loss = 0.04335397
Iteration 12438, loss = 0.04334983
Iteration 12439, loss = 0.04334577
Iteration 12440, loss = 0.04334163
Iteration 12441, loss = 0.04333753
Iteration 12442, loss = 0.04333337
Iteration 12443, loss = 0.04332927
Iteration 12444, loss = 0.04332517
Iteration 12445, loss = 0.04332104
Iteration 12446, loss = 0.04331695
Iteration 12447, loss = 0.04331277
Iteration 12448, loss = 0.04330873
Iteration 12449, loss = 0.04330459
Iteration 12450, loss = 0.04330045
Iteration 12451, loss = 0.04329637
Iteration 12452, loss = 0.04329226
Iteration 12453, loss = 0.04328813
Iteration 12454, loss = 0.04328403
Iteration 12455, loss = 0.04327993
Iteration 12456, loss = 0.04327582
Iteration 12457, loss = 0.04327175
Iteration 12458, loss = 0.04326760
Iteration 12459, loss = 0.04326353
Iteration 12460, loss = 0.04325940
Iteration 12461, loss = 0.04325531
Iteration 12462, loss = 0.04325128
Iteration 12463, loss = 0.04324720
Iteration 12464, loss = 0.04324308
Iteration 12465, loss = 0.04323897
Iteration 12466, loss = 0.04323485
Iteration 12467, loss = 0.04323073
Iteration 12468, loss = 0.04322668
Iteration 12469, loss = 0.04322263
Iteration 12470, loss = 0.04321851
Iteration 12471, loss = 0.04321443
Iteration 12472, loss = 0.04321033
Iteration 12473, loss = 0.04320619
Iteration 12474, loss = 0.04320212
Iteration 12475, loss = 0.04319806
Iteration 12476, loss = 0.04319391
Iteration 12477, loss = 0.04318986
Iteration 12478, loss = 0.04318579
Iteration 12479, loss = 0.04318167
Iteration 12480, loss = 0.04317758
Iteration 12481, loss = 0.04317357
Iteration 12482, loss = 0.04316943
Iteration 12483, loss = 0.04316534
Iteration 12484, loss = 0.04316122
Iteration 12485, loss = 0.04315714
Iteration 12486, loss = 0.04315308
Iteration 12487, loss = 0.04314899
Iteration 12488, loss = 0.04314492
Iteration 12489, loss = 0.04314083
Iteration 12490, loss = 0.04313678
Iteration 12491, loss = 0.04313268
Iteration 12492, loss = 0.04312860
Iteration 12493, loss = 0.04312449
Iteration 12494, loss = 0.04312048
Iteration 12495, loss = 0.04311636
Iteration 12496, loss = 0.04311229
Iteration 12497, loss = 0.04310823
Iteration 12498, loss = 0.04310416
Iteration 12499, loss = 0.04310005
Iteration 12500, loss = 0.04309601
Iteration 12501, loss = 0.04309193
Iteration 12502, loss = 0.04308786
Iteration 12503, loss = 0.04308375
Iteration 12504, loss = 0.04307971
Iteration 12505, loss = 0.04307561
Iteration 12506, loss = 0.04307164
Iteration 12507, loss = 0.04306747
Iteration 12508, loss = 0.04306344
Iteration 12509, loss = 0.04305941
Iteration 12510, loss = 0.04305532
Iteration 12511, loss = 0.04305124
Iteration 12512, loss = 0.04304719
Iteration 12513, loss = 0.04304312
Iteration 12514, loss = 0.04303912
Iteration 12515, loss = 0.04303505
Iteration 12516, loss = 0.04303101
Iteration 12517, loss = 0.04302694
Iteration 12518, loss = 0.04302285
Iteration 12519, loss = 0.04301881
Iteration 12520, loss = 0.04301475
Iteration 12521, loss = 0.04301065
Iteration 12522, loss = 0.04300660
Iteration 12523, loss = 0.04300255
Iteration 12524, loss = 0.04299851
Iteration 12525, loss = 0.04299442
Iteration 12526, loss = 0.04299036
Iteration 12527, loss = 0.04298630
Iteration 12528, loss = 0.04298221
Iteration 12529, loss = 0.04297819
Iteration 12530, loss = 0.04297413
Iteration 12531, loss = 0.04297011
Iteration 12532, loss = 0.04296605
Iteration 12533, loss = 0.04296200
Iteration 12534, loss = 0.04295792
Iteration 12535, loss = 0.04295390
Iteration 12536, loss = 0.04294984
Iteration 12537, loss = 0.04294580
Iteration 12538, loss = 0.04294172
Iteration 12539, loss = 0.04293772
Iteration 12540, loss = 0.04293367
Iteration 12541, loss = 0.04292966
Iteration 12542, loss = 0.04292565
Iteration 12543, loss = 0.04292168
Iteration 12544, loss = 0.04291759
Iteration 12545, loss = 0.04291354
Iteration 12546, loss = 0.04290946
Iteration 12547, loss = 0.04290545
Iteration 12548, loss = 0.04290143
Iteration 12549, loss = 0.04289742
Iteration 12550, loss = 0.04289337
Iteration 12551, loss = 0.04288936
Iteration 12552, loss = 0.04288534
Iteration 12553, loss = 0.04288130
Iteration 12554, loss = 0.04287727
Iteration 12555, loss = 0.04287324
Iteration 12556, loss = 0.04286919
Iteration 12557, loss = 0.04286525
Iteration 12558, loss = 0.04286116
Iteration 12559, loss = 0.04285718
Iteration 12560, loss = 0.04285311
Iteration 12561, loss = 0.04284914
Iteration 12562, loss = 0.04284509
Iteration 12563, loss = 0.04284106
Iteration 12564, loss = 0.04283707
Iteration 12565, loss = 0.04283306
Iteration 12566, loss = 0.04282908
Iteration 12567, loss = 0.04282504
Iteration 12568, loss = 0.04282099
Iteration 12569, loss = 0.04281702
Iteration 12570, loss = 0.04281301
Iteration 12571, loss = 0.04280896
Iteration 12572, loss = 0.04280493
Iteration 12573, loss = 0.04280096
Iteration 12574, loss = 0.04279689
Iteration 12575, loss = 0.04279291
Iteration 12576, loss = 0.04278889
Iteration 12577, loss = 0.04278489
Iteration 12578, loss = 0.04278090
Iteration 12579, loss = 0.04277678
Iteration 12580, loss = 0.04277285
Iteration 12581, loss = 0.04276880
Iteration 12582, loss = 0.04276487
Iteration 12583, loss = 0.04276076
Iteration 12584, loss = 0.04275676
Iteration 12585, loss = 0.04275277
Iteration 12586, loss = 0.04274877
Iteration 12587, loss = 0.04274472
Iteration 12588, loss = 0.04274074
Iteration 12589, loss = 0.04273673
Iteration 12590, loss = 0.04273271
Iteration 12591, loss = 0.04272874
Iteration 12592, loss = 0.04272470
Iteration 12593, loss = 0.04272069
Iteration 12594, loss = 0.04271670
Iteration 12595, loss = 0.04271268
Iteration 12596, loss = 0.04270868
Iteration 12597, loss = 0.04270468
Iteration 12598, loss = 0.04270070
Iteration 12599, loss = 0.04269665
Iteration 12600, loss = 0.04269269
Iteration 12601, loss = 0.04268868
Iteration 12602, loss = 0.04268459
Iteration 12603, loss = 0.04268065
Iteration 12604, loss = 0.04267668
Iteration 12605, loss = 0.04267268
Iteration 12606, loss = 0.04266871
Iteration 12607, loss = 0.04266461
Iteration 12608, loss = 0.04266065
Iteration 12609, loss = 0.04265672
Iteration 12610, loss = 0.04265270
Iteration 12611, loss = 0.04264869
Iteration 12612, loss = 0.04264473
Iteration 12613, loss = 0.04264075
Iteration 12614, loss = 0.04263676
Iteration 12615, loss = 0.04263277
Iteration 12616, loss = 0.04262883
Iteration 12617, loss = 0.04262479
Iteration 12618, loss = 0.04262088
Iteration 12619, loss = 0.04261688
Iteration 12620, loss = 0.04261291
Iteration 12621, loss = 0.04260892
Iteration 12622, loss = 0.04260496
Iteration 12623, loss = 0.04260095
Iteration 12624, loss = 0.04259708
Iteration 12625, loss = 0.04259307
Iteration 12626, loss = 0.04258909
Iteration 12627, loss = 0.04258512
Iteration 12628, loss = 0.04258119
Iteration 12629, loss = 0.04257720
Iteration 12630, loss = 0.04257322
Iteration 12631, loss = 0.04256928
Iteration 12632, loss = 0.04256533
Iteration 12633, loss = 0.04256136
Iteration 12634, loss = 0.04255737
Iteration 12635, loss = 0.04255344
Iteration 12636, loss = 0.04254946
Iteration 12637, loss = 0.04254555
Iteration 12638, loss = 0.04254158
Iteration 12639, loss = 0.04253756
Iteration 12640, loss = 0.04253362
Iteration 12641, loss = 0.04252972
Iteration 12642, loss = 0.04252566
Iteration 12643, loss = 0.04252172
Iteration 12644, loss = 0.04251771
Iteration 12645, loss = 0.04251378
Iteration 12646, loss = 0.04250983
Iteration 12647, loss = 0.04250588
Iteration 12648, loss = 0.04250187
Iteration 12649, loss = 0.04249792
Iteration 12650, loss = 0.04249398
Iteration 12651, loss = 0.04249005
Iteration 12652, loss = 0.04248607
Iteration 12653, loss = 0.04248208
Iteration 12654, loss = 0.04247814
Iteration 12655, loss = 0.04247416
Iteration 12656, loss = 0.04247019
Iteration 12657, loss = 0.04246629
Iteration 12658, loss = 0.04246228
Iteration 12659, loss = 0.04245834
Iteration 12660, loss = 0.04245441
Iteration 12661, loss = 0.04245042
Iteration 12662, loss = 0.04244653
Iteration 12663, loss = 0.04244247
Iteration 12664, loss = 0.04243855
Iteration 12665, loss = 0.04243470
Iteration 12666, loss = 0.04243069
Iteration 12667, loss = 0.04242670
Iteration 12668, loss = 0.04242278
Iteration 12669, loss = 0.04241886
Iteration 12670, loss = 0.04241493
Iteration 12671, loss = 0.04241097
Iteration 12672, loss = 0.04240701
Iteration 12673, loss = 0.04240311
Iteration 12674, loss = 0.04239914
Iteration 12675, loss = 0.04239520
Iteration 12676, loss = 0.04239128
Iteration 12677, loss = 0.04238728
Iteration 12678, loss = 0.04238339
Iteration 12679, loss = 0.04237947
Iteration 12680, loss = 0.04237549
Iteration 12681, loss = 0.04237160
Iteration 12682, loss = 0.04236762
Iteration 12683, loss = 0.04236369
Iteration 12684, loss = 0.04235976
Iteration 12685, loss = 0.04235579
Iteration 12686, loss = 0.04235184
Iteration 12687, loss = 0.04234794
Iteration 12688, loss = 0.04234402
Iteration 12689, loss = 0.04234009
Iteration 12690, loss = 0.04233613
Iteration 12691, loss = 0.04233219
Iteration 12692, loss = 0.04232826
Iteration 12693, loss = 0.04232436
Iteration 12694, loss = 0.04232043
Iteration 12695, loss = 0.04231647
Iteration 12696, loss = 0.04231256
Iteration 12697, loss = 0.04230867
Iteration 12698, loss = 0.04230470
Iteration 12699, loss = 0.04230077
Iteration 12700, loss = 0.04229684
Iteration 12701, loss = 0.04229290
Iteration 12702, loss = 0.04228902
Iteration 12703, loss = 0.04228505
Iteration 12704, loss = 0.04228125
Iteration 12705, loss = 0.04227726
Iteration 12706, loss = 0.04227332
Iteration 12707, loss = 0.04226940
Iteration 12708, loss = 0.04226548
Iteration 12709, loss = 0.04226154
Iteration 12710, loss = 0.04225769
Iteration 12711, loss = 0.04225374
Iteration 12712, loss = 0.04224982
Iteration 12713, loss = 0.04224592
Iteration 12714, loss = 0.04224199
Iteration 12715, loss = 0.04223814
Iteration 12716, loss = 0.04223415
Iteration 12717, loss = 0.04223028
Iteration 12718, loss = 0.04222636
Iteration 12719, loss = 0.04222249
Iteration 12720, loss = 0.04221854
Iteration 12721, loss = 0.04221463
Iteration 12722, loss = 0.04221079
Iteration 12723, loss = 0.04220682
Iteration 12724, loss = 0.04220295
Iteration 12725, loss = 0.04219904
Iteration 12726, loss = 0.04219513
Iteration 12727, loss = 0.04219123
Iteration 12728, loss = 0.04218733
Iteration 12729, loss = 0.04218346
Iteration 12730, loss = 0.04217954
Iteration 12731, loss = 0.04217566
Iteration 12732, loss = 0.04217178
Iteration 12733, loss = 0.04216784
Iteration 12734, loss = 0.04216401
Iteration 12735, loss = 0.04216006
Iteration 12736, loss = 0.04215621
Iteration 12737, loss = 0.04215227
Iteration 12738, loss = 0.04214843
Iteration 12739, loss = 0.04214451
Iteration 12740, loss = 0.04214063
Iteration 12741, loss = 0.04213675
Iteration 12742, loss = 0.04213287
Iteration 12743, loss = 0.04212893
Iteration 12744, loss = 0.04212507
Iteration 12745, loss = 0.04212119
Iteration 12746, loss = 0.04211727
Iteration 12747, loss = 0.04211336
Iteration 12748, loss = 0.04210949
Iteration 12749, loss = 0.04210560
Iteration 12750, loss = 0.04210171
Iteration 12751, loss = 0.04209787
Iteration 12752, loss = 0.04209397
Iteration 12753, loss = 0.04209007
Iteration 12754, loss = 0.04208623
Iteration 12755, loss = 0.04208232
Iteration 12756, loss = 0.04207848
Iteration 12757, loss = 0.04207460
Iteration 12758, loss = 0.04207074
Iteration 12759, loss = 0.04206687
Iteration 12760, loss = 0.04206302
Iteration 12761, loss = 0.04205916
Iteration 12762, loss = 0.04205525
Iteration 12763, loss = 0.04205139
Iteration 12764, loss = 0.04204752
Iteration 12765, loss = 0.04204363
Iteration 12766, loss = 0.04203978
Iteration 12767, loss = 0.04203595
Iteration 12768, loss = 0.04203208
Iteration 12769, loss = 0.04202818
Iteration 12770, loss = 0.04202434
Iteration 12771, loss = 0.04202051
Iteration 12772, loss = 0.04201661
Iteration 12773, loss = 0.04201270
Iteration 12774, loss = 0.04200889
Iteration 12775, loss = 0.04200506
Iteration 12776, loss = 0.04200115
Iteration 12777, loss = 0.04199730
Iteration 12778, loss = 0.04199341
Iteration 12779, loss = 0.04198952
Iteration 12780, loss = 0.04198572
Iteration 12781, loss = 0.04198178
Iteration 12782, loss = 0.04197795
Iteration 12783, loss = 0.04197410
Iteration 12784, loss = 0.04197021
Iteration 12785, loss = 0.04196639
Iteration 12786, loss = 0.04196254
Iteration 12787, loss = 0.04195865
Iteration 12788, loss = 0.04195480
Iteration 12789, loss = 0.04195093
Iteration 12790, loss = 0.04194712
Iteration 12791, loss = 0.04194323
Iteration 12792, loss = 0.04193935
Iteration 12793, loss = 0.04193552
Iteration 12794, loss = 0.04193163
Iteration 12795, loss = 0.04192775
Iteration 12796, loss = 0.04192395
Iteration 12797, loss = 0.04192006
Iteration 12798, loss = 0.04191621
Iteration 12799, loss = 0.04191239
Iteration 12800, loss = 0.04190852
Iteration 12801, loss = 0.04190464
Iteration 12802, loss = 0.04190081
Iteration 12803, loss = 0.04189699
Iteration 12804, loss = 0.04189314
Iteration 12805, loss = 0.04188928
Iteration 12806, loss = 0.04188546
Iteration 12807, loss = 0.04188161
Iteration 12808, loss = 0.04187778
Iteration 12809, loss = 0.04187384
Iteration 12810, loss = 0.04187007
Iteration 12811, loss = 0.04186619
Iteration 12812, loss = 0.04186235
Iteration 12813, loss = 0.04185853
Iteration 12814, loss = 0.04185468
Iteration 12815, loss = 0.04185085
Iteration 12816, loss = 0.04184698
Iteration 12817, loss = 0.04184319
Iteration 12818, loss = 0.04183929
Iteration 12819, loss = 0.04183547
Iteration 12820, loss = 0.04183169
Iteration 12821, loss = 0.04182784
Iteration 12822, loss = 0.04182399
Iteration 12823, loss = 0.04182017
Iteration 12824, loss = 0.04181631
Iteration 12825, loss = 0.04181249
Iteration 12826, loss = 0.04180869
Iteration 12827, loss = 0.04180484
Iteration 12828, loss = 0.04180101
Iteration 12829, loss = 0.04179722
Iteration 12830, loss = 0.04179340
Iteration 12831, loss = 0.04178955
Iteration 12832, loss = 0.04178579
Iteration 12833, loss = 0.04178191
Iteration 12834, loss = 0.04177811
Iteration 12835, loss = 0.04177431
Iteration 12836, loss = 0.04177046
Iteration 12837, loss = 0.04176665
Iteration 12838, loss = 0.04176279
Iteration 12839, loss = 0.04175902
Iteration 12840, loss = 0.04175526
Iteration 12841, loss = 0.04175136
Iteration 12842, loss = 0.04174758
Iteration 12843, loss = 0.04174376
Iteration 12844, loss = 0.04173996
Iteration 12845, loss = 0.04173607
Iteration 12846, loss = 0.04173230
Iteration 12847, loss = 0.04172846
Iteration 12848, loss = 0.04172460
Iteration 12849, loss = 0.04172082
Iteration 12850, loss = 0.04171697
Iteration 12851, loss = 0.04171321
Iteration 12852, loss = 0.04170937
Iteration 12853, loss = 0.04170556
Iteration 12854, loss = 0.04170171
Iteration 12855, loss = 0.04169792
Iteration 12856, loss = 0.04169411
Iteration 12857, loss = 0.04169031
Iteration 12858, loss = 0.04168644
Iteration 12859, loss = 0.04168261
Iteration 12860, loss = 0.04167886
Iteration 12861, loss = 0.04167509
Iteration 12862, loss = 0.04167127
Iteration 12863, loss = 0.04166741
Iteration 12864, loss = 0.04166360
Iteration 12865, loss = 0.04165985
Iteration 12866, loss = 0.04165602
Iteration 12867, loss = 0.04165215
Iteration 12868, loss = 0.04164841
Iteration 12869, loss = 0.04164459
Iteration 12870, loss = 0.04164076
Iteration 12871, loss = 0.04163697
Iteration 12872, loss = 0.04163322
Iteration 12873, loss = 0.04162942
Iteration 12874, loss = 0.04162563
Iteration 12875, loss = 0.04162182
Iteration 12876, loss = 0.04161805
Iteration 12877, loss = 0.04161426
Iteration 12878, loss = 0.04161053
Iteration 12879, loss = 0.04160666
Iteration 12880, loss = 0.04160288
Iteration 12881, loss = 0.04159912
Iteration 12882, loss = 0.04159529
Iteration 12883, loss = 0.04159154
Iteration 12884, loss = 0.04158776
Iteration 12885, loss = 0.04158397
Iteration 12886, loss = 0.04158017
Iteration 12887, loss = 0.04157638
Iteration 12888, loss = 0.04157259
Iteration 12889, loss = 0.04156880
Iteration 12890, loss = 0.04156501
Iteration 12891, loss = 0.04156125
Iteration 12892, loss = 0.04155747
Iteration 12893, loss = 0.04155366
Iteration 12894, loss = 0.04154989
Iteration 12895, loss = 0.04154610
Iteration 12896, loss = 0.04154233
Iteration 12897, loss = 0.04153854
Iteration 12898, loss = 0.04153477
Iteration 12899, loss = 0.04153097
Iteration 12900, loss = 0.04152725
Iteration 12901, loss = 0.04152347
Iteration 12902, loss = 0.04151965
Iteration 12903, loss = 0.04151591
Iteration 12904, loss = 0.04151209
Iteration 12905, loss = 0.04150833
Iteration 12906, loss = 0.04150459
Iteration 12907, loss = 0.04150080
Iteration 12908, loss = 0.04149706
Iteration 12909, loss = 0.04149327
Iteration 12910, loss = 0.04148945
Iteration 12911, loss = 0.04148574
Iteration 12912, loss = 0.04148193
Iteration 12913, loss = 0.04147819
Iteration 12914, loss = 0.04147439
Iteration 12915, loss = 0.04147060
Iteration 12916, loss = 0.04146687
Iteration 12917, loss = 0.04146305
Iteration 12918, loss = 0.04145933
Iteration 12919, loss = 0.04145559
Iteration 12920, loss = 0.04145176
Iteration 12921, loss = 0.04144800
Iteration 12922, loss = 0.04144422
Iteration 12923, loss = 0.04144053
Iteration 12924, loss = 0.04143676
Iteration 12925, loss = 0.04143298
Iteration 12926, loss = 0.04142923
Iteration 12927, loss = 0.04142552
Iteration 12928, loss = 0.04142175
Iteration 12929, loss = 0.04141799
Iteration 12930, loss = 0.04141421
Iteration 12931, loss = 0.04141044
Iteration 12932, loss = 0.04140669
Iteration 12933, loss = 0.04140298
Iteration 12934, loss = 0.04139917
Iteration 12935, loss = 0.04139542
Iteration 12936, loss = 0.04139169
Iteration 12937, loss = 0.04138795
Iteration 12938, loss = 0.04138415
Iteration 12939, loss = 0.04138041
Iteration 12940, loss = 0.04137671
Iteration 12941, loss = 0.04137290
Iteration 12942, loss = 0.04136920
Iteration 12943, loss = 0.04136545
Iteration 12944, loss = 0.04136171
Iteration 12945, loss = 0.04135797
Iteration 12946, loss = 0.04135422
Iteration 12947, loss = 0.04135049
Iteration 12948, loss = 0.04134674
Iteration 12949, loss = 0.04134300
Iteration 12950, loss = 0.04133928
Iteration 12951, loss = 0.04133554
Iteration 12952, loss = 0.04133183
Iteration 12953, loss = 0.04132806
Iteration 12954, loss = 0.04132431
Iteration 12955, loss = 0.04132055
Iteration 12956, loss = 0.04131681
Iteration 12957, loss = 0.04131312
Iteration 12958, loss = 0.04130935
Iteration 12959, loss = 0.04130558
Iteration 12960, loss = 0.04130189
Iteration 12961, loss = 0.04129815
Iteration 12962, loss = 0.04129441
Iteration 12963, loss = 0.04129068
Iteration 12964, loss = 0.04128694
Iteration 12965, loss = 0.04128320
Iteration 12966, loss = 0.04127951
Iteration 12967, loss = 0.04127573
Iteration 12968, loss = 0.04127200
Iteration 12969, loss = 0.04126828
Iteration 12970, loss = 0.04126458
Iteration 12971, loss = 0.04126084
Iteration 12972, loss = 0.04125711
Iteration 12973, loss = 0.04125334
Iteration 12974, loss = 0.04124962
Iteration 12975, loss = 0.04124589
Iteration 12976, loss = 0.04124215
Iteration 12977, loss = 0.04123846
Iteration 12978, loss = 0.04123469
Iteration 12979, loss = 0.04123095
Iteration 12980, loss = 0.04122719
Iteration 12981, loss = 0.04122347
Iteration 12982, loss = 0.04121975
Iteration 12983, loss = 0.04121604
Iteration 12984, loss = 0.04121234
Iteration 12985, loss = 0.04120858
Iteration 12986, loss = 0.04120485
Iteration 12987, loss = 0.04120110
Iteration 12988, loss = 0.04119741
Iteration 12989, loss = 0.04119368
Iteration 12990, loss = 0.04118994
Iteration 12991, loss = 0.04118628
Iteration 12992, loss = 0.04118253
Iteration 12993, loss = 0.04117882
Iteration 12994, loss = 0.04117510
Iteration 12995, loss = 0.04117140
Iteration 12996, loss = 0.04116769
Iteration 12997, loss = 0.04116396
Iteration 12998, loss = 0.04116027
Iteration 12999, loss = 0.04115656
Iteration 13000, loss = 0.04115290
Iteration 13001, loss = 0.04114911
Iteration 13002, loss = 0.04114541
Iteration 13003, loss = 0.04114172
Iteration 13004, loss = 0.04113806
Iteration 13005, loss = 0.04113428
Iteration 13006, loss = 0.04113065
Iteration 13007, loss = 0.04112694
Iteration 13008, loss = 0.04112324
Iteration 13009, loss = 0.04111952
Iteration 13010, loss = 0.04111585
Iteration 13011, loss = 0.04111217
Iteration 13012, loss = 0.04110841
Iteration 13013, loss = 0.04110472
Iteration 13014, loss = 0.04110106
Iteration 13015, loss = 0.04109732
Iteration 13016, loss = 0.04109365
Iteration 13017, loss = 0.04108994
Iteration 13018, loss = 0.04108622
Iteration 13019, loss = 0.04108254
Iteration 13020, loss = 0.04107886
Iteration 13021, loss = 0.04107517
Iteration 13022, loss = 0.04107143
Iteration 13023, loss = 0.04106777
Iteration 13024, loss = 0.04106406
Iteration 13025, loss = 0.04106042
Iteration 13026, loss = 0.04105673
Iteration 13027, loss = 0.04105302
Iteration 13028, loss = 0.04104927
Iteration 13029, loss = 0.04104560
Iteration 13030, loss = 0.04104196
Iteration 13031, loss = 0.04103824
Iteration 13032, loss = 0.04103452
Iteration 13033, loss = 0.04103084
Iteration 13034, loss = 0.04102715
Iteration 13035, loss = 0.04102342
Iteration 13036, loss = 0.04101974
Iteration 13037, loss = 0.04101605
Iteration 13038, loss = 0.04101235
Iteration 13039, loss = 0.04100869
Iteration 13040, loss = 0.04100495
Iteration 13041, loss = 0.04100126
Iteration 13042, loss = 0.04099760
Iteration 13043, loss = 0.04099387
Iteration 13044, loss = 0.04099019
Iteration 13045, loss = 0.04098655
Iteration 13046, loss = 0.04098282
Iteration 13047, loss = 0.04097914
Iteration 13048, loss = 0.04097551
Iteration 13049, loss = 0.04097179
Iteration 13050, loss = 0.04096810
Iteration 13051, loss = 0.04096440
Iteration 13052, loss = 0.04096069
Iteration 13053, loss = 0.04095710
Iteration 13054, loss = 0.04095335
Iteration 13055, loss = 0.04094968
Iteration 13056, loss = 0.04094601
Iteration 13057, loss = 0.04094236
Iteration 13058, loss = 0.04093859
Iteration 13059, loss = 0.04093494
Iteration 13060, loss = 0.04093129
Iteration 13061, loss = 0.04092756
Iteration 13062, loss = 0.04092393
Iteration 13063, loss = 0.04092026
Iteration 13064, loss = 0.04091655
Iteration 13065, loss = 0.04091287
Iteration 13066, loss = 0.04090918
Iteration 13067, loss = 0.04090554
Iteration 13068, loss = 0.04090183
Iteration 13069, loss = 0.04089817
Iteration 13070, loss = 0.04089453
Iteration 13071, loss = 0.04089088
Iteration 13072, loss = 0.04088715
Iteration 13073, loss = 0.04088347
Iteration 13074, loss = 0.04087980
Iteration 13075, loss = 0.04087617
Iteration 13076, loss = 0.04087246
Iteration 13077, loss = 0.04086879
Iteration 13078, loss = 0.04086513
Iteration 13079, loss = 0.04086150
Iteration 13080, loss = 0.04085784
Iteration 13081, loss = 0.04085414
Iteration 13082, loss = 0.04085049
Iteration 13083, loss = 0.04084683
Iteration 13084, loss = 0.04084317
Iteration 13085, loss = 0.04083952
Iteration 13086, loss = 0.04083587
Iteration 13087, loss = 0.04083223
Iteration 13088, loss = 0.04082855
Iteration 13089, loss = 0.04082490
Iteration 13090, loss = 0.04082124
Iteration 13091, loss = 0.04081767
Iteration 13092, loss = 0.04081397
Iteration 13093, loss = 0.04081034
Iteration 13094, loss = 0.04080668
Iteration 13095, loss = 0.04080299
Iteration 13096, loss = 0.04079937
Iteration 13097, loss = 0.04079567
Iteration 13098, loss = 0.04079205
Iteration 13099, loss = 0.04078841
Iteration 13100, loss = 0.04078475
Iteration 13101, loss = 0.04078106
Iteration 13102, loss = 0.04077743
Iteration 13103, loss = 0.04077379
Iteration 13104, loss = 0.04077012
Iteration 13105, loss = 0.04076646
Iteration 13106, loss = 0.04076282
Iteration 13107, loss = 0.04075918
Iteration 13108, loss = 0.04075554
Iteration 13109, loss = 0.04075185
Iteration 13110, loss = 0.04074828
Iteration 13111, loss = 0.04074456
Iteration 13112, loss = 0.04074096
Iteration 13113, loss = 0.04073730
Iteration 13114, loss = 0.04073367
Iteration 13115, loss = 0.04073007
Iteration 13116, loss = 0.04072638
Iteration 13117, loss = 0.04072277
Iteration 13118, loss = 0.04071912
Iteration 13119, loss = 0.04071551
Iteration 13120, loss = 0.04071187
Iteration 13121, loss = 0.04070820
Iteration 13122, loss = 0.04070459
Iteration 13123, loss = 0.04070095
Iteration 13124, loss = 0.04069733
Iteration 13125, loss = 0.04069374
Iteration 13126, loss = 0.04069007
Iteration 13127, loss = 0.04068643
Iteration 13128, loss = 0.04068277
Iteration 13129, loss = 0.04067915
Iteration 13130, loss = 0.04067549
Iteration 13131, loss = 0.04067188
Iteration 13132, loss = 0.04066826
Iteration 13133, loss = 0.04066462
Iteration 13134, loss = 0.04066098
Iteration 13135, loss = 0.04065734
Iteration 13136, loss = 0.04065375
Iteration 13137, loss = 0.04065008
Iteration 13138, loss = 0.04064653
Iteration 13139, loss = 0.04064290
Iteration 13140, loss = 0.04063919
Iteration 13141, loss = 0.04063564
Iteration 13142, loss = 0.04063199
Iteration 13143, loss = 0.04062840
Iteration 13144, loss = 0.04062481
Iteration 13145, loss = 0.04062115
Iteration 13146, loss = 0.04061750
Iteration 13147, loss = 0.04061398
Iteration 13148, loss = 0.04061026
Iteration 13149, loss = 0.04060664
Iteration 13150, loss = 0.04060305
Iteration 13151, loss = 0.04059942
Iteration 13152, loss = 0.04059583
Iteration 13153, loss = 0.04059217
Iteration 13154, loss = 0.04058854
Iteration 13155, loss = 0.04058499
Iteration 13156, loss = 0.04058135
Iteration 13157, loss = 0.04057773
Iteration 13158, loss = 0.04057406
Iteration 13159, loss = 0.04057054
Iteration 13160, loss = 0.04056689
Iteration 13161, loss = 0.04056325
Iteration 13162, loss = 0.04055965
Iteration 13163, loss = 0.04055600
Iteration 13164, loss = 0.04055240
Iteration 13165, loss = 0.04054883
Iteration 13166, loss = 0.04054517
Iteration 13167, loss = 0.04054163
Iteration 13168, loss = 0.04053798
Iteration 13169, loss = 0.04053443
Iteration 13170, loss = 0.04053078
Iteration 13171, loss = 0.04052723
Iteration 13172, loss = 0.04052363
Iteration 13173, loss = 0.04052001
Iteration 13174, loss = 0.04051641
Iteration 13175, loss = 0.04051280
Iteration 13176, loss = 0.04050922
Iteration 13177, loss = 0.04050565
Iteration 13178, loss = 0.04050198
Iteration 13179, loss = 0.04049838
Iteration 13180, loss = 0.04049483
Iteration 13181, loss = 0.04049121
Iteration 13182, loss = 0.04048759
Iteration 13183, loss = 0.04048406
Iteration 13184, loss = 0.04048045
Iteration 13185, loss = 0.04047683
Iteration 13186, loss = 0.04047324
Iteration 13187, loss = 0.04046972
Iteration 13188, loss = 0.04046607
Iteration 13189, loss = 0.04046248
Iteration 13190, loss = 0.04045886
Iteration 13191, loss = 0.04045530
Iteration 13192, loss = 0.04045171
Iteration 13193, loss = 0.04044813
Iteration 13194, loss = 0.04044455
Iteration 13195, loss = 0.04044096
Iteration 13196, loss = 0.04043735
Iteration 13197, loss = 0.04043379
Iteration 13198, loss = 0.04043019
Iteration 13199, loss = 0.04042661
Iteration 13200, loss = 0.04042304
Iteration 13201, loss = 0.04041941
Iteration 13202, loss = 0.04041583
Iteration 13203, loss = 0.04041225
Iteration 13204, loss = 0.04040866
Iteration 13205, loss = 0.04040515
Iteration 13206, loss = 0.04040152
Iteration 13207, loss = 0.04039792
Iteration 13208, loss = 0.04039436
Iteration 13209, loss = 0.04039083
Iteration 13210, loss = 0.04038720
Iteration 13211, loss = 0.04038364
Iteration 13212, loss = 0.04038006
Iteration 13213, loss = 0.04037650
Iteration 13214, loss = 0.04037293
Iteration 13215, loss = 0.04036934
Iteration 13216, loss = 0.04036576
Iteration 13217, loss = 0.04036217
Iteration 13218, loss = 0.04035863
Iteration 13219, loss = 0.04035502
Iteration 13220, loss = 0.04035147
Iteration 13221, loss = 0.04034787
Iteration 13222, loss = 0.04034430
Iteration 13223, loss = 0.04034075
Iteration 13224, loss = 0.04033716
Iteration 13225, loss = 0.04033361
Iteration 13226, loss = 0.04033000
Iteration 13227, loss = 0.04032646
Iteration 13228, loss = 0.04032286
Iteration 13229, loss = 0.04031930
Iteration 13230, loss = 0.04031573
Iteration 13231, loss = 0.04031218
Iteration 13232, loss = 0.04030865
Iteration 13233, loss = 0.04030501
Iteration 13234, loss = 0.04030146
Iteration 13235, loss = 0.04029789
Iteration 13236, loss = 0.04029431
Iteration 13237, loss = 0.04029073
Iteration 13238, loss = 0.04028720
Iteration 13239, loss = 0.04028368
Iteration 13240, loss = 0.04028004
Iteration 13241, loss = 0.04027648
Iteration 13242, loss = 0.04027294
Iteration 13243, loss = 0.04026935
Iteration 13244, loss = 0.04026582
Iteration 13245, loss = 0.04026222
Iteration 13246, loss = 0.04025867
Iteration 13247, loss = 0.04025510
Iteration 13248, loss = 0.04025161
Iteration 13249, loss = 0.04024801
Iteration 13250, loss = 0.04024446
Iteration 13251, loss = 0.04024092
Iteration 13252, loss = 0.04023741
Iteration 13253, loss = 0.04023378
Iteration 13254, loss = 0.04023026
Iteration 13255, loss = 0.04022671
Iteration 13256, loss = 0.04022317
Iteration 13257, loss = 0.04021962
Iteration 13258, loss = 0.04021607
Iteration 13259, loss = 0.04021255
Iteration 13260, loss = 0.04020897
Iteration 13261, loss = 0.04020539
Iteration 13262, loss = 0.04020191
Iteration 13263, loss = 0.04019830
Iteration 13264, loss = 0.04019480
Iteration 13265, loss = 0.04019124
Iteration 13266, loss = 0.04018766
Iteration 13267, loss = 0.04018417
Iteration 13268, loss = 0.04018059
Iteration 13269, loss = 0.04017708
Iteration 13270, loss = 0.04017347
Iteration 13271, loss = 0.04016997
Iteration 13272, loss = 0.04016644
Iteration 13273, loss = 0.04016286
Iteration 13274, loss = 0.04015932
Iteration 13275, loss = 0.04015578
Iteration 13276, loss = 0.04015228
Iteration 13277, loss = 0.04014878
Iteration 13278, loss = 0.04014515
Iteration 13279, loss = 0.04014171
Iteration 13280, loss = 0.04013815
Iteration 13281, loss = 0.04013459
Iteration 13282, loss = 0.04013108
Iteration 13283, loss = 0.04012752
Iteration 13284, loss = 0.04012405
Iteration 13285, loss = 0.04012047
Iteration 13286, loss = 0.04011695
Iteration 13287, loss = 0.04011343
Iteration 13288, loss = 0.04010989
Iteration 13289, loss = 0.04010641
Iteration 13290, loss = 0.04010280
Iteration 13291, loss = 0.04009932
Iteration 13292, loss = 0.04009579
Iteration 13293, loss = 0.04009227
Iteration 13294, loss = 0.04008870
Iteration 13295, loss = 0.04008521
Iteration 13296, loss = 0.04008170
Iteration 13297, loss = 0.04007814
Iteration 13298, loss = 0.04007462
Iteration 13299, loss = 0.04007111
Iteration 13300, loss = 0.04006756
Iteration 13301, loss = 0.04006402
Iteration 13302, loss = 0.04006059
Iteration 13303, loss = 0.04005695
Iteration 13304, loss = 0.04005347
Iteration 13305, loss = 0.04004993
Iteration 13306, loss = 0.04004640
Iteration 13307, loss = 0.04004286
Iteration 13308, loss = 0.04003942
Iteration 13309, loss = 0.04003585
Iteration 13310, loss = 0.04003232
Iteration 13311, loss = 0.04002882
Iteration 13312, loss = 0.04002527
Iteration 13313, loss = 0.04002180
Iteration 13314, loss = 0.04001825
Iteration 13315, loss = 0.04001472
Iteration 13316, loss = 0.04001124
Iteration 13317, loss = 0.04000772
Iteration 13318, loss = 0.04000425
Iteration 13319, loss = 0.04000071
Iteration 13320, loss = 0.03999722
Iteration 13321, loss = 0.03999371
Iteration 13322, loss = 0.03999023
Iteration 13323, loss = 0.03998672
Iteration 13324, loss = 0.03998324
Iteration 13325, loss = 0.03997970
Iteration 13326, loss = 0.03997619
Iteration 13327, loss = 0.03997278
Iteration 13328, loss = 0.03996928
Iteration 13329, loss = 0.03996570
Iteration 13330, loss = 0.03996221
Iteration 13331, loss = 0.03995871
Iteration 13332, loss = 0.03995522
Iteration 13333, loss = 0.03995171
Iteration 13334, loss = 0.03994823
Iteration 13335, loss = 0.03994475
Iteration 13336, loss = 0.03994124
Iteration 13337, loss = 0.03993773
Iteration 13338, loss = 0.03993426
Iteration 13339, loss = 0.03993077
Iteration 13340, loss = 0.03992724
Iteration 13341, loss = 0.03992373
Iteration 13342, loss = 0.03992028
Iteration 13343, loss = 0.03991677
Iteration 13344, loss = 0.03991326
Iteration 13345, loss = 0.03990976
Iteration 13346, loss = 0.03990629
Iteration 13347, loss = 0.03990276
Iteration 13348, loss = 0.03989924
Iteration 13349, loss = 0.03989579
Iteration 13350, loss = 0.03989227
Iteration 13351, loss = 0.03988878
Iteration 13352, loss = 0.03988528
Iteration 13353, loss = 0.03988180
Iteration 13354, loss = 0.03987833
Iteration 13355, loss = 0.03987486
Iteration 13356, loss = 0.03987133
Iteration 13357, loss = 0.03986787
Iteration 13358, loss = 0.03986437
Iteration 13359, loss = 0.03986085
Iteration 13360, loss = 0.03985740
Iteration 13361, loss = 0.03985387
Iteration 13362, loss = 0.03985035
Iteration 13363, loss = 0.03984687
Iteration 13364, loss = 0.03984338
Iteration 13365, loss = 0.03983996
Iteration 13366, loss = 0.03983645
Iteration 13367, loss = 0.03983294
Iteration 13368, loss = 0.03982948
Iteration 13369, loss = 0.03982600
Iteration 13370, loss = 0.03982251
Iteration 13371, loss = 0.03981906
Iteration 13372, loss = 0.03981554
Iteration 13373, loss = 0.03981207
Iteration 13374, loss = 0.03980860
Iteration 13375, loss = 0.03980514
Iteration 13376, loss = 0.03980164
Iteration 13377, loss = 0.03979819
Iteration 13378, loss = 0.03979470
Iteration 13379, loss = 0.03979123
Iteration 13380, loss = 0.03978777
Iteration 13381, loss = 0.03978427
Iteration 13382, loss = 0.03978080
Iteration 13383, loss = 0.03977736
Iteration 13384, loss = 0.03977386
Iteration 13385, loss = 0.03977041
Iteration 13386, loss = 0.03976696
Iteration 13387, loss = 0.03976350
Iteration 13388, loss = 0.03975996
Iteration 13389, loss = 0.03975649
Iteration 13390, loss = 0.03975300
Iteration 13391, loss = 0.03974951
Iteration 13392, loss = 0.03974602
Iteration 13393, loss = 0.03974261
Iteration 13394, loss = 0.03973914
Iteration 13395, loss = 0.03973564
Iteration 13396, loss = 0.03973219
Iteration 13397, loss = 0.03972877
Iteration 13398, loss = 0.03972522
Iteration 13399, loss = 0.03972180
Iteration 13400, loss = 0.03971831
Iteration 13401, loss = 0.03971487
Iteration 13402, loss = 0.03971144
Iteration 13403, loss = 0.03970795
Iteration 13404, loss = 0.03970444
Iteration 13405, loss = 0.03970101
Iteration 13406, loss = 0.03969756
Iteration 13407, loss = 0.03969410
Iteration 13408, loss = 0.03969064
Iteration 13409, loss = 0.03968718
Iteration 13410, loss = 0.03968372
Iteration 13411, loss = 0.03968028
Iteration 13412, loss = 0.03967682
Iteration 13413, loss = 0.03967338
Iteration 13414, loss = 0.03966990
Iteration 13415, loss = 0.03966646
Iteration 13416, loss = 0.03966299
Iteration 13417, loss = 0.03965952
Iteration 13418, loss = 0.03965610
Iteration 13419, loss = 0.03965261
Iteration 13420, loss = 0.03964915
Iteration 13421, loss = 0.03964575
Iteration 13422, loss = 0.03964227
Iteration 13423, loss = 0.03963882
Iteration 13424, loss = 0.03963538
Iteration 13425, loss = 0.03963192
Iteration 13426, loss = 0.03962844
Iteration 13427, loss = 0.03962502
Iteration 13428, loss = 0.03962159
Iteration 13429, loss = 0.03961811
Iteration 13430, loss = 0.03961464
Iteration 13431, loss = 0.03961131
Iteration 13432, loss = 0.03960777
Iteration 13433, loss = 0.03960432
Iteration 13434, loss = 0.03960086
Iteration 13435, loss = 0.03959749
Iteration 13436, loss = 0.03959401
Iteration 13437, loss = 0.03959062
Iteration 13438, loss = 0.03958714
Iteration 13439, loss = 0.03958370
Iteration 13440, loss = 0.03958027
Iteration 13441, loss = 0.03957685
Iteration 13442, loss = 0.03957338
Iteration 13443, loss = 0.03956997
Iteration 13444, loss = 0.03956652
Iteration 13445, loss = 0.03956314
Iteration 13446, loss = 0.03955969
Iteration 13447, loss = 0.03955623
Iteration 13448, loss = 0.03955283
Iteration 13449, loss = 0.03954937
Iteration 13450, loss = 0.03954595
Iteration 13451, loss = 0.03954251
Iteration 13452, loss = 0.03953910
Iteration 13453, loss = 0.03953570
Iteration 13454, loss = 0.03953224
Iteration 13455, loss = 0.03952878
Iteration 13456, loss = 0.03952539
Iteration 13457, loss = 0.03952196
Iteration 13458, loss = 0.03951856
Iteration 13459, loss = 0.03951511
Iteration 13460, loss = 0.03951167
Iteration 13461, loss = 0.03950827
Iteration 13462, loss = 0.03950480
Iteration 13463, loss = 0.03950138
Iteration 13464, loss = 0.03949794
Iteration 13465, loss = 0.03949457
Iteration 13466, loss = 0.03949107
Iteration 13467, loss = 0.03948772
Iteration 13468, loss = 0.03948426
Iteration 13469, loss = 0.03948081
Iteration 13470, loss = 0.03947743
Iteration 13471, loss = 0.03947396
Iteration 13472, loss = 0.03947054
Iteration 13473, loss = 0.03946709
Iteration 13474, loss = 0.03946372
Iteration 13475, loss = 0.03946029
Iteration 13476, loss = 0.03945686
Iteration 13477, loss = 0.03945345
Iteration 13478, loss = 0.03945004
Iteration 13479, loss = 0.03944662
Iteration 13480, loss = 0.03944322
Iteration 13481, loss = 0.03943982
Iteration 13482, loss = 0.03943640
Iteration 13483, loss = 0.03943296
Iteration 13484, loss = 0.03942954
Iteration 13485, loss = 0.03942615
Iteration 13486, loss = 0.03942278
Iteration 13487, loss = 0.03941934
Iteration 13488, loss = 0.03941596
Iteration 13489, loss = 0.03941253
Iteration 13490, loss = 0.03940917
Iteration 13491, loss = 0.03940572
Iteration 13492, loss = 0.03940235
Iteration 13493, loss = 0.03939894
Iteration 13494, loss = 0.03939553
Iteration 13495, loss = 0.03939215
Iteration 13496, loss = 0.03938877
Iteration 13497, loss = 0.03938533
Iteration 13498, loss = 0.03938194
Iteration 13499, loss = 0.03937856
Iteration 13500, loss = 0.03937516
Iteration 13501, loss = 0.03937178
Iteration 13502, loss = 0.03936837
Iteration 13503, loss = 0.03936500
Iteration 13504, loss = 0.03936162
Iteration 13505, loss = 0.03935820
Iteration 13506, loss = 0.03935483
Iteration 13507, loss = 0.03935143
Iteration 13508, loss = 0.03934801
Iteration 13509, loss = 0.03934462
Iteration 13510, loss = 0.03934119
Iteration 13511, loss = 0.03933782
Iteration 13512, loss = 0.03933442
Iteration 13513, loss = 0.03933102
Iteration 13514, loss = 0.03932762
Iteration 13515, loss = 0.03932418
Iteration 13516, loss = 0.03932083
Iteration 13517, loss = 0.03931745
Iteration 13518, loss = 0.03931401
Iteration 13519, loss = 0.03931067
Iteration 13520, loss = 0.03930723
Iteration 13521, loss = 0.03930382
Iteration 13522, loss = 0.03930045
Iteration 13523, loss = 0.03929699
Iteration 13524, loss = 0.03929364
Iteration 13525, loss = 0.03929029
Iteration 13526, loss = 0.03928686
Iteration 13527, loss = 0.03928344
Iteration 13528, loss = 0.03928009
Iteration 13529, loss = 0.03927668
Iteration 13530, loss = 0.03927327
Iteration 13531, loss = 0.03926991
Iteration 13532, loss = 0.03926657
Iteration 13533, loss = 0.03926317
Iteration 13534, loss = 0.03925976
Iteration 13535, loss = 0.03925638
Iteration 13536, loss = 0.03925299
Iteration 13537, loss = 0.03924963
Iteration 13538, loss = 0.03924623
Iteration 13539, loss = 0.03924283
Iteration 13540, loss = 0.03923950
Iteration 13541, loss = 0.03923611
Iteration 13542, loss = 0.03923270
Iteration 13543, loss = 0.03922936
Iteration 13544, loss = 0.03922600
Iteration 13545, loss = 0.03922261
Iteration 13546, loss = 0.03921924
Iteration 13547, loss = 0.03921588
Iteration 13548, loss = 0.03921251
Iteration 13549, loss = 0.03920914
Iteration 13550, loss = 0.03920575
Iteration 13551, loss = 0.03920243
Iteration 13552, loss = 0.03919900
Iteration 13553, loss = 0.03919567
Iteration 13554, loss = 0.03919225
Iteration 13555, loss = 0.03918888
Iteration 13556, loss = 0.03918557
Iteration 13557, loss = 0.03918222
Iteration 13558, loss = 0.03917877
Iteration 13559, loss = 0.03917544
Iteration 13560, loss = 0.03917212
Iteration 13561, loss = 0.03916875
Iteration 13562, loss = 0.03916538
Iteration 13563, loss = 0.03916199
Iteration 13564, loss = 0.03915863
Iteration 13565, loss = 0.03915528
Iteration 13566, loss = 0.03915192
Iteration 13567, loss = 0.03914856
Iteration 13568, loss = 0.03914524
Iteration 13569, loss = 0.03914183
Iteration 13570, loss = 0.03913847
Iteration 13571, loss = 0.03913511
Iteration 13572, loss = 0.03913180
Iteration 13573, loss = 0.03912839
Iteration 13574, loss = 0.03912505
Iteration 13575, loss = 0.03912169
Iteration 13576, loss = 0.03911837
Iteration 13577, loss = 0.03911507
Iteration 13578, loss = 0.03911168
Iteration 13579, loss = 0.03910834
Iteration 13580, loss = 0.03910494
Iteration 13581, loss = 0.03910162
Iteration 13582, loss = 0.03909823
Iteration 13583, loss = 0.03909491
Iteration 13584, loss = 0.03909153
Iteration 13585, loss = 0.03908818
Iteration 13586, loss = 0.03908486
Iteration 13587, loss = 0.03908151
Iteration 13588, loss = 0.03907813
Iteration 13589, loss = 0.03907486
Iteration 13590, loss = 0.03907148
Iteration 13591, loss = 0.03906811
Iteration 13592, loss = 0.03906477
Iteration 13593, loss = 0.03906140
Iteration 13594, loss = 0.03905813
Iteration 13595, loss = 0.03905472
Iteration 13596, loss = 0.03905145
Iteration 13597, loss = 0.03904807
Iteration 13598, loss = 0.03904475
Iteration 13599, loss = 0.03904139
Iteration 13600, loss = 0.03903806
Iteration 13601, loss = 0.03903473
Iteration 13602, loss = 0.03903136
Iteration 13603, loss = 0.03902801
Iteration 13604, loss = 0.03902468
Iteration 13605, loss = 0.03902133
Iteration 13606, loss = 0.03901800
Iteration 13607, loss = 0.03901464
Iteration 13608, loss = 0.03901132
Iteration 13609, loss = 0.03900798
Iteration 13610, loss = 0.03900463
Iteration 13611, loss = 0.03900127
Iteration 13612, loss = 0.03899793
Iteration 13613, loss = 0.03899463
Iteration 13614, loss = 0.03899126
Iteration 13615, loss = 0.03898794
Iteration 13616, loss = 0.03898459
Iteration 13617, loss = 0.03898127
Iteration 13618, loss = 0.03897790
Iteration 13619, loss = 0.03897460
Iteration 13620, loss = 0.03897125
Iteration 13621, loss = 0.03896793
Iteration 13622, loss = 0.03896456
Iteration 13623, loss = 0.03896125
Iteration 13624, loss = 0.03895796
Iteration 13625, loss = 0.03895460
Iteration 13626, loss = 0.03895125
Iteration 13627, loss = 0.03894787
Iteration 13628, loss = 0.03894461
Iteration 13629, loss = 0.03894126
Iteration 13630, loss = 0.03893792
Iteration 13631, loss = 0.03893463
Iteration 13632, loss = 0.03893126
Iteration 13633, loss = 0.03892794
Iteration 13634, loss = 0.03892461
Iteration 13635, loss = 0.03892125
Iteration 13636, loss = 0.03891797
Iteration 13637, loss = 0.03891462
Iteration 13638, loss = 0.03891130
Iteration 13639, loss = 0.03890802
Iteration 13640, loss = 0.03890470
Iteration 13641, loss = 0.03890141
Iteration 13642, loss = 0.03889806
Iteration 13643, loss = 0.03889470
Iteration 13644, loss = 0.03889143
Iteration 13645, loss = 0.03888812
Iteration 13646, loss = 0.03888479
Iteration 13647, loss = 0.03888148
Iteration 13648, loss = 0.03887818
Iteration 13649, loss = 0.03887485
Iteration 13650, loss = 0.03887151
Iteration 13651, loss = 0.03886822
Iteration 13652, loss = 0.03886493
Iteration 13653, loss = 0.03886160
Iteration 13654, loss = 0.03885830
Iteration 13655, loss = 0.03885502
Iteration 13656, loss = 0.03885165
Iteration 13657, loss = 0.03884838
Iteration 13658, loss = 0.03884507
Iteration 13659, loss = 0.03884176
Iteration 13660, loss = 0.03883847
Iteration 13661, loss = 0.03883514
Iteration 13662, loss = 0.03883186
Iteration 13663, loss = 0.03882852
Iteration 13664, loss = 0.03882520
Iteration 13665, loss = 0.03882192
Iteration 13666, loss = 0.03881856
Iteration 13667, loss = 0.03881532
Iteration 13668, loss = 0.03881200
Iteration 13669, loss = 0.03880871
Iteration 13670, loss = 0.03880543
Iteration 13671, loss = 0.03880206
Iteration 13672, loss = 0.03879882
Iteration 13673, loss = 0.03879544
Iteration 13674, loss = 0.03879223
Iteration 13675, loss = 0.03878886
Iteration 13676, loss = 0.03878566
Iteration 13677, loss = 0.03878235
Iteration 13678, loss = 0.03877897
Iteration 13679, loss = 0.03877573
Iteration 13680, loss = 0.03877247
Iteration 13681, loss = 0.03876915
Iteration 13682, loss = 0.03876587
Iteration 13683, loss = 0.03876256
Iteration 13684, loss = 0.03875925
Iteration 13685, loss = 0.03875602
Iteration 13686, loss = 0.03875269
Iteration 13687, loss = 0.03874942
Iteration 13688, loss = 0.03874611
Iteration 13689, loss = 0.03874282
Iteration 13690, loss = 0.03873951
Iteration 13691, loss = 0.03873622
Iteration 13692, loss = 0.03873294
Iteration 13693, loss = 0.03872963
Iteration 13694, loss = 0.03872636
Iteration 13695, loss = 0.03872303
Iteration 13696, loss = 0.03871974
Iteration 13697, loss = 0.03871644
Iteration 13698, loss = 0.03871317
Iteration 13699, loss = 0.03870991
Iteration 13700, loss = 0.03870656
Iteration 13701, loss = 0.03870331
Iteration 13702, loss = 0.03870000
Iteration 13703, loss = 0.03869674
Iteration 13704, loss = 0.03869343
Iteration 13705, loss = 0.03869011
Iteration 13706, loss = 0.03868685
Iteration 13707, loss = 0.03868354
Iteration 13708, loss = 0.03868027
Iteration 13709, loss = 0.03867697
Iteration 13710, loss = 0.03867368
Iteration 13711, loss = 0.03867040
Iteration 13712, loss = 0.03866711
Iteration 13713, loss = 0.03866381
Iteration 13714, loss = 0.03866056
Iteration 13715, loss = 0.03865725
Iteration 13716, loss = 0.03865396
Iteration 13717, loss = 0.03865070
Iteration 13718, loss = 0.03864741
Iteration 13719, loss = 0.03864413
Iteration 13720, loss = 0.03864087
Iteration 13721, loss = 0.03863762
Iteration 13722, loss = 0.03863433
Iteration 13723, loss = 0.03863108
Iteration 13724, loss = 0.03862776
Iteration 13725, loss = 0.03862453
Iteration 13726, loss = 0.03862122
Iteration 13727, loss = 0.03861794
Iteration 13728, loss = 0.03861471
Iteration 13729, loss = 0.03861140
Iteration 13730, loss = 0.03860811
Iteration 13731, loss = 0.03860493
Iteration 13732, loss = 0.03860161
Iteration 13733, loss = 0.03859833
Iteration 13734, loss = 0.03859510
Iteration 13735, loss = 0.03859179
Iteration 13736, loss = 0.03858856
Iteration 13737, loss = 0.03858528
Iteration 13738, loss = 0.03858205
Iteration 13739, loss = 0.03857876
Iteration 13740, loss = 0.03857552
Iteration 13741, loss = 0.03857224
Iteration 13742, loss = 0.03856894
Iteration 13743, loss = 0.03856571
Iteration 13744, loss = 0.03856246
Iteration 13745, loss = 0.03855920
Iteration 13746, loss = 0.03855593
Iteration 13747, loss = 0.03855271
Iteration 13748, loss = 0.03854946
Iteration 13749, loss = 0.03854614
Iteration 13750, loss = 0.03854286
Iteration 13751, loss = 0.03853967
Iteration 13752, loss = 0.03853636
Iteration 13753, loss = 0.03853311
Iteration 13754, loss = 0.03852988
Iteration 13755, loss = 0.03852661
Iteration 13756, loss = 0.03852335
Iteration 13757, loss = 0.03852013
Iteration 13758, loss = 0.03851679
Iteration 13759, loss = 0.03851359
Iteration 13760, loss = 0.03851032
Iteration 13761, loss = 0.03850702
Iteration 13762, loss = 0.03850375
Iteration 13763, loss = 0.03850053
Iteration 13764, loss = 0.03849728
Iteration 13765, loss = 0.03849405
Iteration 13766, loss = 0.03849077
Iteration 13767, loss = 0.03848749
Iteration 13768, loss = 0.03848427
Iteration 13769, loss = 0.03848100
Iteration 13770, loss = 0.03847778
Iteration 13771, loss = 0.03847455
Iteration 13772, loss = 0.03847124
Iteration 13773, loss = 0.03846804
Iteration 13774, loss = 0.03846482
Iteration 13775, loss = 0.03846155
Iteration 13776, loss = 0.03845833
Iteration 13777, loss = 0.03845506
Iteration 13778, loss = 0.03845181
Iteration 13779, loss = 0.03844860
Iteration 13780, loss = 0.03844533
Iteration 13781, loss = 0.03844212
Iteration 13782, loss = 0.03843885
Iteration 13783, loss = 0.03843560
Iteration 13784, loss = 0.03843239
Iteration 13785, loss = 0.03842913
Iteration 13786, loss = 0.03842592
Iteration 13787, loss = 0.03842273
Iteration 13788, loss = 0.03841942
Iteration 13789, loss = 0.03841619
Iteration 13790, loss = 0.03841299
Iteration 13791, loss = 0.03840970
Iteration 13792, loss = 0.03840647
Iteration 13793, loss = 0.03840322
Iteration 13794, loss = 0.03840000
Iteration 13795, loss = 0.03839674
Iteration 13796, loss = 0.03839350
Iteration 13797, loss = 0.03839027
Iteration 13798, loss = 0.03838703
Iteration 13799, loss = 0.03838382
Iteration 13800, loss = 0.03838056
Iteration 13801, loss = 0.03837733
Iteration 13802, loss = 0.03837408
Iteration 13803, loss = 0.03837089
Iteration 13804, loss = 0.03836769
Iteration 13805, loss = 0.03836444
Iteration 13806, loss = 0.03836116
Iteration 13807, loss = 0.03835797
Iteration 13808, loss = 0.03835474
Iteration 13809, loss = 0.03835150
Iteration 13810, loss = 0.03834832
Iteration 13811, loss = 0.03834508
Iteration 13812, loss = 0.03834192
Iteration 13813, loss = 0.03833862
Iteration 13814, loss = 0.03833540
Iteration 13815, loss = 0.03833213
Iteration 13816, loss = 0.03832892
Iteration 13817, loss = 0.03832570
Iteration 13818, loss = 0.03832250
Iteration 13819, loss = 0.03831925
Iteration 13820, loss = 0.03831601
Iteration 13821, loss = 0.03831279
Iteration 13822, loss = 0.03830957
Iteration 13823, loss = 0.03830635
Iteration 13824, loss = 0.03830310
Iteration 13825, loss = 0.03829992
Iteration 13826, loss = 0.03829671
Iteration 13827, loss = 0.03829346
Iteration 13828, loss = 0.03829025
Iteration 13829, loss = 0.03828705
Iteration 13830, loss = 0.03828385
Iteration 13831, loss = 0.03828059
Iteration 13832, loss = 0.03827738
Iteration 13833, loss = 0.03827411
Iteration 13834, loss = 0.03827098
Iteration 13835, loss = 0.03826772
Iteration 13836, loss = 0.03826448
Iteration 13837, loss = 0.03826132
Iteration 13838, loss = 0.03825808
Iteration 13839, loss = 0.03825486
Iteration 13840, loss = 0.03825160
Iteration 13841, loss = 0.03824837
Iteration 13842, loss = 0.03824519
Iteration 13843, loss = 0.03824197
Iteration 13844, loss = 0.03823871
Iteration 13845, loss = 0.03823552
Iteration 13846, loss = 0.03823236
Iteration 13847, loss = 0.03822910
Iteration 13848, loss = 0.03822591
Iteration 13849, loss = 0.03822269
Iteration 13850, loss = 0.03821942
Iteration 13851, loss = 0.03821625
Iteration 13852, loss = 0.03821304
Iteration 13853, loss = 0.03820979
Iteration 13854, loss = 0.03820662
Iteration 13855, loss = 0.03820338
Iteration 13856, loss = 0.03820019
Iteration 13857, loss = 0.03819694
Iteration 13858, loss = 0.03819377
Iteration 13859, loss = 0.03819055
Iteration 13860, loss = 0.03818733
Iteration 13861, loss = 0.03818416
Iteration 13862, loss = 0.03818092
Iteration 13863, loss = 0.03817771
Iteration 13864, loss = 0.03817453
Iteration 13865, loss = 0.03817135
Iteration 13866, loss = 0.03816812
Iteration 13867, loss = 0.03816492
Iteration 13868, loss = 0.03816170
Iteration 13869, loss = 0.03815852
Iteration 13870, loss = 0.03815534
Iteration 13871, loss = 0.03815213
Iteration 13872, loss = 0.03814893
Iteration 13873, loss = 0.03814573
Iteration 13874, loss = 0.03814255
Iteration 13875, loss = 0.03813937
Iteration 13876, loss = 0.03813618
Iteration 13877, loss = 0.03813294
Iteration 13878, loss = 0.03812978
Iteration 13879, loss = 0.03812656
Iteration 13880, loss = 0.03812343
Iteration 13881, loss = 0.03812021
Iteration 13882, loss = 0.03811703
Iteration 13883, loss = 0.03811379
Iteration 13884, loss = 0.03811061
Iteration 13885, loss = 0.03810741
Iteration 13886, loss = 0.03810425
Iteration 13887, loss = 0.03810103
Iteration 13888, loss = 0.03809784
Iteration 13889, loss = 0.03809466
Iteration 13890, loss = 0.03809149
Iteration 13891, loss = 0.03808832
Iteration 13892, loss = 0.03808510
Iteration 13893, loss = 0.03808194
Iteration 13894, loss = 0.03807872
Iteration 13895, loss = 0.03807556
Iteration 13896, loss = 0.03807238
Iteration 13897, loss = 0.03806920
Iteration 13898, loss = 0.03806607
Iteration 13899, loss = 0.03806284
Iteration 13900, loss = 0.03805965
Iteration 13901, loss = 0.03805648
Iteration 13902, loss = 0.03805332
Iteration 13903, loss = 0.03805013
Iteration 13904, loss = 0.03804694
Iteration 13905, loss = 0.03804378
Iteration 13906, loss = 0.03804060
Iteration 13907, loss = 0.03803740
Iteration 13908, loss = 0.03803425
Iteration 13909, loss = 0.03803104
Iteration 13910, loss = 0.03802792
Iteration 13911, loss = 0.03802472
Iteration 13912, loss = 0.03802154
Iteration 13913, loss = 0.03801834
Iteration 13914, loss = 0.03801518
Iteration 13915, loss = 0.03801206
Iteration 13916, loss = 0.03800886
Iteration 13917, loss = 0.03800568
Iteration 13918, loss = 0.03800249
Iteration 13919, loss = 0.03799932
Iteration 13920, loss = 0.03799616
Iteration 13921, loss = 0.03799300
Iteration 13922, loss = 0.03798982
Iteration 13923, loss = 0.03798665
Iteration 13924, loss = 0.03798349
Iteration 13925, loss = 0.03798033
Iteration 13926, loss = 0.03797715
Iteration 13927, loss = 0.03797397
Iteration 13928, loss = 0.03797087
Iteration 13929, loss = 0.03796768
Iteration 13930, loss = 0.03796451
Iteration 13931, loss = 0.03796137
Iteration 13932, loss = 0.03795814
Iteration 13933, loss = 0.03795506
Iteration 13934, loss = 0.03795185
Iteration 13935, loss = 0.03794871
Iteration 13936, loss = 0.03794553
Iteration 13937, loss = 0.03794238
Iteration 13938, loss = 0.03793919
Iteration 13939, loss = 0.03793604
Iteration 13940, loss = 0.03793285
Iteration 13941, loss = 0.03792968
Iteration 13942, loss = 0.03792652
Iteration 13943, loss = 0.03792340
Iteration 13944, loss = 0.03792018
Iteration 13945, loss = 0.03791702
Iteration 13946, loss = 0.03791389
Iteration 13947, loss = 0.03791072
Iteration 13948, loss = 0.03790755
Iteration 13949, loss = 0.03790442
Iteration 13950, loss = 0.03790126
Iteration 13951, loss = 0.03789807
Iteration 13952, loss = 0.03789492
Iteration 13953, loss = 0.03789176
Iteration 13954, loss = 0.03788862
Iteration 13955, loss = 0.03788549
Iteration 13956, loss = 0.03788230
Iteration 13957, loss = 0.03787917
Iteration 13958, loss = 0.03787601
Iteration 13959, loss = 0.03787285
Iteration 13960, loss = 0.03786975
Iteration 13961, loss = 0.03786658
Iteration 13962, loss = 0.03786341
Iteration 13963, loss = 0.03786028
Iteration 13964, loss = 0.03785712
Iteration 13965, loss = 0.03785397
Iteration 13966, loss = 0.03785086
Iteration 13967, loss = 0.03784769
Iteration 13968, loss = 0.03784453
Iteration 13969, loss = 0.03784138
Iteration 13970, loss = 0.03783825
Iteration 13971, loss = 0.03783513
Iteration 13972, loss = 0.03783197
Iteration 13973, loss = 0.03782879
Iteration 13974, loss = 0.03782572
Iteration 13975, loss = 0.03782253
Iteration 13976, loss = 0.03781941
Iteration 13977, loss = 0.03781625
Iteration 13978, loss = 0.03781313
Iteration 13979, loss = 0.03780996
Iteration 13980, loss = 0.03780684
Iteration 13981, loss = 0.03780368
Iteration 13982, loss = 0.03780053
Iteration 13983, loss = 0.03779741
Iteration 13984, loss = 0.03779427
Iteration 13985, loss = 0.03779116
Iteration 13986, loss = 0.03778797
Iteration 13987, loss = 0.03778482
Iteration 13988, loss = 0.03778168
Iteration 13989, loss = 0.03777855
Iteration 13990, loss = 0.03777541
Iteration 13991, loss = 0.03777230
Iteration 13992, loss = 0.03776912
Iteration 13993, loss = 0.03776601
Iteration 13994, loss = 0.03776286
Iteration 13995, loss = 0.03775971
Iteration 13996, loss = 0.03775659
Iteration 13997, loss = 0.03775346
Iteration 13998, loss = 0.03775033
Iteration 13999, loss = 0.03774718
Iteration 14000, loss = 0.03774404
Iteration 14001, loss = 0.03774091
Iteration 14002, loss = 0.03773779
Iteration 14003, loss = 0.03773466
Iteration 14004, loss = 0.03773149
Iteration 14005, loss = 0.03772839
Iteration 14006, loss = 0.03772526
Iteration 14007, loss = 0.03772215
Iteration 14008, loss = 0.03771903
Iteration 14009, loss = 0.03771592
Iteration 14010, loss = 0.03771276
Iteration 14011, loss = 0.03770965
Iteration 14012, loss = 0.03770653
Iteration 14013, loss = 0.03770337
Iteration 14014, loss = 0.03770032
Iteration 14015, loss = 0.03769715
Iteration 14016, loss = 0.03769401
Iteration 14017, loss = 0.03769089
Iteration 14018, loss = 0.03768778
Iteration 14019, loss = 0.03768464
Iteration 14020, loss = 0.03768148
Iteration 14021, loss = 0.03767840
Iteration 14022, loss = 0.03767526
Iteration 14023, loss = 0.03767216
Iteration 14024, loss = 0.03766903
Iteration 14025, loss = 0.03766592
Iteration 14026, loss = 0.03766279
Iteration 14027, loss = 0.03765970
Iteration 14028, loss = 0.03765654
Iteration 14029, loss = 0.03765345
Iteration 14030, loss = 0.03765033
Iteration 14031, loss = 0.03764719
Iteration 14032, loss = 0.03764412
Iteration 14033, loss = 0.03764101
Iteration 14034, loss = 0.03763791
Iteration 14035, loss = 0.03763477
Iteration 14036, loss = 0.03763169
Iteration 14037, loss = 0.03762852
Iteration 14038, loss = 0.03762544
Iteration 14039, loss = 0.03762235
Iteration 14040, loss = 0.03761924
Iteration 14041, loss = 0.03761616
Iteration 14042, loss = 0.03761304
Iteration 14043, loss = 0.03760992
Iteration 14044, loss = 0.03760682
Iteration 14045, loss = 0.03760374
Iteration 14046, loss = 0.03760061
Iteration 14047, loss = 0.03759750
Iteration 14048, loss = 0.03759441
Iteration 14049, loss = 0.03759128
Iteration 14050, loss = 0.03758818
Iteration 14051, loss = 0.03758511
Iteration 14052, loss = 0.03758200
Iteration 14053, loss = 0.03757889
Iteration 14054, loss = 0.03757581
Iteration 14055, loss = 0.03757269
Iteration 14056, loss = 0.03756959
Iteration 14057, loss = 0.03756650
Iteration 14058, loss = 0.03756337
Iteration 14059, loss = 0.03756028
Iteration 14060, loss = 0.03755723
Iteration 14061, loss = 0.03755413
Iteration 14062, loss = 0.03755104
Iteration 14063, loss = 0.03754790
Iteration 14064, loss = 0.03754483
Iteration 14065, loss = 0.03754172
Iteration 14066, loss = 0.03753863
Iteration 14067, loss = 0.03753555
Iteration 14068, loss = 0.03753248
Iteration 14069, loss = 0.03752935
Iteration 14070, loss = 0.03752632
Iteration 14071, loss = 0.03752319
Iteration 14072, loss = 0.03752010
Iteration 14073, loss = 0.03751699
Iteration 14074, loss = 0.03751393
Iteration 14075, loss = 0.03751081
Iteration 14076, loss = 0.03750775
Iteration 14077, loss = 0.03750466
Iteration 14078, loss = 0.03750153
Iteration 14079, loss = 0.03749849
Iteration 14080, loss = 0.03749537
Iteration 14081, loss = 0.03749230
Iteration 14082, loss = 0.03748923
Iteration 14083, loss = 0.03748611
Iteration 14084, loss = 0.03748308
Iteration 14085, loss = 0.03747994
Iteration 14086, loss = 0.03747690
Iteration 14087, loss = 0.03747379
Iteration 14088, loss = 0.03747071
Iteration 14089, loss = 0.03746764
Iteration 14090, loss = 0.03746455
Iteration 14091, loss = 0.03746146
Iteration 14092, loss = 0.03745843
Iteration 14093, loss = 0.03745535
Iteration 14094, loss = 0.03745224
Iteration 14095, loss = 0.03744917
Iteration 14096, loss = 0.03744610
Iteration 14097, loss = 0.03744306
Iteration 14098, loss = 0.03743993
Iteration 14099, loss = 0.03743689
Iteration 14100, loss = 0.03743382
Iteration 14101, loss = 0.03743071
Iteration 14102, loss = 0.03742764
Iteration 14103, loss = 0.03742455
Iteration 14104, loss = 0.03742149
Iteration 14105, loss = 0.03741843
Iteration 14106, loss = 0.03741532
Iteration 14107, loss = 0.03741227
Iteration 14108, loss = 0.03740919
Iteration 14109, loss = 0.03740616
Iteration 14110, loss = 0.03740306
Iteration 14111, loss = 0.03739995
Iteration 14112, loss = 0.03739693
Iteration 14113, loss = 0.03739387
Iteration 14114, loss = 0.03739077
Iteration 14115, loss = 0.03738774
Iteration 14116, loss = 0.03738463
Iteration 14117, loss = 0.03738160
Iteration 14118, loss = 0.03737852
Iteration 14119, loss = 0.03737544
Iteration 14120, loss = 0.03737238
Iteration 14121, loss = 0.03736931
Iteration 14122, loss = 0.03736626
Iteration 14123, loss = 0.03736321
Iteration 14124, loss = 0.03736014
Iteration 14125, loss = 0.03735707
Iteration 14126, loss = 0.03735403
Iteration 14127, loss = 0.03735097
Iteration 14128, loss = 0.03734788
Iteration 14129, loss = 0.03734484
Iteration 14130, loss = 0.03734180
Iteration 14131, loss = 0.03733871
Iteration 14132, loss = 0.03733566
Iteration 14133, loss = 0.03733259
Iteration 14134, loss = 0.03732951
Iteration 14135, loss = 0.03732645
Iteration 14136, loss = 0.03732339
Iteration 14137, loss = 0.03732031
Iteration 14138, loss = 0.03731724
Iteration 14139, loss = 0.03731419
Iteration 14140, loss = 0.03731114
Iteration 14141, loss = 0.03730805
Iteration 14142, loss = 0.03730501
Iteration 14143, loss = 0.03730195
Iteration 14144, loss = 0.03729890
Iteration 14145, loss = 0.03729585
Iteration 14146, loss = 0.03729280
Iteration 14147, loss = 0.03728973
Iteration 14148, loss = 0.03728666
Iteration 14149, loss = 0.03728362
Iteration 14150, loss = 0.03728057
Iteration 14151, loss = 0.03727754
Iteration 14152, loss = 0.03727449
Iteration 14153, loss = 0.03727139
Iteration 14154, loss = 0.03726835
Iteration 14155, loss = 0.03726535
Iteration 14156, loss = 0.03726228
Iteration 14157, loss = 0.03725921
Iteration 14158, loss = 0.03725616
Iteration 14159, loss = 0.03725314
Iteration 14160, loss = 0.03725009
Iteration 14161, loss = 0.03724706
Iteration 14162, loss = 0.03724399
Iteration 14163, loss = 0.03724092
Iteration 14164, loss = 0.03723791
Iteration 14165, loss = 0.03723487
Iteration 14166, loss = 0.03723178
Iteration 14167, loss = 0.03722876
Iteration 14168, loss = 0.03722575
Iteration 14169, loss = 0.03722268
Iteration 14170, loss = 0.03721964
Iteration 14171, loss = 0.03721663
Iteration 14172, loss = 0.03721357
Iteration 14173, loss = 0.03721053
Iteration 14174, loss = 0.03720751
Iteration 14175, loss = 0.03720448
Iteration 14176, loss = 0.03720142
Iteration 14177, loss = 0.03719836
Iteration 14178, loss = 0.03719536
Iteration 14179, loss = 0.03719230
Iteration 14180, loss = 0.03718925
Iteration 14181, loss = 0.03718621
Iteration 14182, loss = 0.03718320
Iteration 14183, loss = 0.03718014
Iteration 14184, loss = 0.03717715
Iteration 14185, loss = 0.03717409
Iteration 14186, loss = 0.03717107
Iteration 14187, loss = 0.03716804
Iteration 14188, loss = 0.03716497
Iteration 14189, loss = 0.03716195
Iteration 14190, loss = 0.03715891
Iteration 14191, loss = 0.03715587
Iteration 14192, loss = 0.03715286
Iteration 14193, loss = 0.03714979
Iteration 14194, loss = 0.03714674
Iteration 14195, loss = 0.03714371
Iteration 14196, loss = 0.03714071
Iteration 14197, loss = 0.03713768
Iteration 14198, loss = 0.03713465
Iteration 14199, loss = 0.03713159
Iteration 14200, loss = 0.03712857
Iteration 14201, loss = 0.03712555
Iteration 14202, loss = 0.03712252
Iteration 14203, loss = 0.03711949
Iteration 14204, loss = 0.03711644
Iteration 14205, loss = 0.03711344
Iteration 14206, loss = 0.03711041
Iteration 14207, loss = 0.03710739
Iteration 14208, loss = 0.03710435
Iteration 14209, loss = 0.03710132
Iteration 14210, loss = 0.03709831
Iteration 14211, loss = 0.03709532
Iteration 14212, loss = 0.03709224
Iteration 14213, loss = 0.03708923
Iteration 14214, loss = 0.03708621
Iteration 14215, loss = 0.03708321
Iteration 14216, loss = 0.03708017
Iteration 14217, loss = 0.03707718
Iteration 14218, loss = 0.03707415
Iteration 14219, loss = 0.03707114
Iteration 14220, loss = 0.03706814
Iteration 14221, loss = 0.03706510
Iteration 14222, loss = 0.03706208
Iteration 14223, loss = 0.03705907
Iteration 14224, loss = 0.03705606
Iteration 14225, loss = 0.03705308
Iteration 14226, loss = 0.03705008
Iteration 14227, loss = 0.03704705
Iteration 14228, loss = 0.03704400
Iteration 14229, loss = 0.03704101
Iteration 14230, loss = 0.03703799
Iteration 14231, loss = 0.03703499
Iteration 14232, loss = 0.03703200
Iteration 14233, loss = 0.03702897
Iteration 14234, loss = 0.03702600
Iteration 14235, loss = 0.03702294
Iteration 14236, loss = 0.03701994
Iteration 14237, loss = 0.03701693
Iteration 14238, loss = 0.03701392
Iteration 14239, loss = 0.03701090
Iteration 14240, loss = 0.03700790
Iteration 14241, loss = 0.03700492
Iteration 14242, loss = 0.03700194
Iteration 14243, loss = 0.03699886
Iteration 14244, loss = 0.03699587
Iteration 14245, loss = 0.03699291
Iteration 14246, loss = 0.03698988
Iteration 14247, loss = 0.03698690
Iteration 14248, loss = 0.03698391
Iteration 14249, loss = 0.03698088
Iteration 14250, loss = 0.03697793
Iteration 14251, loss = 0.03697494
Iteration 14252, loss = 0.03697190
Iteration 14253, loss = 0.03696894
Iteration 14254, loss = 0.03696592
Iteration 14255, loss = 0.03696292
Iteration 14256, loss = 0.03695994
Iteration 14257, loss = 0.03695697
Iteration 14258, loss = 0.03695395
Iteration 14259, loss = 0.03695096
Iteration 14260, loss = 0.03694801
Iteration 14261, loss = 0.03694499
Iteration 14262, loss = 0.03694199
Iteration 14263, loss = 0.03693898
Iteration 14264, loss = 0.03693599
Iteration 14265, loss = 0.03693304
Iteration 14266, loss = 0.03693004
Iteration 14267, loss = 0.03692704
Iteration 14268, loss = 0.03692405
Iteration 14269, loss = 0.03692106
Iteration 14270, loss = 0.03691807
Iteration 14271, loss = 0.03691507
Iteration 14272, loss = 0.03691206
Iteration 14273, loss = 0.03690906
Iteration 14274, loss = 0.03690605
Iteration 14275, loss = 0.03690307
Iteration 14276, loss = 0.03690007
Iteration 14277, loss = 0.03689706
Iteration 14278, loss = 0.03689408
Iteration 14279, loss = 0.03689110
Iteration 14280, loss = 0.03688812
Iteration 14281, loss = 0.03688509
Iteration 14282, loss = 0.03688214
Iteration 14283, loss = 0.03687913
Iteration 14284, loss = 0.03687615
Iteration 14285, loss = 0.03687317
Iteration 14286, loss = 0.03687017
Iteration 14287, loss = 0.03686722
Iteration 14288, loss = 0.03686424
Iteration 14289, loss = 0.03686129
Iteration 14290, loss = 0.03685828
Iteration 14291, loss = 0.03685526
Iteration 14292, loss = 0.03685227
Iteration 14293, loss = 0.03684933
Iteration 14294, loss = 0.03684629
Iteration 14295, loss = 0.03684336
Iteration 14296, loss = 0.03684035
Iteration 14297, loss = 0.03683738
Iteration 14298, loss = 0.03683440
Iteration 14299, loss = 0.03683142
Iteration 14300, loss = 0.03682841
Iteration 14301, loss = 0.03682541
Iteration 14302, loss = 0.03682247
Iteration 14303, loss = 0.03681950
Iteration 14304, loss = 0.03681652
Iteration 14305, loss = 0.03681355
Iteration 14306, loss = 0.03681053
Iteration 14307, loss = 0.03680759
Iteration 14308, loss = 0.03680458
Iteration 14309, loss = 0.03680164
Iteration 14310, loss = 0.03679865
Iteration 14311, loss = 0.03679569
Iteration 14312, loss = 0.03679273
Iteration 14313, loss = 0.03678971
Iteration 14314, loss = 0.03678672
Iteration 14315, loss = 0.03678374
Iteration 14316, loss = 0.03678076
Iteration 14317, loss = 0.03677781
Iteration 14318, loss = 0.03677480
Iteration 14319, loss = 0.03677189
Iteration 14320, loss = 0.03676886
Iteration 14321, loss = 0.03676590
Iteration 14322, loss = 0.03676295
Iteration 14323, loss = 0.03676000
Iteration 14324, loss = 0.03675700
Iteration 14325, loss = 0.03675402
Iteration 14326, loss = 0.03675109
Iteration 14327, loss = 0.03674813
Iteration 14328, loss = 0.03674514
Iteration 14329, loss = 0.03674214
Iteration 14330, loss = 0.03673924
Iteration 14331, loss = 0.03673625
Iteration 14332, loss = 0.03673326
Iteration 14333, loss = 0.03673032
Iteration 14334, loss = 0.03672734
Iteration 14335, loss = 0.03672437
Iteration 14336, loss = 0.03672144
Iteration 14337, loss = 0.03671849
Iteration 14338, loss = 0.03671550
Iteration 14339, loss = 0.03671256
Iteration 14340, loss = 0.03670958
Iteration 14341, loss = 0.03670661
Iteration 14342, loss = 0.03670363
Iteration 14343, loss = 0.03670069
Iteration 14344, loss = 0.03669772
Iteration 14345, loss = 0.03669478
Iteration 14346, loss = 0.03669182
Iteration 14347, loss = 0.03668886
Iteration 14348, loss = 0.03668592
Iteration 14349, loss = 0.03668296
Iteration 14350, loss = 0.03668000
Iteration 14351, loss = 0.03667705
Iteration 14352, loss = 0.03667410
Iteration 14353, loss = 0.03667118
Iteration 14354, loss = 0.03666821
Iteration 14355, loss = 0.03666524
Iteration 14356, loss = 0.03666232
Iteration 14357, loss = 0.03665933
Iteration 14358, loss = 0.03665643
Iteration 14359, loss = 0.03665346
Iteration 14360, loss = 0.03665050
Iteration 14361, loss = 0.03664758
Iteration 14362, loss = 0.03664462
Iteration 14363, loss = 0.03664166
Iteration 14364, loss = 0.03663874
Iteration 14365, loss = 0.03663575
Iteration 14366, loss = 0.03663284
Iteration 14367, loss = 0.03662989
Iteration 14368, loss = 0.03662693
Iteration 14369, loss = 0.03662397
Iteration 14370, loss = 0.03662105
Iteration 14371, loss = 0.03661809
Iteration 14372, loss = 0.03661516
Iteration 14373, loss = 0.03661221
Iteration 14374, loss = 0.03660929
Iteration 14375, loss = 0.03660634
Iteration 14376, loss = 0.03660339
Iteration 14377, loss = 0.03660044
Iteration 14378, loss = 0.03659760
Iteration 14379, loss = 0.03659459
Iteration 14380, loss = 0.03659167
Iteration 14381, loss = 0.03658874
Iteration 14382, loss = 0.03658581
Iteration 14383, loss = 0.03658287
Iteration 14384, loss = 0.03657993
Iteration 14385, loss = 0.03657700
Iteration 14386, loss = 0.03657405
Iteration 14387, loss = 0.03657110
Iteration 14388, loss = 0.03656819
Iteration 14389, loss = 0.03656526
Iteration 14390, loss = 0.03656234
Iteration 14391, loss = 0.03655943
Iteration 14392, loss = 0.03655645
Iteration 14393, loss = 0.03655350
Iteration 14394, loss = 0.03655060
Iteration 14395, loss = 0.03654767
Iteration 14396, loss = 0.03654473
Iteration 14397, loss = 0.03654182
Iteration 14398, loss = 0.03653890
Iteration 14399, loss = 0.03653592
Iteration 14400, loss = 0.03653303
Iteration 14401, loss = 0.03653012
Iteration 14402, loss = 0.03652721
Iteration 14403, loss = 0.03652422
Iteration 14404, loss = 0.03652131
Iteration 14405, loss = 0.03651839
Iteration 14406, loss = 0.03651552
Iteration 14407, loss = 0.03651254
Iteration 14408, loss = 0.03650963
Iteration 14409, loss = 0.03650672
Iteration 14410, loss = 0.03650376
Iteration 14411, loss = 0.03650086
Iteration 14412, loss = 0.03649793
Iteration 14413, loss = 0.03649499
Iteration 14414, loss = 0.03649209
Iteration 14415, loss = 0.03648916
Iteration 14416, loss = 0.03648623
Iteration 14417, loss = 0.03648331
Iteration 14418, loss = 0.03648038
Iteration 14419, loss = 0.03647748
Iteration 14420, loss = 0.03647455
Iteration 14421, loss = 0.03647160
Iteration 14422, loss = 0.03646869
Iteration 14423, loss = 0.03646576
Iteration 14424, loss = 0.03646287
Iteration 14425, loss = 0.03645996
Iteration 14426, loss = 0.03645698
Iteration 14427, loss = 0.03645409
Iteration 14428, loss = 0.03645115
Iteration 14429, loss = 0.03644825
Iteration 14430, loss = 0.03644529
Iteration 14431, loss = 0.03644242
Iteration 14432, loss = 0.03643951
Iteration 14433, loss = 0.03643659
Iteration 14434, loss = 0.03643362
Iteration 14435, loss = 0.03643074
Iteration 14436, loss = 0.03642785
Iteration 14437, loss = 0.03642494
Iteration 14438, loss = 0.03642202
Iteration 14439, loss = 0.03641905
Iteration 14440, loss = 0.03641617
Iteration 14441, loss = 0.03641322
Iteration 14442, loss = 0.03641031
Iteration 14443, loss = 0.03640738
Iteration 14444, loss = 0.03640451
Iteration 14445, loss = 0.03640156
Iteration 14446, loss = 0.03639867
Iteration 14447, loss = 0.03639577
Iteration 14448, loss = 0.03639288
Iteration 14449, loss = 0.03638992
Iteration 14450, loss = 0.03638703
Iteration 14451, loss = 0.03638416
Iteration 14452, loss = 0.03638124
Iteration 14453, loss = 0.03637834
Iteration 14454, loss = 0.03637543
Iteration 14455, loss = 0.03637254
Iteration 14456, loss = 0.03636962
Iteration 14457, loss = 0.03636673
Iteration 14458, loss = 0.03636386
Iteration 14459, loss = 0.03636090
Iteration 14460, loss = 0.03635805
Iteration 14461, loss = 0.03635512
Iteration 14462, loss = 0.03635223
Iteration 14463, loss = 0.03634934
Iteration 14464, loss = 0.03634642
Iteration 14465, loss = 0.03634357
Iteration 14466, loss = 0.03634065
Iteration 14467, loss = 0.03633773
Iteration 14468, loss = 0.03633485
Iteration 14469, loss = 0.03633195
Iteration 14470, loss = 0.03632905
Iteration 14471, loss = 0.03632615
Iteration 14472, loss = 0.03632322
Iteration 14473, loss = 0.03632035
Iteration 14474, loss = 0.03631745
Iteration 14475, loss = 0.03631453
Iteration 14476, loss = 0.03631166
Iteration 14477, loss = 0.03630874
Iteration 14478, loss = 0.03630587
Iteration 14479, loss = 0.03630297
Iteration 14480, loss = 0.03630007
Iteration 14481, loss = 0.03629717
Iteration 14482, loss = 0.03629429
Iteration 14483, loss = 0.03629139
Iteration 14484, loss = 0.03628848
Iteration 14485, loss = 0.03628564
Iteration 14486, loss = 0.03628273
Iteration 14487, loss = 0.03627980
Iteration 14488, loss = 0.03627694
Iteration 14489, loss = 0.03627403
Iteration 14490, loss = 0.03627115
Iteration 14491, loss = 0.03626828
Iteration 14492, loss = 0.03626534
Iteration 14493, loss = 0.03626247
Iteration 14494, loss = 0.03625957
Iteration 14495, loss = 0.03625666
Iteration 14496, loss = 0.03625378
Iteration 14497, loss = 0.03625088
Iteration 14498, loss = 0.03624802
Iteration 14499, loss = 0.03624513
Iteration 14500, loss = 0.03624223
Iteration 14501, loss = 0.03623937
Iteration 14502, loss = 0.03623648
Iteration 14503, loss = 0.03623354
Iteration 14504, loss = 0.03623073
Iteration 14505, loss = 0.03622780
Iteration 14506, loss = 0.03622493
Iteration 14507, loss = 0.03622204
Iteration 14508, loss = 0.03621916
Iteration 14509, loss = 0.03621630
Iteration 14510, loss = 0.03621338
Iteration 14511, loss = 0.03621053
Iteration 14512, loss = 0.03620767
Iteration 14513, loss = 0.03620477
Iteration 14514, loss = 0.03620192
Iteration 14515, loss = 0.03619903
Iteration 14516, loss = 0.03619616
Iteration 14517, loss = 0.03619325
Iteration 14518, loss = 0.03619037
Iteration 14519, loss = 0.03618753
Iteration 14520, loss = 0.03618465
Iteration 14521, loss = 0.03618177
Iteration 14522, loss = 0.03617890
Iteration 14523, loss = 0.03617603
Iteration 14524, loss = 0.03617317
Iteration 14525, loss = 0.03617027
Iteration 14526, loss = 0.03616739
Iteration 14527, loss = 0.03616451
Iteration 14528, loss = 0.03616164
Iteration 14529, loss = 0.03615877
Iteration 14530, loss = 0.03615589
Iteration 14531, loss = 0.03615305
Iteration 14532, loss = 0.03615011
Iteration 14533, loss = 0.03614727
Iteration 14534, loss = 0.03614441
Iteration 14535, loss = 0.03614148
Iteration 14536, loss = 0.03613866
Iteration 14537, loss = 0.03613576
Iteration 14538, loss = 0.03613287
Iteration 14539, loss = 0.03613000
Iteration 14540, loss = 0.03612715
Iteration 14541, loss = 0.03612430
Iteration 14542, loss = 0.03612142
Iteration 14543, loss = 0.03611854
Iteration 14544, loss = 0.03611570
Iteration 14545, loss = 0.03611278
Iteration 14546, loss = 0.03610994
Iteration 14547, loss = 0.03610707
Iteration 14548, loss = 0.03610422
Iteration 14549, loss = 0.03610135
Iteration 14550, loss = 0.03609849
Iteration 14551, loss = 0.03609562
Iteration 14552, loss = 0.03609278
Iteration 14553, loss = 0.03608991
Iteration 14554, loss = 0.03608706
Iteration 14555, loss = 0.03608420
Iteration 14556, loss = 0.03608134
Iteration 14557, loss = 0.03607851
Iteration 14558, loss = 0.03607560
Iteration 14559, loss = 0.03607272
Iteration 14560, loss = 0.03606991
Iteration 14561, loss = 0.03606704
Iteration 14562, loss = 0.03606416
Iteration 14563, loss = 0.03606132
Iteration 14564, loss = 0.03605851
Iteration 14565, loss = 0.03605559
Iteration 14566, loss = 0.03605277
Iteration 14567, loss = 0.03604988
Iteration 14568, loss = 0.03604707
Iteration 14569, loss = 0.03604418
Iteration 14570, loss = 0.03604134
Iteration 14571, loss = 0.03603852
Iteration 14572, loss = 0.03603565
Iteration 14573, loss = 0.03603278
Iteration 14574, loss = 0.03602992
Iteration 14575, loss = 0.03602708
Iteration 14576, loss = 0.03602422
Iteration 14577, loss = 0.03602137
Iteration 14578, loss = 0.03601855
Iteration 14579, loss = 0.03601568
Iteration 14580, loss = 0.03601281
Iteration 14581, loss = 0.03600996
Iteration 14582, loss = 0.03600717
Iteration 14583, loss = 0.03600425
Iteration 14584, loss = 0.03600142
Iteration 14585, loss = 0.03599858
Iteration 14586, loss = 0.03599569
Iteration 14587, loss = 0.03599289
Iteration 14588, loss = 0.03599003
Iteration 14589, loss = 0.03598716
Iteration 14590, loss = 0.03598438
Iteration 14591, loss = 0.03598150
Iteration 14592, loss = 0.03597866
Iteration 14593, loss = 0.03597578
Iteration 14594, loss = 0.03597295
Iteration 14595, loss = 0.03597014
Iteration 14596, loss = 0.03596728
Iteration 14597, loss = 0.03596443
Iteration 14598, loss = 0.03596158
Iteration 14599, loss = 0.03595875
Iteration 14600, loss = 0.03595590
Iteration 14601, loss = 0.03595308
Iteration 14602, loss = 0.03595023
Iteration 14603, loss = 0.03594741
Iteration 14604, loss = 0.03594455
Iteration 14605, loss = 0.03594174
Iteration 14606, loss = 0.03593893
Iteration 14607, loss = 0.03593604
Iteration 14608, loss = 0.03593324
Iteration 14609, loss = 0.03593040
Iteration 14610, loss = 0.03592755
Iteration 14611, loss = 0.03592472
Iteration 14612, loss = 0.03592188
Iteration 14613, loss = 0.03591908
Iteration 14614, loss = 0.03591624
Iteration 14615, loss = 0.03591340
Iteration 14616, loss = 0.03591060
Iteration 14617, loss = 0.03590777
Iteration 14618, loss = 0.03590492
Iteration 14619, loss = 0.03590211
Iteration 14620, loss = 0.03589931
Iteration 14621, loss = 0.03589650
Iteration 14622, loss = 0.03589364
Iteration 14623, loss = 0.03589081
Iteration 14624, loss = 0.03588798
Iteration 14625, loss = 0.03588518
Iteration 14626, loss = 0.03588229
Iteration 14627, loss = 0.03587954
Iteration 14628, loss = 0.03587668
Iteration 14629, loss = 0.03587383
Iteration 14630, loss = 0.03587102
Iteration 14631, loss = 0.03586817
Iteration 14632, loss = 0.03586534
Iteration 14633, loss = 0.03586254
Iteration 14634, loss = 0.03585972
Iteration 14635, loss = 0.03585689
Iteration 14636, loss = 0.03585407
Iteration 14637, loss = 0.03585122
Iteration 14638, loss = 0.03584843
Iteration 14639, loss = 0.03584559
Iteration 14640, loss = 0.03584275
Iteration 14641, loss = 0.03583993
Iteration 14642, loss = 0.03583709
Iteration 14643, loss = 0.03583429
Iteration 14644, loss = 0.03583145
Iteration 14645, loss = 0.03582862
Iteration 14646, loss = 0.03582582
Iteration 14647, loss = 0.03582300
Iteration 14648, loss = 0.03582015
Iteration 14649, loss = 0.03581735
Iteration 14650, loss = 0.03581454
Iteration 14651, loss = 0.03581171
Iteration 14652, loss = 0.03580892
Iteration 14653, loss = 0.03580606
Iteration 14654, loss = 0.03580326
Iteration 14655, loss = 0.03580041
Iteration 14656, loss = 0.03579757
Iteration 14657, loss = 0.03579482
Iteration 14658, loss = 0.03579195
Iteration 14659, loss = 0.03578914
Iteration 14660, loss = 0.03578635
Iteration 14661, loss = 0.03578353
Iteration 14662, loss = 0.03578069
Iteration 14663, loss = 0.03577787
Iteration 14664, loss = 0.03577505
Iteration 14665, loss = 0.03577227
Iteration 14666, loss = 0.03576943
Iteration 14667, loss = 0.03576664
Iteration 14668, loss = 0.03576382
Iteration 14669, loss = 0.03576101
Iteration 14670, loss = 0.03575820
Iteration 14671, loss = 0.03575538
Iteration 14672, loss = 0.03575259
Iteration 14673, loss = 0.03574977
Iteration 14674, loss = 0.03574697
Iteration 14675, loss = 0.03574418
Iteration 14676, loss = 0.03574133
Iteration 14677, loss = 0.03573850
Iteration 14678, loss = 0.03573574
Iteration 14679, loss = 0.03573289
Iteration 14680, loss = 0.03573014
Iteration 14681, loss = 0.03572732
Iteration 14682, loss = 0.03572450
Iteration 14683, loss = 0.03572171
Iteration 14684, loss = 0.03571890
Iteration 14685, loss = 0.03571608
Iteration 14686, loss = 0.03571326
Iteration 14687, loss = 0.03571051
Iteration 14688, loss = 0.03570766
Iteration 14689, loss = 0.03570482
Iteration 14690, loss = 0.03570202
Iteration 14691, loss = 0.03569923
Iteration 14692, loss = 0.03569641
Iteration 14693, loss = 0.03569362
Iteration 14694, loss = 0.03569079
Iteration 14695, loss = 0.03568803
Iteration 14696, loss = 0.03568520
Iteration 14697, loss = 0.03568241
Iteration 14698, loss = 0.03567963
Iteration 14699, loss = 0.03567683
Iteration 14700, loss = 0.03567402
Iteration 14701, loss = 0.03567123
Iteration 14702, loss = 0.03566844
Iteration 14703, loss = 0.03566562
Iteration 14704, loss = 0.03566284
Iteration 14705, loss = 0.03566006
Iteration 14706, loss = 0.03565727
Iteration 14707, loss = 0.03565447
Iteration 14708, loss = 0.03565169
Iteration 14709, loss = 0.03564890
Iteration 14710, loss = 0.03564610
Iteration 14711, loss = 0.03564334
Iteration 14712, loss = 0.03564047
Iteration 14713, loss = 0.03563773
Iteration 14714, loss = 0.03563491
Iteration 14715, loss = 0.03563215
Iteration 14716, loss = 0.03562934
Iteration 14717, loss = 0.03562655
Iteration 14718, loss = 0.03562373
Iteration 14719, loss = 0.03562098
Iteration 14720, loss = 0.03561823
Iteration 14721, loss = 0.03561538
Iteration 14722, loss = 0.03561263
Iteration 14723, loss = 0.03560984
Iteration 14724, loss = 0.03560705
Iteration 14725, loss = 0.03560427
Iteration 14726, loss = 0.03560149
Iteration 14727, loss = 0.03559873
Iteration 14728, loss = 0.03559590
Iteration 14729, loss = 0.03559313
Iteration 14730, loss = 0.03559036
Iteration 14731, loss = 0.03558757
Iteration 14732, loss = 0.03558480
Iteration 14733, loss = 0.03558196
Iteration 14734, loss = 0.03557918
Iteration 14735, loss = 0.03557643
Iteration 14736, loss = 0.03557365
Iteration 14737, loss = 0.03557084
Iteration 14738, loss = 0.03556806
Iteration 14739, loss = 0.03556526
Iteration 14740, loss = 0.03556248
Iteration 14741, loss = 0.03555971
Iteration 14742, loss = 0.03555693
Iteration 14743, loss = 0.03555413
Iteration 14744, loss = 0.03555137
Iteration 14745, loss = 0.03554858
Iteration 14746, loss = 0.03554583
Iteration 14747, loss = 0.03554303
Iteration 14748, loss = 0.03554026
Iteration 14749, loss = 0.03553748
Iteration 14750, loss = 0.03553475
Iteration 14751, loss = 0.03553192
Iteration 14752, loss = 0.03552914
Iteration 14753, loss = 0.03552639
Iteration 14754, loss = 0.03552360
Iteration 14755, loss = 0.03552084
Iteration 14756, loss = 0.03551807
Iteration 14757, loss = 0.03551531
Iteration 14758, loss = 0.03551252
Iteration 14759, loss = 0.03550979
Iteration 14760, loss = 0.03550694
Iteration 14761, loss = 0.03550420
Iteration 14762, loss = 0.03550144
Iteration 14763, loss = 0.03549868
Iteration 14764, loss = 0.03549587
Iteration 14765, loss = 0.03549312
Iteration 14766, loss = 0.03549038
Iteration 14767, loss = 0.03548756
Iteration 14768, loss = 0.03548478
Iteration 14769, loss = 0.03548202
Iteration 14770, loss = 0.03547925
Iteration 14771, loss = 0.03547652
Iteration 14772, loss = 0.03547373
Iteration 14773, loss = 0.03547093
Iteration 14774, loss = 0.03546818
Iteration 14775, loss = 0.03546542
Iteration 14776, loss = 0.03546268
Iteration 14777, loss = 0.03545987
Iteration 14778, loss = 0.03545715
Iteration 14779, loss = 0.03545438
Iteration 14780, loss = 0.03545158
Iteration 14781, loss = 0.03544883
Iteration 14782, loss = 0.03544604
Iteration 14783, loss = 0.03544329
Iteration 14784, loss = 0.03544055
Iteration 14785, loss = 0.03543775
Iteration 14786, loss = 0.03543502
Iteration 14787, loss = 0.03543220
Iteration 14788, loss = 0.03542948
Iteration 14789, loss = 0.03542672
Iteration 14790, loss = 0.03542395
Iteration 14791, loss = 0.03542119
Iteration 14792, loss = 0.03541845
Iteration 14793, loss = 0.03541572
Iteration 14794, loss = 0.03541292
Iteration 14795, loss = 0.03541017
Iteration 14796, loss = 0.03540740
Iteration 14797, loss = 0.03540464
Iteration 14798, loss = 0.03540190
Iteration 14799, loss = 0.03539914
Iteration 14800, loss = 0.03539637
Iteration 14801, loss = 0.03539363
Iteration 14802, loss = 0.03539086
Iteration 14803, loss = 0.03538810
Iteration 14804, loss = 0.03538534
Iteration 14805, loss = 0.03538263
Iteration 14806, loss = 0.03537983
Iteration 14807, loss = 0.03537708
Iteration 14808, loss = 0.03537436
Iteration 14809, loss = 0.03537161
Iteration 14810, loss = 0.03536883
Iteration 14811, loss = 0.03536610
Iteration 14812, loss = 0.03536335
Iteration 14813, loss = 0.03536060
Iteration 14814, loss = 0.03535787
Iteration 14815, loss = 0.03535510
Iteration 14816, loss = 0.03535239
Iteration 14817, loss = 0.03534958
Iteration 14818, loss = 0.03534693
Iteration 14819, loss = 0.03534412
Iteration 14820, loss = 0.03534136
Iteration 14821, loss = 0.03533867
Iteration 14822, loss = 0.03533588
Iteration 14823, loss = 0.03533318
Iteration 14824, loss = 0.03533046
Iteration 14825, loss = 0.03532771
Iteration 14826, loss = 0.03532495
Iteration 14827, loss = 0.03532227
Iteration 14828, loss = 0.03531949
Iteration 14829, loss = 0.03531675
Iteration 14830, loss = 0.03531398
Iteration 14831, loss = 0.03531129
Iteration 14832, loss = 0.03530853
Iteration 14833, loss = 0.03530581
Iteration 14834, loss = 0.03530310
Iteration 14835, loss = 0.03530035
Iteration 14836, loss = 0.03529761
Iteration 14837, loss = 0.03529489
Iteration 14838, loss = 0.03529213
Iteration 14839, loss = 0.03528940
Iteration 14840, loss = 0.03528667
Iteration 14841, loss = 0.03528390
Iteration 14842, loss = 0.03528118
Iteration 14843, loss = 0.03527844
Iteration 14844, loss = 0.03527573
Iteration 14845, loss = 0.03527296
Iteration 14846, loss = 0.03527023
Iteration 14847, loss = 0.03526753
Iteration 14848, loss = 0.03526475
Iteration 14849, loss = 0.03526204
Iteration 14850, loss = 0.03525928
Iteration 14851, loss = 0.03525661
Iteration 14852, loss = 0.03525387
Iteration 14853, loss = 0.03525110
Iteration 14854, loss = 0.03524834
Iteration 14855, loss = 0.03524563
Iteration 14856, loss = 0.03524292
Iteration 14857, loss = 0.03524017
Iteration 14858, loss = 0.03523746
Iteration 14859, loss = 0.03523470
Iteration 14860, loss = 0.03523202
Iteration 14861, loss = 0.03522929
Iteration 14862, loss = 0.03522656
Iteration 14863, loss = 0.03522383
Iteration 14864, loss = 0.03522112
Iteration 14865, loss = 0.03521837
Iteration 14866, loss = 0.03521566
Iteration 14867, loss = 0.03521291
Iteration 14868, loss = 0.03521026
Iteration 14869, loss = 0.03520748
Iteration 14870, loss = 0.03520475
Iteration 14871, loss = 0.03520203
Iteration 14872, loss = 0.03519930
Iteration 14873, loss = 0.03519659
Iteration 14874, loss = 0.03519387
Iteration 14875, loss = 0.03519117
Iteration 14876, loss = 0.03518843
Iteration 14877, loss = 0.03518572
Iteration 14878, loss = 0.03518296
Iteration 14879, loss = 0.03518028
Iteration 14880, loss = 0.03517752
Iteration 14881, loss = 0.03517482
Iteration 14882, loss = 0.03517211
Iteration 14883, loss = 0.03516938
Iteration 14884, loss = 0.03516669
Iteration 14885, loss = 0.03516395
Iteration 14886, loss = 0.03516122
Iteration 14887, loss = 0.03515851
Iteration 14888, loss = 0.03515579
Iteration 14889, loss = 0.03515309
Iteration 14890, loss = 0.03515036
Iteration 14891, loss = 0.03514767
Iteration 14892, loss = 0.03514495
Iteration 14893, loss = 0.03514227
Iteration 14894, loss = 0.03513954
Iteration 14895, loss = 0.03513685
Iteration 14896, loss = 0.03513417
Iteration 14897, loss = 0.03513140
Iteration 14898, loss = 0.03512870
Iteration 14899, loss = 0.03512601
Iteration 14900, loss = 0.03512331
Iteration 14901, loss = 0.03512057
Iteration 14902, loss = 0.03511787
Iteration 14903, loss = 0.03511517
Iteration 14904, loss = 0.03511245
Iteration 14905, loss = 0.03510973
Iteration 14906, loss = 0.03510702
Iteration 14907, loss = 0.03510434
Iteration 14908, loss = 0.03510162
Iteration 14909, loss = 0.03509890
Iteration 14910, loss = 0.03509621
Iteration 14911, loss = 0.03509352
Iteration 14912, loss = 0.03509080
Iteration 14913, loss = 0.03508808
Iteration 14914, loss = 0.03508544
Iteration 14915, loss = 0.03508271
Iteration 14916, loss = 0.03508000
Iteration 14917, loss = 0.03507729
Iteration 14918, loss = 0.03507457
Iteration 14919, loss = 0.03507189
Iteration 14920, loss = 0.03506919
Iteration 14921, loss = 0.03506651
Iteration 14922, loss = 0.03506377
Iteration 14923, loss = 0.03506110
Iteration 14924, loss = 0.03505840
Iteration 14925, loss = 0.03505572
Iteration 14926, loss = 0.03505300
Iteration 14927, loss = 0.03505032
Iteration 14928, loss = 0.03504764
Iteration 14929, loss = 0.03504492
Iteration 14930, loss = 0.03504223
Iteration 14931, loss = 0.03503953
Iteration 14932, loss = 0.03503681
Iteration 14933, loss = 0.03503412
Iteration 14934, loss = 0.03503141
Iteration 14935, loss = 0.03502869
Iteration 14936, loss = 0.03502599
Iteration 14937, loss = 0.03502328
Iteration 14938, loss = 0.03502059
Iteration 14939, loss = 0.03501792
Iteration 14940, loss = 0.03501516
Iteration 14941, loss = 0.03501249
Iteration 14942, loss = 0.03500980
Iteration 14943, loss = 0.03500704
Iteration 14944, loss = 0.03500441
Iteration 14945, loss = 0.03500167
Iteration 14946, loss = 0.03499898
Iteration 14947, loss = 0.03499631
Iteration 14948, loss = 0.03499360
Iteration 14949, loss = 0.03499092
Iteration 14950, loss = 0.03498818
Iteration 14951, loss = 0.03498557
Iteration 14952, loss = 0.03498286
Iteration 14953, loss = 0.03498015
Iteration 14954, loss = 0.03497744
Iteration 14955, loss = 0.03497478
Iteration 14956, loss = 0.03497207
Iteration 14957, loss = 0.03496943
Iteration 14958, loss = 0.03496671
Iteration 14959, loss = 0.03496400
Iteration 14960, loss = 0.03496129
Iteration 14961, loss = 0.03495864
Iteration 14962, loss = 0.03495595
Iteration 14963, loss = 0.03495326
Iteration 14964, loss = 0.03495058
Iteration 14965, loss = 0.03494791
Iteration 14966, loss = 0.03494518
Iteration 14967, loss = 0.03494251
Iteration 14968, loss = 0.03493985
Iteration 14969, loss = 0.03493716
Iteration 14970, loss = 0.03493447
Iteration 14971, loss = 0.03493178
Iteration 14972, loss = 0.03492909
Iteration 14973, loss = 0.03492641
Iteration 14974, loss = 0.03492374
Iteration 14975, loss = 0.03492104
Iteration 14976, loss = 0.03491838
Iteration 14977, loss = 0.03491570
Iteration 14978, loss = 0.03491302
Iteration 14979, loss = 0.03491032
Iteration 14980, loss = 0.03490763
Iteration 14981, loss = 0.03490494
Iteration 14982, loss = 0.03490232
Iteration 14983, loss = 0.03489961
Iteration 14984, loss = 0.03489696
Iteration 14985, loss = 0.03489427
Iteration 14986, loss = 0.03489160
Iteration 14987, loss = 0.03488890
Iteration 14988, loss = 0.03488627
Iteration 14989, loss = 0.03488357
Iteration 14990, loss = 0.03488090
Iteration 14991, loss = 0.03487823
Iteration 14992, loss = 0.03487556
Iteration 14993, loss = 0.03487287
Iteration 14994, loss = 0.03487020
Iteration 14995, loss = 0.03486755
Iteration 14996, loss = 0.03486489
Iteration 14997, loss = 0.03486221
Iteration 14998, loss = 0.03485952
Iteration 14999, loss = 0.03485690
Iteration 15000, loss = 0.03485420
Iteration 15001, loss = 0.03485154
Iteration 15002, loss = 0.03484888
Iteration 15003, loss = 0.03484623
Iteration 15004, loss = 0.03484354
Iteration 15005, loss = 0.03484088
Iteration 15006, loss = 0.03483821
Iteration 15007, loss = 0.03483559
Iteration 15008, loss = 0.03483290
Iteration 15009, loss = 0.03483024
Iteration 15010, loss = 0.03482757
Iteration 15011, loss = 0.03482492
Iteration 15012, loss = 0.03482229
Iteration 15013, loss = 0.03481961
Iteration 15014, loss = 0.03481692
Iteration 15015, loss = 0.03481427
Iteration 15016, loss = 0.03481160
Iteration 15017, loss = 0.03480898
Iteration 15018, loss = 0.03480635
Iteration 15019, loss = 0.03480364
Iteration 15020, loss = 0.03480094
Iteration 15021, loss = 0.03479828
Iteration 15022, loss = 0.03479566
Iteration 15023, loss = 0.03479295
Iteration 15024, loss = 0.03479029
Iteration 15025, loss = 0.03478766
Iteration 15026, loss = 0.03478500
Iteration 15027, loss = 0.03478230
Iteration 15028, loss = 0.03477963
Iteration 15029, loss = 0.03477700
Iteration 15030, loss = 0.03477433
Iteration 15031, loss = 0.03477162
Iteration 15032, loss = 0.03476900
Iteration 15033, loss = 0.03476634
Iteration 15034, loss = 0.03476368
Iteration 15035, loss = 0.03476101
Iteration 15036, loss = 0.03475837
Iteration 15037, loss = 0.03475571
Iteration 15038, loss = 0.03475308
Iteration 15039, loss = 0.03475041
Iteration 15040, loss = 0.03474774
Iteration 15041, loss = 0.03474509
Iteration 15042, loss = 0.03474241
Iteration 15043, loss = 0.03473978
Iteration 15044, loss = 0.03473710
Iteration 15045, loss = 0.03473442
Iteration 15046, loss = 0.03473180
Iteration 15047, loss = 0.03472912
Iteration 15048, loss = 0.03472647
Iteration 15049, loss = 0.03472379
Iteration 15050, loss = 0.03472118
Iteration 15051, loss = 0.03471849
Iteration 15052, loss = 0.03471585
Iteration 15053, loss = 0.03471319
Iteration 15054, loss = 0.03471052
Iteration 15055, loss = 0.03470787
Iteration 15056, loss = 0.03470522
Iteration 15057, loss = 0.03470256
Iteration 15058, loss = 0.03469993
Iteration 15059, loss = 0.03469727
Iteration 15060, loss = 0.03469461
Iteration 15061, loss = 0.03469195
Iteration 15062, loss = 0.03468934
Iteration 15063, loss = 0.03468669
Iteration 15064, loss = 0.03468400
Iteration 15065, loss = 0.03468136
Iteration 15066, loss = 0.03467869
Iteration 15067, loss = 0.03467607
Iteration 15068, loss = 0.03467343
Iteration 15069, loss = 0.03467080
Iteration 15070, loss = 0.03466812
Iteration 15071, loss = 0.03466553
Iteration 15072, loss = 0.03466283
Iteration 15073, loss = 0.03466019
Iteration 15074, loss = 0.03465756
Iteration 15075, loss = 0.03465493
Iteration 15076, loss = 0.03465227
Iteration 15077, loss = 0.03464965
Iteration 15078, loss = 0.03464698
Iteration 15079, loss = 0.03464439
Iteration 15080, loss = 0.03464170
Iteration 15081, loss = 0.03463907
Iteration 15082, loss = 0.03463643
Iteration 15083, loss = 0.03463379
Iteration 15084, loss = 0.03463112
Iteration 15085, loss = 0.03462855
Iteration 15086, loss = 0.03462589
Iteration 15087, loss = 0.03462325
Iteration 15088, loss = 0.03462062
Iteration 15089, loss = 0.03461795
Iteration 15090, loss = 0.03461535
Iteration 15091, loss = 0.03461271
Iteration 15092, loss = 0.03461007
Iteration 15093, loss = 0.03460744
Iteration 15094, loss = 0.03460481
Iteration 15095, loss = 0.03460217
Iteration 15096, loss = 0.03459957
Iteration 15097, loss = 0.03459691
Iteration 15098, loss = 0.03459431
Iteration 15099, loss = 0.03459166
Iteration 15100, loss = 0.03458905
Iteration 15101, loss = 0.03458639
Iteration 15102, loss = 0.03458377
Iteration 15103, loss = 0.03458113
Iteration 15104, loss = 0.03457851
Iteration 15105, loss = 0.03457589
Iteration 15106, loss = 0.03457327
Iteration 15107, loss = 0.03457061
Iteration 15108, loss = 0.03456800
Iteration 15109, loss = 0.03456537
Iteration 15110, loss = 0.03456272
Iteration 15111, loss = 0.03456013
Iteration 15112, loss = 0.03455750
Iteration 15113, loss = 0.03455484
Iteration 15114, loss = 0.03455224
Iteration 15115, loss = 0.03454960
Iteration 15116, loss = 0.03454696
Iteration 15117, loss = 0.03454433
Iteration 15118, loss = 0.03454171
Iteration 15119, loss = 0.03453910
Iteration 15120, loss = 0.03453644
Iteration 15121, loss = 0.03453380
Iteration 15122, loss = 0.03453116
Iteration 15123, loss = 0.03452852
Iteration 15124, loss = 0.03452593
Iteration 15125, loss = 0.03452331
Iteration 15126, loss = 0.03452066
Iteration 15127, loss = 0.03451805
Iteration 15128, loss = 0.03451544
Iteration 15129, loss = 0.03451280
Iteration 15130, loss = 0.03451018
Iteration 15131, loss = 0.03450756
Iteration 15132, loss = 0.03450494
Iteration 15133, loss = 0.03450230
Iteration 15134, loss = 0.03449972
Iteration 15135, loss = 0.03449710
Iteration 15136, loss = 0.03449444
Iteration 15137, loss = 0.03449186
Iteration 15138, loss = 0.03448927
Iteration 15139, loss = 0.03448666
Iteration 15140, loss = 0.03448406
Iteration 15141, loss = 0.03448143
Iteration 15142, loss = 0.03447879
Iteration 15143, loss = 0.03447618
Iteration 15144, loss = 0.03447356
Iteration 15145, loss = 0.03447095
Iteration 15146, loss = 0.03446833
Iteration 15147, loss = 0.03446569
Iteration 15148, loss = 0.03446307
Iteration 15149, loss = 0.03446045
Iteration 15150, loss = 0.03445784
Iteration 15151, loss = 0.03445518
Iteration 15152, loss = 0.03445258
Iteration 15153, loss = 0.03444997
Iteration 15154, loss = 0.03444734
Iteration 15155, loss = 0.03444472
Iteration 15156, loss = 0.03444207
Iteration 15157, loss = 0.03443944
Iteration 15158, loss = 0.03443684
Iteration 15159, loss = 0.03443424
Iteration 15160, loss = 0.03443164
Iteration 15161, loss = 0.03442900
Iteration 15162, loss = 0.03442641
Iteration 15163, loss = 0.03442376
Iteration 15164, loss = 0.03442118
Iteration 15165, loss = 0.03441855
Iteration 15166, loss = 0.03441595
Iteration 15167, loss = 0.03441336
Iteration 15168, loss = 0.03441075
Iteration 15169, loss = 0.03440812
Iteration 15170, loss = 0.03440553
Iteration 15171, loss = 0.03440290
Iteration 15172, loss = 0.03440029
Iteration 15173, loss = 0.03439770
Iteration 15174, loss = 0.03439509
Iteration 15175, loss = 0.03439247
Iteration 15176, loss = 0.03438990
Iteration 15177, loss = 0.03438725
Iteration 15178, loss = 0.03438465
Iteration 15179, loss = 0.03438204
Iteration 15180, loss = 0.03437947
Iteration 15181, loss = 0.03437685
Iteration 15182, loss = 0.03437426
Iteration 15183, loss = 0.03437163
Iteration 15184, loss = 0.03436901
Iteration 15185, loss = 0.03436643
Iteration 15186, loss = 0.03436383
Iteration 15187, loss = 0.03436125
Iteration 15188, loss = 0.03435865
Iteration 15189, loss = 0.03435601
Iteration 15190, loss = 0.03435343
Iteration 15191, loss = 0.03435082
Iteration 15192, loss = 0.03434826
Iteration 15193, loss = 0.03434563
Iteration 15194, loss = 0.03434304
Iteration 15195, loss = 0.03434045
Iteration 15196, loss = 0.03433786
Iteration 15197, loss = 0.03433526
Iteration 15198, loss = 0.03433268
Iteration 15199, loss = 0.03433004
Iteration 15200, loss = 0.03432747
Iteration 15201, loss = 0.03432486
Iteration 15202, loss = 0.03432227
Iteration 15203, loss = 0.03431965
Iteration 15204, loss = 0.03431711
Iteration 15205, loss = 0.03431445
Iteration 15206, loss = 0.03431187
Iteration 15207, loss = 0.03430927
Iteration 15208, loss = 0.03430666
Iteration 15209, loss = 0.03430406
Iteration 15210, loss = 0.03430145
Iteration 15211, loss = 0.03429889
Iteration 15212, loss = 0.03429631
Iteration 15213, loss = 0.03429372
Iteration 15214, loss = 0.03429109
Iteration 15215, loss = 0.03428852
Iteration 15216, loss = 0.03428595
Iteration 15217, loss = 0.03428336
Iteration 15218, loss = 0.03428077
Iteration 15219, loss = 0.03427819
Iteration 15220, loss = 0.03427557
Iteration 15221, loss = 0.03427299
Iteration 15222, loss = 0.03427041
Iteration 15223, loss = 0.03426785
Iteration 15224, loss = 0.03426524
Iteration 15225, loss = 0.03426264
Iteration 15226, loss = 0.03426004
Iteration 15227, loss = 0.03425750
Iteration 15228, loss = 0.03425491
Iteration 15229, loss = 0.03425228
Iteration 15230, loss = 0.03424972
Iteration 15231, loss = 0.03424717
Iteration 15232, loss = 0.03424455
Iteration 15233, loss = 0.03424196
Iteration 15234, loss = 0.03423937
Iteration 15235, loss = 0.03423681
Iteration 15236, loss = 0.03423423
Iteration 15237, loss = 0.03423164
Iteration 15238, loss = 0.03422910
Iteration 15239, loss = 0.03422648
Iteration 15240, loss = 0.03422392
Iteration 15241, loss = 0.03422135
Iteration 15242, loss = 0.03421875
Iteration 15243, loss = 0.03421624
Iteration 15244, loss = 0.03421361
Iteration 15245, loss = 0.03421103
Iteration 15246, loss = 0.03420848
Iteration 15247, loss = 0.03420590
Iteration 15248, loss = 0.03420330
Iteration 15249, loss = 0.03420074
Iteration 15250, loss = 0.03419814
Iteration 15251, loss = 0.03419560
Iteration 15252, loss = 0.03419301
Iteration 15253, loss = 0.03419042
Iteration 15254, loss = 0.03418786
Iteration 15255, loss = 0.03418529
Iteration 15256, loss = 0.03418273
Iteration 15257, loss = 0.03418015
Iteration 15258, loss = 0.03417760
Iteration 15259, loss = 0.03417501
Iteration 15260, loss = 0.03417247
Iteration 15261, loss = 0.03416987
Iteration 15262, loss = 0.03416731
Iteration 15263, loss = 0.03416474
Iteration 15264, loss = 0.03416218
Iteration 15265, loss = 0.03415960
Iteration 15266, loss = 0.03415706
Iteration 15267, loss = 0.03415449
Iteration 15268, loss = 0.03415195
Iteration 15269, loss = 0.03414938
Iteration 15270, loss = 0.03414678
Iteration 15271, loss = 0.03414423
Iteration 15272, loss = 0.03414168
Iteration 15273, loss = 0.03413912
Iteration 15274, loss = 0.03413654
Iteration 15275, loss = 0.03413400
Iteration 15276, loss = 0.03413142
Iteration 15277, loss = 0.03412885
Iteration 15278, loss = 0.03412631
Iteration 15279, loss = 0.03412378
Iteration 15280, loss = 0.03412120
Iteration 15281, loss = 0.03411866
Iteration 15282, loss = 0.03411609
Iteration 15283, loss = 0.03411353
Iteration 15284, loss = 0.03411100
Iteration 15285, loss = 0.03410840
Iteration 15286, loss = 0.03410585
Iteration 15287, loss = 0.03410327
Iteration 15288, loss = 0.03410072
Iteration 15289, loss = 0.03409818
Iteration 15290, loss = 0.03409564
Iteration 15291, loss = 0.03409308
Iteration 15292, loss = 0.03409050
Iteration 15293, loss = 0.03408793
Iteration 15294, loss = 0.03408539
Iteration 15295, loss = 0.03408285
Iteration 15296, loss = 0.03408028
Iteration 15297, loss = 0.03407773
Iteration 15298, loss = 0.03407519
Iteration 15299, loss = 0.03407260
Iteration 15300, loss = 0.03407008
Iteration 15301, loss = 0.03406752
Iteration 15302, loss = 0.03406499
Iteration 15303, loss = 0.03406237
Iteration 15304, loss = 0.03405986
Iteration 15305, loss = 0.03405732
Iteration 15306, loss = 0.03405474
Iteration 15307, loss = 0.03405220
Iteration 15308, loss = 0.03404967
Iteration 15309, loss = 0.03404712
Iteration 15310, loss = 0.03404452
Iteration 15311, loss = 0.03404200
Iteration 15312, loss = 0.03403944
Iteration 15313, loss = 0.03403689
Iteration 15314, loss = 0.03403434
Iteration 15315, loss = 0.03403178
Iteration 15316, loss = 0.03402921
Iteration 15317, loss = 0.03402666
Iteration 15318, loss = 0.03402412
Iteration 15319, loss = 0.03402159
Iteration 15320, loss = 0.03401904
Iteration 15321, loss = 0.03401648
Iteration 15322, loss = 0.03401393
Iteration 15323, loss = 0.03401136
Iteration 15324, loss = 0.03400885
Iteration 15325, loss = 0.03400631
Iteration 15326, loss = 0.03400374
Iteration 15327, loss = 0.03400116
Iteration 15328, loss = 0.03399863
Iteration 15329, loss = 0.03399606
Iteration 15330, loss = 0.03399353
Iteration 15331, loss = 0.03399097
Iteration 15332, loss = 0.03398843
Iteration 15333, loss = 0.03398585
Iteration 15334, loss = 0.03398338
Iteration 15335, loss = 0.03398080
Iteration 15336, loss = 0.03397822
Iteration 15337, loss = 0.03397570
Iteration 15338, loss = 0.03397318
Iteration 15339, loss = 0.03397060
Iteration 15340, loss = 0.03396806
Iteration 15341, loss = 0.03396556
Iteration 15342, loss = 0.03396300
Iteration 15343, loss = 0.03396049
Iteration 15344, loss = 0.03395794
Iteration 15345, loss = 0.03395545
Iteration 15346, loss = 0.03395287
Iteration 15347, loss = 0.03395032
Iteration 15348, loss = 0.03394779
Iteration 15349, loss = 0.03394531
Iteration 15350, loss = 0.03394276
Iteration 15351, loss = 0.03394022
Iteration 15352, loss = 0.03393767
Iteration 15353, loss = 0.03393512
Iteration 15354, loss = 0.03393264
Iteration 15355, loss = 0.03393008
Iteration 15356, loss = 0.03392755
Iteration 15357, loss = 0.03392502
Iteration 15358, loss = 0.03392250
Iteration 15359, loss = 0.03391999
Iteration 15360, loss = 0.03391745
Iteration 15361, loss = 0.03391495
Iteration 15362, loss = 0.03391238
Iteration 15363, loss = 0.03390990
Iteration 15364, loss = 0.03390738
Iteration 15365, loss = 0.03390483
Iteration 15366, loss = 0.03390229
Iteration 15367, loss = 0.03389977
Iteration 15368, loss = 0.03389726
Iteration 15369, loss = 0.03389472
Iteration 15370, loss = 0.03389224
Iteration 15371, loss = 0.03388966
Iteration 15372, loss = 0.03388717
Iteration 15373, loss = 0.03388462
Iteration 15374, loss = 0.03388212
Iteration 15375, loss = 0.03387958
Iteration 15376, loss = 0.03387708
Iteration 15377, loss = 0.03387455
Iteration 15378, loss = 0.03387202
Iteration 15379, loss = 0.03386949
Iteration 15380, loss = 0.03386696
Iteration 15381, loss = 0.03386443
Iteration 15382, loss = 0.03386190
Iteration 15383, loss = 0.03385939
Iteration 15384, loss = 0.03385689
Iteration 15385, loss = 0.03385433
Iteration 15386, loss = 0.03385183
Iteration 15387, loss = 0.03384934
Iteration 15388, loss = 0.03384678
Iteration 15389, loss = 0.03384429
Iteration 15390, loss = 0.03384176
Iteration 15391, loss = 0.03383925
Iteration 15392, loss = 0.03383675
Iteration 15393, loss = 0.03383422
Iteration 15394, loss = 0.03383173
Iteration 15395, loss = 0.03382921
Iteration 15396, loss = 0.03382666
Iteration 15397, loss = 0.03382416
Iteration 15398, loss = 0.03382165
Iteration 15399, loss = 0.03381912
Iteration 15400, loss = 0.03381662
Iteration 15401, loss = 0.03381411
Iteration 15402, loss = 0.03381158
Iteration 15403, loss = 0.03380908
Iteration 15404, loss = 0.03380657
Iteration 15405, loss = 0.03380407
Iteration 15406, loss = 0.03380151
Iteration 15407, loss = 0.03379900
Iteration 15408, loss = 0.03379646
Iteration 15409, loss = 0.03379400
Iteration 15410, loss = 0.03379147
Iteration 15411, loss = 0.03378897
Iteration 15412, loss = 0.03378644
Iteration 15413, loss = 0.03378395
Iteration 15414, loss = 0.03378140
Iteration 15415, loss = 0.03377890
Iteration 15416, loss = 0.03377637
Iteration 15417, loss = 0.03377388
Iteration 15418, loss = 0.03377137
Iteration 15419, loss = 0.03376885
Iteration 15420, loss = 0.03376635
Iteration 15421, loss = 0.03376383
Iteration 15422, loss = 0.03376131
Iteration 15423, loss = 0.03375881
Iteration 15424, loss = 0.03375630
Iteration 15425, loss = 0.03375382
Iteration 15426, loss = 0.03375128
Iteration 15427, loss = 0.03374876
Iteration 15428, loss = 0.03374627
Iteration 15429, loss = 0.03374376
Iteration 15430, loss = 0.03374126
Iteration 15431, loss = 0.03373876
Iteration 15432, loss = 0.03373624
Iteration 15433, loss = 0.03373377
Iteration 15434, loss = 0.03373126
Iteration 15435, loss = 0.03372875
Iteration 15436, loss = 0.03372625
Iteration 15437, loss = 0.03372374
Iteration 15438, loss = 0.03372122
Iteration 15439, loss = 0.03371876
Iteration 15440, loss = 0.03371625
Iteration 15441, loss = 0.03371372
Iteration 15442, loss = 0.03371127
Iteration 15443, loss = 0.03370871
Iteration 15444, loss = 0.03370623
Iteration 15445, loss = 0.03370372
Iteration 15446, loss = 0.03370125
Iteration 15447, loss = 0.03369872
Iteration 15448, loss = 0.03369621
Iteration 15449, loss = 0.03369371
Iteration 15450, loss = 0.03369121
Iteration 15451, loss = 0.03368870
Iteration 15452, loss = 0.03368618
Iteration 15453, loss = 0.03368369
Iteration 15454, loss = 0.03368119
Iteration 15455, loss = 0.03367865
Iteration 15456, loss = 0.03367619
Iteration 15457, loss = 0.03367369
Iteration 15458, loss = 0.03367117
Iteration 15459, loss = 0.03366871
Iteration 15460, loss = 0.03366616
Iteration 15461, loss = 0.03366366
Iteration 15462, loss = 0.03366118
Iteration 15463, loss = 0.03365874
Iteration 15464, loss = 0.03365619
Iteration 15465, loss = 0.03365371
Iteration 15466, loss = 0.03365121
Iteration 15467, loss = 0.03364873
Iteration 15468, loss = 0.03364627
Iteration 15469, loss = 0.03364374
Iteration 15470, loss = 0.03364126
Iteration 15471, loss = 0.03363875
Iteration 15472, loss = 0.03363630
Iteration 15473, loss = 0.03363377
Iteration 15474, loss = 0.03363130
Iteration 15475, loss = 0.03362881
Iteration 15476, loss = 0.03362634
Iteration 15477, loss = 0.03362385
Iteration 15478, loss = 0.03362140
Iteration 15479, loss = 0.03361887
Iteration 15480, loss = 0.03361638
Iteration 15481, loss = 0.03361391
Iteration 15482, loss = 0.03361145
Iteration 15483, loss = 0.03360895
Iteration 15484, loss = 0.03360645
Iteration 15485, loss = 0.03360397
Iteration 15486, loss = 0.03360150
Iteration 15487, loss = 0.03359902
Iteration 15488, loss = 0.03359655
Iteration 15489, loss = 0.03359406
Iteration 15490, loss = 0.03359159
Iteration 15491, loss = 0.03358910
Iteration 15492, loss = 0.03358658
Iteration 15493, loss = 0.03358410
Iteration 15494, loss = 0.03358164
Iteration 15495, loss = 0.03357915
Iteration 15496, loss = 0.03357666
Iteration 15497, loss = 0.03357418
Iteration 15498, loss = 0.03357167
Iteration 15499, loss = 0.03356920
Iteration 15500, loss = 0.03356672
Iteration 15501, loss = 0.03356422
Iteration 15502, loss = 0.03356175
Iteration 15503, loss = 0.03355925
Iteration 15504, loss = 0.03355679
Iteration 15505, loss = 0.03355430
Iteration 15506, loss = 0.03355181
Iteration 15507, loss = 0.03354936
Iteration 15508, loss = 0.03354685
Iteration 15509, loss = 0.03354438
Iteration 15510, loss = 0.03354191
Iteration 15511, loss = 0.03353940
Iteration 15512, loss = 0.03353693
Iteration 15513, loss = 0.03353448
Iteration 15514, loss = 0.03353201
Iteration 15515, loss = 0.03352953
Iteration 15516, loss = 0.03352703
Iteration 15517, loss = 0.03352457
Iteration 15518, loss = 0.03352210
Iteration 15519, loss = 0.03351964
Iteration 15520, loss = 0.03351718
Iteration 15521, loss = 0.03351470
Iteration 15522, loss = 0.03351225
Iteration 15523, loss = 0.03350975
Iteration 15524, loss = 0.03350726
Iteration 15525, loss = 0.03350480
Iteration 15526, loss = 0.03350232
Iteration 15527, loss = 0.03349988
Iteration 15528, loss = 0.03349742
Iteration 15529, loss = 0.03349495
Iteration 15530, loss = 0.03349247
Iteration 15531, loss = 0.03349004
Iteration 15532, loss = 0.03348755
Iteration 15533, loss = 0.03348507
Iteration 15534, loss = 0.03348260
Iteration 15535, loss = 0.03348014
Iteration 15536, loss = 0.03347766
Iteration 15537, loss = 0.03347516
Iteration 15538, loss = 0.03347275
Iteration 15539, loss = 0.03347023
Iteration 15540, loss = 0.03346778
Iteration 15541, loss = 0.03346532
Iteration 15542, loss = 0.03346280
Iteration 15543, loss = 0.03346038
Iteration 15544, loss = 0.03345790
Iteration 15545, loss = 0.03345544
Iteration 15546, loss = 0.03345295
Iteration 15547, loss = 0.03345048
Iteration 15548, loss = 0.03344802
Iteration 15549, loss = 0.03344555
Iteration 15550, loss = 0.03344311
Iteration 15551, loss = 0.03344064
Iteration 15552, loss = 0.03343812
Iteration 15553, loss = 0.03343568
Iteration 15554, loss = 0.03343321
Iteration 15555, loss = 0.03343076
Iteration 15556, loss = 0.03342827
Iteration 15557, loss = 0.03342585
Iteration 15558, loss = 0.03342338
Iteration 15559, loss = 0.03342092
Iteration 15560, loss = 0.03341843
Iteration 15561, loss = 0.03341597
Iteration 15562, loss = 0.03341355
Iteration 15563, loss = 0.03341105
Iteration 15564, loss = 0.03340861
Iteration 15565, loss = 0.03340615
Iteration 15566, loss = 0.03340370
Iteration 15567, loss = 0.03340124
Iteration 15568, loss = 0.03339881
Iteration 15569, loss = 0.03339636
Iteration 15570, loss = 0.03339390
Iteration 15571, loss = 0.03339145
Iteration 15572, loss = 0.03338901
Iteration 15573, loss = 0.03338650
Iteration 15574, loss = 0.03338409
Iteration 15575, loss = 0.03338162
Iteration 15576, loss = 0.03337913
Iteration 15577, loss = 0.03337672
Iteration 15578, loss = 0.03337426
Iteration 15579, loss = 0.03337180
Iteration 15580, loss = 0.03336932
Iteration 15581, loss = 0.03336687
Iteration 15582, loss = 0.03336444
Iteration 15583, loss = 0.03336198
Iteration 15584, loss = 0.03335955
Iteration 15585, loss = 0.03335712
Iteration 15586, loss = 0.03335466
Iteration 15587, loss = 0.03335220
Iteration 15588, loss = 0.03334974
Iteration 15589, loss = 0.03334731
Iteration 15590, loss = 0.03334487
Iteration 15591, loss = 0.03334243
Iteration 15592, loss = 0.03333996
Iteration 15593, loss = 0.03333751
Iteration 15594, loss = 0.03333507
Iteration 15595, loss = 0.03333264
Iteration 15596, loss = 0.03333019
Iteration 15597, loss = 0.03332778
Iteration 15598, loss = 0.03332534
Iteration 15599, loss = 0.03332290
Iteration 15600, loss = 0.03332044
Iteration 15601, loss = 0.03331804
Iteration 15602, loss = 0.03331554
Iteration 15603, loss = 0.03331313
Iteration 15604, loss = 0.03331068
Iteration 15605, loss = 0.03330824
Iteration 15606, loss = 0.03330579
Iteration 15607, loss = 0.03330336
Iteration 15608, loss = 0.03330091
Iteration 15609, loss = 0.03329848
Iteration 15610, loss = 0.03329603
Iteration 15611, loss = 0.03329362
Iteration 15612, loss = 0.03329115
Iteration 15613, loss = 0.03328872
Iteration 15614, loss = 0.03328630
Iteration 15615, loss = 0.03328384
Iteration 15616, loss = 0.03328141
Iteration 15617, loss = 0.03327896
Iteration 15618, loss = 0.03327654
Iteration 15619, loss = 0.03327409
Iteration 15620, loss = 0.03327167
Iteration 15621, loss = 0.03326921
Iteration 15622, loss = 0.03326678
Iteration 15623, loss = 0.03326436
Iteration 15624, loss = 0.03326188
Iteration 15625, loss = 0.03325948
Iteration 15626, loss = 0.03325705
Iteration 15627, loss = 0.03325460
Iteration 15628, loss = 0.03325215
Iteration 15629, loss = 0.03324974
Iteration 15630, loss = 0.03324731
Iteration 15631, loss = 0.03324485
Iteration 15632, loss = 0.03324243
Iteration 15633, loss = 0.03324003
Iteration 15634, loss = 0.03323757
Iteration 15635, loss = 0.03323514
Iteration 15636, loss = 0.03323273
Iteration 15637, loss = 0.03323027
Iteration 15638, loss = 0.03322786
Iteration 15639, loss = 0.03322542
Iteration 15640, loss = 0.03322299
Iteration 15641, loss = 0.03322058
Iteration 15642, loss = 0.03321811
Iteration 15643, loss = 0.03321570
Iteration 15644, loss = 0.03321330
Iteration 15645, loss = 0.03321086
Iteration 15646, loss = 0.03320843
Iteration 15647, loss = 0.03320599
Iteration 15648, loss = 0.03320357
Iteration 15649, loss = 0.03320117
Iteration 15650, loss = 0.03319874
Iteration 15651, loss = 0.03319631
Iteration 15652, loss = 0.03319390
Iteration 15653, loss = 0.03319147
Iteration 15654, loss = 0.03318906
Iteration 15655, loss = 0.03318661
Iteration 15656, loss = 0.03318420
Iteration 15657, loss = 0.03318175
Iteration 15658, loss = 0.03317932
Iteration 15659, loss = 0.03317690
Iteration 15660, loss = 0.03317446
Iteration 15661, loss = 0.03317205
Iteration 15662, loss = 0.03316963
Iteration 15663, loss = 0.03316722
Iteration 15664, loss = 0.03316476
Iteration 15665, loss = 0.03316236
Iteration 15666, loss = 0.03315991
Iteration 15667, loss = 0.03315746
Iteration 15668, loss = 0.03315505
Iteration 15669, loss = 0.03315267
Iteration 15670, loss = 0.03315020
Iteration 15671, loss = 0.03314780
Iteration 15672, loss = 0.03314535
Iteration 15673, loss = 0.03314297
Iteration 15674, loss = 0.03314052
Iteration 15675, loss = 0.03313811
Iteration 15676, loss = 0.03313566
Iteration 15677, loss = 0.03313322
Iteration 15678, loss = 0.03313080
Iteration 15679, loss = 0.03312837
Iteration 15680, loss = 0.03312597
Iteration 15681, loss = 0.03312355
Iteration 15682, loss = 0.03312112
Iteration 15683, loss = 0.03311870
Iteration 15684, loss = 0.03311633
Iteration 15685, loss = 0.03311388
Iteration 15686, loss = 0.03311147
Iteration 15687, loss = 0.03310907
Iteration 15688, loss = 0.03310665
Iteration 15689, loss = 0.03310421
Iteration 15690, loss = 0.03310181
Iteration 15691, loss = 0.03309941
Iteration 15692, loss = 0.03309700
Iteration 15693, loss = 0.03309458
Iteration 15694, loss = 0.03309215
Iteration 15695, loss = 0.03308978
Iteration 15696, loss = 0.03308732
Iteration 15697, loss = 0.03308492
Iteration 15698, loss = 0.03308255
Iteration 15699, loss = 0.03308008
Iteration 15700, loss = 0.03307770
Iteration 15701, loss = 0.03307526
Iteration 15702, loss = 0.03307285
Iteration 15703, loss = 0.03307043
Iteration 15704, loss = 0.03306805
Iteration 15705, loss = 0.03306560
Iteration 15706, loss = 0.03306320
Iteration 15707, loss = 0.03306080
Iteration 15708, loss = 0.03305840
Iteration 15709, loss = 0.03305601
Iteration 15710, loss = 0.03305358
Iteration 15711, loss = 0.03305116
Iteration 15712, loss = 0.03304879
Iteration 15713, loss = 0.03304638
Iteration 15714, loss = 0.03304396
Iteration 15715, loss = 0.03304156
Iteration 15716, loss = 0.03303918
Iteration 15717, loss = 0.03303676
Iteration 15718, loss = 0.03303435
Iteration 15719, loss = 0.03303197
Iteration 15720, loss = 0.03302956
Iteration 15721, loss = 0.03302720
Iteration 15722, loss = 0.03302480
Iteration 15723, loss = 0.03302238
Iteration 15724, loss = 0.03301996
Iteration 15725, loss = 0.03301762
Iteration 15726, loss = 0.03301520
Iteration 15727, loss = 0.03301280
Iteration 15728, loss = 0.03301038
Iteration 15729, loss = 0.03300798
Iteration 15730, loss = 0.03300558
Iteration 15731, loss = 0.03300321
Iteration 15732, loss = 0.03300082
Iteration 15733, loss = 0.03299841
Iteration 15734, loss = 0.03299603
Iteration 15735, loss = 0.03299362
Iteration 15736, loss = 0.03299120
Iteration 15737, loss = 0.03298881
Iteration 15738, loss = 0.03298642
Iteration 15739, loss = 0.03298399
Iteration 15740, loss = 0.03298161
Iteration 15741, loss = 0.03297920
Iteration 15742, loss = 0.03297684
Iteration 15743, loss = 0.03297443
Iteration 15744, loss = 0.03297205
Iteration 15745, loss = 0.03296962
Iteration 15746, loss = 0.03296725
Iteration 15747, loss = 0.03296487
Iteration 15748, loss = 0.03296245
Iteration 15749, loss = 0.03296010
Iteration 15750, loss = 0.03295767
Iteration 15751, loss = 0.03295534
Iteration 15752, loss = 0.03295291
Iteration 15753, loss = 0.03295052
Iteration 15754, loss = 0.03294813
Iteration 15755, loss = 0.03294573
Iteration 15756, loss = 0.03294334
Iteration 15757, loss = 0.03294101
Iteration 15758, loss = 0.03293857
Iteration 15759, loss = 0.03293619
Iteration 15760, loss = 0.03293380
Iteration 15761, loss = 0.03293139
Iteration 15762, loss = 0.03292901
Iteration 15763, loss = 0.03292663
Iteration 15764, loss = 0.03292425
Iteration 15765, loss = 0.03292186
Iteration 15766, loss = 0.03291947
Iteration 15767, loss = 0.03291709
Iteration 15768, loss = 0.03291467
Iteration 15769, loss = 0.03291229
Iteration 15770, loss = 0.03290990
Iteration 15771, loss = 0.03290751
Iteration 15772, loss = 0.03290513
Iteration 15773, loss = 0.03290273
Iteration 15774, loss = 0.03290039
Iteration 15775, loss = 0.03289796
Iteration 15776, loss = 0.03289559
Iteration 15777, loss = 0.03289321
Iteration 15778, loss = 0.03289084
Iteration 15779, loss = 0.03288840
Iteration 15780, loss = 0.03288603
Iteration 15781, loss = 0.03288365
Iteration 15782, loss = 0.03288126
Iteration 15783, loss = 0.03287888
Iteration 15784, loss = 0.03287652
Iteration 15785, loss = 0.03287410
Iteration 15786, loss = 0.03287173
Iteration 15787, loss = 0.03286934
Iteration 15788, loss = 0.03286696
Iteration 15789, loss = 0.03286458
Iteration 15790, loss = 0.03286218
Iteration 15791, loss = 0.03285981
Iteration 15792, loss = 0.03285743
Iteration 15793, loss = 0.03285507
Iteration 15794, loss = 0.03285269
Iteration 15795, loss = 0.03285031
Iteration 15796, loss = 0.03284796
Iteration 15797, loss = 0.03284554
Iteration 15798, loss = 0.03284313
Iteration 15799, loss = 0.03284078
Iteration 15800, loss = 0.03283840
Iteration 15801, loss = 0.03283601
Iteration 15802, loss = 0.03283363
Iteration 15803, loss = 0.03283125
Iteration 15804, loss = 0.03282890
Iteration 15805, loss = 0.03282651
Iteration 15806, loss = 0.03282413
Iteration 15807, loss = 0.03282175
Iteration 15808, loss = 0.03281939
Iteration 15809, loss = 0.03281700
Iteration 15810, loss = 0.03281461
Iteration 15811, loss = 0.03281225
Iteration 15812, loss = 0.03280988
Iteration 15813, loss = 0.03280752
Iteration 15814, loss = 0.03280513
Iteration 15815, loss = 0.03280276
Iteration 15816, loss = 0.03280037
Iteration 15817, loss = 0.03279801
Iteration 15818, loss = 0.03279568
Iteration 15819, loss = 0.03279327
Iteration 15820, loss = 0.03279091
Iteration 15821, loss = 0.03278853
Iteration 15822, loss = 0.03278621
Iteration 15823, loss = 0.03278384
Iteration 15824, loss = 0.03278148
Iteration 15825, loss = 0.03277911
Iteration 15826, loss = 0.03277672
Iteration 15827, loss = 0.03277436
Iteration 15828, loss = 0.03277199
Iteration 15829, loss = 0.03276965
Iteration 15830, loss = 0.03276726
Iteration 15831, loss = 0.03276489
Iteration 15832, loss = 0.03276253
Iteration 15833, loss = 0.03276017
Iteration 15834, loss = 0.03275783
Iteration 15835, loss = 0.03275542
Iteration 15836, loss = 0.03275310
Iteration 15837, loss = 0.03275071
Iteration 15838, loss = 0.03274835
Iteration 15839, loss = 0.03274599
Iteration 15840, loss = 0.03274363
Iteration 15841, loss = 0.03274128
Iteration 15842, loss = 0.03273889
Iteration 15843, loss = 0.03273654
Iteration 15844, loss = 0.03273418
Iteration 15845, loss = 0.03273184
Iteration 15846, loss = 0.03272946
Iteration 15847, loss = 0.03272711
Iteration 15848, loss = 0.03272473
Iteration 15849, loss = 0.03272243
Iteration 15850, loss = 0.03272000
Iteration 15851, loss = 0.03271767
Iteration 15852, loss = 0.03271531
Iteration 15853, loss = 0.03271298
Iteration 15854, loss = 0.03271063
Iteration 15855, loss = 0.03270826
Iteration 15856, loss = 0.03270590
Iteration 15857, loss = 0.03270355
Iteration 15858, loss = 0.03270120
Iteration 15859, loss = 0.03269886
Iteration 15860, loss = 0.03269651
Iteration 15861, loss = 0.03269414
Iteration 15862, loss = 0.03269178
Iteration 15863, loss = 0.03268946
Iteration 15864, loss = 0.03268709
Iteration 15865, loss = 0.03268475
Iteration 15866, loss = 0.03268243
Iteration 15867, loss = 0.03268004
Iteration 15868, loss = 0.03267768
Iteration 15869, loss = 0.03267535
Iteration 15870, loss = 0.03267296
Iteration 15871, loss = 0.03267063
Iteration 15872, loss = 0.03266827
Iteration 15873, loss = 0.03266596
Iteration 15874, loss = 0.03266358
Iteration 15875, loss = 0.03266125
Iteration 15876, loss = 0.03265891
Iteration 15877, loss = 0.03265651
Iteration 15878, loss = 0.03265418
Iteration 15879, loss = 0.03265187
Iteration 15880, loss = 0.03264950
Iteration 15881, loss = 0.03264713
Iteration 15882, loss = 0.03264483
Iteration 15883, loss = 0.03264248
Iteration 15884, loss = 0.03264013
Iteration 15885, loss = 0.03263777
Iteration 15886, loss = 0.03263543
Iteration 15887, loss = 0.03263307
Iteration 15888, loss = 0.03263074
Iteration 15889, loss = 0.03262840
Iteration 15890, loss = 0.03262606
Iteration 15891, loss = 0.03262371
Iteration 15892, loss = 0.03262137
Iteration 15893, loss = 0.03261904
Iteration 15894, loss = 0.03261670
Iteration 15895, loss = 0.03261433
Iteration 15896, loss = 0.03261200
Iteration 15897, loss = 0.03260965
Iteration 15898, loss = 0.03260732
Iteration 15899, loss = 0.03260496
Iteration 15900, loss = 0.03260262
Iteration 15901, loss = 0.03260025
Iteration 15902, loss = 0.03259794
Iteration 15903, loss = 0.03259560
Iteration 15904, loss = 0.03259324
Iteration 15905, loss = 0.03259091
Iteration 15906, loss = 0.03258860
Iteration 15907, loss = 0.03258624
Iteration 15908, loss = 0.03258388
Iteration 15909, loss = 0.03258156
Iteration 15910, loss = 0.03257927
Iteration 15911, loss = 0.03257691
Iteration 15912, loss = 0.03257455
Iteration 15913, loss = 0.03257224
Iteration 15914, loss = 0.03256991
Iteration 15915, loss = 0.03256755
Iteration 15916, loss = 0.03256521
Iteration 15917, loss = 0.03256288
Iteration 15918, loss = 0.03256054
Iteration 15919, loss = 0.03255824
Iteration 15920, loss = 0.03255587
Iteration 15921, loss = 0.03255356
Iteration 15922, loss = 0.03255121
Iteration 15923, loss = 0.03254887
Iteration 15924, loss = 0.03254652
Iteration 15925, loss = 0.03254420
Iteration 15926, loss = 0.03254186
Iteration 15927, loss = 0.03253955
Iteration 15928, loss = 0.03253719
Iteration 15929, loss = 0.03253485
Iteration 15930, loss = 0.03253256
Iteration 15931, loss = 0.03253020
Iteration 15932, loss = 0.03252787
Iteration 15933, loss = 0.03252552
Iteration 15934, loss = 0.03252323
Iteration 15935, loss = 0.03252086
Iteration 15936, loss = 0.03251856
Iteration 15937, loss = 0.03251622
Iteration 15938, loss = 0.03251388
Iteration 15939, loss = 0.03251155
Iteration 15940, loss = 0.03250925
Iteration 15941, loss = 0.03250694
Iteration 15942, loss = 0.03250461
Iteration 15943, loss = 0.03250229
Iteration 15944, loss = 0.03249996
Iteration 15945, loss = 0.03249763
Iteration 15946, loss = 0.03249530
Iteration 15947, loss = 0.03249297
Iteration 15948, loss = 0.03249064
Iteration 15949, loss = 0.03248834
Iteration 15950, loss = 0.03248599
Iteration 15951, loss = 0.03248367
Iteration 15952, loss = 0.03248135
Iteration 15953, loss = 0.03247903
Iteration 15954, loss = 0.03247671
Iteration 15955, loss = 0.03247438
Iteration 15956, loss = 0.03247206
Iteration 15957, loss = 0.03246974
Iteration 15958, loss = 0.03246742
Iteration 15959, loss = 0.03246510
Iteration 15960, loss = 0.03246277
Iteration 15961, loss = 0.03246046
Iteration 15962, loss = 0.03245811
Iteration 15963, loss = 0.03245582
Iteration 15964, loss = 0.03245350
Iteration 15965, loss = 0.03245116
Iteration 15966, loss = 0.03244885
Iteration 15967, loss = 0.03244653
Iteration 15968, loss = 0.03244425
Iteration 15969, loss = 0.03244191
Iteration 15970, loss = 0.03243960
Iteration 15971, loss = 0.03243729
Iteration 15972, loss = 0.03243499
Iteration 15973, loss = 0.03243264
Iteration 15974, loss = 0.03243037
Iteration 15975, loss = 0.03242805
Iteration 15976, loss = 0.03242572
Iteration 15977, loss = 0.03242341
Iteration 15978, loss = 0.03242108
Iteration 15979, loss = 0.03241875
Iteration 15980, loss = 0.03241648
Iteration 15981, loss = 0.03241417
Iteration 15982, loss = 0.03241182
Iteration 15983, loss = 0.03240950
Iteration 15984, loss = 0.03240721
Iteration 15985, loss = 0.03240487
Iteration 15986, loss = 0.03240258
Iteration 15987, loss = 0.03240025
Iteration 15988, loss = 0.03239791
Iteration 15989, loss = 0.03239563
Iteration 15990, loss = 0.03239332
Iteration 15991, loss = 0.03239101
Iteration 15992, loss = 0.03238868
Iteration 15993, loss = 0.03238638
Iteration 15994, loss = 0.03238407
Iteration 15995, loss = 0.03238175
Iteration 15996, loss = 0.03237944
Iteration 15997, loss = 0.03237711
Iteration 15998, loss = 0.03237485
Iteration 15999, loss = 0.03237250
Iteration 16000, loss = 0.03237022
Iteration 16001, loss = 0.03236788
Iteration 16002, loss = 0.03236557
Iteration 16003, loss = 0.03236324
Iteration 16004, loss = 0.03236099
Iteration 16005, loss = 0.03235866
Iteration 16006, loss = 0.03235633
Iteration 16007, loss = 0.03235402
Iteration 16008, loss = 0.03235171
Iteration 16009, loss = 0.03234941
Iteration 16010, loss = 0.03234711
Iteration 16011, loss = 0.03234477
Iteration 16012, loss = 0.03234249
Iteration 16013, loss = 0.03234024
Iteration 16014, loss = 0.03233788
Iteration 16015, loss = 0.03233556
Iteration 16016, loss = 0.03233327
Iteration 16017, loss = 0.03233096
Iteration 16018, loss = 0.03232868
Iteration 16019, loss = 0.03232637
Iteration 16020, loss = 0.03232408
Iteration 16021, loss = 0.03232177
Iteration 16022, loss = 0.03231947
Iteration 16023, loss = 0.03231718
Iteration 16024, loss = 0.03231488
Iteration 16025, loss = 0.03231256
Iteration 16026, loss = 0.03231028
Iteration 16027, loss = 0.03230797
Iteration 16028, loss = 0.03230564
Iteration 16029, loss = 0.03230338
Iteration 16030, loss = 0.03230105
Iteration 16031, loss = 0.03229872
Iteration 16032, loss = 0.03229645
Iteration 16033, loss = 0.03229415
Iteration 16034, loss = 0.03229184
Iteration 16035, loss = 0.03228955
Iteration 16036, loss = 0.03228724
Iteration 16037, loss = 0.03228493
Iteration 16038, loss = 0.03228267
Iteration 16039, loss = 0.03228038
Iteration 16040, loss = 0.03227806
Iteration 16041, loss = 0.03227575
Iteration 16042, loss = 0.03227349
Iteration 16043, loss = 0.03227118
Iteration 16044, loss = 0.03226891
Iteration 16045, loss = 0.03226663
Iteration 16046, loss = 0.03226435
Iteration 16047, loss = 0.03226204
Iteration 16048, loss = 0.03225975
Iteration 16049, loss = 0.03225748
Iteration 16050, loss = 0.03225519
Iteration 16051, loss = 0.03225289
Iteration 16052, loss = 0.03225063
Iteration 16053, loss = 0.03224832
Iteration 16054, loss = 0.03224606
Iteration 16055, loss = 0.03224377
Iteration 16056, loss = 0.03224145
Iteration 16057, loss = 0.03223918
Iteration 16058, loss = 0.03223689
Iteration 16059, loss = 0.03223460
Iteration 16060, loss = 0.03223233
Iteration 16061, loss = 0.03223003
Iteration 16062, loss = 0.03222773
Iteration 16063, loss = 0.03222544
Iteration 16064, loss = 0.03222317
Iteration 16065, loss = 0.03222087
Iteration 16066, loss = 0.03221858
Iteration 16067, loss = 0.03221631
Iteration 16068, loss = 0.03221403
Iteration 16069, loss = 0.03221174
Iteration 16070, loss = 0.03220943
Iteration 16071, loss = 0.03220713
Iteration 16072, loss = 0.03220489
Iteration 16073, loss = 0.03220260
Iteration 16074, loss = 0.03220030
Iteration 16075, loss = 0.03219801
Iteration 16076, loss = 0.03219573
Iteration 16077, loss = 0.03219345
Iteration 16078, loss = 0.03219119
Iteration 16079, loss = 0.03218889
Iteration 16080, loss = 0.03218660
Iteration 16081, loss = 0.03218435
Iteration 16082, loss = 0.03218204
Iteration 16083, loss = 0.03217976
Iteration 16084, loss = 0.03217746
Iteration 16085, loss = 0.03217520
Iteration 16086, loss = 0.03217292
Iteration 16087, loss = 0.03217062
Iteration 16088, loss = 0.03216836
Iteration 16089, loss = 0.03216605
Iteration 16090, loss = 0.03216378
Iteration 16091, loss = 0.03216155
Iteration 16092, loss = 0.03215924
Iteration 16093, loss = 0.03215690
Iteration 16094, loss = 0.03215468
Iteration 16095, loss = 0.03215240
Iteration 16096, loss = 0.03215009
Iteration 16097, loss = 0.03214782
Iteration 16098, loss = 0.03214554
Iteration 16099, loss = 0.03214328
Iteration 16100, loss = 0.03214102
Iteration 16101, loss = 0.03213871
Iteration 16102, loss = 0.03213645
Iteration 16103, loss = 0.03213419
Iteration 16104, loss = 0.03213190
Iteration 16105, loss = 0.03212962
Iteration 16106, loss = 0.03212736
Iteration 16107, loss = 0.03212511
Iteration 16108, loss = 0.03212284
Iteration 16109, loss = 0.03212054
Iteration 16110, loss = 0.03211828
Iteration 16111, loss = 0.03211600
Iteration 16112, loss = 0.03211373
Iteration 16113, loss = 0.03211146
Iteration 16114, loss = 0.03210918
Iteration 16115, loss = 0.03210693
Iteration 16116, loss = 0.03210462
Iteration 16117, loss = 0.03210238
Iteration 16118, loss = 0.03210010
Iteration 16119, loss = 0.03209786
Iteration 16120, loss = 0.03209559
Iteration 16121, loss = 0.03209330
Iteration 16122, loss = 0.03209105
Iteration 16123, loss = 0.03208879
Iteration 16124, loss = 0.03208652
Iteration 16125, loss = 0.03208425
Iteration 16126, loss = 0.03208199
Iteration 16127, loss = 0.03207973
Iteration 16128, loss = 0.03207747
Iteration 16129, loss = 0.03207518
Iteration 16130, loss = 0.03207295
Iteration 16131, loss = 0.03207067
Iteration 16132, loss = 0.03206841
Iteration 16133, loss = 0.03206614
Iteration 16134, loss = 0.03206390
Iteration 16135, loss = 0.03206160
Iteration 16136, loss = 0.03205937
Iteration 16137, loss = 0.03205712
Iteration 16138, loss = 0.03205483
Iteration 16139, loss = 0.03205255
Iteration 16140, loss = 0.03205031
Iteration 16141, loss = 0.03204805
Iteration 16142, loss = 0.03204579
Iteration 16143, loss = 0.03204352
Iteration 16144, loss = 0.03204126
Iteration 16145, loss = 0.03203903
Iteration 16146, loss = 0.03203676
Iteration 16147, loss = 0.03203452
Iteration 16148, loss = 0.03203226
Iteration 16149, loss = 0.03203002
Iteration 16150, loss = 0.03202774
Iteration 16151, loss = 0.03202549
Iteration 16152, loss = 0.03202323
Iteration 16153, loss = 0.03202100
Iteration 16154, loss = 0.03201874
Iteration 16155, loss = 0.03201648
Iteration 16156, loss = 0.03201423
Iteration 16157, loss = 0.03201197
Iteration 16158, loss = 0.03200973
Iteration 16159, loss = 0.03200745
Iteration 16160, loss = 0.03200518
Iteration 16161, loss = 0.03200294
Iteration 16162, loss = 0.03200069
Iteration 16163, loss = 0.03199842
Iteration 16164, loss = 0.03199619
Iteration 16165, loss = 0.03199395
Iteration 16166, loss = 0.03199166
Iteration 16167, loss = 0.03198940
Iteration 16168, loss = 0.03198714
Iteration 16169, loss = 0.03198489
Iteration 16170, loss = 0.03198264
Iteration 16171, loss = 0.03198041
Iteration 16172, loss = 0.03197812
Iteration 16173, loss = 0.03197589
Iteration 16174, loss = 0.03197364
Iteration 16175, loss = 0.03197138
Iteration 16176, loss = 0.03196911
Iteration 16177, loss = 0.03196686
Iteration 16178, loss = 0.03196464
Iteration 16179, loss = 0.03196238
Iteration 16180, loss = 0.03196012
Iteration 16181, loss = 0.03195789
Iteration 16182, loss = 0.03195566
Iteration 16183, loss = 0.03195336
Iteration 16184, loss = 0.03195112
Iteration 16185, loss = 0.03194887
Iteration 16186, loss = 0.03194666
Iteration 16187, loss = 0.03194440
Iteration 16188, loss = 0.03194215
Iteration 16189, loss = 0.03193989
Iteration 16190, loss = 0.03193766
Iteration 16191, loss = 0.03193543
Iteration 16192, loss = 0.03193318
Iteration 16193, loss = 0.03193093
Iteration 16194, loss = 0.03192869
Iteration 16195, loss = 0.03192645
Iteration 16196, loss = 0.03192419
Iteration 16197, loss = 0.03192196
Iteration 16198, loss = 0.03191971
Iteration 16199, loss = 0.03191748
Iteration 16200, loss = 0.03191524
Iteration 16201, loss = 0.03191301
Iteration 16202, loss = 0.03191075
Iteration 16203, loss = 0.03190852
Iteration 16204, loss = 0.03190626
Iteration 16205, loss = 0.03190402
Iteration 16206, loss = 0.03190178
Iteration 16207, loss = 0.03189951
Iteration 16208, loss = 0.03189731
Iteration 16209, loss = 0.03189504
Iteration 16210, loss = 0.03189279
Iteration 16211, loss = 0.03189059
Iteration 16212, loss = 0.03188833
Iteration 16213, loss = 0.03188608
Iteration 16214, loss = 0.03188383
Iteration 16215, loss = 0.03188158
Iteration 16216, loss = 0.03187933
Iteration 16217, loss = 0.03187711
Iteration 16218, loss = 0.03187485
Iteration 16219, loss = 0.03187263
Iteration 16220, loss = 0.03187036
Iteration 16221, loss = 0.03186812
Iteration 16222, loss = 0.03186587
Iteration 16223, loss = 0.03186365
Iteration 16224, loss = 0.03186141
Iteration 16225, loss = 0.03185919
Iteration 16226, loss = 0.03185692
Iteration 16227, loss = 0.03185470
Iteration 16228, loss = 0.03185247
Iteration 16229, loss = 0.03185023
Iteration 16230, loss = 0.03184801
Iteration 16231, loss = 0.03184577
Iteration 16232, loss = 0.03184352
Iteration 16233, loss = 0.03184130
Iteration 16234, loss = 0.03183905
Iteration 16235, loss = 0.03183683
Iteration 16236, loss = 0.03183459
Iteration 16237, loss = 0.03183237
Iteration 16238, loss = 0.03183013
Iteration 16239, loss = 0.03182787
Iteration 16240, loss = 0.03182565
Iteration 16241, loss = 0.03182346
Iteration 16242, loss = 0.03182118
Iteration 16243, loss = 0.03181892
Iteration 16244, loss = 0.03181670
Iteration 16245, loss = 0.03181448
Iteration 16246, loss = 0.03181226
Iteration 16247, loss = 0.03181000
Iteration 16248, loss = 0.03180780
Iteration 16249, loss = 0.03180556
Iteration 16250, loss = 0.03180332
Iteration 16251, loss = 0.03180111
Iteration 16252, loss = 0.03179888
Iteration 16253, loss = 0.03179669
Iteration 16254, loss = 0.03179445
Iteration 16255, loss = 0.03179221
Iteration 16256, loss = 0.03178997
Iteration 16257, loss = 0.03178775
Iteration 16258, loss = 0.03178555
Iteration 16259, loss = 0.03178333
Iteration 16260, loss = 0.03178108
Iteration 16261, loss = 0.03177886
Iteration 16262, loss = 0.03177665
Iteration 16263, loss = 0.03177442
Iteration 16264, loss = 0.03177219
Iteration 16265, loss = 0.03177002
Iteration 16266, loss = 0.03176778
Iteration 16267, loss = 0.03176554
Iteration 16268, loss = 0.03176332
Iteration 16269, loss = 0.03176110
Iteration 16270, loss = 0.03175887
Iteration 16271, loss = 0.03175665
Iteration 16272, loss = 0.03175446
Iteration 16273, loss = 0.03175221
Iteration 16274, loss = 0.03175002
Iteration 16275, loss = 0.03174778
Iteration 16276, loss = 0.03174558
Iteration 16277, loss = 0.03174335
Iteration 16278, loss = 0.03174113
Iteration 16279, loss = 0.03173893
Iteration 16280, loss = 0.03173669
Iteration 16281, loss = 0.03173451
Iteration 16282, loss = 0.03173226
Iteration 16283, loss = 0.03173003
Iteration 16284, loss = 0.03172783
Iteration 16285, loss = 0.03172562
Iteration 16286, loss = 0.03172342
Iteration 16287, loss = 0.03172118
Iteration 16288, loss = 0.03171896
Iteration 16289, loss = 0.03171674
Iteration 16290, loss = 0.03171455
Iteration 16291, loss = 0.03171232
Iteration 16292, loss = 0.03171011
Iteration 16293, loss = 0.03170791
Iteration 16294, loss = 0.03170568
Iteration 16295, loss = 0.03170348
Iteration 16296, loss = 0.03170127
Iteration 16297, loss = 0.03169907
Iteration 16298, loss = 0.03169686
Iteration 16299, loss = 0.03169464
Iteration 16300, loss = 0.03169241
Iteration 16301, loss = 0.03169022
Iteration 16302, loss = 0.03168798
Iteration 16303, loss = 0.03168578
Iteration 16304, loss = 0.03168359
Iteration 16305, loss = 0.03168136
Iteration 16306, loss = 0.03167915
Iteration 16307, loss = 0.03167693
Iteration 16308, loss = 0.03167476
Iteration 16309, loss = 0.03167252
Iteration 16310, loss = 0.03167032
Iteration 16311, loss = 0.03166814
Iteration 16312, loss = 0.03166592
Iteration 16313, loss = 0.03166371
Iteration 16314, loss = 0.03166152
Iteration 16315, loss = 0.03165930
Iteration 16316, loss = 0.03165710
Iteration 16317, loss = 0.03165490
Iteration 16318, loss = 0.03165270
Iteration 16319, loss = 0.03165051
Iteration 16320, loss = 0.03164828
Iteration 16321, loss = 0.03164606
Iteration 16322, loss = 0.03164386
Iteration 16323, loss = 0.03164166
Iteration 16324, loss = 0.03163947
Iteration 16325, loss = 0.03163724
Iteration 16326, loss = 0.03163508
Iteration 16327, loss = 0.03163286
Iteration 16328, loss = 0.03163068
Iteration 16329, loss = 0.03162842
Iteration 16330, loss = 0.03162627
Iteration 16331, loss = 0.03162405
Iteration 16332, loss = 0.03162188
Iteration 16333, loss = 0.03161965
Iteration 16334, loss = 0.03161745
Iteration 16335, loss = 0.03161523
Iteration 16336, loss = 0.03161304
Iteration 16337, loss = 0.03161083
Iteration 16338, loss = 0.03160863
Iteration 16339, loss = 0.03160646
Iteration 16340, loss = 0.03160424
Iteration 16341, loss = 0.03160206
Iteration 16342, loss = 0.03159984
Iteration 16343, loss = 0.03159767
Iteration 16344, loss = 0.03159547
Iteration 16345, loss = 0.03159325
Iteration 16346, loss = 0.03159107
Iteration 16347, loss = 0.03158887
Iteration 16348, loss = 0.03158669
Iteration 16349, loss = 0.03158451
Iteration 16350, loss = 0.03158229
Iteration 16351, loss = 0.03158012
Iteration 16352, loss = 0.03157790
Iteration 16353, loss = 0.03157572
Iteration 16354, loss = 0.03157352
Iteration 16355, loss = 0.03157134
Iteration 16356, loss = 0.03156916
Iteration 16357, loss = 0.03156692
Iteration 16358, loss = 0.03156473
Iteration 16359, loss = 0.03156255
Iteration 16360, loss = 0.03156036
Iteration 16361, loss = 0.03155815
Iteration 16362, loss = 0.03155597
Iteration 16363, loss = 0.03155377
Iteration 16364, loss = 0.03155156
Iteration 16365, loss = 0.03154939
Iteration 16366, loss = 0.03154719
Iteration 16367, loss = 0.03154502
Iteration 16368, loss = 0.03154280
Iteration 16369, loss = 0.03154060
Iteration 16370, loss = 0.03153843
Iteration 16371, loss = 0.03153623
Iteration 16372, loss = 0.03153402
Iteration 16373, loss = 0.03153185
Iteration 16374, loss = 0.03152965
Iteration 16375, loss = 0.03152748
Iteration 16376, loss = 0.03152527
Iteration 16377, loss = 0.03152309
Iteration 16378, loss = 0.03152089
Iteration 16379, loss = 0.03151876
Iteration 16380, loss = 0.03151652
Iteration 16381, loss = 0.03151435
Iteration 16382, loss = 0.03151217
Iteration 16383, loss = 0.03150996
Iteration 16384, loss = 0.03150778
Iteration 16385, loss = 0.03150562
Iteration 16386, loss = 0.03150343
Iteration 16387, loss = 0.03150123
Iteration 16388, loss = 0.03149906
Iteration 16389, loss = 0.03149688
Iteration 16390, loss = 0.03149471
Iteration 16391, loss = 0.03149253
Iteration 16392, loss = 0.03149034
Iteration 16393, loss = 0.03148815
Iteration 16394, loss = 0.03148597
Iteration 16395, loss = 0.03148379
Iteration 16396, loss = 0.03148162
Iteration 16397, loss = 0.03147945
Iteration 16398, loss = 0.03147727
Iteration 16399, loss = 0.03147506
Iteration 16400, loss = 0.03147291
Iteration 16401, loss = 0.03147071
Iteration 16402, loss = 0.03146853
Iteration 16403, loss = 0.03146636
Iteration 16404, loss = 0.03146423
Iteration 16405, loss = 0.03146199
Iteration 16406, loss = 0.03145985
Iteration 16407, loss = 0.03145764
Iteration 16408, loss = 0.03145547
Iteration 16409, loss = 0.03145329
Iteration 16410, loss = 0.03145109
Iteration 16411, loss = 0.03144895
Iteration 16412, loss = 0.03144672
Iteration 16413, loss = 0.03144457
Iteration 16414, loss = 0.03144238
Iteration 16415, loss = 0.03144022
Iteration 16416, loss = 0.03143801
Iteration 16417, loss = 0.03143587
Iteration 16418, loss = 0.03143366
Iteration 16419, loss = 0.03143149
Iteration 16420, loss = 0.03142930
Iteration 16421, loss = 0.03142713
Iteration 16422, loss = 0.03142497
Iteration 16423, loss = 0.03142281
Iteration 16424, loss = 0.03142063
Iteration 16425, loss = 0.03141843
Iteration 16426, loss = 0.03141628
Iteration 16427, loss = 0.03141410
Iteration 16428, loss = 0.03141194
Iteration 16429, loss = 0.03140972
Iteration 16430, loss = 0.03140759
Iteration 16431, loss = 0.03140540
Iteration 16432, loss = 0.03140323
Iteration 16433, loss = 0.03140105
Iteration 16434, loss = 0.03139888
Iteration 16435, loss = 0.03139668
Iteration 16436, loss = 0.03139453
Iteration 16437, loss = 0.03139236
Iteration 16438, loss = 0.03139017
Iteration 16439, loss = 0.03138800
Iteration 16440, loss = 0.03138584
Iteration 16441, loss = 0.03138367
Iteration 16442, loss = 0.03138148
Iteration 16443, loss = 0.03137933
Iteration 16444, loss = 0.03137717
Iteration 16445, loss = 0.03137499
Iteration 16446, loss = 0.03137281
Iteration 16447, loss = 0.03137067
Iteration 16448, loss = 0.03136849
Iteration 16449, loss = 0.03136632
Iteration 16450, loss = 0.03136414
Iteration 16451, loss = 0.03136199
Iteration 16452, loss = 0.03135982
Iteration 16453, loss = 0.03135765
Iteration 16454, loss = 0.03135548
Iteration 16455, loss = 0.03135331
Iteration 16456, loss = 0.03135113
Iteration 16457, loss = 0.03134900
Iteration 16458, loss = 0.03134683
Iteration 16459, loss = 0.03134465
Iteration 16460, loss = 0.03134248
Iteration 16461, loss = 0.03134032
Iteration 16462, loss = 0.03133815
Iteration 16463, loss = 0.03133599
Iteration 16464, loss = 0.03133378
Iteration 16465, loss = 0.03133164
Iteration 16466, loss = 0.03132947
Iteration 16467, loss = 0.03132729
Iteration 16468, loss = 0.03132512
Iteration 16469, loss = 0.03132297
Iteration 16470, loss = 0.03132080
Iteration 16471, loss = 0.03131864
Iteration 16472, loss = 0.03131649
Iteration 16473, loss = 0.03131432
Iteration 16474, loss = 0.03131219
Iteration 16475, loss = 0.03131001
Iteration 16476, loss = 0.03130782
Iteration 16477, loss = 0.03130566
Iteration 16478, loss = 0.03130349
Iteration 16479, loss = 0.03130135
Iteration 16480, loss = 0.03129917
Iteration 16481, loss = 0.03129701
Iteration 16482, loss = 0.03129484
Iteration 16483, loss = 0.03129265
Iteration 16484, loss = 0.03129052
Iteration 16485, loss = 0.03128835
Iteration 16486, loss = 0.03128622
Iteration 16487, loss = 0.03128404
Iteration 16488, loss = 0.03128187
Iteration 16489, loss = 0.03127972
Iteration 16490, loss = 0.03127756
Iteration 16491, loss = 0.03127541
Iteration 16492, loss = 0.03127326
Iteration 16493, loss = 0.03127109
Iteration 16494, loss = 0.03126896
Iteration 16495, loss = 0.03126680
Iteration 16496, loss = 0.03126463
Iteration 16497, loss = 0.03126248
Iteration 16498, loss = 0.03126032
Iteration 16499, loss = 0.03125816
Iteration 16500, loss = 0.03125604
Iteration 16501, loss = 0.03125389
Iteration 16502, loss = 0.03125177
Iteration 16503, loss = 0.03124960
Iteration 16504, loss = 0.03124747
Iteration 16505, loss = 0.03124530
Iteration 16506, loss = 0.03124315
Iteration 16507, loss = 0.03124099
Iteration 16508, loss = 0.03123885
Iteration 16509, loss = 0.03123667
Iteration 16510, loss = 0.03123451
Iteration 16511, loss = 0.03123241
Iteration 16512, loss = 0.03123023
Iteration 16513, loss = 0.03122808
Iteration 16514, loss = 0.03122593
Iteration 16515, loss = 0.03122379
Iteration 16516, loss = 0.03122162
Iteration 16517, loss = 0.03121950
Iteration 16518, loss = 0.03121738
Iteration 16519, loss = 0.03121520
Iteration 16520, loss = 0.03121307
Iteration 16521, loss = 0.03121092
Iteration 16522, loss = 0.03120878
Iteration 16523, loss = 0.03120666
Iteration 16524, loss = 0.03120452
Iteration 16525, loss = 0.03120238
Iteration 16526, loss = 0.03120022
Iteration 16527, loss = 0.03119805
Iteration 16528, loss = 0.03119595
Iteration 16529, loss = 0.03119381
Iteration 16530, loss = 0.03119163
Iteration 16531, loss = 0.03118953
Iteration 16532, loss = 0.03118734
Iteration 16533, loss = 0.03118519
Iteration 16534, loss = 0.03118305
Iteration 16535, loss = 0.03118093
Iteration 16536, loss = 0.03117876
Iteration 16537, loss = 0.03117666
Iteration 16538, loss = 0.03117451
Iteration 16539, loss = 0.03117235
Iteration 16540, loss = 0.03117021
Iteration 16541, loss = 0.03116810
Iteration 16542, loss = 0.03116593
Iteration 16543, loss = 0.03116378
Iteration 16544, loss = 0.03116167
Iteration 16545, loss = 0.03115950
Iteration 16546, loss = 0.03115736
Iteration 16547, loss = 0.03115526
Iteration 16548, loss = 0.03115309
Iteration 16549, loss = 0.03115097
Iteration 16550, loss = 0.03114883
Iteration 16551, loss = 0.03114670
Iteration 16552, loss = 0.03114457
Iteration 16553, loss = 0.03114240
Iteration 16554, loss = 0.03114029
Iteration 16555, loss = 0.03113817
Iteration 16556, loss = 0.03113601
Iteration 16557, loss = 0.03113391
Iteration 16558, loss = 0.03113175
Iteration 16559, loss = 0.03112962
Iteration 16560, loss = 0.03112750
Iteration 16561, loss = 0.03112537
Iteration 16562, loss = 0.03112323
Iteration 16563, loss = 0.03112111
Iteration 16564, loss = 0.03111896
Iteration 16565, loss = 0.03111682
Iteration 16566, loss = 0.03111469
Iteration 16567, loss = 0.03111257
Iteration 16568, loss = 0.03111043
Iteration 16569, loss = 0.03110830
Iteration 16570, loss = 0.03110617
Iteration 16571, loss = 0.03110401
Iteration 16572, loss = 0.03110192
Iteration 16573, loss = 0.03109980
Iteration 16574, loss = 0.03109763
Iteration 16575, loss = 0.03109551
Iteration 16576, loss = 0.03109336
Iteration 16577, loss = 0.03109124
Iteration 16578, loss = 0.03108911
Iteration 16579, loss = 0.03108698
Iteration 16580, loss = 0.03108487
Iteration 16581, loss = 0.03108277
Iteration 16582, loss = 0.03108059
Iteration 16583, loss = 0.03107847
Iteration 16584, loss = 0.03107634
Iteration 16585, loss = 0.03107419
Iteration 16586, loss = 0.03107207
Iteration 16587, loss = 0.03106994
Iteration 16588, loss = 0.03106779
Iteration 16589, loss = 0.03106567
Iteration 16590, loss = 0.03106356
Iteration 16591, loss = 0.03106139
Iteration 16592, loss = 0.03105926
Iteration 16593, loss = 0.03105714
Iteration 16594, loss = 0.03105499
Iteration 16595, loss = 0.03105288
Iteration 16596, loss = 0.03105072
Iteration 16597, loss = 0.03104861
Iteration 16598, loss = 0.03104651
Iteration 16599, loss = 0.03104435
Iteration 16600, loss = 0.03104222
Iteration 16601, loss = 0.03104007
Iteration 16602, loss = 0.03103795
Iteration 16603, loss = 0.03103587
Iteration 16604, loss = 0.03103369
Iteration 16605, loss = 0.03103158
Iteration 16606, loss = 0.03102945
Iteration 16607, loss = 0.03102735
Iteration 16608, loss = 0.03102520
Iteration 16609, loss = 0.03102307
Iteration 16610, loss = 0.03102095
Iteration 16611, loss = 0.03101885
Iteration 16612, loss = 0.03101673
Iteration 16613, loss = 0.03101460
Iteration 16614, loss = 0.03101246
Iteration 16615, loss = 0.03101037
Iteration 16616, loss = 0.03100823
Iteration 16617, loss = 0.03100612
Iteration 16618, loss = 0.03100402
Iteration 16619, loss = 0.03100190
Iteration 16620, loss = 0.03099976
Iteration 16621, loss = 0.03099765
Iteration 16622, loss = 0.03099552
Iteration 16623, loss = 0.03099341
Iteration 16624, loss = 0.03099129
Iteration 16625, loss = 0.03098919
Iteration 16626, loss = 0.03098703
Iteration 16627, loss = 0.03098495
Iteration 16628, loss = 0.03098284
Iteration 16629, loss = 0.03098071
Iteration 16630, loss = 0.03097861
Iteration 16631, loss = 0.03097650
Iteration 16632, loss = 0.03097436
Iteration 16633, loss = 0.03097224
Iteration 16634, loss = 0.03097014
Iteration 16635, loss = 0.03096803
Iteration 16636, loss = 0.03096594
Iteration 16637, loss = 0.03096378
Iteration 16638, loss = 0.03096168
Iteration 16639, loss = 0.03095956
Iteration 16640, loss = 0.03095748
Iteration 16641, loss = 0.03095533
Iteration 16642, loss = 0.03095324
Iteration 16643, loss = 0.03095112
Iteration 16644, loss = 0.03094900
Iteration 16645, loss = 0.03094689
Iteration 16646, loss = 0.03094483
Iteration 16647, loss = 0.03094265
Iteration 16648, loss = 0.03094054
Iteration 16649, loss = 0.03093844
Iteration 16650, loss = 0.03093630
Iteration 16651, loss = 0.03093419
Iteration 16652, loss = 0.03093208
Iteration 16653, loss = 0.03092996
Iteration 16654, loss = 0.03092783
Iteration 16655, loss = 0.03092575
Iteration 16656, loss = 0.03092361
Iteration 16657, loss = 0.03092151
Iteration 16658, loss = 0.03091938
Iteration 16659, loss = 0.03091725
Iteration 16660, loss = 0.03091516
Iteration 16661, loss = 0.03091301
Iteration 16662, loss = 0.03091094
Iteration 16663, loss = 0.03090880
Iteration 16664, loss = 0.03090671
Iteration 16665, loss = 0.03090458
Iteration 16666, loss = 0.03090249
Iteration 16667, loss = 0.03090034
Iteration 16668, loss = 0.03089826
Iteration 16669, loss = 0.03089613
Iteration 16670, loss = 0.03089405
Iteration 16671, loss = 0.03089193
Iteration 16672, loss = 0.03088981
Iteration 16673, loss = 0.03088773
Iteration 16674, loss = 0.03088563
Iteration 16675, loss = 0.03088351
Iteration 16676, loss = 0.03088140
Iteration 16677, loss = 0.03087933
Iteration 16678, loss = 0.03087721
Iteration 16679, loss = 0.03087511
Iteration 16680, loss = 0.03087301
Iteration 16681, loss = 0.03087088
Iteration 16682, loss = 0.03086878
Iteration 16683, loss = 0.03086667
Iteration 16684, loss = 0.03086458
Iteration 16685, loss = 0.03086248
Iteration 16686, loss = 0.03086038
Iteration 16687, loss = 0.03085830
Iteration 16688, loss = 0.03085615
Iteration 16689, loss = 0.03085406
Iteration 16690, loss = 0.03085195
Iteration 16691, loss = 0.03084985
Iteration 16692, loss = 0.03084776
Iteration 16693, loss = 0.03084567
Iteration 16694, loss = 0.03084356
Iteration 16695, loss = 0.03084143
Iteration 16696, loss = 0.03083935
Iteration 16697, loss = 0.03083727
Iteration 16698, loss = 0.03083517
Iteration 16699, loss = 0.03083305
Iteration 16700, loss = 0.03083096
Iteration 16701, loss = 0.03082887
Iteration 16702, loss = 0.03082676
Iteration 16703, loss = 0.03082469
Iteration 16704, loss = 0.03082258
Iteration 16705, loss = 0.03082047
Iteration 16706, loss = 0.03081840
Iteration 16707, loss = 0.03081630
Iteration 16708, loss = 0.03081421
Iteration 16709, loss = 0.03081213
Iteration 16710, loss = 0.03081004
Iteration 16711, loss = 0.03080792
Iteration 16712, loss = 0.03080583
Iteration 16713, loss = 0.03080375
Iteration 16714, loss = 0.03080165
Iteration 16715, loss = 0.03079956
Iteration 16716, loss = 0.03079747
Iteration 16717, loss = 0.03079539
Iteration 16718, loss = 0.03079329
Iteration 16719, loss = 0.03079121
Iteration 16720, loss = 0.03078912
Iteration 16721, loss = 0.03078702
Iteration 16722, loss = 0.03078495
Iteration 16723, loss = 0.03078285
Iteration 16724, loss = 0.03078076
Iteration 16725, loss = 0.03077868
Iteration 16726, loss = 0.03077659
Iteration 16727, loss = 0.03077448
Iteration 16728, loss = 0.03077244
Iteration 16729, loss = 0.03077035
Iteration 16730, loss = 0.03076825
Iteration 16731, loss = 0.03076616
Iteration 16732, loss = 0.03076410
Iteration 16733, loss = 0.03076199
Iteration 16734, loss = 0.03075994
Iteration 16735, loss = 0.03075784
Iteration 16736, loss = 0.03075578
Iteration 16737, loss = 0.03075369
Iteration 16738, loss = 0.03075162
Iteration 16739, loss = 0.03074954
Iteration 16740, loss = 0.03074745
Iteration 16741, loss = 0.03074537
Iteration 16742, loss = 0.03074329
Iteration 16743, loss = 0.03074123
Iteration 16744, loss = 0.03073915
Iteration 16745, loss = 0.03073707
Iteration 16746, loss = 0.03073496
Iteration 16747, loss = 0.03073291
Iteration 16748, loss = 0.03073082
Iteration 16749, loss = 0.03072875
Iteration 16750, loss = 0.03072667
Iteration 16751, loss = 0.03072461
Iteration 16752, loss = 0.03072251
Iteration 16753, loss = 0.03072041
Iteration 16754, loss = 0.03071835
Iteration 16755, loss = 0.03071626
Iteration 16756, loss = 0.03071421
Iteration 16757, loss = 0.03071213
Iteration 16758, loss = 0.03071004
Iteration 16759, loss = 0.03070794
Iteration 16760, loss = 0.03070588
Iteration 16761, loss = 0.03070380
Iteration 16762, loss = 0.03070172
Iteration 16763, loss = 0.03069963
Iteration 16764, loss = 0.03069755
Iteration 16765, loss = 0.03069550
Iteration 16766, loss = 0.03069340
Iteration 16767, loss = 0.03069134
Iteration 16768, loss = 0.03068925
Iteration 16769, loss = 0.03068718
Iteration 16770, loss = 0.03068510
Iteration 16771, loss = 0.03068303
Iteration 16772, loss = 0.03068097
Iteration 16773, loss = 0.03067887
Iteration 16774, loss = 0.03067678
Iteration 16775, loss = 0.03067473
Iteration 16776, loss = 0.03067262
Iteration 16777, loss = 0.03067057
Iteration 16778, loss = 0.03066848
Iteration 16779, loss = 0.03066641
Iteration 16780, loss = 0.03066431
Iteration 16781, loss = 0.03066228
Iteration 16782, loss = 0.03066019
Iteration 16783, loss = 0.03065811
Iteration 16784, loss = 0.03065604
Iteration 16785, loss = 0.03065399
Iteration 16786, loss = 0.03065189
Iteration 16787, loss = 0.03064982
Iteration 16788, loss = 0.03064776
Iteration 16789, loss = 0.03064569
Iteration 16790, loss = 0.03064360
Iteration 16791, loss = 0.03064153
Iteration 16792, loss = 0.03063948
Iteration 16793, loss = 0.03063740
Iteration 16794, loss = 0.03063534
Iteration 16795, loss = 0.03063328
Iteration 16796, loss = 0.03063120
Iteration 16797, loss = 0.03062915
Iteration 16798, loss = 0.03062707
Iteration 16799, loss = 0.03062501
Iteration 16800, loss = 0.03062296
Iteration 16801, loss = 0.03062088
Iteration 16802, loss = 0.03061883
Iteration 16803, loss = 0.03061676
Iteration 16804, loss = 0.03061470
Iteration 16805, loss = 0.03061263
Iteration 16806, loss = 0.03061057
Iteration 16807, loss = 0.03060852
Iteration 16808, loss = 0.03060644
Iteration 16809, loss = 0.03060437
Iteration 16810, loss = 0.03060230
Iteration 16811, loss = 0.03060025
Iteration 16812, loss = 0.03059820
Iteration 16813, loss = 0.03059612
Iteration 16814, loss = 0.03059408
Iteration 16815, loss = 0.03059202
Iteration 16816, loss = 0.03058994
Iteration 16817, loss = 0.03058788
Iteration 16818, loss = 0.03058584
Iteration 16819, loss = 0.03058375
Iteration 16820, loss = 0.03058168
Iteration 16821, loss = 0.03057964
Iteration 16822, loss = 0.03057756
Iteration 16823, loss = 0.03057550
Iteration 16824, loss = 0.03057341
Iteration 16825, loss = 0.03057140
Iteration 16826, loss = 0.03056934
Iteration 16827, loss = 0.03056726
Iteration 16828, loss = 0.03056520
Iteration 16829, loss = 0.03056312
Iteration 16830, loss = 0.03056110
Iteration 16831, loss = 0.03055902
Iteration 16832, loss = 0.03055696
Iteration 16833, loss = 0.03055491
Iteration 16834, loss = 0.03055288
Iteration 16835, loss = 0.03055081
Iteration 16836, loss = 0.03054876
Iteration 16837, loss = 0.03054673
Iteration 16838, loss = 0.03054465
Iteration 16839, loss = 0.03054259
Iteration 16840, loss = 0.03054055
Iteration 16841, loss = 0.03053853
Iteration 16842, loss = 0.03053645
Iteration 16843, loss = 0.03053441
Iteration 16844, loss = 0.03053235
Iteration 16845, loss = 0.03053028
Iteration 16846, loss = 0.03052822
Iteration 16847, loss = 0.03052616
Iteration 16848, loss = 0.03052413
Iteration 16849, loss = 0.03052208
Iteration 16850, loss = 0.03052001
Iteration 16851, loss = 0.03051796
Iteration 16852, loss = 0.03051595
Iteration 16853, loss = 0.03051388
Iteration 16854, loss = 0.03051179
Iteration 16855, loss = 0.03050977
Iteration 16856, loss = 0.03050771
Iteration 16857, loss = 0.03050567
Iteration 16858, loss = 0.03050359
Iteration 16859, loss = 0.03050159
Iteration 16860, loss = 0.03049950
Iteration 16861, loss = 0.03049747
Iteration 16862, loss = 0.03049540
Iteration 16863, loss = 0.03049337
Iteration 16864, loss = 0.03049132
Iteration 16865, loss = 0.03048925
Iteration 16866, loss = 0.03048722
Iteration 16867, loss = 0.03048517
Iteration 16868, loss = 0.03048312
Iteration 16869, loss = 0.03048107
Iteration 16870, loss = 0.03047900
Iteration 16871, loss = 0.03047697
Iteration 16872, loss = 0.03047489
Iteration 16873, loss = 0.03047288
Iteration 16874, loss = 0.03047083
Iteration 16875, loss = 0.03046880
Iteration 16876, loss = 0.03046673
Iteration 16877, loss = 0.03046469
Iteration 16878, loss = 0.03046266
Iteration 16879, loss = 0.03046061
Iteration 16880, loss = 0.03045857
Iteration 16881, loss = 0.03045651
Iteration 16882, loss = 0.03045447
Iteration 16883, loss = 0.03045242
Iteration 16884, loss = 0.03045037
Iteration 16885, loss = 0.03044834
Iteration 16886, loss = 0.03044628
Iteration 16887, loss = 0.03044423
Iteration 16888, loss = 0.03044219
Iteration 16889, loss = 0.03044016
Iteration 16890, loss = 0.03043813
Iteration 16891, loss = 0.03043607
Iteration 16892, loss = 0.03043403
Iteration 16893, loss = 0.03043199
Iteration 16894, loss = 0.03042993
Iteration 16895, loss = 0.03042788
Iteration 16896, loss = 0.03042586
Iteration 16897, loss = 0.03042381
Iteration 16898, loss = 0.03042175
Iteration 16899, loss = 0.03041972
Iteration 16900, loss = 0.03041771
Iteration 16901, loss = 0.03041565
Iteration 16902, loss = 0.03041363
Iteration 16903, loss = 0.03041157
Iteration 16904, loss = 0.03040954
Iteration 16905, loss = 0.03040749
Iteration 16906, loss = 0.03040547
Iteration 16907, loss = 0.03040339
Iteration 16908, loss = 0.03040137
Iteration 16909, loss = 0.03039933
Iteration 16910, loss = 0.03039729
Iteration 16911, loss = 0.03039527
Iteration 16912, loss = 0.03039322
Iteration 16913, loss = 0.03039118
Iteration 16914, loss = 0.03038914
Iteration 16915, loss = 0.03038714
Iteration 16916, loss = 0.03038508
Iteration 16917, loss = 0.03038304
Iteration 16918, loss = 0.03038099
Iteration 16919, loss = 0.03037895
Iteration 16920, loss = 0.03037699
Iteration 16921, loss = 0.03037490
Iteration 16922, loss = 0.03037285
Iteration 16923, loss = 0.03037086
Iteration 16924, loss = 0.03036882
Iteration 16925, loss = 0.03036678
Iteration 16926, loss = 0.03036476
Iteration 16927, loss = 0.03036274
Iteration 16928, loss = 0.03036069
Iteration 16929, loss = 0.03035865
Iteration 16930, loss = 0.03035662
Iteration 16931, loss = 0.03035461
Iteration 16932, loss = 0.03035255
Iteration 16933, loss = 0.03035052
Iteration 16934, loss = 0.03034849
Iteration 16935, loss = 0.03034646
Iteration 16936, loss = 0.03034441
Iteration 16937, loss = 0.03034238
Iteration 16938, loss = 0.03034036
Iteration 16939, loss = 0.03033831
Iteration 16940, loss = 0.03033627
Iteration 16941, loss = 0.03033426
Iteration 16942, loss = 0.03033222
Iteration 16943, loss = 0.03033018
Iteration 16944, loss = 0.03032817
Iteration 16945, loss = 0.03032612
Iteration 16946, loss = 0.03032410
Iteration 16947, loss = 0.03032207
Iteration 16948, loss = 0.03032006
Iteration 16949, loss = 0.03031801
Iteration 16950, loss = 0.03031600
Iteration 16951, loss = 0.03031395
Iteration 16952, loss = 0.03031194
Iteration 16953, loss = 0.03030993
Iteration 16954, loss = 0.03030787
Iteration 16955, loss = 0.03030586
Iteration 16956, loss = 0.03030384
Iteration 16957, loss = 0.03030179
Iteration 16958, loss = 0.03029975
Iteration 16959, loss = 0.03029774
Iteration 16960, loss = 0.03029570
Iteration 16961, loss = 0.03029369
Iteration 16962, loss = 0.03029168
Iteration 16963, loss = 0.03028961
Iteration 16964, loss = 0.03028761
Iteration 16965, loss = 0.03028559
Iteration 16966, loss = 0.03028356
Iteration 16967, loss = 0.03028152
Iteration 16968, loss = 0.03027951
Iteration 16969, loss = 0.03027752
Iteration 16970, loss = 0.03027548
Iteration 16971, loss = 0.03027346
Iteration 16972, loss = 0.03027144
Iteration 16973, loss = 0.03026943
Iteration 16974, loss = 0.03026742
Iteration 16975, loss = 0.03026541
Iteration 16976, loss = 0.03026338
Iteration 16977, loss = 0.03026135
Iteration 16978, loss = 0.03025935
Iteration 16979, loss = 0.03025735
Iteration 16980, loss = 0.03025536
Iteration 16981, loss = 0.03025329
Iteration 16982, loss = 0.03025130
Iteration 16983, loss = 0.03024927
Iteration 16984, loss = 0.03024727
Iteration 16985, loss = 0.03024524
Iteration 16986, loss = 0.03024321
Iteration 16987, loss = 0.03024122
Iteration 16988, loss = 0.03023920
Iteration 16989, loss = 0.03023720
Iteration 16990, loss = 0.03023517
Iteration 16991, loss = 0.03023315
Iteration 16992, loss = 0.03023118
Iteration 16993, loss = 0.03022912
Iteration 16994, loss = 0.03022711
Iteration 16995, loss = 0.03022508
Iteration 16996, loss = 0.03022307
Iteration 16997, loss = 0.03022109
Iteration 16998, loss = 0.03021906
Iteration 16999, loss = 0.03021703
Iteration 17000, loss = 0.03021504
Iteration 17001, loss = 0.03021302
Iteration 17002, loss = 0.03021101
Iteration 17003, loss = 0.03020899
Iteration 17004, loss = 0.03020699
Iteration 17005, loss = 0.03020495
Iteration 17006, loss = 0.03020295
Iteration 17007, loss = 0.03020092
Iteration 17008, loss = 0.03019891
Iteration 17009, loss = 0.03019691
Iteration 17010, loss = 0.03019490
Iteration 17011, loss = 0.03019291
Iteration 17012, loss = 0.03019088
Iteration 17013, loss = 0.03018888
Iteration 17014, loss = 0.03018688
Iteration 17015, loss = 0.03018486
Iteration 17016, loss = 0.03018286
Iteration 17017, loss = 0.03018085
Iteration 17018, loss = 0.03017887
Iteration 17019, loss = 0.03017683
Iteration 17020, loss = 0.03017483
Iteration 17021, loss = 0.03017282
Iteration 17022, loss = 0.03017084
Iteration 17023, loss = 0.03016881
Iteration 17024, loss = 0.03016683
Iteration 17025, loss = 0.03016481
Iteration 17026, loss = 0.03016281
Iteration 17027, loss = 0.03016081
Iteration 17028, loss = 0.03015884
Iteration 17029, loss = 0.03015680
Iteration 17030, loss = 0.03015479
Iteration 17031, loss = 0.03015279
Iteration 17032, loss = 0.03015077
Iteration 17033, loss = 0.03014877
Iteration 17034, loss = 0.03014675
Iteration 17035, loss = 0.03014474
Iteration 17036, loss = 0.03014276
Iteration 17037, loss = 0.03014072
Iteration 17038, loss = 0.03013871
Iteration 17039, loss = 0.03013675
Iteration 17040, loss = 0.03013470
Iteration 17041, loss = 0.03013273
Iteration 17042, loss = 0.03013071
Iteration 17043, loss = 0.03012869
Iteration 17044, loss = 0.03012670
Iteration 17045, loss = 0.03012469
Iteration 17046, loss = 0.03012268
Iteration 17047, loss = 0.03012069
Iteration 17048, loss = 0.03011869
Iteration 17049, loss = 0.03011670
Iteration 17050, loss = 0.03011467
Iteration 17051, loss = 0.03011270
Iteration 17052, loss = 0.03011068
Iteration 17053, loss = 0.03010867
Iteration 17054, loss = 0.03010670
Iteration 17055, loss = 0.03010470
Iteration 17056, loss = 0.03010272
Iteration 17057, loss = 0.03010073
Iteration 17058, loss = 0.03009873
Iteration 17059, loss = 0.03009675
Iteration 17060, loss = 0.03009473
Iteration 17061, loss = 0.03009277
Iteration 17062, loss = 0.03009077
Iteration 17063, loss = 0.03008875
Iteration 17064, loss = 0.03008677
Iteration 17065, loss = 0.03008479
Iteration 17066, loss = 0.03008280
Iteration 17067, loss = 0.03008081
Iteration 17068, loss = 0.03007882
Iteration 17069, loss = 0.03007682
Iteration 17070, loss = 0.03007483
Iteration 17071, loss = 0.03007284
Iteration 17072, loss = 0.03007083
Iteration 17073, loss = 0.03006888
Iteration 17074, loss = 0.03006683
Iteration 17075, loss = 0.03006486
Iteration 17076, loss = 0.03006290
Iteration 17077, loss = 0.03006087
Iteration 17078, loss = 0.03005887
Iteration 17079, loss = 0.03005690
Iteration 17080, loss = 0.03005490
Iteration 17081, loss = 0.03005289
Iteration 17082, loss = 0.03005091
Iteration 17083, loss = 0.03004890
Iteration 17084, loss = 0.03004691
Iteration 17085, loss = 0.03004493
Iteration 17086, loss = 0.03004292
Iteration 17087, loss = 0.03004093
Iteration 17088, loss = 0.03003898
Iteration 17089, loss = 0.03003695
Iteration 17090, loss = 0.03003495
Iteration 17091, loss = 0.03003298
Iteration 17092, loss = 0.03003097
Iteration 17093, loss = 0.03002897
Iteration 17094, loss = 0.03002701
Iteration 17095, loss = 0.03002504
Iteration 17096, loss = 0.03002301
Iteration 17097, loss = 0.03002104
Iteration 17098, loss = 0.03001904
Iteration 17099, loss = 0.03001704
Iteration 17100, loss = 0.03001505
Iteration 17101, loss = 0.03001306
Iteration 17102, loss = 0.03001104
Iteration 17103, loss = 0.03000907
Iteration 17104, loss = 0.03000707
Iteration 17105, loss = 0.03000507
Iteration 17106, loss = 0.03000308
Iteration 17107, loss = 0.03000109
Iteration 17108, loss = 0.02999911
Iteration 17109, loss = 0.02999710
Iteration 17110, loss = 0.02999512
Iteration 17111, loss = 0.02999313
Iteration 17112, loss = 0.02999118
Iteration 17113, loss = 0.02998918
Iteration 17114, loss = 0.02998722
Iteration 17115, loss = 0.02998521
Iteration 17116, loss = 0.02998321
Iteration 17117, loss = 0.02998123
Iteration 17118, loss = 0.02997925
Iteration 17119, loss = 0.02997730
Iteration 17120, loss = 0.02997529
Iteration 17121, loss = 0.02997334
Iteration 17122, loss = 0.02997134
Iteration 17123, loss = 0.02996936
Iteration 17124, loss = 0.02996737
Iteration 17125, loss = 0.02996539
Iteration 17126, loss = 0.02996345
Iteration 17127, loss = 0.02996143
Iteration 17128, loss = 0.02995946
Iteration 17129, loss = 0.02995750
Iteration 17130, loss = 0.02995551
Iteration 17131, loss = 0.02995352
Iteration 17132, loss = 0.02995154
Iteration 17133, loss = 0.02994959
Iteration 17134, loss = 0.02994759
Iteration 17135, loss = 0.02994564
Iteration 17136, loss = 0.02994363
Iteration 17137, loss = 0.02994167
Iteration 17138, loss = 0.02993972
Iteration 17139, loss = 0.02993769
Iteration 17140, loss = 0.02993573
Iteration 17141, loss = 0.02993376
Iteration 17142, loss = 0.02993178
Iteration 17143, loss = 0.02992981
Iteration 17144, loss = 0.02992784
Iteration 17145, loss = 0.02992587
Iteration 17146, loss = 0.02992389
Iteration 17147, loss = 0.02992193
Iteration 17148, loss = 0.02991992
Iteration 17149, loss = 0.02991798
Iteration 17150, loss = 0.02991600
Iteration 17151, loss = 0.02991403
Iteration 17152, loss = 0.02991205
Iteration 17153, loss = 0.02991009
Iteration 17154, loss = 0.02990810
Iteration 17155, loss = 0.02990612
Iteration 17156, loss = 0.02990418
Iteration 17157, loss = 0.02990220
Iteration 17158, loss = 0.02990025
Iteration 17159, loss = 0.02989825
Iteration 17160, loss = 0.02989630
Iteration 17161, loss = 0.02989432
Iteration 17162, loss = 0.02989238
Iteration 17163, loss = 0.02989039
Iteration 17164, loss = 0.02988842
Iteration 17165, loss = 0.02988644
Iteration 17166, loss = 0.02988448
Iteration 17167, loss = 0.02988253
Iteration 17168, loss = 0.02988057
Iteration 17169, loss = 0.02987859
Iteration 17170, loss = 0.02987665
Iteration 17171, loss = 0.02987464
Iteration 17172, loss = 0.02987268
Iteration 17173, loss = 0.02987072
Iteration 17174, loss = 0.02986875
Iteration 17175, loss = 0.02986678
Iteration 17176, loss = 0.02986481
Iteration 17177, loss = 0.02986284
Iteration 17178, loss = 0.02986086
Iteration 17179, loss = 0.02985894
Iteration 17180, loss = 0.02985696
Iteration 17181, loss = 0.02985499
Iteration 17182, loss = 0.02985301
Iteration 17183, loss = 0.02985107
Iteration 17184, loss = 0.02984910
Iteration 17185, loss = 0.02984713
Iteration 17186, loss = 0.02984517
Iteration 17187, loss = 0.02984323
Iteration 17188, loss = 0.02984126
Iteration 17189, loss = 0.02983929
Iteration 17190, loss = 0.02983733
Iteration 17191, loss = 0.02983536
Iteration 17192, loss = 0.02983340
Iteration 17193, loss = 0.02983143
Iteration 17194, loss = 0.02982948
Iteration 17195, loss = 0.02982752
Iteration 17196, loss = 0.02982553
Iteration 17197, loss = 0.02982360
Iteration 17198, loss = 0.02982164
Iteration 17199, loss = 0.02981966
Iteration 17200, loss = 0.02981771
Iteration 17201, loss = 0.02981576
Iteration 17202, loss = 0.02981379
Iteration 17203, loss = 0.02981184
Iteration 17204, loss = 0.02980988
Iteration 17205, loss = 0.02980794
Iteration 17206, loss = 0.02980597
Iteration 17207, loss = 0.02980401
Iteration 17208, loss = 0.02980206
Iteration 17209, loss = 0.02980010
Iteration 17210, loss = 0.02979815
Iteration 17211, loss = 0.02979617
Iteration 17212, loss = 0.02979423
Iteration 17213, loss = 0.02979231
Iteration 17214, loss = 0.02979032
Iteration 17215, loss = 0.02978836
Iteration 17216, loss = 0.02978641
Iteration 17217, loss = 0.02978446
Iteration 17218, loss = 0.02978251
Iteration 17219, loss = 0.02978056
Iteration 17220, loss = 0.02977862
Iteration 17221, loss = 0.02977666
Iteration 17222, loss = 0.02977471
Iteration 17223, loss = 0.02977273
Iteration 17224, loss = 0.02977079
Iteration 17225, loss = 0.02976884
Iteration 17226, loss = 0.02976687
Iteration 17227, loss = 0.02976492
Iteration 17228, loss = 0.02976297
Iteration 17229, loss = 0.02976101
Iteration 17230, loss = 0.02975906
Iteration 17231, loss = 0.02975708
Iteration 17232, loss = 0.02975513
Iteration 17233, loss = 0.02975317
Iteration 17234, loss = 0.02975121
Iteration 17235, loss = 0.02974925
Iteration 17236, loss = 0.02974732
Iteration 17237, loss = 0.02974538
Iteration 17238, loss = 0.02974342
Iteration 17239, loss = 0.02974144
Iteration 17240, loss = 0.02973948
Iteration 17241, loss = 0.02973754
Iteration 17242, loss = 0.02973560
Iteration 17243, loss = 0.02973364
Iteration 17244, loss = 0.02973169
Iteration 17245, loss = 0.02972974
Iteration 17246, loss = 0.02972781
Iteration 17247, loss = 0.02972584
Iteration 17248, loss = 0.02972391
Iteration 17249, loss = 0.02972195
Iteration 17250, loss = 0.02972003
Iteration 17251, loss = 0.02971806
Iteration 17252, loss = 0.02971613
Iteration 17253, loss = 0.02971419
Iteration 17254, loss = 0.02971225
Iteration 17255, loss = 0.02971032
Iteration 17256, loss = 0.02970834
Iteration 17257, loss = 0.02970640
Iteration 17258, loss = 0.02970446
Iteration 17259, loss = 0.02970253
Iteration 17260, loss = 0.02970057
Iteration 17261, loss = 0.02969863
Iteration 17262, loss = 0.02969667
Iteration 17263, loss = 0.02969474
Iteration 17264, loss = 0.02969278
Iteration 17265, loss = 0.02969085
Iteration 17266, loss = 0.02968890
Iteration 17267, loss = 0.02968693
Iteration 17268, loss = 0.02968503
Iteration 17269, loss = 0.02968307
Iteration 17270, loss = 0.02968111
Iteration 17271, loss = 0.02967920
Iteration 17272, loss = 0.02967722
Iteration 17273, loss = 0.02967529
Iteration 17274, loss = 0.02967337
Iteration 17275, loss = 0.02967143
Iteration 17276, loss = 0.02966948
Iteration 17277, loss = 0.02966753
Iteration 17278, loss = 0.02966560
Iteration 17279, loss = 0.02966366
Iteration 17280, loss = 0.02966174
Iteration 17281, loss = 0.02965980
Iteration 17282, loss = 0.02965787
Iteration 17283, loss = 0.02965592
Iteration 17284, loss = 0.02965399
Iteration 17285, loss = 0.02965207
Iteration 17286, loss = 0.02965014
Iteration 17287, loss = 0.02964819
Iteration 17288, loss = 0.02964623
Iteration 17289, loss = 0.02964433
Iteration 17290, loss = 0.02964239
Iteration 17291, loss = 0.02964047
Iteration 17292, loss = 0.02963854
Iteration 17293, loss = 0.02963658
Iteration 17294, loss = 0.02963467
Iteration 17295, loss = 0.02963269
Iteration 17296, loss = 0.02963078
Iteration 17297, loss = 0.02962887
Iteration 17298, loss = 0.02962692
Iteration 17299, loss = 0.02962498
Iteration 17300, loss = 0.02962304
Iteration 17301, loss = 0.02962110
Iteration 17302, loss = 0.02961917
Iteration 17303, loss = 0.02961725
Iteration 17304, loss = 0.02961532
Iteration 17305, loss = 0.02961337
Iteration 17306, loss = 0.02961144
Iteration 17307, loss = 0.02960953
Iteration 17308, loss = 0.02960758
Iteration 17309, loss = 0.02960564
Iteration 17310, loss = 0.02960373
Iteration 17311, loss = 0.02960180
Iteration 17312, loss = 0.02959985
Iteration 17313, loss = 0.02959794
Iteration 17314, loss = 0.02959599
Iteration 17315, loss = 0.02959405
Iteration 17316, loss = 0.02959216
Iteration 17317, loss = 0.02959021
Iteration 17318, loss = 0.02958829
Iteration 17319, loss = 0.02958636
Iteration 17320, loss = 0.02958444
Iteration 17321, loss = 0.02958251
Iteration 17322, loss = 0.02958060
Iteration 17323, loss = 0.02957868
Iteration 17324, loss = 0.02957674
Iteration 17325, loss = 0.02957486
Iteration 17326, loss = 0.02957290
Iteration 17327, loss = 0.02957098
Iteration 17328, loss = 0.02956905
Iteration 17329, loss = 0.02956711
Iteration 17330, loss = 0.02956520
Iteration 17331, loss = 0.02956325
Iteration 17332, loss = 0.02956133
Iteration 17333, loss = 0.02955940
Iteration 17334, loss = 0.02955749
Iteration 17335, loss = 0.02955554
Iteration 17336, loss = 0.02955363
Iteration 17337, loss = 0.02955169
Iteration 17338, loss = 0.02954976
Iteration 17339, loss = 0.02954787
Iteration 17340, loss = 0.02954591
Iteration 17341, loss = 0.02954399
Iteration 17342, loss = 0.02954207
Iteration 17343, loss = 0.02954014
Iteration 17344, loss = 0.02953821
Iteration 17345, loss = 0.02953632
Iteration 17346, loss = 0.02953440
Iteration 17347, loss = 0.02953246
Iteration 17348, loss = 0.02953055
Iteration 17349, loss = 0.02952861
Iteration 17350, loss = 0.02952669
Iteration 17351, loss = 0.02952479
Iteration 17352, loss = 0.02952286
Iteration 17353, loss = 0.02952092
Iteration 17354, loss = 0.02951904
Iteration 17355, loss = 0.02951708
Iteration 17356, loss = 0.02951517
Iteration 17357, loss = 0.02951326
Iteration 17358, loss = 0.02951133
Iteration 17359, loss = 0.02950943
Iteration 17360, loss = 0.02950751
Iteration 17361, loss = 0.02950558
Iteration 17362, loss = 0.02950366
Iteration 17363, loss = 0.02950173
Iteration 17364, loss = 0.02949982
Iteration 17365, loss = 0.02949787
Iteration 17366, loss = 0.02949597
Iteration 17367, loss = 0.02949404
Iteration 17368, loss = 0.02949212
Iteration 17369, loss = 0.02949019
Iteration 17370, loss = 0.02948827
Iteration 17371, loss = 0.02948635
Iteration 17372, loss = 0.02948443
Iteration 17373, loss = 0.02948251
Iteration 17374, loss = 0.02948060
Iteration 17375, loss = 0.02947868
Iteration 17376, loss = 0.02947674
Iteration 17377, loss = 0.02947483
Iteration 17378, loss = 0.02947292
Iteration 17379, loss = 0.02947098
Iteration 17380, loss = 0.02946905
Iteration 17381, loss = 0.02946713
Iteration 17382, loss = 0.02946523
Iteration 17383, loss = 0.02946333
Iteration 17384, loss = 0.02946140
Iteration 17385, loss = 0.02945948
Iteration 17386, loss = 0.02945757
Iteration 17387, loss = 0.02945565
Iteration 17388, loss = 0.02945375
Iteration 17389, loss = 0.02945183
Iteration 17390, loss = 0.02944992
Iteration 17391, loss = 0.02944798
Iteration 17392, loss = 0.02944606
Iteration 17393, loss = 0.02944414
Iteration 17394, loss = 0.02944224
Iteration 17395, loss = 0.02944033
Iteration 17396, loss = 0.02943841
Iteration 17397, loss = 0.02943647
Iteration 17398, loss = 0.02943457
Iteration 17399, loss = 0.02943266
Iteration 17400, loss = 0.02943075
Iteration 17401, loss = 0.02942883
Iteration 17402, loss = 0.02942693
Iteration 17403, loss = 0.02942501
Iteration 17404, loss = 0.02942309
Iteration 17405, loss = 0.02942118
Iteration 17406, loss = 0.02941928
Iteration 17407, loss = 0.02941738
Iteration 17408, loss = 0.02941545
Iteration 17409, loss = 0.02941356
Iteration 17410, loss = 0.02941162
Iteration 17411, loss = 0.02940975
Iteration 17412, loss = 0.02940782
Iteration 17413, loss = 0.02940593
Iteration 17414, loss = 0.02940401
Iteration 17415, loss = 0.02940212
Iteration 17416, loss = 0.02940021
Iteration 17417, loss = 0.02939829
Iteration 17418, loss = 0.02939640
Iteration 17419, loss = 0.02939450
Iteration 17420, loss = 0.02939258
Iteration 17421, loss = 0.02939071
Iteration 17422, loss = 0.02938876
Iteration 17423, loss = 0.02938688
Iteration 17424, loss = 0.02938496
Iteration 17425, loss = 0.02938307
Iteration 17426, loss = 0.02938118
Iteration 17427, loss = 0.02937926
Iteration 17428, loss = 0.02937735
Iteration 17429, loss = 0.02937544
Iteration 17430, loss = 0.02937355
Iteration 17431, loss = 0.02937165
Iteration 17432, loss = 0.02936975
Iteration 17433, loss = 0.02936786
Iteration 17434, loss = 0.02936596
Iteration 17435, loss = 0.02936406
Iteration 17436, loss = 0.02936216
Iteration 17437, loss = 0.02936025
Iteration 17438, loss = 0.02935836
Iteration 17439, loss = 0.02935645
Iteration 17440, loss = 0.02935454
Iteration 17441, loss = 0.02935265
Iteration 17442, loss = 0.02935075
Iteration 17443, loss = 0.02934888
Iteration 17444, loss = 0.02934697
Iteration 17445, loss = 0.02934506
Iteration 17446, loss = 0.02934314
Iteration 17447, loss = 0.02934125
Iteration 17448, loss = 0.02933937
Iteration 17449, loss = 0.02933746
Iteration 17450, loss = 0.02933556
Iteration 17451, loss = 0.02933366
Iteration 17452, loss = 0.02933178
Iteration 17453, loss = 0.02932986
Iteration 17454, loss = 0.02932795
Iteration 17455, loss = 0.02932606
Iteration 17456, loss = 0.02932413
Iteration 17457, loss = 0.02932226
Iteration 17458, loss = 0.02932034
Iteration 17459, loss = 0.02931844
Iteration 17460, loss = 0.02931653
Iteration 17461, loss = 0.02931464
Iteration 17462, loss = 0.02931272
Iteration 17463, loss = 0.02931085
Iteration 17464, loss = 0.02930895
Iteration 17465, loss = 0.02930704
Iteration 17466, loss = 0.02930516
Iteration 17467, loss = 0.02930327
Iteration 17468, loss = 0.02930136
Iteration 17469, loss = 0.02929944
Iteration 17470, loss = 0.02929757
Iteration 17471, loss = 0.02929568
Iteration 17472, loss = 0.02929377
Iteration 17473, loss = 0.02929190
Iteration 17474, loss = 0.02928999
Iteration 17475, loss = 0.02928811
Iteration 17476, loss = 0.02928621
Iteration 17477, loss = 0.02928431
Iteration 17478, loss = 0.02928243
Iteration 17479, loss = 0.02928054
Iteration 17480, loss = 0.02927863
Iteration 17481, loss = 0.02927676
Iteration 17482, loss = 0.02927483
Iteration 17483, loss = 0.02927291
Iteration 17484, loss = 0.02927103
Iteration 17485, loss = 0.02926914
Iteration 17486, loss = 0.02926724
Iteration 17487, loss = 0.02926535
Iteration 17488, loss = 0.02926343
Iteration 17489, loss = 0.02926154
Iteration 17490, loss = 0.02925965
Iteration 17491, loss = 0.02925778
Iteration 17492, loss = 0.02925588
Iteration 17493, loss = 0.02925398
Iteration 17494, loss = 0.02925209
Iteration 17495, loss = 0.02925019
Iteration 17496, loss = 0.02924830
Iteration 17497, loss = 0.02924644
Iteration 17498, loss = 0.02924452
Iteration 17499, loss = 0.02924263
Iteration 17500, loss = 0.02924076
Iteration 17501, loss = 0.02923883
Iteration 17502, loss = 0.02923698
Iteration 17503, loss = 0.02923508
Iteration 17504, loss = 0.02923321
Iteration 17505, loss = 0.02923132
Iteration 17506, loss = 0.02922944
Iteration 17507, loss = 0.02922756
Iteration 17508, loss = 0.02922566
Iteration 17509, loss = 0.02922380
Iteration 17510, loss = 0.02922190
Iteration 17511, loss = 0.02922004
Iteration 17512, loss = 0.02921812
Iteration 17513, loss = 0.02921624
Iteration 17514, loss = 0.02921434
Iteration 17515, loss = 0.02921246
Iteration 17516, loss = 0.02921058
Iteration 17517, loss = 0.02920869
Iteration 17518, loss = 0.02920682
Iteration 17519, loss = 0.02920493
Iteration 17520, loss = 0.02920305
Iteration 17521, loss = 0.02920118
Iteration 17522, loss = 0.02919928
Iteration 17523, loss = 0.02919740
Iteration 17524, loss = 0.02919551
Iteration 17525, loss = 0.02919363
Iteration 17526, loss = 0.02919177
Iteration 17527, loss = 0.02918988
Iteration 17528, loss = 0.02918800
Iteration 17529, loss = 0.02918615
Iteration 17530, loss = 0.02918428
Iteration 17531, loss = 0.02918238
Iteration 17532, loss = 0.02918053
Iteration 17533, loss = 0.02917864
Iteration 17534, loss = 0.02917677
Iteration 17535, loss = 0.02917489
Iteration 17536, loss = 0.02917302
Iteration 17537, loss = 0.02917114
Iteration 17538, loss = 0.02916925
Iteration 17539, loss = 0.02916739
Iteration 17540, loss = 0.02916550
Iteration 17541, loss = 0.02916366
Iteration 17542, loss = 0.02916176
Iteration 17543, loss = 0.02915991
Iteration 17544, loss = 0.02915802
Iteration 17545, loss = 0.02915616
Iteration 17546, loss = 0.02915428
Iteration 17547, loss = 0.02915244
Iteration 17548, loss = 0.02915057
Iteration 17549, loss = 0.02914868
Iteration 17550, loss = 0.02914683
Iteration 17551, loss = 0.02914496
Iteration 17552, loss = 0.02914307
Iteration 17553, loss = 0.02914121
Iteration 17554, loss = 0.02913933
Iteration 17555, loss = 0.02913747
Iteration 17556, loss = 0.02913559
Iteration 17557, loss = 0.02913373
Iteration 17558, loss = 0.02913187
Iteration 17559, loss = 0.02913000
Iteration 17560, loss = 0.02912813
Iteration 17561, loss = 0.02912624
Iteration 17562, loss = 0.02912436
Iteration 17563, loss = 0.02912252
Iteration 17564, loss = 0.02912066
Iteration 17565, loss = 0.02911878
Iteration 17566, loss = 0.02911689
Iteration 17567, loss = 0.02911503
Iteration 17568, loss = 0.02911317
Iteration 17569, loss = 0.02911128
Iteration 17570, loss = 0.02910945
Iteration 17571, loss = 0.02910756
Iteration 17572, loss = 0.02910569
Iteration 17573, loss = 0.02910380
Iteration 17574, loss = 0.02910194
Iteration 17575, loss = 0.02910007
Iteration 17576, loss = 0.02909821
Iteration 17577, loss = 0.02909632
Iteration 17578, loss = 0.02909447
Iteration 17579, loss = 0.02909264
Iteration 17580, loss = 0.02909072
Iteration 17581, loss = 0.02908886
Iteration 17582, loss = 0.02908702
Iteration 17583, loss = 0.02908516
Iteration 17584, loss = 0.02908327
Iteration 17585, loss = 0.02908141
Iteration 17586, loss = 0.02907956
Iteration 17587, loss = 0.02907769
Iteration 17588, loss = 0.02907584
Iteration 17589, loss = 0.02907396
Iteration 17590, loss = 0.02907211
Iteration 17591, loss = 0.02907023
Iteration 17592, loss = 0.02906839
Iteration 17593, loss = 0.02906651
Iteration 17594, loss = 0.02906465
Iteration 17595, loss = 0.02906278
Iteration 17596, loss = 0.02906090
Iteration 17597, loss = 0.02905905
Iteration 17598, loss = 0.02905719
Iteration 17599, loss = 0.02905534
Iteration 17600, loss = 0.02905345
Iteration 17601, loss = 0.02905160
Iteration 17602, loss = 0.02904972
Iteration 17603, loss = 0.02904785
Iteration 17604, loss = 0.02904602
Iteration 17605, loss = 0.02904415
Iteration 17606, loss = 0.02904228
Iteration 17607, loss = 0.02904042
Iteration 17608, loss = 0.02903856
Iteration 17609, loss = 0.02903668
Iteration 17610, loss = 0.02903483
Iteration 17611, loss = 0.02903295
Iteration 17612, loss = 0.02903112
Iteration 17613, loss = 0.02902927
Iteration 17614, loss = 0.02902737
Iteration 17615, loss = 0.02902555
Iteration 17616, loss = 0.02902367
Iteration 17617, loss = 0.02902184
Iteration 17618, loss = 0.02901999
Iteration 17619, loss = 0.02901812
Iteration 17620, loss = 0.02901626
Iteration 17621, loss = 0.02901443
Iteration 17622, loss = 0.02901258
Iteration 17623, loss = 0.02901073
Iteration 17624, loss = 0.02900888
Iteration 17625, loss = 0.02900702
Iteration 17626, loss = 0.02900519
Iteration 17627, loss = 0.02900332
Iteration 17628, loss = 0.02900150
Iteration 17629, loss = 0.02899962
Iteration 17630, loss = 0.02899776
Iteration 17631, loss = 0.02899590
Iteration 17632, loss = 0.02899407
Iteration 17633, loss = 0.02899218
Iteration 17634, loss = 0.02899037
Iteration 17635, loss = 0.02898849
Iteration 17636, loss = 0.02898662
Iteration 17637, loss = 0.02898479
Iteration 17638, loss = 0.02898294
Iteration 17639, loss = 0.02898107
Iteration 17640, loss = 0.02897923
Iteration 17641, loss = 0.02897736
Iteration 17642, loss = 0.02897550
Iteration 17643, loss = 0.02897365
Iteration 17644, loss = 0.02897182
Iteration 17645, loss = 0.02896996
Iteration 17646, loss = 0.02896810
Iteration 17647, loss = 0.02896625
Iteration 17648, loss = 0.02896442
Iteration 17649, loss = 0.02896255
Iteration 17650, loss = 0.02896069
Iteration 17651, loss = 0.02895883
Iteration 17652, loss = 0.02895698
Iteration 17653, loss = 0.02895514
Iteration 17654, loss = 0.02895332
Iteration 17655, loss = 0.02895145
Iteration 17656, loss = 0.02894959
Iteration 17657, loss = 0.02894775
Iteration 17658, loss = 0.02894592
Iteration 17659, loss = 0.02894407
Iteration 17660, loss = 0.02894221
Iteration 17661, loss = 0.02894038
Iteration 17662, loss = 0.02893852
Iteration 17663, loss = 0.02893670
Iteration 17664, loss = 0.02893484
Iteration 17665, loss = 0.02893297
Iteration 17666, loss = 0.02893113
Iteration 17667, loss = 0.02892929
Iteration 17668, loss = 0.02892743
Iteration 17669, loss = 0.02892556
Iteration 17670, loss = 0.02892373
Iteration 17671, loss = 0.02892190
Iteration 17672, loss = 0.02892004
Iteration 17673, loss = 0.02891820
Iteration 17674, loss = 0.02891635
Iteration 17675, loss = 0.02891450
Iteration 17676, loss = 0.02891263
Iteration 17677, loss = 0.02891084
Iteration 17678, loss = 0.02890897
Iteration 17679, loss = 0.02890710
Iteration 17680, loss = 0.02890527
Iteration 17681, loss = 0.02890340
Iteration 17682, loss = 0.02890159
Iteration 17683, loss = 0.02889975
Iteration 17684, loss = 0.02889789
Iteration 17685, loss = 0.02889605
Iteration 17686, loss = 0.02889421
Iteration 17687, loss = 0.02889238
Iteration 17688, loss = 0.02889055
Iteration 17689, loss = 0.02888872
Iteration 17690, loss = 0.02888686
Iteration 17691, loss = 0.02888504
Iteration 17692, loss = 0.02888320
Iteration 17693, loss = 0.02888136
Iteration 17694, loss = 0.02887951
Iteration 17695, loss = 0.02887769
Iteration 17696, loss = 0.02887585
Iteration 17697, loss = 0.02887402
Iteration 17698, loss = 0.02887218
Iteration 17699, loss = 0.02887035
Iteration 17700, loss = 0.02886851
Iteration 17701, loss = 0.02886667
Iteration 17702, loss = 0.02886484
Iteration 17703, loss = 0.02886301
Iteration 17704, loss = 0.02886116
Iteration 17705, loss = 0.02885934
Iteration 17706, loss = 0.02885750
Iteration 17707, loss = 0.02885566
Iteration 17708, loss = 0.02885383
Iteration 17709, loss = 0.02885197
Iteration 17710, loss = 0.02885016
Iteration 17711, loss = 0.02884832
Iteration 17712, loss = 0.02884649
Iteration 17713, loss = 0.02884465
Iteration 17714, loss = 0.02884280
Iteration 17715, loss = 0.02884096
Iteration 17716, loss = 0.02883913
Iteration 17717, loss = 0.02883730
Iteration 17718, loss = 0.02883545
Iteration 17719, loss = 0.02883362
Iteration 17720, loss = 0.02883178
Iteration 17721, loss = 0.02882995
Iteration 17722, loss = 0.02882811
Iteration 17723, loss = 0.02882628
Iteration 17724, loss = 0.02882444
Iteration 17725, loss = 0.02882264
Iteration 17726, loss = 0.02882078
Iteration 17727, loss = 0.02881894
Iteration 17728, loss = 0.02881711
Iteration 17729, loss = 0.02881529
Iteration 17730, loss = 0.02881344
Iteration 17731, loss = 0.02881161
Iteration 17732, loss = 0.02880980
Iteration 17733, loss = 0.02880797
Iteration 17734, loss = 0.02880611
Iteration 17735, loss = 0.02880427
Iteration 17736, loss = 0.02880244
Iteration 17737, loss = 0.02880062
Iteration 17738, loss = 0.02879878
Iteration 17739, loss = 0.02879693
Iteration 17740, loss = 0.02879511
Iteration 17741, loss = 0.02879327
Iteration 17742, loss = 0.02879142
Iteration 17743, loss = 0.02878958
Iteration 17744, loss = 0.02878777
Iteration 17745, loss = 0.02878594
Iteration 17746, loss = 0.02878411
Iteration 17747, loss = 0.02878228
Iteration 17748, loss = 0.02878045
Iteration 17749, loss = 0.02877862
Iteration 17750, loss = 0.02877682
Iteration 17751, loss = 0.02877497
Iteration 17752, loss = 0.02877317
Iteration 17753, loss = 0.02877133
Iteration 17754, loss = 0.02876949
Iteration 17755, loss = 0.02876769
Iteration 17756, loss = 0.02876586
Iteration 17757, loss = 0.02876403
Iteration 17758, loss = 0.02876224
Iteration 17759, loss = 0.02876039
Iteration 17760, loss = 0.02875857
Iteration 17761, loss = 0.02875672
Iteration 17762, loss = 0.02875490
Iteration 17763, loss = 0.02875310
Iteration 17764, loss = 0.02875126
Iteration 17765, loss = 0.02874946
Iteration 17766, loss = 0.02874763
Iteration 17767, loss = 0.02874578
Iteration 17768, loss = 0.02874397
Iteration 17769, loss = 0.02874216
Iteration 17770, loss = 0.02874031
Iteration 17771, loss = 0.02873849
Iteration 17772, loss = 0.02873666
Iteration 17773, loss = 0.02873485
Iteration 17774, loss = 0.02873303
Iteration 17775, loss = 0.02873119
Iteration 17776, loss = 0.02872937
Iteration 17777, loss = 0.02872753
Iteration 17778, loss = 0.02872572
Iteration 17779, loss = 0.02872393
Iteration 17780, loss = 0.02872208
Iteration 17781, loss = 0.02872028
Iteration 17782, loss = 0.02871845
Iteration 17783, loss = 0.02871664
Iteration 17784, loss = 0.02871480
Iteration 17785, loss = 0.02871297
Iteration 17786, loss = 0.02871118
Iteration 17787, loss = 0.02870934
Iteration 17788, loss = 0.02870752
Iteration 17789, loss = 0.02870571
Iteration 17790, loss = 0.02870390
Iteration 17791, loss = 0.02870204
Iteration 17792, loss = 0.02870024
Iteration 17793, loss = 0.02869844
Iteration 17794, loss = 0.02869661
Iteration 17795, loss = 0.02869480
Iteration 17796, loss = 0.02869300
Iteration 17797, loss = 0.02869118
Iteration 17798, loss = 0.02868937
Iteration 17799, loss = 0.02868756
Iteration 17800, loss = 0.02868573
Iteration 17801, loss = 0.02868392
Iteration 17802, loss = 0.02868212
Iteration 17803, loss = 0.02868029
Iteration 17804, loss = 0.02867848
Iteration 17805, loss = 0.02867665
Iteration 17806, loss = 0.02867487
Iteration 17807, loss = 0.02867306
Iteration 17808, loss = 0.02867122
Iteration 17809, loss = 0.02866943
Iteration 17810, loss = 0.02866763
Iteration 17811, loss = 0.02866580
Iteration 17812, loss = 0.02866402
Iteration 17813, loss = 0.02866217
Iteration 17814, loss = 0.02866038
Iteration 17815, loss = 0.02865857
Iteration 17816, loss = 0.02865675
Iteration 17817, loss = 0.02865497
Iteration 17818, loss = 0.02865313
Iteration 17819, loss = 0.02865134
Iteration 17820, loss = 0.02864953
Iteration 17821, loss = 0.02864772
Iteration 17822, loss = 0.02864593
Iteration 17823, loss = 0.02864411
Iteration 17824, loss = 0.02864228
Iteration 17825, loss = 0.02864049
Iteration 17826, loss = 0.02863868
Iteration 17827, loss = 0.02863688
Iteration 17828, loss = 0.02863507
Iteration 17829, loss = 0.02863327
Iteration 17830, loss = 0.02863148
Iteration 17831, loss = 0.02862964
Iteration 17832, loss = 0.02862786
Iteration 17833, loss = 0.02862603
Iteration 17834, loss = 0.02862423
Iteration 17835, loss = 0.02862243
Iteration 17836, loss = 0.02862063
Iteration 17837, loss = 0.02861881
Iteration 17838, loss = 0.02861701
Iteration 17839, loss = 0.02861522
Iteration 17840, loss = 0.02861342
Iteration 17841, loss = 0.02861160
Iteration 17842, loss = 0.02860977
Iteration 17843, loss = 0.02860799
Iteration 17844, loss = 0.02860618
Iteration 17845, loss = 0.02860436
Iteration 17846, loss = 0.02860258
Iteration 17847, loss = 0.02860076
Iteration 17848, loss = 0.02859899
Iteration 17849, loss = 0.02859718
Iteration 17850, loss = 0.02859536
Iteration 17851, loss = 0.02859357
Iteration 17852, loss = 0.02859174
Iteration 17853, loss = 0.02858995
Iteration 17854, loss = 0.02858814
Iteration 17855, loss = 0.02858633
Iteration 17856, loss = 0.02858454
Iteration 17857, loss = 0.02858273
Iteration 17858, loss = 0.02858095
Iteration 17859, loss = 0.02857914
Iteration 17860, loss = 0.02857733
Iteration 17861, loss = 0.02857553
Iteration 17862, loss = 0.02857373
Iteration 17863, loss = 0.02857195
Iteration 17864, loss = 0.02857011
Iteration 17865, loss = 0.02856832
Iteration 17866, loss = 0.02856652
Iteration 17867, loss = 0.02856472
Iteration 17868, loss = 0.02856293
Iteration 17869, loss = 0.02856114
Iteration 17870, loss = 0.02855934
Iteration 17871, loss = 0.02855753
Iteration 17872, loss = 0.02855574
Iteration 17873, loss = 0.02855393
Iteration 17874, loss = 0.02855214
Iteration 17875, loss = 0.02855035
Iteration 17876, loss = 0.02854853
Iteration 17877, loss = 0.02854673
Iteration 17878, loss = 0.02854494
Iteration 17879, loss = 0.02854314
Iteration 17880, loss = 0.02854134
Iteration 17881, loss = 0.02853956
Iteration 17882, loss = 0.02853775
Iteration 17883, loss = 0.02853597
Iteration 17884, loss = 0.02853416
Iteration 17885, loss = 0.02853238
Iteration 17886, loss = 0.02853057
Iteration 17887, loss = 0.02852877
Iteration 17888, loss = 0.02852699
Iteration 17889, loss = 0.02852520
Iteration 17890, loss = 0.02852340
Iteration 17891, loss = 0.02852162
Iteration 17892, loss = 0.02851981
Iteration 17893, loss = 0.02851804
Iteration 17894, loss = 0.02851626
Iteration 17895, loss = 0.02851445
Iteration 17896, loss = 0.02851267
Iteration 17897, loss = 0.02851089
Iteration 17898, loss = 0.02850911
Iteration 17899, loss = 0.02850731
Iteration 17900, loss = 0.02850552
Iteration 17901, loss = 0.02850371
Iteration 17902, loss = 0.02850196
Iteration 17903, loss = 0.02850016
Iteration 17904, loss = 0.02849837
Iteration 17905, loss = 0.02849658
Iteration 17906, loss = 0.02849480
Iteration 17907, loss = 0.02849301
Iteration 17908, loss = 0.02849122
Iteration 17909, loss = 0.02848945
Iteration 17910, loss = 0.02848765
Iteration 17911, loss = 0.02848586
Iteration 17912, loss = 0.02848409
Iteration 17913, loss = 0.02848228
Iteration 17914, loss = 0.02848051
Iteration 17915, loss = 0.02847869
Iteration 17916, loss = 0.02847692
Iteration 17917, loss = 0.02847514
Iteration 17918, loss = 0.02847336
Iteration 17919, loss = 0.02847155
Iteration 17920, loss = 0.02846977
Iteration 17921, loss = 0.02846799
Iteration 17922, loss = 0.02846621
Iteration 17923, loss = 0.02846441
Iteration 17924, loss = 0.02846261
Iteration 17925, loss = 0.02846083
Iteration 17926, loss = 0.02845904
Iteration 17927, loss = 0.02845725
Iteration 17928, loss = 0.02845548
Iteration 17929, loss = 0.02845368
Iteration 17930, loss = 0.02845191
Iteration 17931, loss = 0.02845011
Iteration 17932, loss = 0.02844831
Iteration 17933, loss = 0.02844655
Iteration 17934, loss = 0.02844475
Iteration 17935, loss = 0.02844296
Iteration 17936, loss = 0.02844118
Iteration 17937, loss = 0.02843939
Iteration 17938, loss = 0.02843763
Iteration 17939, loss = 0.02843580
Iteration 17940, loss = 0.02843404
Iteration 17941, loss = 0.02843225
Iteration 17942, loss = 0.02843048
Iteration 17943, loss = 0.02842868
Iteration 17944, loss = 0.02842688
Iteration 17945, loss = 0.02842511
Iteration 17946, loss = 0.02842333
Iteration 17947, loss = 0.02842154
Iteration 17948, loss = 0.02841977
Iteration 17949, loss = 0.02841797
Iteration 17950, loss = 0.02841618
Iteration 17951, loss = 0.02841440
Iteration 17952, loss = 0.02841266
Iteration 17953, loss = 0.02841084
Iteration 17954, loss = 0.02840909
Iteration 17955, loss = 0.02840728
Iteration 17956, loss = 0.02840550
Iteration 17957, loss = 0.02840372
Iteration 17958, loss = 0.02840196
Iteration 17959, loss = 0.02840017
Iteration 17960, loss = 0.02839841
Iteration 17961, loss = 0.02839661
Iteration 17962, loss = 0.02839486
Iteration 17963, loss = 0.02839305
Iteration 17964, loss = 0.02839127
Iteration 17965, loss = 0.02838950
Iteration 17966, loss = 0.02838774
Iteration 17967, loss = 0.02838595
Iteration 17968, loss = 0.02838416
Iteration 17969, loss = 0.02838240
Iteration 17970, loss = 0.02838060
Iteration 17971, loss = 0.02837884
Iteration 17972, loss = 0.02837708
Iteration 17973, loss = 0.02837526
Iteration 17974, loss = 0.02837349
Iteration 17975, loss = 0.02837174
Iteration 17976, loss = 0.02836996
Iteration 17977, loss = 0.02836819
Iteration 17978, loss = 0.02836641
Iteration 17979, loss = 0.02836464
Iteration 17980, loss = 0.02836287
Iteration 17981, loss = 0.02836110
Iteration 17982, loss = 0.02835933
Iteration 17983, loss = 0.02835759
Iteration 17984, loss = 0.02835579
Iteration 17985, loss = 0.02835401
Iteration 17986, loss = 0.02835224
Iteration 17987, loss = 0.02835048
Iteration 17988, loss = 0.02834869
Iteration 17989, loss = 0.02834694
Iteration 17990, loss = 0.02834515
Iteration 17991, loss = 0.02834338
Iteration 17992, loss = 0.02834163
Iteration 17993, loss = 0.02833982
Iteration 17994, loss = 0.02833807
Iteration 17995, loss = 0.02833629
Iteration 17996, loss = 0.02833451
Iteration 17997, loss = 0.02833275
Iteration 17998, loss = 0.02833098
Iteration 17999, loss = 0.02832922
Iteration 18000, loss = 0.02832746
Iteration 18001, loss = 0.02832567
Iteration 18002, loss = 0.02832392
Iteration 18003, loss = 0.02832215
Iteration 18004, loss = 0.02832038
Iteration 18005, loss = 0.02831863
Iteration 18006, loss = 0.02831685
Iteration 18007, loss = 0.02831508
Iteration 18008, loss = 0.02831333
Iteration 18009, loss = 0.02831159
Iteration 18010, loss = 0.02830980
Iteration 18011, loss = 0.02830802
Iteration 18012, loss = 0.02830625
Iteration 18013, loss = 0.02830452
Iteration 18014, loss = 0.02830275
Iteration 18015, loss = 0.02830098
Iteration 18016, loss = 0.02829922
Iteration 18017, loss = 0.02829746
Iteration 18018, loss = 0.02829567
Iteration 18019, loss = 0.02829392
Iteration 18020, loss = 0.02829217
Iteration 18021, loss = 0.02829039
Iteration 18022, loss = 0.02828864
Iteration 18023, loss = 0.02828685
Iteration 18024, loss = 0.02828510
Iteration 18025, loss = 0.02828334
Iteration 18026, loss = 0.02828157
Iteration 18027, loss = 0.02827983
Iteration 18028, loss = 0.02827805
Iteration 18029, loss = 0.02827628
Iteration 18030, loss = 0.02827451
Iteration 18031, loss = 0.02827275
Iteration 18032, loss = 0.02827101
Iteration 18033, loss = 0.02826924
Iteration 18034, loss = 0.02826748
Iteration 18035, loss = 0.02826572
Iteration 18036, loss = 0.02826394
Iteration 18037, loss = 0.02826218
Iteration 18038, loss = 0.02826043
Iteration 18039, loss = 0.02825866
Iteration 18040, loss = 0.02825692
Iteration 18041, loss = 0.02825514
Iteration 18042, loss = 0.02825340
Iteration 18043, loss = 0.02825165
Iteration 18044, loss = 0.02824988
Iteration 18045, loss = 0.02824811
Iteration 18046, loss = 0.02824638
Iteration 18047, loss = 0.02824461
Iteration 18048, loss = 0.02824285
Iteration 18049, loss = 0.02824110
Iteration 18050, loss = 0.02823936
Iteration 18051, loss = 0.02823758
Iteration 18052, loss = 0.02823584
Iteration 18053, loss = 0.02823409
Iteration 18054, loss = 0.02823230
Iteration 18055, loss = 0.02823056
Iteration 18056, loss = 0.02822882
Iteration 18057, loss = 0.02822705
Iteration 18058, loss = 0.02822532
Iteration 18059, loss = 0.02822355
Iteration 18060, loss = 0.02822180
Iteration 18061, loss = 0.02822004
Iteration 18062, loss = 0.02821827
Iteration 18063, loss = 0.02821652
Iteration 18064, loss = 0.02821477
Iteration 18065, loss = 0.02821300
Iteration 18066, loss = 0.02821126
Iteration 18067, loss = 0.02820949
Iteration 18068, loss = 0.02820773
Iteration 18069, loss = 0.02820597
Iteration 18070, loss = 0.02820422
Iteration 18071, loss = 0.02820246
Iteration 18072, loss = 0.02820071
Iteration 18073, loss = 0.02819895
Iteration 18074, loss = 0.02819719
Iteration 18075, loss = 0.02819545
Iteration 18076, loss = 0.02819368
Iteration 18077, loss = 0.02819193
Iteration 18078, loss = 0.02819017
Iteration 18079, loss = 0.02818843
Iteration 18080, loss = 0.02818668
Iteration 18081, loss = 0.02818492
Iteration 18082, loss = 0.02818317
Iteration 18083, loss = 0.02818141
Iteration 18084, loss = 0.02817964
Iteration 18085, loss = 0.02817792
Iteration 18086, loss = 0.02817618
Iteration 18087, loss = 0.02817443
Iteration 18088, loss = 0.02817266
Iteration 18089, loss = 0.02817092
Iteration 18090, loss = 0.02816915
Iteration 18091, loss = 0.02816742
Iteration 18092, loss = 0.02816566
Iteration 18093, loss = 0.02816394
Iteration 18094, loss = 0.02816217
Iteration 18095, loss = 0.02816042
Iteration 18096, loss = 0.02815868
Iteration 18097, loss = 0.02815694
Iteration 18098, loss = 0.02815520
Iteration 18099, loss = 0.02815343
Iteration 18100, loss = 0.02815171
Iteration 18101, loss = 0.02814994
Iteration 18102, loss = 0.02814818
Iteration 18103, loss = 0.02814645
Iteration 18104, loss = 0.02814472
Iteration 18105, loss = 0.02814296
Iteration 18106, loss = 0.02814120
Iteration 18107, loss = 0.02813948
Iteration 18108, loss = 0.02813770
Iteration 18109, loss = 0.02813596
Iteration 18110, loss = 0.02813422
Iteration 18111, loss = 0.02813247
Iteration 18112, loss = 0.02813071
Iteration 18113, loss = 0.02812900
Iteration 18114, loss = 0.02812724
Iteration 18115, loss = 0.02812549
Iteration 18116, loss = 0.02812375
Iteration 18117, loss = 0.02812201
Iteration 18118, loss = 0.02812026
Iteration 18119, loss = 0.02811851
Iteration 18120, loss = 0.02811676
Iteration 18121, loss = 0.02811505
Iteration 18122, loss = 0.02811330
Iteration 18123, loss = 0.02811155
Iteration 18124, loss = 0.02810983
Iteration 18125, loss = 0.02810808
Iteration 18126, loss = 0.02810633
Iteration 18127, loss = 0.02810462
Iteration 18128, loss = 0.02810286
Iteration 18129, loss = 0.02810109
Iteration 18130, loss = 0.02809938
Iteration 18131, loss = 0.02809761
Iteration 18132, loss = 0.02809588
Iteration 18133, loss = 0.02809416
Iteration 18134, loss = 0.02809241
Iteration 18135, loss = 0.02809065
Iteration 18136, loss = 0.02808890
Iteration 18137, loss = 0.02808717
Iteration 18138, loss = 0.02808543
Iteration 18139, loss = 0.02808368
Iteration 18140, loss = 0.02808194
Iteration 18141, loss = 0.02808020
Iteration 18142, loss = 0.02807847
Iteration 18143, loss = 0.02807675
Iteration 18144, loss = 0.02807500
Iteration 18145, loss = 0.02807326
Iteration 18146, loss = 0.02807151
Iteration 18147, loss = 0.02806979
Iteration 18148, loss = 0.02806805
Iteration 18149, loss = 0.02806629
Iteration 18150, loss = 0.02806455
Iteration 18151, loss = 0.02806283
Iteration 18152, loss = 0.02806110
Iteration 18153, loss = 0.02805937
Iteration 18154, loss = 0.02805763
Iteration 18155, loss = 0.02805588
Iteration 18156, loss = 0.02805414
Iteration 18157, loss = 0.02805240
Iteration 18158, loss = 0.02805068
Iteration 18159, loss = 0.02804894
Iteration 18160, loss = 0.02804721
Iteration 18161, loss = 0.02804549
Iteration 18162, loss = 0.02804375
Iteration 18163, loss = 0.02804200
Iteration 18164, loss = 0.02804029
Iteration 18165, loss = 0.02803855
Iteration 18166, loss = 0.02803681
Iteration 18167, loss = 0.02803509
Iteration 18168, loss = 0.02803336
Iteration 18169, loss = 0.02803162
Iteration 18170, loss = 0.02802988
Iteration 18171, loss = 0.02802817
Iteration 18172, loss = 0.02802640
Iteration 18173, loss = 0.02802470
Iteration 18174, loss = 0.02802297
Iteration 18175, loss = 0.02802121
Iteration 18176, loss = 0.02801948
Iteration 18177, loss = 0.02801777
Iteration 18178, loss = 0.02801601
Iteration 18179, loss = 0.02801428
Iteration 18180, loss = 0.02801256
Iteration 18181, loss = 0.02801082
Iteration 18182, loss = 0.02800909
Iteration 18183, loss = 0.02800736
Iteration 18184, loss = 0.02800565
Iteration 18185, loss = 0.02800389
Iteration 18186, loss = 0.02800220
Iteration 18187, loss = 0.02800045
Iteration 18188, loss = 0.02799874
Iteration 18189, loss = 0.02799700
Iteration 18190, loss = 0.02799528
Iteration 18191, loss = 0.02799357
Iteration 18192, loss = 0.02799184
Iteration 18193, loss = 0.02799011
Iteration 18194, loss = 0.02798839
Iteration 18195, loss = 0.02798666
Iteration 18196, loss = 0.02798494
Iteration 18197, loss = 0.02798323
Iteration 18198, loss = 0.02798148
Iteration 18199, loss = 0.02797981
Iteration 18200, loss = 0.02797807
Iteration 18201, loss = 0.02797633
Iteration 18202, loss = 0.02797461
Iteration 18203, loss = 0.02797288
Iteration 18204, loss = 0.02797116
Iteration 18205, loss = 0.02796942
Iteration 18206, loss = 0.02796771
Iteration 18207, loss = 0.02796599
Iteration 18208, loss = 0.02796424
Iteration 18209, loss = 0.02796252
Iteration 18210, loss = 0.02796081
Iteration 18211, loss = 0.02795908
Iteration 18212, loss = 0.02795735
Iteration 18213, loss = 0.02795562
Iteration 18214, loss = 0.02795390
Iteration 18215, loss = 0.02795220
Iteration 18216, loss = 0.02795047
Iteration 18217, loss = 0.02794876
Iteration 18218, loss = 0.02794702
Iteration 18219, loss = 0.02794529
Iteration 18220, loss = 0.02794359
Iteration 18221, loss = 0.02794187
Iteration 18222, loss = 0.02794011
Iteration 18223, loss = 0.02793840
Iteration 18224, loss = 0.02793668
Iteration 18225, loss = 0.02793494
Iteration 18226, loss = 0.02793325
Iteration 18227, loss = 0.02793153
Iteration 18228, loss = 0.02792981
Iteration 18229, loss = 0.02792808
Iteration 18230, loss = 0.02792637
Iteration 18231, loss = 0.02792465
Iteration 18232, loss = 0.02792293
Iteration 18233, loss = 0.02792121
Iteration 18234, loss = 0.02791951
Iteration 18235, loss = 0.02791776
Iteration 18236, loss = 0.02791607
Iteration 18237, loss = 0.02791435
Iteration 18238, loss = 0.02791263
Iteration 18239, loss = 0.02791090
Iteration 18240, loss = 0.02790922
Iteration 18241, loss = 0.02790747
Iteration 18242, loss = 0.02790576
Iteration 18243, loss = 0.02790404
Iteration 18244, loss = 0.02790234
Iteration 18245, loss = 0.02790059
Iteration 18246, loss = 0.02789887
Iteration 18247, loss = 0.02789715
Iteration 18248, loss = 0.02789545
Iteration 18249, loss = 0.02789374
Iteration 18250, loss = 0.02789200
Iteration 18251, loss = 0.02789029
Iteration 18252, loss = 0.02788856
Iteration 18253, loss = 0.02788683
Iteration 18254, loss = 0.02788512
Iteration 18255, loss = 0.02788343
Iteration 18256, loss = 0.02788169
Iteration 18257, loss = 0.02787997
Iteration 18258, loss = 0.02787823
Iteration 18259, loss = 0.02787654
Iteration 18260, loss = 0.02787485
Iteration 18261, loss = 0.02787311
Iteration 18262, loss = 0.02787138
Iteration 18263, loss = 0.02786969
Iteration 18264, loss = 0.02786797
Iteration 18265, loss = 0.02786624
Iteration 18266, loss = 0.02786452
Iteration 18267, loss = 0.02786281
Iteration 18268, loss = 0.02786110
Iteration 18269, loss = 0.02785938
Iteration 18270, loss = 0.02785765
Iteration 18271, loss = 0.02785595
Iteration 18272, loss = 0.02785422
Iteration 18273, loss = 0.02785251
Iteration 18274, loss = 0.02785082
Iteration 18275, loss = 0.02784907
Iteration 18276, loss = 0.02784738
Iteration 18277, loss = 0.02784564
Iteration 18278, loss = 0.02784397
Iteration 18279, loss = 0.02784227
Iteration 18280, loss = 0.02784052
Iteration 18281, loss = 0.02783882
Iteration 18282, loss = 0.02783711
Iteration 18283, loss = 0.02783541
Iteration 18284, loss = 0.02783369
Iteration 18285, loss = 0.02783197
Iteration 18286, loss = 0.02783028
Iteration 18287, loss = 0.02782855
Iteration 18288, loss = 0.02782684
Iteration 18289, loss = 0.02782515
Iteration 18290, loss = 0.02782345
Iteration 18291, loss = 0.02782171
Iteration 18292, loss = 0.02782003
Iteration 18293, loss = 0.02781830
Iteration 18294, loss = 0.02781663
Iteration 18295, loss = 0.02781491
Iteration 18296, loss = 0.02781320
Iteration 18297, loss = 0.02781147
Iteration 18298, loss = 0.02780980
Iteration 18299, loss = 0.02780809
Iteration 18300, loss = 0.02780638
Iteration 18301, loss = 0.02780466
Iteration 18302, loss = 0.02780297
Iteration 18303, loss = 0.02780125
Iteration 18304, loss = 0.02779958
Iteration 18305, loss = 0.02779787
Iteration 18306, loss = 0.02779614
Iteration 18307, loss = 0.02779445
Iteration 18308, loss = 0.02779274
Iteration 18309, loss = 0.02779106
Iteration 18310, loss = 0.02778933
Iteration 18311, loss = 0.02778764
Iteration 18312, loss = 0.02778593
Iteration 18313, loss = 0.02778422
Iteration 18314, loss = 0.02778254
Iteration 18315, loss = 0.02778083
Iteration 18316, loss = 0.02777912
Iteration 18317, loss = 0.02777741
Iteration 18318, loss = 0.02777573
Iteration 18319, loss = 0.02777400
Iteration 18320, loss = 0.02777235
Iteration 18321, loss = 0.02777060
Iteration 18322, loss = 0.02776892
Iteration 18323, loss = 0.02776721
Iteration 18324, loss = 0.02776551
Iteration 18325, loss = 0.02776379
Iteration 18326, loss = 0.02776212
Iteration 18327, loss = 0.02776041
Iteration 18328, loss = 0.02775870
Iteration 18329, loss = 0.02775702
Iteration 18330, loss = 0.02775532
Iteration 18331, loss = 0.02775363
Iteration 18332, loss = 0.02775190
Iteration 18333, loss = 0.02775022
Iteration 18334, loss = 0.02774853
Iteration 18335, loss = 0.02774683
Iteration 18336, loss = 0.02774512
Iteration 18337, loss = 0.02774340
Iteration 18338, loss = 0.02774172
Iteration 18339, loss = 0.02774003
Iteration 18340, loss = 0.02773833
Iteration 18341, loss = 0.02773664
Iteration 18342, loss = 0.02773493
Iteration 18343, loss = 0.02773323
Iteration 18344, loss = 0.02773154
Iteration 18345, loss = 0.02772983
Iteration 18346, loss = 0.02772813
Iteration 18347, loss = 0.02772644
Iteration 18348, loss = 0.02772472
Iteration 18349, loss = 0.02772305
Iteration 18350, loss = 0.02772134
Iteration 18351, loss = 0.02771964
Iteration 18352, loss = 0.02771794
Iteration 18353, loss = 0.02771627
Iteration 18354, loss = 0.02771455
Iteration 18355, loss = 0.02771287
Iteration 18356, loss = 0.02771116
Iteration 18357, loss = 0.02770949
Iteration 18358, loss = 0.02770777
Iteration 18359, loss = 0.02770608
Iteration 18360, loss = 0.02770438
Iteration 18361, loss = 0.02770272
Iteration 18362, loss = 0.02770100
Iteration 18363, loss = 0.02769932
Iteration 18364, loss = 0.02769762
Iteration 18365, loss = 0.02769592
Iteration 18366, loss = 0.02769423
Iteration 18367, loss = 0.02769254
Iteration 18368, loss = 0.02769086
Iteration 18369, loss = 0.02768916
Iteration 18370, loss = 0.02768746
Iteration 18371, loss = 0.02768576
Iteration 18372, loss = 0.02768411
Iteration 18373, loss = 0.02768241
Iteration 18374, loss = 0.02768068
Iteration 18375, loss = 0.02767900
Iteration 18376, loss = 0.02767732
Iteration 18377, loss = 0.02767561
Iteration 18378, loss = 0.02767392
Iteration 18379, loss = 0.02767223
Iteration 18380, loss = 0.02767055
Iteration 18381, loss = 0.02766887
Iteration 18382, loss = 0.02766717
Iteration 18383, loss = 0.02766547
Iteration 18384, loss = 0.02766379
Iteration 18385, loss = 0.02766213
Iteration 18386, loss = 0.02766043
Iteration 18387, loss = 0.02765875
Iteration 18388, loss = 0.02765705
Iteration 18389, loss = 0.02765535
Iteration 18390, loss = 0.02765367
Iteration 18391, loss = 0.02765202
Iteration 18392, loss = 0.02765030
Iteration 18393, loss = 0.02764860
Iteration 18394, loss = 0.02764695
Iteration 18395, loss = 0.02764524
Iteration 18396, loss = 0.02764356
Iteration 18397, loss = 0.02764189
Iteration 18398, loss = 0.02764019
Iteration 18399, loss = 0.02763850
Iteration 18400, loss = 0.02763684
Iteration 18401, loss = 0.02763515
Iteration 18402, loss = 0.02763347
Iteration 18403, loss = 0.02763179
Iteration 18404, loss = 0.02763010
Iteration 18405, loss = 0.02762843
Iteration 18406, loss = 0.02762672
Iteration 18407, loss = 0.02762506
Iteration 18408, loss = 0.02762335
Iteration 18409, loss = 0.02762168
Iteration 18410, loss = 0.02762002
Iteration 18411, loss = 0.02761831
Iteration 18412, loss = 0.02761664
Iteration 18413, loss = 0.02761496
Iteration 18414, loss = 0.02761327
Iteration 18415, loss = 0.02761159
Iteration 18416, loss = 0.02760991
Iteration 18417, loss = 0.02760822
Iteration 18418, loss = 0.02760653
Iteration 18419, loss = 0.02760485
Iteration 18420, loss = 0.02760317
Iteration 18421, loss = 0.02760148
Iteration 18422, loss = 0.02759982
Iteration 18423, loss = 0.02759813
Iteration 18424, loss = 0.02759644
Iteration 18425, loss = 0.02759479
Iteration 18426, loss = 0.02759311
Iteration 18427, loss = 0.02759144
Iteration 18428, loss = 0.02758976
Iteration 18429, loss = 0.02758810
Iteration 18430, loss = 0.02758641
Iteration 18431, loss = 0.02758473
Iteration 18432, loss = 0.02758306
Iteration 18433, loss = 0.02758138
Iteration 18434, loss = 0.02757972
Iteration 18435, loss = 0.02757803
Iteration 18436, loss = 0.02757634
Iteration 18437, loss = 0.02757470
Iteration 18438, loss = 0.02757299
Iteration 18439, loss = 0.02757134
Iteration 18440, loss = 0.02756964
Iteration 18441, loss = 0.02756796
Iteration 18442, loss = 0.02756630
Iteration 18443, loss = 0.02756463
Iteration 18444, loss = 0.02756295
Iteration 18445, loss = 0.02756127
Iteration 18446, loss = 0.02755961
Iteration 18447, loss = 0.02755793
Iteration 18448, loss = 0.02755623
Iteration 18449, loss = 0.02755459
Iteration 18450, loss = 0.02755289
Iteration 18451, loss = 0.02755122
Iteration 18452, loss = 0.02754955
Iteration 18453, loss = 0.02754788
Iteration 18454, loss = 0.02754619
Iteration 18455, loss = 0.02754453
Iteration 18456, loss = 0.02754285
Iteration 18457, loss = 0.02754118
Iteration 18458, loss = 0.02753952
Iteration 18459, loss = 0.02753783
Iteration 18460, loss = 0.02753620
Iteration 18461, loss = 0.02753451
Iteration 18462, loss = 0.02753283
Iteration 18463, loss = 0.02753118
Iteration 18464, loss = 0.02752949
Iteration 18465, loss = 0.02752782
Iteration 18466, loss = 0.02752617
Iteration 18467, loss = 0.02752447
Iteration 18468, loss = 0.02752280
Iteration 18469, loss = 0.02752114
Iteration 18470, loss = 0.02751946
Iteration 18471, loss = 0.02751780
Iteration 18472, loss = 0.02751611
Iteration 18473, loss = 0.02751441
Iteration 18474, loss = 0.02751276
Iteration 18475, loss = 0.02751108
Iteration 18476, loss = 0.02750940
Iteration 18477, loss = 0.02750775
Iteration 18478, loss = 0.02750607
Iteration 18479, loss = 0.02750439
Iteration 18480, loss = 0.02750271
Iteration 18481, loss = 0.02750108
Iteration 18482, loss = 0.02749939
Iteration 18483, loss = 0.02749771
Iteration 18484, loss = 0.02749608
Iteration 18485, loss = 0.02749438
Iteration 18486, loss = 0.02749272
Iteration 18487, loss = 0.02749109
Iteration 18488, loss = 0.02748940
Iteration 18489, loss = 0.02748773
Iteration 18490, loss = 0.02748607
Iteration 18491, loss = 0.02748443
Iteration 18492, loss = 0.02748275
Iteration 18493, loss = 0.02748109
Iteration 18494, loss = 0.02747944
Iteration 18495, loss = 0.02747776
Iteration 18496, loss = 0.02747609
Iteration 18497, loss = 0.02747442
Iteration 18498, loss = 0.02747278
Iteration 18499, loss = 0.02747110
Iteration 18500, loss = 0.02746944
Iteration 18501, loss = 0.02746778
Iteration 18502, loss = 0.02746609
Iteration 18503, loss = 0.02746443
Iteration 18504, loss = 0.02746278
Iteration 18505, loss = 0.02746112
Iteration 18506, loss = 0.02745943
Iteration 18507, loss = 0.02745777
Iteration 18508, loss = 0.02745611
Iteration 18509, loss = 0.02745445
Iteration 18510, loss = 0.02745278
Iteration 18511, loss = 0.02745112
Iteration 18512, loss = 0.02744945
Iteration 18513, loss = 0.02744777
Iteration 18514, loss = 0.02744614
Iteration 18515, loss = 0.02744450
Iteration 18516, loss = 0.02744281
Iteration 18517, loss = 0.02744114
Iteration 18518, loss = 0.02743951
Iteration 18519, loss = 0.02743783
Iteration 18520, loss = 0.02743618
Iteration 18521, loss = 0.02743452
Iteration 18522, loss = 0.02743287
Iteration 18523, loss = 0.02743120
Iteration 18524, loss = 0.02742955
Iteration 18525, loss = 0.02742790
Iteration 18526, loss = 0.02742623
Iteration 18527, loss = 0.02742455
Iteration 18528, loss = 0.02742291
Iteration 18529, loss = 0.02742124
Iteration 18530, loss = 0.02741958
Iteration 18531, loss = 0.02741792
Iteration 18532, loss = 0.02741627
Iteration 18533, loss = 0.02741459
Iteration 18534, loss = 0.02741296
Iteration 18535, loss = 0.02741131
Iteration 18536, loss = 0.02740965
Iteration 18537, loss = 0.02740800
Iteration 18538, loss = 0.02740633
Iteration 18539, loss = 0.02740467
Iteration 18540, loss = 0.02740301
Iteration 18541, loss = 0.02740135
Iteration 18542, loss = 0.02739970
Iteration 18543, loss = 0.02739808
Iteration 18544, loss = 0.02739639
Iteration 18545, loss = 0.02739475
Iteration 18546, loss = 0.02739310
Iteration 18547, loss = 0.02739143
Iteration 18548, loss = 0.02738977
Iteration 18549, loss = 0.02738811
Iteration 18550, loss = 0.02738646
Iteration 18551, loss = 0.02738482
Iteration 18552, loss = 0.02738315
Iteration 18553, loss = 0.02738149
Iteration 18554, loss = 0.02737982
Iteration 18555, loss = 0.02737817
Iteration 18556, loss = 0.02737653
Iteration 18557, loss = 0.02737487
Iteration 18558, loss = 0.02737322
Iteration 18559, loss = 0.02737156
Iteration 18560, loss = 0.02736992
Iteration 18561, loss = 0.02736827
Iteration 18562, loss = 0.02736663
Iteration 18563, loss = 0.02736496
Iteration 18564, loss = 0.02736331
Iteration 18565, loss = 0.02736167
Iteration 18566, loss = 0.02736001
Iteration 18567, loss = 0.02735837
Iteration 18568, loss = 0.02735671
Iteration 18569, loss = 0.02735506
Iteration 18570, loss = 0.02735341
Iteration 18571, loss = 0.02735176
Iteration 18572, loss = 0.02735013
Iteration 18573, loss = 0.02734848
Iteration 18574, loss = 0.02734682
Iteration 18575, loss = 0.02734516
Iteration 18576, loss = 0.02734353
Iteration 18577, loss = 0.02734188
Iteration 18578, loss = 0.02734026
Iteration 18579, loss = 0.02733858
Iteration 18580, loss = 0.02733697
Iteration 18581, loss = 0.02733533
Iteration 18582, loss = 0.02733368
Iteration 18583, loss = 0.02733201
Iteration 18584, loss = 0.02733036
Iteration 18585, loss = 0.02732874
Iteration 18586, loss = 0.02732707
Iteration 18587, loss = 0.02732544
Iteration 18588, loss = 0.02732380
Iteration 18589, loss = 0.02732215
Iteration 18590, loss = 0.02732051
Iteration 18591, loss = 0.02731886
Iteration 18592, loss = 0.02731722
Iteration 18593, loss = 0.02731558
Iteration 18594, loss = 0.02731392
Iteration 18595, loss = 0.02731228
Iteration 18596, loss = 0.02731064
Iteration 18597, loss = 0.02730900
Iteration 18598, loss = 0.02730734
Iteration 18599, loss = 0.02730570
Iteration 18600, loss = 0.02730407
Iteration 18601, loss = 0.02730243
Iteration 18602, loss = 0.02730077
Iteration 18603, loss = 0.02729913
Iteration 18604, loss = 0.02729749
Iteration 18605, loss = 0.02729584
Iteration 18606, loss = 0.02729421
Iteration 18607, loss = 0.02729256
Iteration 18608, loss = 0.02729091
Iteration 18609, loss = 0.02728930
Iteration 18610, loss = 0.02728763
Iteration 18611, loss = 0.02728599
Iteration 18612, loss = 0.02728434
Iteration 18613, loss = 0.02728272
Iteration 18614, loss = 0.02728107
Iteration 18615, loss = 0.02727944
Iteration 18616, loss = 0.02727776
Iteration 18617, loss = 0.02727614
Iteration 18618, loss = 0.02727447
Iteration 18619, loss = 0.02727283
Iteration 18620, loss = 0.02727120
Iteration 18621, loss = 0.02726955
Iteration 18622, loss = 0.02726790
Iteration 18623, loss = 0.02726626
Iteration 18624, loss = 0.02726463
Iteration 18625, loss = 0.02726297
Iteration 18626, loss = 0.02726134
Iteration 18627, loss = 0.02725973
Iteration 18628, loss = 0.02725805
Iteration 18629, loss = 0.02725643
Iteration 18630, loss = 0.02725478
Iteration 18631, loss = 0.02725315
Iteration 18632, loss = 0.02725151
Iteration 18633, loss = 0.02724986
Iteration 18634, loss = 0.02724825
Iteration 18635, loss = 0.02724659
Iteration 18636, loss = 0.02724497
Iteration 18637, loss = 0.02724332
Iteration 18638, loss = 0.02724168
Iteration 18639, loss = 0.02724006
Iteration 18640, loss = 0.02723841
Iteration 18641, loss = 0.02723678
Iteration 18642, loss = 0.02723514
Iteration 18643, loss = 0.02723352
Iteration 18644, loss = 0.02723185
Iteration 18645, loss = 0.02723023
Iteration 18646, loss = 0.02722860
Iteration 18647, loss = 0.02722696
Iteration 18648, loss = 0.02722532
Iteration 18649, loss = 0.02722370
Iteration 18650, loss = 0.02722206
Iteration 18651, loss = 0.02722043
Iteration 18652, loss = 0.02721879
Iteration 18653, loss = 0.02721715
Iteration 18654, loss = 0.02721553
Iteration 18655, loss = 0.02721388
Iteration 18656, loss = 0.02721226
Iteration 18657, loss = 0.02721064
Iteration 18658, loss = 0.02720899
Iteration 18659, loss = 0.02720737
Iteration 18660, loss = 0.02720575
Iteration 18661, loss = 0.02720411
Iteration 18662, loss = 0.02720249
Iteration 18663, loss = 0.02720085
Iteration 18664, loss = 0.02719924
Iteration 18665, loss = 0.02719759
Iteration 18666, loss = 0.02719598
Iteration 18667, loss = 0.02719437
Iteration 18668, loss = 0.02719273
Iteration 18669, loss = 0.02719109
Iteration 18670, loss = 0.02718948
Iteration 18671, loss = 0.02718783
Iteration 18672, loss = 0.02718619
Iteration 18673, loss = 0.02718459
Iteration 18674, loss = 0.02718295
Iteration 18675, loss = 0.02718132
Iteration 18676, loss = 0.02717970
Iteration 18677, loss = 0.02717807
Iteration 18678, loss = 0.02717645
Iteration 18679, loss = 0.02717481
Iteration 18680, loss = 0.02717318
Iteration 18681, loss = 0.02717157
Iteration 18682, loss = 0.02716994
Iteration 18683, loss = 0.02716831
Iteration 18684, loss = 0.02716667
Iteration 18685, loss = 0.02716503
Iteration 18686, loss = 0.02716341
Iteration 18687, loss = 0.02716178
Iteration 18688, loss = 0.02716015
Iteration 18689, loss = 0.02715853
Iteration 18690, loss = 0.02715690
Iteration 18691, loss = 0.02715527
Iteration 18692, loss = 0.02715364
Iteration 18693, loss = 0.02715201
Iteration 18694, loss = 0.02715037
Iteration 18695, loss = 0.02714876
Iteration 18696, loss = 0.02714712
Iteration 18697, loss = 0.02714549
Iteration 18698, loss = 0.02714390
Iteration 18699, loss = 0.02714225
Iteration 18700, loss = 0.02714061
Iteration 18701, loss = 0.02713901
Iteration 18702, loss = 0.02713738
Iteration 18703, loss = 0.02713575
Iteration 18704, loss = 0.02713411
Iteration 18705, loss = 0.02713249
Iteration 18706, loss = 0.02713089
Iteration 18707, loss = 0.02712926
Iteration 18708, loss = 0.02712764
Iteration 18709, loss = 0.02712600
Iteration 18710, loss = 0.02712438
Iteration 18711, loss = 0.02712276
Iteration 18712, loss = 0.02712114
Iteration 18713, loss = 0.02711952
Iteration 18714, loss = 0.02711787
Iteration 18715, loss = 0.02711628
Iteration 18716, loss = 0.02711465
Iteration 18717, loss = 0.02711303
Iteration 18718, loss = 0.02711141
Iteration 18719, loss = 0.02710978
Iteration 18720, loss = 0.02710818
Iteration 18721, loss = 0.02710654
Iteration 18722, loss = 0.02710495
Iteration 18723, loss = 0.02710331
Iteration 18724, loss = 0.02710169
Iteration 18725, loss = 0.02710008
Iteration 18726, loss = 0.02709845
Iteration 18727, loss = 0.02709682
Iteration 18728, loss = 0.02709520
Iteration 18729, loss = 0.02709358
Iteration 18730, loss = 0.02709194
Iteration 18731, loss = 0.02709036
Iteration 18732, loss = 0.02708871
Iteration 18733, loss = 0.02708711
Iteration 18734, loss = 0.02708549
Iteration 18735, loss = 0.02708387
Iteration 18736, loss = 0.02708225
Iteration 18737, loss = 0.02708063
Iteration 18738, loss = 0.02707903
Iteration 18739, loss = 0.02707741
Iteration 18740, loss = 0.02707581
Iteration 18741, loss = 0.02707418
Iteration 18742, loss = 0.02707258
Iteration 18743, loss = 0.02707096
Iteration 18744, loss = 0.02706934
Iteration 18745, loss = 0.02706773
Iteration 18746, loss = 0.02706613
Iteration 18747, loss = 0.02706450
Iteration 18748, loss = 0.02706291
Iteration 18749, loss = 0.02706128
Iteration 18750, loss = 0.02705967
Iteration 18751, loss = 0.02705805
Iteration 18752, loss = 0.02705645
Iteration 18753, loss = 0.02705483
Iteration 18754, loss = 0.02705321
Iteration 18755, loss = 0.02705160
Iteration 18756, loss = 0.02704999
Iteration 18757, loss = 0.02704839
Iteration 18758, loss = 0.02704676
Iteration 18759, loss = 0.02704516
Iteration 18760, loss = 0.02704354
Iteration 18761, loss = 0.02704195
Iteration 18762, loss = 0.02704034
Iteration 18763, loss = 0.02703872
Iteration 18764, loss = 0.02703711
Iteration 18765, loss = 0.02703550
Iteration 18766, loss = 0.02703390
Iteration 18767, loss = 0.02703227
Iteration 18768, loss = 0.02703066
Iteration 18769, loss = 0.02702906
Iteration 18770, loss = 0.02702745
Iteration 18771, loss = 0.02702585
Iteration 18772, loss = 0.02702423
Iteration 18773, loss = 0.02702262
Iteration 18774, loss = 0.02702101
Iteration 18775, loss = 0.02701939
Iteration 18776, loss = 0.02701779
Iteration 18777, loss = 0.02701615
Iteration 18778, loss = 0.02701458
Iteration 18779, loss = 0.02701296
Iteration 18780, loss = 0.02701135
Iteration 18781, loss = 0.02700974
Iteration 18782, loss = 0.02700814
Iteration 18783, loss = 0.02700652
Iteration 18784, loss = 0.02700492
Iteration 18785, loss = 0.02700334
Iteration 18786, loss = 0.02700170
Iteration 18787, loss = 0.02700012
Iteration 18788, loss = 0.02699850
Iteration 18789, loss = 0.02699687
Iteration 18790, loss = 0.02699530
Iteration 18791, loss = 0.02699366
Iteration 18792, loss = 0.02699208
Iteration 18793, loss = 0.02699046
Iteration 18794, loss = 0.02698885
Iteration 18795, loss = 0.02698726
Iteration 18796, loss = 0.02698564
Iteration 18797, loss = 0.02698404
Iteration 18798, loss = 0.02698242
Iteration 18799, loss = 0.02698081
Iteration 18800, loss = 0.02697920
Iteration 18801, loss = 0.02697758
Iteration 18802, loss = 0.02697599
Iteration 18803, loss = 0.02697440
Iteration 18804, loss = 0.02697276
Iteration 18805, loss = 0.02697116
Iteration 18806, loss = 0.02696956
Iteration 18807, loss = 0.02696796
Iteration 18808, loss = 0.02696636
Iteration 18809, loss = 0.02696476
Iteration 18810, loss = 0.02696313
Iteration 18811, loss = 0.02696154
Iteration 18812, loss = 0.02695994
Iteration 18813, loss = 0.02695834
Iteration 18814, loss = 0.02695675
Iteration 18815, loss = 0.02695512
Iteration 18816, loss = 0.02695353
Iteration 18817, loss = 0.02695193
Iteration 18818, loss = 0.02695033
Iteration 18819, loss = 0.02694872
Iteration 18820, loss = 0.02694713
Iteration 18821, loss = 0.02694553
Iteration 18822, loss = 0.02694393
Iteration 18823, loss = 0.02694231
Iteration 18824, loss = 0.02694073
Iteration 18825, loss = 0.02693911
Iteration 18826, loss = 0.02693752
Iteration 18827, loss = 0.02693591
Iteration 18828, loss = 0.02693431
Iteration 18829, loss = 0.02693272
Iteration 18830, loss = 0.02693111
Iteration 18831, loss = 0.02692950
Iteration 18832, loss = 0.02692792
Iteration 18833, loss = 0.02692632
Iteration 18834, loss = 0.02692471
Iteration 18835, loss = 0.02692312
Iteration 18836, loss = 0.02692152
Iteration 18837, loss = 0.02691992
Iteration 18838, loss = 0.02691832
Iteration 18839, loss = 0.02691672
Iteration 18840, loss = 0.02691512
Iteration 18841, loss = 0.02691352
Iteration 18842, loss = 0.02691193
Iteration 18843, loss = 0.02691032
Iteration 18844, loss = 0.02690873
Iteration 18845, loss = 0.02690711
Iteration 18846, loss = 0.02690553
Iteration 18847, loss = 0.02690393
Iteration 18848, loss = 0.02690235
Iteration 18849, loss = 0.02690073
Iteration 18850, loss = 0.02689915
Iteration 18851, loss = 0.02689755
Iteration 18852, loss = 0.02689594
Iteration 18853, loss = 0.02689436
Iteration 18854, loss = 0.02689275
Iteration 18855, loss = 0.02689114
Iteration 18856, loss = 0.02688956
Iteration 18857, loss = 0.02688795
Iteration 18858, loss = 0.02688635
Iteration 18859, loss = 0.02688475
Iteration 18860, loss = 0.02688317
Iteration 18861, loss = 0.02688156
Iteration 18862, loss = 0.02687998
Iteration 18863, loss = 0.02687838
Iteration 18864, loss = 0.02687680
Iteration 18865, loss = 0.02687518
Iteration 18866, loss = 0.02687359
Iteration 18867, loss = 0.02687200
Iteration 18868, loss = 0.02687041
Iteration 18869, loss = 0.02686881
Iteration 18870, loss = 0.02686722
Iteration 18871, loss = 0.02686562
Iteration 18872, loss = 0.02686404
Iteration 18873, loss = 0.02686246
Iteration 18874, loss = 0.02686085
Iteration 18875, loss = 0.02685927
Iteration 18876, loss = 0.02685765
Iteration 18877, loss = 0.02685607
Iteration 18878, loss = 0.02685448
Iteration 18879, loss = 0.02685288
Iteration 18880, loss = 0.02685129
Iteration 18881, loss = 0.02684972
Iteration 18882, loss = 0.02684811
Iteration 18883, loss = 0.02684652
Iteration 18884, loss = 0.02684493
Iteration 18885, loss = 0.02684332
Iteration 18886, loss = 0.02684174
Iteration 18887, loss = 0.02684015
Iteration 18888, loss = 0.02683856
Iteration 18889, loss = 0.02683697
Iteration 18890, loss = 0.02683538
Iteration 18891, loss = 0.02683380
Iteration 18892, loss = 0.02683219
Iteration 18893, loss = 0.02683062
Iteration 18894, loss = 0.02682902
Iteration 18895, loss = 0.02682744
Iteration 18896, loss = 0.02682585
Iteration 18897, loss = 0.02682426
Iteration 18898, loss = 0.02682267
Iteration 18899, loss = 0.02682110
Iteration 18900, loss = 0.02681950
Iteration 18901, loss = 0.02681794
Iteration 18902, loss = 0.02681634
Iteration 18903, loss = 0.02681475
Iteration 18904, loss = 0.02681318
Iteration 18905, loss = 0.02681158
Iteration 18906, loss = 0.02681000
Iteration 18907, loss = 0.02680844
Iteration 18908, loss = 0.02680684
Iteration 18909, loss = 0.02680528
Iteration 18910, loss = 0.02680370
Iteration 18911, loss = 0.02680212
Iteration 18912, loss = 0.02680056
Iteration 18913, loss = 0.02679895
Iteration 18914, loss = 0.02679738
Iteration 18915, loss = 0.02679580
Iteration 18916, loss = 0.02679422
Iteration 18917, loss = 0.02679265
Iteration 18918, loss = 0.02679107
Iteration 18919, loss = 0.02678948
Iteration 18920, loss = 0.02678792
Iteration 18921, loss = 0.02678632
Iteration 18922, loss = 0.02678475
Iteration 18923, loss = 0.02678316
Iteration 18924, loss = 0.02678159
Iteration 18925, loss = 0.02678000
Iteration 18926, loss = 0.02677842
Iteration 18927, loss = 0.02677685
Iteration 18928, loss = 0.02677527
Iteration 18929, loss = 0.02677369
Iteration 18930, loss = 0.02677212
Iteration 18931, loss = 0.02677053
Iteration 18932, loss = 0.02676894
Iteration 18933, loss = 0.02676737
Iteration 18934, loss = 0.02676582
Iteration 18935, loss = 0.02676421
Iteration 18936, loss = 0.02676262
Iteration 18937, loss = 0.02676104
Iteration 18938, loss = 0.02675949
Iteration 18939, loss = 0.02675789
Iteration 18940, loss = 0.02675633
Iteration 18941, loss = 0.02675475
Iteration 18942, loss = 0.02675316
Iteration 18943, loss = 0.02675160
Iteration 18944, loss = 0.02675000
Iteration 18945, loss = 0.02674845
Iteration 18946, loss = 0.02674687
Iteration 18947, loss = 0.02674528
Iteration 18948, loss = 0.02674371
Iteration 18949, loss = 0.02674213
Iteration 18950, loss = 0.02674055
Iteration 18951, loss = 0.02673897
Iteration 18952, loss = 0.02673739
Iteration 18953, loss = 0.02673584
Iteration 18954, loss = 0.02673426
Iteration 18955, loss = 0.02673266
Iteration 18956, loss = 0.02673111
Iteration 18957, loss = 0.02672952
Iteration 18958, loss = 0.02672794
Iteration 18959, loss = 0.02672638
Iteration 18960, loss = 0.02672479
Iteration 18961, loss = 0.02672322
Iteration 18962, loss = 0.02672164
Iteration 18963, loss = 0.02672006
Iteration 18964, loss = 0.02671849
Iteration 18965, loss = 0.02671691
Iteration 18966, loss = 0.02671536
Iteration 18967, loss = 0.02671378
Iteration 18968, loss = 0.02671220
Iteration 18969, loss = 0.02671062
Iteration 18970, loss = 0.02670905
Iteration 18971, loss = 0.02670749
Iteration 18972, loss = 0.02670590
Iteration 18973, loss = 0.02670436
Iteration 18974, loss = 0.02670276
Iteration 18975, loss = 0.02670120
Iteration 18976, loss = 0.02669961
Iteration 18977, loss = 0.02669804
Iteration 18978, loss = 0.02669648
Iteration 18979, loss = 0.02669490
Iteration 18980, loss = 0.02669332
Iteration 18981, loss = 0.02669174
Iteration 18982, loss = 0.02669017
Iteration 18983, loss = 0.02668862
Iteration 18984, loss = 0.02668704
Iteration 18985, loss = 0.02668545
Iteration 18986, loss = 0.02668387
Iteration 18987, loss = 0.02668230
Iteration 18988, loss = 0.02668074
Iteration 18989, loss = 0.02667917
Iteration 18990, loss = 0.02667758
Iteration 18991, loss = 0.02667601
Iteration 18992, loss = 0.02667445
Iteration 18993, loss = 0.02667286
Iteration 18994, loss = 0.02667132
Iteration 18995, loss = 0.02666973
Iteration 18996, loss = 0.02666816
Iteration 18997, loss = 0.02666657
Iteration 18998, loss = 0.02666504
Iteration 18999, loss = 0.02666345
Iteration 19000, loss = 0.02666189
Iteration 19001, loss = 0.02666031
Iteration 19002, loss = 0.02665876
Iteration 19003, loss = 0.02665719
Iteration 19004, loss = 0.02665562
Iteration 19005, loss = 0.02665405
Iteration 19006, loss = 0.02665249
Iteration 19007, loss = 0.02665093
Iteration 19008, loss = 0.02664935
Iteration 19009, loss = 0.02664777
Iteration 19010, loss = 0.02664623
Iteration 19011, loss = 0.02664467
Iteration 19012, loss = 0.02664309
Iteration 19013, loss = 0.02664156
Iteration 19014, loss = 0.02664000
Iteration 19015, loss = 0.02663843
Iteration 19016, loss = 0.02663687
Iteration 19017, loss = 0.02663529
Iteration 19018, loss = 0.02663374
Iteration 19019, loss = 0.02663217
Iteration 19020, loss = 0.02663061
Iteration 19021, loss = 0.02662905
Iteration 19022, loss = 0.02662750
Iteration 19023, loss = 0.02662595
Iteration 19024, loss = 0.02662437
Iteration 19025, loss = 0.02662282
Iteration 19026, loss = 0.02662125
Iteration 19027, loss = 0.02661969
Iteration 19028, loss = 0.02661812
Iteration 19029, loss = 0.02661656
Iteration 19030, loss = 0.02661500
Iteration 19031, loss = 0.02661343
Iteration 19032, loss = 0.02661188
Iteration 19033, loss = 0.02661032
Iteration 19034, loss = 0.02660874
Iteration 19035, loss = 0.02660717
Iteration 19036, loss = 0.02660563
Iteration 19037, loss = 0.02660405
Iteration 19038, loss = 0.02660250
Iteration 19039, loss = 0.02660093
Iteration 19040, loss = 0.02659937
Iteration 19041, loss = 0.02659780
Iteration 19042, loss = 0.02659624
Iteration 19043, loss = 0.02659469
Iteration 19044, loss = 0.02659311
Iteration 19045, loss = 0.02659157
Iteration 19046, loss = 0.02659004
Iteration 19047, loss = 0.02658846
Iteration 19048, loss = 0.02658689
Iteration 19049, loss = 0.02658535
Iteration 19050, loss = 0.02658376
Iteration 19051, loss = 0.02658222
Iteration 19052, loss = 0.02658068
Iteration 19053, loss = 0.02657911
Iteration 19054, loss = 0.02657755
Iteration 19055, loss = 0.02657601
Iteration 19056, loss = 0.02657445
Iteration 19057, loss = 0.02657287
Iteration 19058, loss = 0.02657133
Iteration 19059, loss = 0.02656978
Iteration 19060, loss = 0.02656822
Iteration 19061, loss = 0.02656668
Iteration 19062, loss = 0.02656511
Iteration 19063, loss = 0.02656355
Iteration 19064, loss = 0.02656200
Iteration 19065, loss = 0.02656045
Iteration 19066, loss = 0.02655887
Iteration 19067, loss = 0.02655733
Iteration 19068, loss = 0.02655576
Iteration 19069, loss = 0.02655421
Iteration 19070, loss = 0.02655265
Iteration 19071, loss = 0.02655112
Iteration 19072, loss = 0.02654954
Iteration 19073, loss = 0.02654798
Iteration 19074, loss = 0.02654643
Iteration 19075, loss = 0.02654488
Iteration 19076, loss = 0.02654333
Iteration 19077, loss = 0.02654179
Iteration 19078, loss = 0.02654021
Iteration 19079, loss = 0.02653865
Iteration 19080, loss = 0.02653709
Iteration 19081, loss = 0.02653552
Iteration 19082, loss = 0.02653399
Iteration 19083, loss = 0.02653242
Iteration 19084, loss = 0.02653087
Iteration 19085, loss = 0.02652934
Iteration 19086, loss = 0.02652775
Iteration 19087, loss = 0.02652622
Iteration 19088, loss = 0.02652466
Iteration 19089, loss = 0.02652313
Iteration 19090, loss = 0.02652154
Iteration 19091, loss = 0.02652000
Iteration 19092, loss = 0.02651843
Iteration 19093, loss = 0.02651688
Iteration 19094, loss = 0.02651533
Iteration 19095, loss = 0.02651378
Iteration 19096, loss = 0.02651223
Iteration 19097, loss = 0.02651070
Iteration 19098, loss = 0.02650912
Iteration 19099, loss = 0.02650757
Iteration 19100, loss = 0.02650602
Iteration 19101, loss = 0.02650451
Iteration 19102, loss = 0.02650293
Iteration 19103, loss = 0.02650138
Iteration 19104, loss = 0.02649982
Iteration 19105, loss = 0.02649829
Iteration 19106, loss = 0.02649675
Iteration 19107, loss = 0.02649518
Iteration 19108, loss = 0.02649365
Iteration 19109, loss = 0.02649210
Iteration 19110, loss = 0.02649056
Iteration 19111, loss = 0.02648900
Iteration 19112, loss = 0.02648747
Iteration 19113, loss = 0.02648592
Iteration 19114, loss = 0.02648437
Iteration 19115, loss = 0.02648282
Iteration 19116, loss = 0.02648127
Iteration 19117, loss = 0.02647971
Iteration 19118, loss = 0.02647818
Iteration 19119, loss = 0.02647663
Iteration 19120, loss = 0.02647508
Iteration 19121, loss = 0.02647354
Iteration 19122, loss = 0.02647199
Iteration 19123, loss = 0.02647045
Iteration 19124, loss = 0.02646889
Iteration 19125, loss = 0.02646735
Iteration 19126, loss = 0.02646578
Iteration 19127, loss = 0.02646424
Iteration 19128, loss = 0.02646271
Iteration 19129, loss = 0.02646116
Iteration 19130, loss = 0.02645961
Iteration 19131, loss = 0.02645807
Iteration 19132, loss = 0.02645652
Iteration 19133, loss = 0.02645497
Iteration 19134, loss = 0.02645345
Iteration 19135, loss = 0.02645190
Iteration 19136, loss = 0.02645035
Iteration 19137, loss = 0.02644880
Iteration 19138, loss = 0.02644726
Iteration 19139, loss = 0.02644571
Iteration 19140, loss = 0.02644419
Iteration 19141, loss = 0.02644264
Iteration 19142, loss = 0.02644109
Iteration 19143, loss = 0.02643953
Iteration 19144, loss = 0.02643802
Iteration 19145, loss = 0.02643646
Iteration 19146, loss = 0.02643492
Iteration 19147, loss = 0.02643337
Iteration 19148, loss = 0.02643183
Iteration 19149, loss = 0.02643029
Iteration 19150, loss = 0.02642875
Iteration 19151, loss = 0.02642720
Iteration 19152, loss = 0.02642567
Iteration 19153, loss = 0.02642411
Iteration 19154, loss = 0.02642260
Iteration 19155, loss = 0.02642106
Iteration 19156, loss = 0.02641949
Iteration 19157, loss = 0.02641796
Iteration 19158, loss = 0.02641643
Iteration 19159, loss = 0.02641487
Iteration 19160, loss = 0.02641332
Iteration 19161, loss = 0.02641178
Iteration 19162, loss = 0.02641026
Iteration 19163, loss = 0.02640870
Iteration 19164, loss = 0.02640718
Iteration 19165, loss = 0.02640563
Iteration 19166, loss = 0.02640408
Iteration 19167, loss = 0.02640255
Iteration 19168, loss = 0.02640100
Iteration 19169, loss = 0.02639950
Iteration 19170, loss = 0.02639793
Iteration 19171, loss = 0.02639639
Iteration 19172, loss = 0.02639487
Iteration 19173, loss = 0.02639334
Iteration 19174, loss = 0.02639180
Iteration 19175, loss = 0.02639027
Iteration 19176, loss = 0.02638876
Iteration 19177, loss = 0.02638721
Iteration 19178, loss = 0.02638567
Iteration 19179, loss = 0.02638413
Iteration 19180, loss = 0.02638260
Iteration 19181, loss = 0.02638107
Iteration 19182, loss = 0.02637956
Iteration 19183, loss = 0.02637799
Iteration 19184, loss = 0.02637645
Iteration 19185, loss = 0.02637493
Iteration 19186, loss = 0.02637339
Iteration 19187, loss = 0.02637187
Iteration 19188, loss = 0.02637031
Iteration 19189, loss = 0.02636879
Iteration 19190, loss = 0.02636724
Iteration 19191, loss = 0.02636572
Iteration 19192, loss = 0.02636419
Iteration 19193, loss = 0.02636265
Iteration 19194, loss = 0.02636114
Iteration 19195, loss = 0.02635961
Iteration 19196, loss = 0.02635806
Iteration 19197, loss = 0.02635653
Iteration 19198, loss = 0.02635500
Iteration 19199, loss = 0.02635347
Iteration 19200, loss = 0.02635194
Iteration 19201, loss = 0.02635042
Iteration 19202, loss = 0.02634888
Iteration 19203, loss = 0.02634737
Iteration 19204, loss = 0.02634583
Iteration 19205, loss = 0.02634430
Iteration 19206, loss = 0.02634276
Iteration 19207, loss = 0.02634124
Iteration 19208, loss = 0.02633969
Iteration 19209, loss = 0.02633817
Iteration 19210, loss = 0.02633663
Iteration 19211, loss = 0.02633511
Iteration 19212, loss = 0.02633357
Iteration 19213, loss = 0.02633204
Iteration 19214, loss = 0.02633051
Iteration 19215, loss = 0.02632900
Iteration 19216, loss = 0.02632746
Iteration 19217, loss = 0.02632591
Iteration 19218, loss = 0.02632438
Iteration 19219, loss = 0.02632287
Iteration 19220, loss = 0.02632135
Iteration 19221, loss = 0.02631981
Iteration 19222, loss = 0.02631829
Iteration 19223, loss = 0.02631676
Iteration 19224, loss = 0.02631523
Iteration 19225, loss = 0.02631371
Iteration 19226, loss = 0.02631219
Iteration 19227, loss = 0.02631066
Iteration 19228, loss = 0.02630915
Iteration 19229, loss = 0.02630762
Iteration 19230, loss = 0.02630610
Iteration 19231, loss = 0.02630458
Iteration 19232, loss = 0.02630305
Iteration 19233, loss = 0.02630151
Iteration 19234, loss = 0.02630000
Iteration 19235, loss = 0.02629848
Iteration 19236, loss = 0.02629695
Iteration 19237, loss = 0.02629544
Iteration 19238, loss = 0.02629390
Iteration 19239, loss = 0.02629239
Iteration 19240, loss = 0.02629088
Iteration 19241, loss = 0.02628934
Iteration 19242, loss = 0.02628781
Iteration 19243, loss = 0.02628630
Iteration 19244, loss = 0.02628477
Iteration 19245, loss = 0.02628326
Iteration 19246, loss = 0.02628173
Iteration 19247, loss = 0.02628019
Iteration 19248, loss = 0.02627867
Iteration 19249, loss = 0.02627715
Iteration 19250, loss = 0.02627564
Iteration 19251, loss = 0.02627412
Iteration 19252, loss = 0.02627261
Iteration 19253, loss = 0.02627107
Iteration 19254, loss = 0.02626957
Iteration 19255, loss = 0.02626807
Iteration 19256, loss = 0.02626653
Iteration 19257, loss = 0.02626502
Iteration 19258, loss = 0.02626350
Iteration 19259, loss = 0.02626197
Iteration 19260, loss = 0.02626047
Iteration 19261, loss = 0.02625894
Iteration 19262, loss = 0.02625743
Iteration 19263, loss = 0.02625592
Iteration 19264, loss = 0.02625441
Iteration 19265, loss = 0.02625287
Iteration 19266, loss = 0.02625136
Iteration 19267, loss = 0.02624985
Iteration 19268, loss = 0.02624833
Iteration 19269, loss = 0.02624681
Iteration 19270, loss = 0.02624531
Iteration 19271, loss = 0.02624376
Iteration 19272, loss = 0.02624225
Iteration 19273, loss = 0.02624073
Iteration 19274, loss = 0.02623921
Iteration 19275, loss = 0.02623770
Iteration 19276, loss = 0.02623618
Iteration 19277, loss = 0.02623467
Iteration 19278, loss = 0.02623314
Iteration 19279, loss = 0.02623163
Iteration 19280, loss = 0.02623010
Iteration 19281, loss = 0.02622859
Iteration 19282, loss = 0.02622706
Iteration 19283, loss = 0.02622555
Iteration 19284, loss = 0.02622403
Iteration 19285, loss = 0.02622251
Iteration 19286, loss = 0.02622098
Iteration 19287, loss = 0.02621949
Iteration 19288, loss = 0.02621796
Iteration 19289, loss = 0.02621645
Iteration 19290, loss = 0.02621494
Iteration 19291, loss = 0.02621343
Iteration 19292, loss = 0.02621191
Iteration 19293, loss = 0.02621041
Iteration 19294, loss = 0.02620889
Iteration 19295, loss = 0.02620739
Iteration 19296, loss = 0.02620586
Iteration 19297, loss = 0.02620435
Iteration 19298, loss = 0.02620284
Iteration 19299, loss = 0.02620136
Iteration 19300, loss = 0.02619983
Iteration 19301, loss = 0.02619834
Iteration 19302, loss = 0.02619682
Iteration 19303, loss = 0.02619531
Iteration 19304, loss = 0.02619379
Iteration 19305, loss = 0.02619230
Iteration 19306, loss = 0.02619078
Iteration 19307, loss = 0.02618929
Iteration 19308, loss = 0.02618778
Iteration 19309, loss = 0.02618626
Iteration 19310, loss = 0.02618472
Iteration 19311, loss = 0.02618322
Iteration 19312, loss = 0.02618173
Iteration 19313, loss = 0.02618020
Iteration 19314, loss = 0.02617870
Iteration 19315, loss = 0.02617719
Iteration 19316, loss = 0.02617569
Iteration 19317, loss = 0.02617415
Iteration 19318, loss = 0.02617265
Iteration 19319, loss = 0.02617114
Iteration 19320, loss = 0.02616961
Iteration 19321, loss = 0.02616811
Iteration 19322, loss = 0.02616657
Iteration 19323, loss = 0.02616508
Iteration 19324, loss = 0.02616355
Iteration 19325, loss = 0.02616203
Iteration 19326, loss = 0.02616054
Iteration 19327, loss = 0.02615902
Iteration 19328, loss = 0.02615749
Iteration 19329, loss = 0.02615599
Iteration 19330, loss = 0.02615446
Iteration 19331, loss = 0.02615297
Iteration 19332, loss = 0.02615145
Iteration 19333, loss = 0.02614995
Iteration 19334, loss = 0.02614842
Iteration 19335, loss = 0.02614691
Iteration 19336, loss = 0.02614539
Iteration 19337, loss = 0.02614392
Iteration 19338, loss = 0.02614240
Iteration 19339, loss = 0.02614090
Iteration 19340, loss = 0.02613937
Iteration 19341, loss = 0.02613787
Iteration 19342, loss = 0.02613638
Iteration 19343, loss = 0.02613486
Iteration 19344, loss = 0.02613338
Iteration 19345, loss = 0.02613186
Iteration 19346, loss = 0.02613035
Iteration 19347, loss = 0.02612886
Iteration 19348, loss = 0.02612734
Iteration 19349, loss = 0.02612584
Iteration 19350, loss = 0.02612433
Iteration 19351, loss = 0.02612284
Iteration 19352, loss = 0.02612133
Iteration 19353, loss = 0.02611982
Iteration 19354, loss = 0.02611834
Iteration 19355, loss = 0.02611684
Iteration 19356, loss = 0.02611533
Iteration 19357, loss = 0.02611384
Iteration 19358, loss = 0.02611232
Iteration 19359, loss = 0.02611084
Iteration 19360, loss = 0.02610934
Iteration 19361, loss = 0.02610785
Iteration 19362, loss = 0.02610634
Iteration 19363, loss = 0.02610486
Iteration 19364, loss = 0.02610334
Iteration 19365, loss = 0.02610186
Iteration 19366, loss = 0.02610036
Iteration 19367, loss = 0.02609884
Iteration 19368, loss = 0.02609734
Iteration 19369, loss = 0.02609585
Iteration 19370, loss = 0.02609435
Iteration 19371, loss = 0.02609284
Iteration 19372, loss = 0.02609137
Iteration 19373, loss = 0.02608985
Iteration 19374, loss = 0.02608835
Iteration 19375, loss = 0.02608683
Iteration 19376, loss = 0.02608534
Iteration 19377, loss = 0.02608384
Iteration 19378, loss = 0.02608235
Iteration 19379, loss = 0.02608085
Iteration 19380, loss = 0.02607933
Iteration 19381, loss = 0.02607784
Iteration 19382, loss = 0.02607636
Iteration 19383, loss = 0.02607488
Iteration 19384, loss = 0.02607335
Iteration 19385, loss = 0.02607184
Iteration 19386, loss = 0.02607034
Iteration 19387, loss = 0.02606886
Iteration 19388, loss = 0.02606735
Iteration 19389, loss = 0.02606588
Iteration 19390, loss = 0.02606438
Iteration 19391, loss = 0.02606288
Iteration 19392, loss = 0.02606138
Iteration 19393, loss = 0.02605990
Iteration 19394, loss = 0.02605840
Iteration 19395, loss = 0.02605689
Iteration 19396, loss = 0.02605539
Iteration 19397, loss = 0.02605391
Iteration 19398, loss = 0.02605240
Iteration 19399, loss = 0.02605093
Iteration 19400, loss = 0.02604941
Iteration 19401, loss = 0.02604793
Iteration 19402, loss = 0.02604642
Iteration 19403, loss = 0.02604495
Iteration 19404, loss = 0.02604345
Iteration 19405, loss = 0.02604195
Iteration 19406, loss = 0.02604045
Iteration 19407, loss = 0.02603896
Iteration 19408, loss = 0.02603748
Iteration 19409, loss = 0.02603597
Iteration 19410, loss = 0.02603448
Iteration 19411, loss = 0.02603301
Iteration 19412, loss = 0.02603149
Iteration 19413, loss = 0.02603001
Iteration 19414, loss = 0.02602853
Iteration 19415, loss = 0.02602704
Iteration 19416, loss = 0.02602554
Iteration 19417, loss = 0.02602405
Iteration 19418, loss = 0.02602256
Iteration 19419, loss = 0.02602108
Iteration 19420, loss = 0.02601956
Iteration 19421, loss = 0.02601808
Iteration 19422, loss = 0.02601659
Iteration 19423, loss = 0.02601510
Iteration 19424, loss = 0.02601361
Iteration 19425, loss = 0.02601211
Iteration 19426, loss = 0.02601063
Iteration 19427, loss = 0.02600913
Iteration 19428, loss = 0.02600766
Iteration 19429, loss = 0.02600615
Iteration 19430, loss = 0.02600467
Iteration 19431, loss = 0.02600318
Iteration 19432, loss = 0.02600170
Iteration 19433, loss = 0.02600020
Iteration 19434, loss = 0.02599871
Iteration 19435, loss = 0.02599723
Iteration 19436, loss = 0.02599575
Iteration 19437, loss = 0.02599426
Iteration 19438, loss = 0.02599275
Iteration 19439, loss = 0.02599129
Iteration 19440, loss = 0.02598977
Iteration 19441, loss = 0.02598829
Iteration 19442, loss = 0.02598681
Iteration 19443, loss = 0.02598530
Iteration 19444, loss = 0.02598381
Iteration 19445, loss = 0.02598233
Iteration 19446, loss = 0.02598084
Iteration 19447, loss = 0.02597936
Iteration 19448, loss = 0.02597786
Iteration 19449, loss = 0.02597637
Iteration 19450, loss = 0.02597489
Iteration 19451, loss = 0.02597341
Iteration 19452, loss = 0.02597192
Iteration 19453, loss = 0.02597043
Iteration 19454, loss = 0.02596893
Iteration 19455, loss = 0.02596745
Iteration 19456, loss = 0.02596598
Iteration 19457, loss = 0.02596449
Iteration 19458, loss = 0.02596300
Iteration 19459, loss = 0.02596153
Iteration 19460, loss = 0.02596004
Iteration 19461, loss = 0.02595856
Iteration 19462, loss = 0.02595706
Iteration 19463, loss = 0.02595558
Iteration 19464, loss = 0.02595410
Iteration 19465, loss = 0.02595262
Iteration 19466, loss = 0.02595113
Iteration 19467, loss = 0.02594964
Iteration 19468, loss = 0.02594816
Iteration 19469, loss = 0.02594670
Iteration 19470, loss = 0.02594516
Iteration 19471, loss = 0.02594370
Iteration 19472, loss = 0.02594220
Iteration 19473, loss = 0.02594072
Iteration 19474, loss = 0.02593924
Iteration 19475, loss = 0.02593775
Iteration 19476, loss = 0.02593626
Iteration 19477, loss = 0.02593477
Iteration 19478, loss = 0.02593329
Iteration 19479, loss = 0.02593182
Iteration 19480, loss = 0.02593034
Iteration 19481, loss = 0.02592884
Iteration 19482, loss = 0.02592734
Iteration 19483, loss = 0.02592588
Iteration 19484, loss = 0.02592439
Iteration 19485, loss = 0.02592292
Iteration 19486, loss = 0.02592141
Iteration 19487, loss = 0.02591994
Iteration 19488, loss = 0.02591848
Iteration 19489, loss = 0.02591698
Iteration 19490, loss = 0.02591549
Iteration 19491, loss = 0.02591403
Iteration 19492, loss = 0.02591253
Iteration 19493, loss = 0.02591107
Iteration 19494, loss = 0.02590958
Iteration 19495, loss = 0.02590809
Iteration 19496, loss = 0.02590662
Iteration 19497, loss = 0.02590515
Iteration 19498, loss = 0.02590363
Iteration 19499, loss = 0.02590218
Iteration 19500, loss = 0.02590071
Iteration 19501, loss = 0.02589923
Iteration 19502, loss = 0.02589772
Iteration 19503, loss = 0.02589625
Iteration 19504, loss = 0.02589478
Iteration 19505, loss = 0.02589330
Iteration 19506, loss = 0.02589182
Iteration 19507, loss = 0.02589033
Iteration 19508, loss = 0.02588886
Iteration 19509, loss = 0.02588737
Iteration 19510, loss = 0.02588591
Iteration 19511, loss = 0.02588443
Iteration 19512, loss = 0.02588295
Iteration 19513, loss = 0.02588149
Iteration 19514, loss = 0.02587999
Iteration 19515, loss = 0.02587855
Iteration 19516, loss = 0.02587705
Iteration 19517, loss = 0.02587557
Iteration 19518, loss = 0.02587411
Iteration 19519, loss = 0.02587263
Iteration 19520, loss = 0.02587114
Iteration 19521, loss = 0.02586967
Iteration 19522, loss = 0.02586819
Iteration 19523, loss = 0.02586674
Iteration 19524, loss = 0.02586525
Iteration 19525, loss = 0.02586376
Iteration 19526, loss = 0.02586230
Iteration 19527, loss = 0.02586082
Iteration 19528, loss = 0.02585934
Iteration 19529, loss = 0.02585788
Iteration 19530, loss = 0.02585640
Iteration 19531, loss = 0.02585492
Iteration 19532, loss = 0.02585349
Iteration 19533, loss = 0.02585198
Iteration 19534, loss = 0.02585052
Iteration 19535, loss = 0.02584904
Iteration 19536, loss = 0.02584758
Iteration 19537, loss = 0.02584610
Iteration 19538, loss = 0.02584463
Iteration 19539, loss = 0.02584315
Iteration 19540, loss = 0.02584168
Iteration 19541, loss = 0.02584022
Iteration 19542, loss = 0.02583874
Iteration 19543, loss = 0.02583727
Iteration 19544, loss = 0.02583580
Iteration 19545, loss = 0.02583431
Iteration 19546, loss = 0.02583286
Iteration 19547, loss = 0.02583139
Iteration 19548, loss = 0.02582992
Iteration 19549, loss = 0.02582844
Iteration 19550, loss = 0.02582696
Iteration 19551, loss = 0.02582551
Iteration 19552, loss = 0.02582404
Iteration 19553, loss = 0.02582258
Iteration 19554, loss = 0.02582109
Iteration 19555, loss = 0.02581963
Iteration 19556, loss = 0.02581816
Iteration 19557, loss = 0.02581671
Iteration 19558, loss = 0.02581523
Iteration 19559, loss = 0.02581377
Iteration 19560, loss = 0.02581230
Iteration 19561, loss = 0.02581083
Iteration 19562, loss = 0.02580937
Iteration 19563, loss = 0.02580791
Iteration 19564, loss = 0.02580643
Iteration 19565, loss = 0.02580496
Iteration 19566, loss = 0.02580351
Iteration 19567, loss = 0.02580202
Iteration 19568, loss = 0.02580057
Iteration 19569, loss = 0.02579910
Iteration 19570, loss = 0.02579762
Iteration 19571, loss = 0.02579616
Iteration 19572, loss = 0.02579468
Iteration 19573, loss = 0.02579323
Iteration 19574, loss = 0.02579175
Iteration 19575, loss = 0.02579029
Iteration 19576, loss = 0.02578883
Iteration 19577, loss = 0.02578734
Iteration 19578, loss = 0.02578588
Iteration 19579, loss = 0.02578442
Iteration 19580, loss = 0.02578293
Iteration 19581, loss = 0.02578147
Iteration 19582, loss = 0.02578002
Iteration 19583, loss = 0.02577854
Iteration 19584, loss = 0.02577709
Iteration 19585, loss = 0.02577561
Iteration 19586, loss = 0.02577417
Iteration 19587, loss = 0.02577268
Iteration 19588, loss = 0.02577123
Iteration 19589, loss = 0.02576976
Iteration 19590, loss = 0.02576830
Iteration 19591, loss = 0.02576683
Iteration 19592, loss = 0.02576538
Iteration 19593, loss = 0.02576391
Iteration 19594, loss = 0.02576244
Iteration 19595, loss = 0.02576098
Iteration 19596, loss = 0.02575953
Iteration 19597, loss = 0.02575808
Iteration 19598, loss = 0.02575660
Iteration 19599, loss = 0.02575515
Iteration 19600, loss = 0.02575370
Iteration 19601, loss = 0.02575223
Iteration 19602, loss = 0.02575077
Iteration 19603, loss = 0.02574930
Iteration 19604, loss = 0.02574785
Iteration 19605, loss = 0.02574639
Iteration 19606, loss = 0.02574494
Iteration 19607, loss = 0.02574347
Iteration 19608, loss = 0.02574202
Iteration 19609, loss = 0.02574054
Iteration 19610, loss = 0.02573911
Iteration 19611, loss = 0.02573765
Iteration 19612, loss = 0.02573618
Iteration 19613, loss = 0.02573472
Iteration 19614, loss = 0.02573325
Iteration 19615, loss = 0.02573181
Iteration 19616, loss = 0.02573034
Iteration 19617, loss = 0.02572889
Iteration 19618, loss = 0.02572740
Iteration 19619, loss = 0.02572596
Iteration 19620, loss = 0.02572450
Iteration 19621, loss = 0.02572303
Iteration 19622, loss = 0.02572155
Iteration 19623, loss = 0.02572009
Iteration 19624, loss = 0.02571863
Iteration 19625, loss = 0.02571717
Iteration 19626, loss = 0.02571571
Iteration 19627, loss = 0.02571427
Iteration 19628, loss = 0.02571279
Iteration 19629, loss = 0.02571136
Iteration 19630, loss = 0.02570988
Iteration 19631, loss = 0.02570843
Iteration 19632, loss = 0.02570695
Iteration 19633, loss = 0.02570550
Iteration 19634, loss = 0.02570404
Iteration 19635, loss = 0.02570258
Iteration 19636, loss = 0.02570112
Iteration 19637, loss = 0.02569967
Iteration 19638, loss = 0.02569820
Iteration 19639, loss = 0.02569676
Iteration 19640, loss = 0.02569529
Iteration 19641, loss = 0.02569383
Iteration 19642, loss = 0.02569239
Iteration 19643, loss = 0.02569095
Iteration 19644, loss = 0.02568946
Iteration 19645, loss = 0.02568803
Iteration 19646, loss = 0.02568656
Iteration 19647, loss = 0.02568510
Iteration 19648, loss = 0.02568366
Iteration 19649, loss = 0.02568220
Iteration 19650, loss = 0.02568075
Iteration 19651, loss = 0.02567929
Iteration 19652, loss = 0.02567782
Iteration 19653, loss = 0.02567637
Iteration 19654, loss = 0.02567494
Iteration 19655, loss = 0.02567346
Iteration 19656, loss = 0.02567203
Iteration 19657, loss = 0.02567056
Iteration 19658, loss = 0.02566911
Iteration 19659, loss = 0.02566764
Iteration 19660, loss = 0.02566620
Iteration 19661, loss = 0.02566475
Iteration 19662, loss = 0.02566328
Iteration 19663, loss = 0.02566185
Iteration 19664, loss = 0.02566040
Iteration 19665, loss = 0.02565892
Iteration 19666, loss = 0.02565748
Iteration 19667, loss = 0.02565603
Iteration 19668, loss = 0.02565459
Iteration 19669, loss = 0.02565311
Iteration 19670, loss = 0.02565166
Iteration 19671, loss = 0.02565022
Iteration 19672, loss = 0.02564877
Iteration 19673, loss = 0.02564731
Iteration 19674, loss = 0.02564586
Iteration 19675, loss = 0.02564440
Iteration 19676, loss = 0.02564294
Iteration 19677, loss = 0.02564150
Iteration 19678, loss = 0.02564005
Iteration 19679, loss = 0.02563861
Iteration 19680, loss = 0.02563715
Iteration 19681, loss = 0.02563571
Iteration 19682, loss = 0.02563425
Iteration 19683, loss = 0.02563280
Iteration 19684, loss = 0.02563134
Iteration 19685, loss = 0.02562990
Iteration 19686, loss = 0.02562846
Iteration 19687, loss = 0.02562699
Iteration 19688, loss = 0.02562556
Iteration 19689, loss = 0.02562409
Iteration 19690, loss = 0.02562265
Iteration 19691, loss = 0.02562121
Iteration 19692, loss = 0.02561974
Iteration 19693, loss = 0.02561829
Iteration 19694, loss = 0.02561686
Iteration 19695, loss = 0.02561542
Iteration 19696, loss = 0.02561396
Iteration 19697, loss = 0.02561250
Iteration 19698, loss = 0.02561107
Iteration 19699, loss = 0.02560961
Iteration 19700, loss = 0.02560815
Iteration 19701, loss = 0.02560674
Iteration 19702, loss = 0.02560527
Iteration 19703, loss = 0.02560383
Iteration 19704, loss = 0.02560237
Iteration 19705, loss = 0.02560093
Iteration 19706, loss = 0.02559949
Iteration 19707, loss = 0.02559806
Iteration 19708, loss = 0.02559658
Iteration 19709, loss = 0.02559516
Iteration 19710, loss = 0.02559371
Iteration 19711, loss = 0.02559229
Iteration 19712, loss = 0.02559082
Iteration 19713, loss = 0.02558938
Iteration 19714, loss = 0.02558794
Iteration 19715, loss = 0.02558650
Iteration 19716, loss = 0.02558505
Iteration 19717, loss = 0.02558362
Iteration 19718, loss = 0.02558216
Iteration 19719, loss = 0.02558071
Iteration 19720, loss = 0.02557927
Iteration 19721, loss = 0.02557781
Iteration 19722, loss = 0.02557640
Iteration 19723, loss = 0.02557493
Iteration 19724, loss = 0.02557349
Iteration 19725, loss = 0.02557205
Iteration 19726, loss = 0.02557062
Iteration 19727, loss = 0.02556919
Iteration 19728, loss = 0.02556774
Iteration 19729, loss = 0.02556628
Iteration 19730, loss = 0.02556485
Iteration 19731, loss = 0.02556342
Iteration 19732, loss = 0.02556197
Iteration 19733, loss = 0.02556054
Iteration 19734, loss = 0.02555913
Iteration 19735, loss = 0.02555768
Iteration 19736, loss = 0.02555623
Iteration 19737, loss = 0.02555481
Iteration 19738, loss = 0.02555336
Iteration 19739, loss = 0.02555193
Iteration 19740, loss = 0.02555050
Iteration 19741, loss = 0.02554905
Iteration 19742, loss = 0.02554761
Iteration 19743, loss = 0.02554618
Iteration 19744, loss = 0.02554474
Iteration 19745, loss = 0.02554331
Iteration 19746, loss = 0.02554187
Iteration 19747, loss = 0.02554043
Iteration 19748, loss = 0.02553900
Iteration 19749, loss = 0.02553754
Iteration 19750, loss = 0.02553613
Iteration 19751, loss = 0.02553469
Iteration 19752, loss = 0.02553326
Iteration 19753, loss = 0.02553183
Iteration 19754, loss = 0.02553038
Iteration 19755, loss = 0.02552895
Iteration 19756, loss = 0.02552753
Iteration 19757, loss = 0.02552609
Iteration 19758, loss = 0.02552464
Iteration 19759, loss = 0.02552322
Iteration 19760, loss = 0.02552177
Iteration 19761, loss = 0.02552033
Iteration 19762, loss = 0.02551891
Iteration 19763, loss = 0.02551746
Iteration 19764, loss = 0.02551604
Iteration 19765, loss = 0.02551459
Iteration 19766, loss = 0.02551316
Iteration 19767, loss = 0.02551173
Iteration 19768, loss = 0.02551029
Iteration 19769, loss = 0.02550887
Iteration 19770, loss = 0.02550743
Iteration 19771, loss = 0.02550600
Iteration 19772, loss = 0.02550457
Iteration 19773, loss = 0.02550312
Iteration 19774, loss = 0.02550169
Iteration 19775, loss = 0.02550026
Iteration 19776, loss = 0.02549882
Iteration 19777, loss = 0.02549738
Iteration 19778, loss = 0.02549596
Iteration 19779, loss = 0.02549452
Iteration 19780, loss = 0.02549310
Iteration 19781, loss = 0.02549166
Iteration 19782, loss = 0.02549022
Iteration 19783, loss = 0.02548878
Iteration 19784, loss = 0.02548736
Iteration 19785, loss = 0.02548592
Iteration 19786, loss = 0.02548449
Iteration 19787, loss = 0.02548308
Iteration 19788, loss = 0.02548162
Iteration 19789, loss = 0.02548019
Iteration 19790, loss = 0.02547877
Iteration 19791, loss = 0.02547732
Iteration 19792, loss = 0.02547589
Iteration 19793, loss = 0.02547445
Iteration 19794, loss = 0.02547302
Iteration 19795, loss = 0.02547160
Iteration 19796, loss = 0.02547017
Iteration 19797, loss = 0.02546872
Iteration 19798, loss = 0.02546730
Iteration 19799, loss = 0.02546587
Iteration 19800, loss = 0.02546442
Iteration 19801, loss = 0.02546300
Iteration 19802, loss = 0.02546156
Iteration 19803, loss = 0.02546016
Iteration 19804, loss = 0.02545871
Iteration 19805, loss = 0.02545728
Iteration 19806, loss = 0.02545585
Iteration 19807, loss = 0.02545442
Iteration 19808, loss = 0.02545298
Iteration 19809, loss = 0.02545158
Iteration 19810, loss = 0.02545014
Iteration 19811, loss = 0.02544873
Iteration 19812, loss = 0.02544728
Iteration 19813, loss = 0.02544587
Iteration 19814, loss = 0.02544443
Iteration 19815, loss = 0.02544300
Iteration 19816, loss = 0.02544161
Iteration 19817, loss = 0.02544016
Iteration 19818, loss = 0.02543876
Iteration 19819, loss = 0.02543730
Iteration 19820, loss = 0.02543588
Iteration 19821, loss = 0.02543445
Iteration 19822, loss = 0.02543304
Iteration 19823, loss = 0.02543159
Iteration 19824, loss = 0.02543016
Iteration 19825, loss = 0.02542874
Iteration 19826, loss = 0.02542730
Iteration 19827, loss = 0.02542590
Iteration 19828, loss = 0.02542446
Iteration 19829, loss = 0.02542305
Iteration 19830, loss = 0.02542162
Iteration 19831, loss = 0.02542021
Iteration 19832, loss = 0.02541877
Iteration 19833, loss = 0.02541734
Iteration 19834, loss = 0.02541596
Iteration 19835, loss = 0.02541452
Iteration 19836, loss = 0.02541309
Iteration 19837, loss = 0.02541165
Iteration 19838, loss = 0.02541026
Iteration 19839, loss = 0.02540883
Iteration 19840, loss = 0.02540741
Iteration 19841, loss = 0.02540600
Iteration 19842, loss = 0.02540455
Iteration 19843, loss = 0.02540313
Iteration 19844, loss = 0.02540172
Iteration 19845, loss = 0.02540030
Iteration 19846, loss = 0.02539887
Iteration 19847, loss = 0.02539744
Iteration 19848, loss = 0.02539603
Iteration 19849, loss = 0.02539462
Iteration 19850, loss = 0.02539318
Iteration 19851, loss = 0.02539177
Iteration 19852, loss = 0.02539035
Iteration 19853, loss = 0.02538892
Iteration 19854, loss = 0.02538751
Iteration 19855, loss = 0.02538608
Iteration 19856, loss = 0.02538466
Iteration 19857, loss = 0.02538322
Iteration 19858, loss = 0.02538183
Iteration 19859, loss = 0.02538041
Iteration 19860, loss = 0.02537898
Iteration 19861, loss = 0.02537757
Iteration 19862, loss = 0.02537614
Iteration 19863, loss = 0.02537473
Iteration 19864, loss = 0.02537331
Iteration 19865, loss = 0.02537189
Iteration 19866, loss = 0.02537048
Iteration 19867, loss = 0.02536905
Iteration 19868, loss = 0.02536763
Iteration 19869, loss = 0.02536621
Iteration 19870, loss = 0.02536478
Iteration 19871, loss = 0.02536338
Iteration 19872, loss = 0.02536197
Iteration 19873, loss = 0.02536058
Iteration 19874, loss = 0.02535913
Iteration 19875, loss = 0.02535770
Iteration 19876, loss = 0.02535628
Iteration 19877, loss = 0.02535489
Iteration 19878, loss = 0.02535345
Iteration 19879, loss = 0.02535203
Iteration 19880, loss = 0.02535061
Iteration 19881, loss = 0.02534919
Iteration 19882, loss = 0.02534779
Iteration 19883, loss = 0.02534636
Iteration 19884, loss = 0.02534495
Iteration 19885, loss = 0.02534355
Iteration 19886, loss = 0.02534211
Iteration 19887, loss = 0.02534071
Iteration 19888, loss = 0.02533928
Iteration 19889, loss = 0.02533787
Iteration 19890, loss = 0.02533645
Iteration 19891, loss = 0.02533503
Iteration 19892, loss = 0.02533362
Iteration 19893, loss = 0.02533222
Iteration 19894, loss = 0.02533079
Iteration 19895, loss = 0.02532939
Iteration 19896, loss = 0.02532795
Iteration 19897, loss = 0.02532655
Iteration 19898, loss = 0.02532514
Iteration 19899, loss = 0.02532372
Iteration 19900, loss = 0.02532229
Iteration 19901, loss = 0.02532089
Iteration 19902, loss = 0.02531948
Iteration 19903, loss = 0.02531807
Iteration 19904, loss = 0.02531666
Iteration 19905, loss = 0.02531526
Iteration 19906, loss = 0.02531384
Iteration 19907, loss = 0.02531242
Iteration 19908, loss = 0.02531101
Iteration 19909, loss = 0.02530957
Iteration 19910, loss = 0.02530818
Iteration 19911, loss = 0.02530677
Iteration 19912, loss = 0.02530534
Iteration 19913, loss = 0.02530393
Iteration 19914, loss = 0.02530251
Iteration 19915, loss = 0.02530110
Iteration 19916, loss = 0.02529969
Iteration 19917, loss = 0.02529830
Iteration 19918, loss = 0.02529687
Iteration 19919, loss = 0.02529547
Iteration 19920, loss = 0.02529406
Iteration 19921, loss = 0.02529263
Iteration 19922, loss = 0.02529122
Iteration 19923, loss = 0.02528983
Iteration 19924, loss = 0.02528844
Iteration 19925, loss = 0.02528700
Iteration 19926, loss = 0.02528558
Iteration 19927, loss = 0.02528421
Iteration 19928, loss = 0.02528278
Iteration 19929, loss = 0.02528136
Iteration 19930, loss = 0.02527997
Iteration 19931, loss = 0.02527856
Iteration 19932, loss = 0.02527713
Iteration 19933, loss = 0.02527574
Iteration 19934, loss = 0.02527432
Iteration 19935, loss = 0.02527291
Iteration 19936, loss = 0.02527150
Iteration 19937, loss = 0.02527010
Iteration 19938, loss = 0.02526868
Iteration 19939, loss = 0.02526727
Iteration 19940, loss = 0.02526585
Iteration 19941, loss = 0.02526446
Iteration 19942, loss = 0.02526304
Iteration 19943, loss = 0.02526165
Iteration 19944, loss = 0.02526024
Iteration 19945, loss = 0.02525882
Iteration 19946, loss = 0.02525741
Iteration 19947, loss = 0.02525600
Iteration 19948, loss = 0.02525460
Iteration 19949, loss = 0.02525318
Iteration 19950, loss = 0.02525179
Iteration 19951, loss = 0.02525038
Iteration 19952, loss = 0.02524897
Iteration 19953, loss = 0.02524758
Iteration 19954, loss = 0.02524616
Iteration 19955, loss = 0.02524474
Iteration 19956, loss = 0.02524335
Iteration 19957, loss = 0.02524194
Iteration 19958, loss = 0.02524055
Iteration 19959, loss = 0.02523914
Iteration 19960, loss = 0.02523773
Iteration 19961, loss = 0.02523633
Iteration 19962, loss = 0.02523492
Iteration 19963, loss = 0.02523352
Iteration 19964, loss = 0.02523211
Iteration 19965, loss = 0.02523072
Iteration 19966, loss = 0.02522930
Iteration 19967, loss = 0.02522790
Iteration 19968, loss = 0.02522650
Iteration 19969, loss = 0.02522511
Iteration 19970, loss = 0.02522371
Iteration 19971, loss = 0.02522229
Iteration 19972, loss = 0.02522089
Iteration 19973, loss = 0.02521950
Iteration 19974, loss = 0.02521810
Iteration 19975, loss = 0.02521668
Iteration 19976, loss = 0.02521527
Iteration 19977, loss = 0.02521388
Iteration 19978, loss = 0.02521247
Iteration 19979, loss = 0.02521108
Iteration 19980, loss = 0.02520966
Iteration 19981, loss = 0.02520827
Iteration 19982, loss = 0.02520685
Iteration 19983, loss = 0.02520547
Iteration 19984, loss = 0.02520405
Iteration 19985, loss = 0.02520265
Iteration 19986, loss = 0.02520125
Iteration 19987, loss = 0.02519985
Iteration 19988, loss = 0.02519844
Iteration 19989, loss = 0.02519704
Iteration 19990, loss = 0.02519564
Iteration 19991, loss = 0.02519423
Iteration 19992, loss = 0.02519284
Iteration 19993, loss = 0.02519144
Iteration 19994, loss = 0.02519003
Iteration 19995, loss = 0.02518863
Iteration 19996, loss = 0.02518723
Iteration 19997, loss = 0.02518583
Iteration 19998, loss = 0.02518445
Iteration 19999, loss = 0.02518305
Iteration 20000, loss = 0.02518165
Iteration 20001, loss = 0.02518023
Iteration 20002, loss = 0.02517887
Iteration 20003, loss = 0.02517745
Iteration 20004, loss = 0.02517605
Iteration 20005, loss = 0.02517464
Iteration 20006, loss = 0.02517328
Iteration 20007, loss = 0.02517186
Iteration 20008, loss = 0.02517046
Iteration 20009, loss = 0.02516906
Iteration 20010, loss = 0.02516767
Iteration 20011, loss = 0.02516628
Iteration 20012, loss = 0.02516489
Iteration 20013, loss = 0.02516348
Iteration 20014, loss = 0.02516209
Iteration 20015, loss = 0.02516069
Iteration 20016, loss = 0.02515932
Iteration 20017, loss = 0.02515791
Iteration 20018, loss = 0.02515652
Iteration 20019, loss = 0.02515513
Iteration 20020, loss = 0.02515374
Iteration 20021, loss = 0.02515234
Iteration 20022, loss = 0.02515097
Iteration 20023, loss = 0.02514954
Iteration 20024, loss = 0.02514817
Iteration 20025, loss = 0.02514677
Iteration 20026, loss = 0.02514538
Iteration 20027, loss = 0.02514398
Iteration 20028, loss = 0.02514260
Iteration 20029, loss = 0.02514120
Iteration 20030, loss = 0.02513980
Iteration 20031, loss = 0.02513843
Iteration 20032, loss = 0.02513704
Iteration 20033, loss = 0.02513561
Iteration 20034, loss = 0.02513423
Iteration 20035, loss = 0.02513283
Iteration 20036, loss = 0.02513145
Iteration 20037, loss = 0.02513004
Iteration 20038, loss = 0.02512865
Iteration 20039, loss = 0.02512727
Iteration 20040, loss = 0.02512587
Iteration 20041, loss = 0.02512447
Iteration 20042, loss = 0.02512309
Iteration 20043, loss = 0.02512169
Iteration 20044, loss = 0.02512029
Iteration 20045, loss = 0.02511889
Iteration 20046, loss = 0.02511752
Iteration 20047, loss = 0.02511612
Iteration 20048, loss = 0.02511473
Iteration 20049, loss = 0.02511334
Iteration 20050, loss = 0.02511192
Iteration 20051, loss = 0.02511056
Iteration 20052, loss = 0.02510915
Iteration 20053, loss = 0.02510776
Iteration 20054, loss = 0.02510637
Iteration 20055, loss = 0.02510497
Iteration 20056, loss = 0.02510359
Iteration 20057, loss = 0.02510220
Iteration 20058, loss = 0.02510081
Iteration 20059, loss = 0.02509941
Iteration 20060, loss = 0.02509803
Iteration 20061, loss = 0.02509664
Iteration 20062, loss = 0.02509526
Iteration 20063, loss = 0.02509387
Iteration 20064, loss = 0.02509248
Iteration 20065, loss = 0.02509109
Iteration 20066, loss = 0.02508971
Iteration 20067, loss = 0.02508834
Iteration 20068, loss = 0.02508694
Iteration 20069, loss = 0.02508554
Iteration 20070, loss = 0.02508418
Iteration 20071, loss = 0.02508281
Iteration 20072, loss = 0.02508141
Iteration 20073, loss = 0.02508002
Iteration 20074, loss = 0.02507864
Iteration 20075, loss = 0.02507726
Iteration 20076, loss = 0.02507589
Iteration 20077, loss = 0.02507451
Iteration 20078, loss = 0.02507311
Iteration 20079, loss = 0.02507173
Iteration 20080, loss = 0.02507034
Iteration 20081, loss = 0.02506898
Iteration 20082, loss = 0.02506757
Iteration 20083, loss = 0.02506619
Iteration 20084, loss = 0.02506481
Iteration 20085, loss = 0.02506342
Iteration 20086, loss = 0.02506205
Iteration 20087, loss = 0.02506064
Iteration 20088, loss = 0.02505927
Iteration 20089, loss = 0.02505788
Iteration 20090, loss = 0.02505650
Iteration 20091, loss = 0.02505511
Iteration 20092, loss = 0.02505372
Iteration 20093, loss = 0.02505234
Iteration 20094, loss = 0.02505097
Iteration 20095, loss = 0.02504957
Iteration 20096, loss = 0.02504819
Iteration 20097, loss = 0.02504682
Iteration 20098, loss = 0.02504543
Iteration 20099, loss = 0.02504405
Iteration 20100, loss = 0.02504266
Iteration 20101, loss = 0.02504126
Iteration 20102, loss = 0.02503992
Iteration 20103, loss = 0.02503851
Iteration 20104, loss = 0.02503713
Iteration 20105, loss = 0.02503572
Iteration 20106, loss = 0.02503438
Iteration 20107, loss = 0.02503298
Iteration 20108, loss = 0.02503159
Iteration 20109, loss = 0.02503022
Iteration 20110, loss = 0.02502884
Iteration 20111, loss = 0.02502744
Iteration 20112, loss = 0.02502606
Iteration 20113, loss = 0.02502468
Iteration 20114, loss = 0.02502332
Iteration 20115, loss = 0.02502192
Iteration 20116, loss = 0.02502054
Iteration 20117, loss = 0.02501916
Iteration 20118, loss = 0.02501777
Iteration 20119, loss = 0.02501640
Iteration 20120, loss = 0.02501502
Iteration 20121, loss = 0.02501363
Iteration 20122, loss = 0.02501225
Iteration 20123, loss = 0.02501088
Iteration 20124, loss = 0.02500949
Iteration 20125, loss = 0.02500811
Iteration 20126, loss = 0.02500672
Iteration 20127, loss = 0.02500535
Iteration 20128, loss = 0.02500397
Iteration 20129, loss = 0.02500259
Iteration 20130, loss = 0.02500122
Iteration 20131, loss = 0.02499984
Iteration 20132, loss = 0.02499847
Iteration 20133, loss = 0.02499709
Iteration 20134, loss = 0.02499572
Iteration 20135, loss = 0.02499433
Iteration 20136, loss = 0.02499297
Iteration 20137, loss = 0.02499159
Iteration 20138, loss = 0.02499023
Iteration 20139, loss = 0.02498884
Iteration 20140, loss = 0.02498746
Iteration 20141, loss = 0.02498610
Iteration 20142, loss = 0.02498472
Iteration 20143, loss = 0.02498333
Iteration 20144, loss = 0.02498195
Iteration 20145, loss = 0.02498059
Iteration 20146, loss = 0.02497921
Iteration 20147, loss = 0.02497784
Iteration 20148, loss = 0.02497646
Iteration 20149, loss = 0.02497510
Iteration 20150, loss = 0.02497372
Iteration 20151, loss = 0.02497232
Iteration 20152, loss = 0.02497096
Iteration 20153, loss = 0.02496958
Iteration 20154, loss = 0.02496822
Iteration 20155, loss = 0.02496684
Iteration 20156, loss = 0.02496547
Iteration 20157, loss = 0.02496410
Iteration 20158, loss = 0.02496272
Iteration 20159, loss = 0.02496135
Iteration 20160, loss = 0.02495998
Iteration 20161, loss = 0.02495860
Iteration 20162, loss = 0.02495721
Iteration 20163, loss = 0.02495585
Iteration 20164, loss = 0.02495449
Iteration 20165, loss = 0.02495311
Iteration 20166, loss = 0.02495172
Iteration 20167, loss = 0.02495034
Iteration 20168, loss = 0.02494898
Iteration 20169, loss = 0.02494761
Iteration 20170, loss = 0.02494625
Iteration 20171, loss = 0.02494486
Iteration 20172, loss = 0.02494350
Iteration 20173, loss = 0.02494212
Iteration 20174, loss = 0.02494075
Iteration 20175, loss = 0.02493939
Iteration 20176, loss = 0.02493801
Iteration 20177, loss = 0.02493663
Iteration 20178, loss = 0.02493527
Iteration 20179, loss = 0.02493391
Iteration 20180, loss = 0.02493252
Iteration 20181, loss = 0.02493114
Iteration 20182, loss = 0.02492977
Iteration 20183, loss = 0.02492842
Iteration 20184, loss = 0.02492703
Iteration 20185, loss = 0.02492566
Iteration 20186, loss = 0.02492430
Iteration 20187, loss = 0.02492293
Iteration 20188, loss = 0.02492156
Iteration 20189, loss = 0.02492019
Iteration 20190, loss = 0.02491881
Iteration 20191, loss = 0.02491745
Iteration 20192, loss = 0.02491611
Iteration 20193, loss = 0.02491471
Iteration 20194, loss = 0.02491337
Iteration 20195, loss = 0.02491199
Iteration 20196, loss = 0.02491062
Iteration 20197, loss = 0.02490926
Iteration 20198, loss = 0.02490787
Iteration 20199, loss = 0.02490652
Iteration 20200, loss = 0.02490516
Iteration 20201, loss = 0.02490380
Iteration 20202, loss = 0.02490243
Iteration 20203, loss = 0.02490105
Iteration 20204, loss = 0.02489968
Iteration 20205, loss = 0.02489831
Iteration 20206, loss = 0.02489694
Iteration 20207, loss = 0.02489559
Iteration 20208, loss = 0.02489422
Iteration 20209, loss = 0.02489285
Iteration 20210, loss = 0.02489148
Iteration 20211, loss = 0.02489012
Iteration 20212, loss = 0.02488875
Iteration 20213, loss = 0.02488740
Iteration 20214, loss = 0.02488603
Iteration 20215, loss = 0.02488466
Iteration 20216, loss = 0.02488329
Iteration 20217, loss = 0.02488192
Iteration 20218, loss = 0.02488058
Iteration 20219, loss = 0.02487921
Iteration 20220, loss = 0.02487784
Iteration 20221, loss = 0.02487647
Iteration 20222, loss = 0.02487511
Iteration 20223, loss = 0.02487375
Iteration 20224, loss = 0.02487238
Iteration 20225, loss = 0.02487102
Iteration 20226, loss = 0.02486964
Iteration 20227, loss = 0.02486830
Iteration 20228, loss = 0.02486693
Iteration 20229, loss = 0.02486558
Iteration 20230, loss = 0.02486420
Iteration 20231, loss = 0.02486285
Iteration 20232, loss = 0.02486150
Iteration 20233, loss = 0.02486012
Iteration 20234, loss = 0.02485877
Iteration 20235, loss = 0.02485741
Iteration 20236, loss = 0.02485604
Iteration 20237, loss = 0.02485469
Iteration 20238, loss = 0.02485329
Iteration 20239, loss = 0.02485195
Iteration 20240, loss = 0.02485061
Iteration 20241, loss = 0.02484924
Iteration 20242, loss = 0.02484786
Iteration 20243, loss = 0.02484648
Iteration 20244, loss = 0.02484515
Iteration 20245, loss = 0.02484377
Iteration 20246, loss = 0.02484241
Iteration 20247, loss = 0.02484105
Iteration 20248, loss = 0.02483968
Iteration 20249, loss = 0.02483833
Iteration 20250, loss = 0.02483695
Iteration 20251, loss = 0.02483559
Iteration 20252, loss = 0.02483423
Iteration 20253, loss = 0.02483288
Iteration 20254, loss = 0.02483153
Iteration 20255, loss = 0.02483016
Iteration 20256, loss = 0.02482880
Iteration 20257, loss = 0.02482742
Iteration 20258, loss = 0.02482607
Iteration 20259, loss = 0.02482471
Iteration 20260, loss = 0.02482335
Iteration 20261, loss = 0.02482200
Iteration 20262, loss = 0.02482063
Iteration 20263, loss = 0.02481928
Iteration 20264, loss = 0.02481789
Iteration 20265, loss = 0.02481655
Iteration 20266, loss = 0.02481518
Iteration 20267, loss = 0.02481383
Iteration 20268, loss = 0.02481248
Iteration 20269, loss = 0.02481112
Iteration 20270, loss = 0.02480975
Iteration 20271, loss = 0.02480838
Iteration 20272, loss = 0.02480703
Iteration 20273, loss = 0.02480567
Iteration 20274, loss = 0.02480431
Iteration 20275, loss = 0.02480295
Iteration 20276, loss = 0.02480160
Iteration 20277, loss = 0.02480024
Iteration 20278, loss = 0.02479890
Iteration 20279, loss = 0.02479752
Iteration 20280, loss = 0.02479618
Iteration 20281, loss = 0.02479482
Iteration 20282, loss = 0.02479346
Iteration 20283, loss = 0.02479211
Iteration 20284, loss = 0.02479075
Iteration 20285, loss = 0.02478938
Iteration 20286, loss = 0.02478804
Iteration 20287, loss = 0.02478668
Iteration 20288, loss = 0.02478532
Iteration 20289, loss = 0.02478397
Iteration 20290, loss = 0.02478260
Iteration 20291, loss = 0.02478126
Iteration 20292, loss = 0.02477991
Iteration 20293, loss = 0.02477853
Iteration 20294, loss = 0.02477719
Iteration 20295, loss = 0.02477582
Iteration 20296, loss = 0.02477448
Iteration 20297, loss = 0.02477313
Iteration 20298, loss = 0.02477177
Iteration 20299, loss = 0.02477043
Iteration 20300, loss = 0.02476906
Iteration 20301, loss = 0.02476769
Iteration 20302, loss = 0.02476637
Iteration 20303, loss = 0.02476500
Iteration 20304, loss = 0.02476363
Iteration 20305, loss = 0.02476229
Iteration 20306, loss = 0.02476094
Iteration 20307, loss = 0.02475957
Iteration 20308, loss = 0.02475823
Iteration 20309, loss = 0.02475688
Iteration 20310, loss = 0.02475553
Iteration 20311, loss = 0.02475418
Iteration 20312, loss = 0.02475282
Iteration 20313, loss = 0.02475147
Iteration 20314, loss = 0.02475012
Iteration 20315, loss = 0.02474879
Iteration 20316, loss = 0.02474742
Iteration 20317, loss = 0.02474607
Iteration 20318, loss = 0.02474472
Iteration 20319, loss = 0.02474337
Iteration 20320, loss = 0.02474201
Iteration 20321, loss = 0.02474069
Iteration 20322, loss = 0.02473932
Iteration 20323, loss = 0.02473797
Iteration 20324, loss = 0.02473661
Iteration 20325, loss = 0.02473526
Iteration 20326, loss = 0.02473392
Iteration 20327, loss = 0.02473259
Iteration 20328, loss = 0.02473122
Iteration 20329, loss = 0.02472987
Iteration 20330, loss = 0.02472852
Iteration 20331, loss = 0.02472718
Iteration 20332, loss = 0.02472583
Iteration 20333, loss = 0.02472448
Iteration 20334, loss = 0.02472312
Iteration 20335, loss = 0.02472179
Iteration 20336, loss = 0.02472045
Iteration 20337, loss = 0.02471909
Iteration 20338, loss = 0.02471774
Iteration 20339, loss = 0.02471642
Iteration 20340, loss = 0.02471506
Iteration 20341, loss = 0.02471371
Iteration 20342, loss = 0.02471236
Iteration 20343, loss = 0.02471102
Iteration 20344, loss = 0.02470967
Iteration 20345, loss = 0.02470833
Iteration 20346, loss = 0.02470700
Iteration 20347, loss = 0.02470566
Iteration 20348, loss = 0.02470433
Iteration 20349, loss = 0.02470296
Iteration 20350, loss = 0.02470163
Iteration 20351, loss = 0.02470028
Iteration 20352, loss = 0.02469893
Iteration 20353, loss = 0.02469759
Iteration 20354, loss = 0.02469624
Iteration 20355, loss = 0.02469490
Iteration 20356, loss = 0.02469356
Iteration 20357, loss = 0.02469221
Iteration 20358, loss = 0.02469086
Iteration 20359, loss = 0.02468953
Iteration 20360, loss = 0.02468818
Iteration 20361, loss = 0.02468684
Iteration 20362, loss = 0.02468550
Iteration 20363, loss = 0.02468415
Iteration 20364, loss = 0.02468282
Iteration 20365, loss = 0.02468147
Iteration 20366, loss = 0.02468013
Iteration 20367, loss = 0.02467878
Iteration 20368, loss = 0.02467744
Iteration 20369, loss = 0.02467611
Iteration 20370, loss = 0.02467475
Iteration 20371, loss = 0.02467341
Iteration 20372, loss = 0.02467208
Iteration 20373, loss = 0.02467072
Iteration 20374, loss = 0.02466940
Iteration 20375, loss = 0.02466805
Iteration 20376, loss = 0.02466670
Iteration 20377, loss = 0.02466536
Iteration 20378, loss = 0.02466404
Iteration 20379, loss = 0.02466269
Iteration 20380, loss = 0.02466136
Iteration 20381, loss = 0.02466001
Iteration 20382, loss = 0.02465869
Iteration 20383, loss = 0.02465735
Iteration 20384, loss = 0.02465600
Iteration 20385, loss = 0.02465467
Iteration 20386, loss = 0.02465332
Iteration 20387, loss = 0.02465196
Iteration 20388, loss = 0.02465063
Iteration 20389, loss = 0.02464928
Iteration 20390, loss = 0.02464794
Iteration 20391, loss = 0.02464661
Iteration 20392, loss = 0.02464526
Iteration 20393, loss = 0.02464391
Iteration 20394, loss = 0.02464260
Iteration 20395, loss = 0.02464125
Iteration 20396, loss = 0.02463989
Iteration 20397, loss = 0.02463858
Iteration 20398, loss = 0.02463723
Iteration 20399, loss = 0.02463589
Iteration 20400, loss = 0.02463453
Iteration 20401, loss = 0.02463321
Iteration 20402, loss = 0.02463187
Iteration 20403, loss = 0.02463052
Iteration 20404, loss = 0.02462919
Iteration 20405, loss = 0.02462784
Iteration 20406, loss = 0.02462649
Iteration 20407, loss = 0.02462517
Iteration 20408, loss = 0.02462383
Iteration 20409, loss = 0.02462249
Iteration 20410, loss = 0.02462114
Iteration 20411, loss = 0.02461979
Iteration 20412, loss = 0.02461844
Iteration 20413, loss = 0.02461713
Iteration 20414, loss = 0.02461578
Iteration 20415, loss = 0.02461444
Iteration 20416, loss = 0.02461311
Iteration 20417, loss = 0.02461176
Iteration 20418, loss = 0.02461042
Iteration 20419, loss = 0.02460909
Iteration 20420, loss = 0.02460774
Iteration 20421, loss = 0.02460641
Iteration 20422, loss = 0.02460507
Iteration 20423, loss = 0.02460374
Iteration 20424, loss = 0.02460241
Iteration 20425, loss = 0.02460107
Iteration 20426, loss = 0.02459974
Iteration 20427, loss = 0.02459841
Iteration 20428, loss = 0.02459708
Iteration 20429, loss = 0.02459575
Iteration 20430, loss = 0.02459442
Iteration 20431, loss = 0.02459308
Iteration 20432, loss = 0.02459175
Iteration 20433, loss = 0.02459043
Iteration 20434, loss = 0.02458908
Iteration 20435, loss = 0.02458775
Iteration 20436, loss = 0.02458643
Iteration 20437, loss = 0.02458509
Iteration 20438, loss = 0.02458375
Iteration 20439, loss = 0.02458241
Iteration 20440, loss = 0.02458109
Iteration 20441, loss = 0.02457974
Iteration 20442, loss = 0.02457842
Iteration 20443, loss = 0.02457708
Iteration 20444, loss = 0.02457572
Iteration 20445, loss = 0.02457444
Iteration 20446, loss = 0.02457307
Iteration 20447, loss = 0.02457172
Iteration 20448, loss = 0.02457039
Iteration 20449, loss = 0.02456906
Iteration 20450, loss = 0.02456776
Iteration 20451, loss = 0.02456640
Iteration 20452, loss = 0.02456507
Iteration 20453, loss = 0.02456374
Iteration 20454, loss = 0.02456241
Iteration 20455, loss = 0.02456110
Iteration 20456, loss = 0.02455974
Iteration 20457, loss = 0.02455841
Iteration 20458, loss = 0.02455708
Iteration 20459, loss = 0.02455575
Iteration 20460, loss = 0.02455442
Iteration 20461, loss = 0.02455308
Iteration 20462, loss = 0.02455176
Iteration 20463, loss = 0.02455043
Iteration 20464, loss = 0.02454911
Iteration 20465, loss = 0.02454778
Iteration 20466, loss = 0.02454645
Iteration 20467, loss = 0.02454512
Iteration 20468, loss = 0.02454379
Iteration 20469, loss = 0.02454248
Iteration 20470, loss = 0.02454114
Iteration 20471, loss = 0.02453980
Iteration 20472, loss = 0.02453848
Iteration 20473, loss = 0.02453716
Iteration 20474, loss = 0.02453582
Iteration 20475, loss = 0.02453449
Iteration 20476, loss = 0.02453318
Iteration 20477, loss = 0.02453187
Iteration 20478, loss = 0.02453053
Iteration 20479, loss = 0.02452920
Iteration 20480, loss = 0.02452787
Iteration 20481, loss = 0.02452654
Iteration 20482, loss = 0.02452522
Iteration 20483, loss = 0.02452391
Iteration 20484, loss = 0.02452257
Iteration 20485, loss = 0.02452125
Iteration 20486, loss = 0.02451995
Iteration 20487, loss = 0.02451861
Iteration 20488, loss = 0.02451728
Iteration 20489, loss = 0.02451596
Iteration 20490, loss = 0.02451463
Iteration 20491, loss = 0.02451331
Iteration 20492, loss = 0.02451199
Iteration 20493, loss = 0.02451067
Iteration 20494, loss = 0.02450935
Iteration 20495, loss = 0.02450803
Iteration 20496, loss = 0.02450670
Iteration 20497, loss = 0.02450539
Iteration 20498, loss = 0.02450404
Iteration 20499, loss = 0.02450273
Iteration 20500, loss = 0.02450142
Iteration 20501, loss = 0.02450008
Iteration 20502, loss = 0.02449876
Iteration 20503, loss = 0.02449744
Iteration 20504, loss = 0.02449612
Iteration 20505, loss = 0.02449480
Iteration 20506, loss = 0.02449349
Iteration 20507, loss = 0.02449216
Iteration 20508, loss = 0.02449081
Iteration 20509, loss = 0.02448950
Iteration 20510, loss = 0.02448818
Iteration 20511, loss = 0.02448687
Iteration 20512, loss = 0.02448555
Iteration 20513, loss = 0.02448421
Iteration 20514, loss = 0.02448290
Iteration 20515, loss = 0.02448158
Iteration 20516, loss = 0.02448024
Iteration 20517, loss = 0.02447895
Iteration 20518, loss = 0.02447760
Iteration 20519, loss = 0.02447629
Iteration 20520, loss = 0.02447497
Iteration 20521, loss = 0.02447364
Iteration 20522, loss = 0.02447235
Iteration 20523, loss = 0.02447101
Iteration 20524, loss = 0.02446971
Iteration 20525, loss = 0.02446836
Iteration 20526, loss = 0.02446705
Iteration 20527, loss = 0.02446574
Iteration 20528, loss = 0.02446441
Iteration 20529, loss = 0.02446309
Iteration 20530, loss = 0.02446177
Iteration 20531, loss = 0.02446045
Iteration 20532, loss = 0.02445912
Iteration 20533, loss = 0.02445780
Iteration 20534, loss = 0.02445648
Iteration 20535, loss = 0.02445516
Iteration 20536, loss = 0.02445384
Iteration 20537, loss = 0.02445253
Iteration 20538, loss = 0.02445120
Iteration 20539, loss = 0.02444990
Iteration 20540, loss = 0.02444857
Iteration 20541, loss = 0.02444724
Iteration 20542, loss = 0.02444592
Iteration 20543, loss = 0.02444462
Iteration 20544, loss = 0.02444329
Iteration 20545, loss = 0.02444197
Iteration 20546, loss = 0.02444065
Iteration 20547, loss = 0.02443935
Iteration 20548, loss = 0.02443803
Iteration 20549, loss = 0.02443670
Iteration 20550, loss = 0.02443540
Iteration 20551, loss = 0.02443406
Iteration 20552, loss = 0.02443274
Iteration 20553, loss = 0.02443143
Iteration 20554, loss = 0.02443010
Iteration 20555, loss = 0.02442880
Iteration 20556, loss = 0.02442748
Iteration 20557, loss = 0.02442615
Iteration 20558, loss = 0.02442485
Iteration 20559, loss = 0.02442351
Iteration 20560, loss = 0.02442222
Iteration 20561, loss = 0.02442090
Iteration 20562, loss = 0.02441958
Iteration 20563, loss = 0.02441828
Iteration 20564, loss = 0.02441694
Iteration 20565, loss = 0.02441565
Iteration 20566, loss = 0.02441431
Iteration 20567, loss = 0.02441301
Iteration 20568, loss = 0.02441169
Iteration 20569, loss = 0.02441039
Iteration 20570, loss = 0.02440906
Iteration 20571, loss = 0.02440775
Iteration 20572, loss = 0.02440645
Iteration 20573, loss = 0.02440513
Iteration 20574, loss = 0.02440383
Iteration 20575, loss = 0.02440252
Iteration 20576, loss = 0.02440120
Iteration 20577, loss = 0.02439988
Iteration 20578, loss = 0.02439857
Iteration 20579, loss = 0.02439727
Iteration 20580, loss = 0.02439594
Iteration 20581, loss = 0.02439465
Iteration 20582, loss = 0.02439333
Iteration 20583, loss = 0.02439200
Iteration 20584, loss = 0.02439069
Iteration 20585, loss = 0.02438940
Iteration 20586, loss = 0.02438805
Iteration 20587, loss = 0.02438675
Iteration 20588, loss = 0.02438544
Iteration 20589, loss = 0.02438414
Iteration 20590, loss = 0.02438284
Iteration 20591, loss = 0.02438150
Iteration 20592, loss = 0.02438020
Iteration 20593, loss = 0.02437888
Iteration 20594, loss = 0.02437758
Iteration 20595, loss = 0.02437628
Iteration 20596, loss = 0.02437496
Iteration 20597, loss = 0.02437366
Iteration 20598, loss = 0.02437232
Iteration 20599, loss = 0.02437104
Iteration 20600, loss = 0.02436973
Iteration 20601, loss = 0.02436841
Iteration 20602, loss = 0.02436709
Iteration 20603, loss = 0.02436578
Iteration 20604, loss = 0.02436448
Iteration 20605, loss = 0.02436316
Iteration 20606, loss = 0.02436187
Iteration 20607, loss = 0.02436055
Iteration 20608, loss = 0.02435925
Iteration 20609, loss = 0.02435793
Iteration 20610, loss = 0.02435663
Iteration 20611, loss = 0.02435531
Iteration 20612, loss = 0.02435402
Iteration 20613, loss = 0.02435269
Iteration 20614, loss = 0.02435139
Iteration 20615, loss = 0.02435008
Iteration 20616, loss = 0.02434877
Iteration 20617, loss = 0.02434749
Iteration 20618, loss = 0.02434618
Iteration 20619, loss = 0.02434488
Iteration 20620, loss = 0.02434358
Iteration 20621, loss = 0.02434225
Iteration 20622, loss = 0.02434097
Iteration 20623, loss = 0.02433966
Iteration 20624, loss = 0.02433836
Iteration 20625, loss = 0.02433707
Iteration 20626, loss = 0.02433575
Iteration 20627, loss = 0.02433444
Iteration 20628, loss = 0.02433314
Iteration 20629, loss = 0.02433183
Iteration 20630, loss = 0.02433053
Iteration 20631, loss = 0.02432923
Iteration 20632, loss = 0.02432794
Iteration 20633, loss = 0.02432662
Iteration 20634, loss = 0.02432532
Iteration 20635, loss = 0.02432401
Iteration 20636, loss = 0.02432271
Iteration 20637, loss = 0.02432142
Iteration 20638, loss = 0.02432011
Iteration 20639, loss = 0.02431881
Iteration 20640, loss = 0.02431750
Iteration 20641, loss = 0.02431620
Iteration 20642, loss = 0.02431490
Iteration 20643, loss = 0.02431361
Iteration 20644, loss = 0.02431232
Iteration 20645, loss = 0.02431099
Iteration 20646, loss = 0.02430970
Iteration 20647, loss = 0.02430840
Iteration 20648, loss = 0.02430710
Iteration 20649, loss = 0.02430580
Iteration 20650, loss = 0.02430450
Iteration 20651, loss = 0.02430322
Iteration 20652, loss = 0.02430190
Iteration 20653, loss = 0.02430060
Iteration 20654, loss = 0.02429929
Iteration 20655, loss = 0.02429801
Iteration 20656, loss = 0.02429669
Iteration 20657, loss = 0.02429539
Iteration 20658, loss = 0.02429409
Iteration 20659, loss = 0.02429279
Iteration 20660, loss = 0.02429147
Iteration 20661, loss = 0.02429022
Iteration 20662, loss = 0.02428888
Iteration 20663, loss = 0.02428760
Iteration 20664, loss = 0.02428630
Iteration 20665, loss = 0.02428500
Iteration 20666, loss = 0.02428370
Iteration 20667, loss = 0.02428240
Iteration 20668, loss = 0.02428111
Iteration 20669, loss = 0.02427982
Iteration 20670, loss = 0.02427851
Iteration 20671, loss = 0.02427723
Iteration 20672, loss = 0.02427593
Iteration 20673, loss = 0.02427463
Iteration 20674, loss = 0.02427333
Iteration 20675, loss = 0.02427204
Iteration 20676, loss = 0.02427074
Iteration 20677, loss = 0.02426946
Iteration 20678, loss = 0.02426818
Iteration 20679, loss = 0.02426687
Iteration 20680, loss = 0.02426558
Iteration 20681, loss = 0.02426428
Iteration 20682, loss = 0.02426297
Iteration 20683, loss = 0.02426170
Iteration 20684, loss = 0.02426040
Iteration 20685, loss = 0.02425909
Iteration 20686, loss = 0.02425779
Iteration 20687, loss = 0.02425650
Iteration 20688, loss = 0.02425520
Iteration 20689, loss = 0.02425391
Iteration 20690, loss = 0.02425261
Iteration 20691, loss = 0.02425131
Iteration 20692, loss = 0.02425001
Iteration 20693, loss = 0.02424871
Iteration 20694, loss = 0.02424741
Iteration 20695, loss = 0.02424612
Iteration 20696, loss = 0.02424482
Iteration 20697, loss = 0.02424352
Iteration 20698, loss = 0.02424222
Iteration 20699, loss = 0.02424093
Iteration 20700, loss = 0.02423964
Iteration 20701, loss = 0.02423833
Iteration 20702, loss = 0.02423704
Iteration 20703, loss = 0.02423576
Iteration 20704, loss = 0.02423445
Iteration 20705, loss = 0.02423316
Iteration 20706, loss = 0.02423186
Iteration 20707, loss = 0.02423058
Iteration 20708, loss = 0.02422927
Iteration 20709, loss = 0.02422797
Iteration 20710, loss = 0.02422667
Iteration 20711, loss = 0.02422538
Iteration 20712, loss = 0.02422410
Iteration 20713, loss = 0.02422278
Iteration 20714, loss = 0.02422149
Iteration 20715, loss = 0.02422019
Iteration 20716, loss = 0.02421890
Iteration 20717, loss = 0.02421760
Iteration 20718, loss = 0.02421630
Iteration 20719, loss = 0.02421503
Iteration 20720, loss = 0.02421373
Iteration 20721, loss = 0.02421242
Iteration 20722, loss = 0.02421114
Iteration 20723, loss = 0.02420983
Iteration 20724, loss = 0.02420854
Iteration 20725, loss = 0.02420726
Iteration 20726, loss = 0.02420595
Iteration 20727, loss = 0.02420465
Iteration 20728, loss = 0.02420336
Iteration 20729, loss = 0.02420207
Iteration 20730, loss = 0.02420079
Iteration 20731, loss = 0.02419950
Iteration 20732, loss = 0.02419820
Iteration 20733, loss = 0.02419691
Iteration 20734, loss = 0.02419560
Iteration 20735, loss = 0.02419432
Iteration 20736, loss = 0.02419303
Iteration 20737, loss = 0.02419174
Iteration 20738, loss = 0.02419043
Iteration 20739, loss = 0.02418914
Iteration 20740, loss = 0.02418783
Iteration 20741, loss = 0.02418654
Iteration 20742, loss = 0.02418527
Iteration 20743, loss = 0.02418394
Iteration 20744, loss = 0.02418266
Iteration 20745, loss = 0.02418137
Iteration 20746, loss = 0.02418009
Iteration 20747, loss = 0.02417879
Iteration 20748, loss = 0.02417749
Iteration 20749, loss = 0.02417621
Iteration 20750, loss = 0.02417492
Iteration 20751, loss = 0.02417363
Iteration 20752, loss = 0.02417235
Iteration 20753, loss = 0.02417105
Iteration 20754, loss = 0.02416976
Iteration 20755, loss = 0.02416850
Iteration 20756, loss = 0.02416720
Iteration 20757, loss = 0.02416590
Iteration 20758, loss = 0.02416464
Iteration 20759, loss = 0.02416334
Iteration 20760, loss = 0.02416204
Iteration 20761, loss = 0.02416076
Iteration 20762, loss = 0.02415946
Iteration 20763, loss = 0.02415818
Iteration 20764, loss = 0.02415688
Iteration 20765, loss = 0.02415559
Iteration 20766, loss = 0.02415431
Iteration 20767, loss = 0.02415301
Iteration 20768, loss = 0.02415173
Iteration 20769, loss = 0.02415044
Iteration 20770, loss = 0.02414915
Iteration 20771, loss = 0.02414787
Iteration 20772, loss = 0.02414658
Iteration 20773, loss = 0.02414529
Iteration 20774, loss = 0.02414401
Iteration 20775, loss = 0.02414272
Iteration 20776, loss = 0.02414146
Iteration 20777, loss = 0.02414015
Iteration 20778, loss = 0.02413887
Iteration 20779, loss = 0.02413759
Iteration 20780, loss = 0.02413629
Iteration 20781, loss = 0.02413502
Iteration 20782, loss = 0.02413372
Iteration 20783, loss = 0.02413246
Iteration 20784, loss = 0.02413117
Iteration 20785, loss = 0.02412988
Iteration 20786, loss = 0.02412861
Iteration 20787, loss = 0.02412732
Iteration 20788, loss = 0.02412603
Iteration 20789, loss = 0.02412474
Iteration 20790, loss = 0.02412346
Iteration 20791, loss = 0.02412217
Iteration 20792, loss = 0.02412090
Iteration 20793, loss = 0.02411961
Iteration 20794, loss = 0.02411831
Iteration 20795, loss = 0.02411702
Iteration 20796, loss = 0.02411576
Iteration 20797, loss = 0.02411448
Iteration 20798, loss = 0.02411318
Iteration 20799, loss = 0.02411189
Iteration 20800, loss = 0.02411060
Iteration 20801, loss = 0.02410932
Iteration 20802, loss = 0.02410805
Iteration 20803, loss = 0.02410674
Iteration 20804, loss = 0.02410546
Iteration 20805, loss = 0.02410417
Iteration 20806, loss = 0.02410291
Iteration 20807, loss = 0.02410163
Iteration 20808, loss = 0.02410032
Iteration 20809, loss = 0.02409905
Iteration 20810, loss = 0.02409776
Iteration 20811, loss = 0.02409649
Iteration 20812, loss = 0.02409519
Iteration 20813, loss = 0.02409393
Iteration 20814, loss = 0.02409265
Iteration 20815, loss = 0.02409136
Iteration 20816, loss = 0.02409009
Iteration 20817, loss = 0.02408882
Iteration 20818, loss = 0.02408752
Iteration 20819, loss = 0.02408623
Iteration 20820, loss = 0.02408497
Iteration 20821, loss = 0.02408368
Iteration 20822, loss = 0.02408241
Iteration 20823, loss = 0.02408113
Iteration 20824, loss = 0.02407984
Iteration 20825, loss = 0.02407857
Iteration 20826, loss = 0.02407732
Iteration 20827, loss = 0.02407602
Iteration 20828, loss = 0.02407474
Iteration 20829, loss = 0.02407345
Iteration 20830, loss = 0.02407219
Iteration 20831, loss = 0.02407091
Iteration 20832, loss = 0.02406963
Iteration 20833, loss = 0.02406837
Iteration 20834, loss = 0.02406710
Iteration 20835, loss = 0.02406581
Iteration 20836, loss = 0.02406453
Iteration 20837, loss = 0.02406326
Iteration 20838, loss = 0.02406198
Iteration 20839, loss = 0.02406072
Iteration 20840, loss = 0.02405942
Iteration 20841, loss = 0.02405816
Iteration 20842, loss = 0.02405688
Iteration 20843, loss = 0.02405560
Iteration 20844, loss = 0.02405435
Iteration 20845, loss = 0.02405307
Iteration 20846, loss = 0.02405180
Iteration 20847, loss = 0.02405052
Iteration 20848, loss = 0.02404924
Iteration 20849, loss = 0.02404797
Iteration 20850, loss = 0.02404670
Iteration 20851, loss = 0.02404542
Iteration 20852, loss = 0.02404416
Iteration 20853, loss = 0.02404288
Iteration 20854, loss = 0.02404161
Iteration 20855, loss = 0.02404036
Iteration 20856, loss = 0.02403906
Iteration 20857, loss = 0.02403780
Iteration 20858, loss = 0.02403655
Iteration 20859, loss = 0.02403526
Iteration 20860, loss = 0.02403398
Iteration 20861, loss = 0.02403273
Iteration 20862, loss = 0.02403143
Iteration 20863, loss = 0.02403017
Iteration 20864, loss = 0.02402889
Iteration 20865, loss = 0.02402762
Iteration 20866, loss = 0.02402635
Iteration 20867, loss = 0.02402506
Iteration 20868, loss = 0.02402380
Iteration 20869, loss = 0.02402254
Iteration 20870, loss = 0.02402126
Iteration 20871, loss = 0.02401998
Iteration 20872, loss = 0.02401870
Iteration 20873, loss = 0.02401743
Iteration 20874, loss = 0.02401616
Iteration 20875, loss = 0.02401490
Iteration 20876, loss = 0.02401363
Iteration 20877, loss = 0.02401233
Iteration 20878, loss = 0.02401109
Iteration 20879, loss = 0.02400980
Iteration 20880, loss = 0.02400852
Iteration 20881, loss = 0.02400726
Iteration 20882, loss = 0.02400599
Iteration 20883, loss = 0.02400472
Iteration 20884, loss = 0.02400346
Iteration 20885, loss = 0.02400218
Iteration 20886, loss = 0.02400090
Iteration 20887, loss = 0.02399965
Iteration 20888, loss = 0.02399837
Iteration 20889, loss = 0.02399712
Iteration 20890, loss = 0.02399582
Iteration 20891, loss = 0.02399457
Iteration 20892, loss = 0.02399329
Iteration 20893, loss = 0.02399205
Iteration 20894, loss = 0.02399075
Iteration 20895, loss = 0.02398947
Iteration 20896, loss = 0.02398820
Iteration 20897, loss = 0.02398694
Iteration 20898, loss = 0.02398567
Iteration 20899, loss = 0.02398440
Iteration 20900, loss = 0.02398313
Iteration 20901, loss = 0.02398185
Iteration 20902, loss = 0.02398058
Iteration 20903, loss = 0.02397933
Iteration 20904, loss = 0.02397805
Iteration 20905, loss = 0.02397680
Iteration 20906, loss = 0.02397554
Iteration 20907, loss = 0.02397426
Iteration 20908, loss = 0.02397301
Iteration 20909, loss = 0.02397173
Iteration 20910, loss = 0.02397047
Iteration 20911, loss = 0.02396922
Iteration 20912, loss = 0.02396794
Iteration 20913, loss = 0.02396670
Iteration 20914, loss = 0.02396542
Iteration 20915, loss = 0.02396418
Iteration 20916, loss = 0.02396290
Iteration 20917, loss = 0.02396165
Iteration 20918, loss = 0.02396038
Iteration 20919, loss = 0.02395911
Iteration 20920, loss = 0.02395786
Iteration 20921, loss = 0.02395662
Iteration 20922, loss = 0.02395532
Iteration 20923, loss = 0.02395406
Iteration 20924, loss = 0.02395279
Iteration 20925, loss = 0.02395153
Iteration 20926, loss = 0.02395028
Iteration 20927, loss = 0.02394901
Iteration 20928, loss = 0.02394774
Iteration 20929, loss = 0.02394647
Iteration 20930, loss = 0.02394521
Iteration 20931, loss = 0.02394395
Iteration 20932, loss = 0.02394269
Iteration 20933, loss = 0.02394142
Iteration 20934, loss = 0.02394015
Iteration 20935, loss = 0.02393892
Iteration 20936, loss = 0.02393763
Iteration 20937, loss = 0.02393637
Iteration 20938, loss = 0.02393510
Iteration 20939, loss = 0.02393386
Iteration 20940, loss = 0.02393260
Iteration 20941, loss = 0.02393132
Iteration 20942, loss = 0.02393006
Iteration 20943, loss = 0.02392880
Iteration 20944, loss = 0.02392754
Iteration 20945, loss = 0.02392630
Iteration 20946, loss = 0.02392502
Iteration 20947, loss = 0.02392375
Iteration 20948, loss = 0.02392250
Iteration 20949, loss = 0.02392123
Iteration 20950, loss = 0.02391999
Iteration 20951, loss = 0.02391873
Iteration 20952, loss = 0.02391745
Iteration 20953, loss = 0.02391622
Iteration 20954, loss = 0.02391493
Iteration 20955, loss = 0.02391366
Iteration 20956, loss = 0.02391240
Iteration 20957, loss = 0.02391114
Iteration 20958, loss = 0.02390988
Iteration 20959, loss = 0.02390863
Iteration 20960, loss = 0.02390738
Iteration 20961, loss = 0.02390610
Iteration 20962, loss = 0.02390484
Iteration 20963, loss = 0.02390358
Iteration 20964, loss = 0.02390232
Iteration 20965, loss = 0.02390108
Iteration 20966, loss = 0.02389982
Iteration 20967, loss = 0.02389856
Iteration 20968, loss = 0.02389729
Iteration 20969, loss = 0.02389604
Iteration 20970, loss = 0.02389478
Iteration 20971, loss = 0.02389353
Iteration 20972, loss = 0.02389228
Iteration 20973, loss = 0.02389102
Iteration 20974, loss = 0.02388977
Iteration 20975, loss = 0.02388852
Iteration 20976, loss = 0.02388724
Iteration 20977, loss = 0.02388600
Iteration 20978, loss = 0.02388473
Iteration 20979, loss = 0.02388347
Iteration 20980, loss = 0.02388222
Iteration 20981, loss = 0.02388097
Iteration 20982, loss = 0.02387971
Iteration 20983, loss = 0.02387847
Iteration 20984, loss = 0.02387720
Iteration 20985, loss = 0.02387594
Iteration 20986, loss = 0.02387467
Iteration 20987, loss = 0.02387343
Iteration 20988, loss = 0.02387218
Iteration 20989, loss = 0.02387093
Iteration 20990, loss = 0.02386968
Iteration 20991, loss = 0.02386840
Iteration 20992, loss = 0.02386716
Iteration 20993, loss = 0.02386590
Iteration 20994, loss = 0.02386465
Iteration 20995, loss = 0.02386338
Iteration 20996, loss = 0.02386213
Iteration 20997, loss = 0.02386088
Iteration 20998, loss = 0.02385962
Iteration 20999, loss = 0.02385837
Iteration 21000, loss = 0.02385711
Iteration 21001, loss = 0.02385585
Iteration 21002, loss = 0.02385459
Iteration 21003, loss = 0.02385335
Iteration 21004, loss = 0.02385207
Iteration 21005, loss = 0.02385083
Iteration 21006, loss = 0.02384958
Iteration 21007, loss = 0.02384831
Iteration 21008, loss = 0.02384708
Iteration 21009, loss = 0.02384582
Iteration 21010, loss = 0.02384456
Iteration 21011, loss = 0.02384330
Iteration 21012, loss = 0.02384204
Iteration 21013, loss = 0.02384078
Iteration 21014, loss = 0.02383954
Iteration 21015, loss = 0.02383829
Iteration 21016, loss = 0.02383704
Iteration 21017, loss = 0.02383578
Iteration 21018, loss = 0.02383453
Iteration 21019, loss = 0.02383327
Iteration 21020, loss = 0.02383202
Iteration 21021, loss = 0.02383077
Iteration 21022, loss = 0.02382952
Iteration 21023, loss = 0.02382829
Iteration 21024, loss = 0.02382701
Iteration 21025, loss = 0.02382578
Iteration 21026, loss = 0.02382451
Iteration 21027, loss = 0.02382327
Iteration 21028, loss = 0.02382200
Iteration 21029, loss = 0.02382075
Iteration 21030, loss = 0.02381950
Iteration 21031, loss = 0.02381825
Iteration 21032, loss = 0.02381700
Iteration 21033, loss = 0.02381574
Iteration 21034, loss = 0.02381451
Iteration 21035, loss = 0.02381324
Iteration 21036, loss = 0.02381199
Iteration 21037, loss = 0.02381076
Iteration 21038, loss = 0.02380947
Iteration 21039, loss = 0.02380822
Iteration 21040, loss = 0.02380698
Iteration 21041, loss = 0.02380571
Iteration 21042, loss = 0.02380447
Iteration 21043, loss = 0.02380320
Iteration 21044, loss = 0.02380195
Iteration 21045, loss = 0.02380071
Iteration 21046, loss = 0.02379945
Iteration 21047, loss = 0.02379818
Iteration 21048, loss = 0.02379694
Iteration 21049, loss = 0.02379568
Iteration 21050, loss = 0.02379444
Iteration 21051, loss = 0.02379319
Iteration 21052, loss = 0.02379193
Iteration 21053, loss = 0.02379068
Iteration 21054, loss = 0.02378943
Iteration 21055, loss = 0.02378820
Iteration 21056, loss = 0.02378693
Iteration 21057, loss = 0.02378568
Iteration 21058, loss = 0.02378444
Iteration 21059, loss = 0.02378318
Iteration 21060, loss = 0.02378193
Iteration 21061, loss = 0.02378067
Iteration 21062, loss = 0.02377943
Iteration 21063, loss = 0.02377818
Iteration 21064, loss = 0.02377693
Iteration 21065, loss = 0.02377570
Iteration 21066, loss = 0.02377443
Iteration 21067, loss = 0.02377319
Iteration 21068, loss = 0.02377195
Iteration 21069, loss = 0.02377069
Iteration 21070, loss = 0.02376945
Iteration 21071, loss = 0.02376821
Iteration 21072, loss = 0.02376695
Iteration 21073, loss = 0.02376570
Iteration 21074, loss = 0.02376446
Iteration 21075, loss = 0.02376321
Iteration 21076, loss = 0.02376196
Iteration 21077, loss = 0.02376074
Iteration 21078, loss = 0.02375948
Iteration 21079, loss = 0.02375823
Iteration 21080, loss = 0.02375699
Iteration 21081, loss = 0.02375575
Iteration 21082, loss = 0.02375449
Iteration 21083, loss = 0.02375326
Iteration 21084, loss = 0.02375202
Iteration 21085, loss = 0.02375077
Iteration 21086, loss = 0.02374953
Iteration 21087, loss = 0.02374828
Iteration 21088, loss = 0.02374707
Iteration 21089, loss = 0.02374581
Iteration 21090, loss = 0.02374455
Iteration 21091, loss = 0.02374332
Iteration 21092, loss = 0.02374208
Iteration 21093, loss = 0.02374083
Iteration 21094, loss = 0.02373960
Iteration 21095, loss = 0.02373834
Iteration 21096, loss = 0.02373711
Iteration 21097, loss = 0.02373588
Iteration 21098, loss = 0.02373463
Iteration 21099, loss = 0.02373338
Iteration 21100, loss = 0.02373215
Iteration 21101, loss = 0.02373090
Iteration 21102, loss = 0.02372967
Iteration 21103, loss = 0.02372843
Iteration 21104, loss = 0.02372718
Iteration 21105, loss = 0.02372596
Iteration 21106, loss = 0.02372471
Iteration 21107, loss = 0.02372347
Iteration 21108, loss = 0.02372223
Iteration 21109, loss = 0.02372098
Iteration 21110, loss = 0.02371974
Iteration 21111, loss = 0.02371852
Iteration 21112, loss = 0.02371726
Iteration 21113, loss = 0.02371603
Iteration 21114, loss = 0.02371479
Iteration 21115, loss = 0.02371355
Iteration 21116, loss = 0.02371231
Iteration 21117, loss = 0.02371106
Iteration 21118, loss = 0.02370980
Iteration 21119, loss = 0.02370858
Iteration 21120, loss = 0.02370735
Iteration 21121, loss = 0.02370611
Iteration 21122, loss = 0.02370487
Iteration 21123, loss = 0.02370361
Iteration 21124, loss = 0.02370239
Iteration 21125, loss = 0.02370116
Iteration 21126, loss = 0.02369991
Iteration 21127, loss = 0.02369868
Iteration 21128, loss = 0.02369744
Iteration 21129, loss = 0.02369622
Iteration 21130, loss = 0.02369497
Iteration 21131, loss = 0.02369373
Iteration 21132, loss = 0.02369248
Iteration 21133, loss = 0.02369126
Iteration 21134, loss = 0.02369002
Iteration 21135, loss = 0.02368879
Iteration 21136, loss = 0.02368755
Iteration 21137, loss = 0.02368631
Iteration 21138, loss = 0.02368509
Iteration 21139, loss = 0.02368384
Iteration 21140, loss = 0.02368260
Iteration 21141, loss = 0.02368137
Iteration 21142, loss = 0.02368013
Iteration 21143, loss = 0.02367890
Iteration 21144, loss = 0.02367763
Iteration 21145, loss = 0.02367642
Iteration 21146, loss = 0.02367518
Iteration 21147, loss = 0.02367394
Iteration 21148, loss = 0.02367269
Iteration 21149, loss = 0.02367146
Iteration 21150, loss = 0.02367024
Iteration 21151, loss = 0.02366898
Iteration 21152, loss = 0.02366776
Iteration 21153, loss = 0.02366653
Iteration 21154, loss = 0.02366528
Iteration 21155, loss = 0.02366406
Iteration 21156, loss = 0.02366282
Iteration 21157, loss = 0.02366158
Iteration 21158, loss = 0.02366032
Iteration 21159, loss = 0.02365911
Iteration 21160, loss = 0.02365787
Iteration 21161, loss = 0.02365663
Iteration 21162, loss = 0.02365538
Iteration 21163, loss = 0.02365415
Iteration 21164, loss = 0.02365291
Iteration 21165, loss = 0.02365168
Iteration 21166, loss = 0.02365046
Iteration 21167, loss = 0.02364921
Iteration 21168, loss = 0.02364798
Iteration 21169, loss = 0.02364675
Iteration 21170, loss = 0.02364552
Iteration 21171, loss = 0.02364427
Iteration 21172, loss = 0.02364305
Iteration 21173, loss = 0.02364181
Iteration 21174, loss = 0.02364058
Iteration 21175, loss = 0.02363936
Iteration 21176, loss = 0.02363813
Iteration 21177, loss = 0.02363688
Iteration 21178, loss = 0.02363564
Iteration 21179, loss = 0.02363441
Iteration 21180, loss = 0.02363318
Iteration 21181, loss = 0.02363195
Iteration 21182, loss = 0.02363073
Iteration 21183, loss = 0.02362950
Iteration 21184, loss = 0.02362826
Iteration 21185, loss = 0.02362704
Iteration 21186, loss = 0.02362581
Iteration 21187, loss = 0.02362459
Iteration 21188, loss = 0.02362335
Iteration 21189, loss = 0.02362210
Iteration 21190, loss = 0.02362088
Iteration 21191, loss = 0.02361966
Iteration 21192, loss = 0.02361844
Iteration 21193, loss = 0.02361720
Iteration 21194, loss = 0.02361597
Iteration 21195, loss = 0.02361476
Iteration 21196, loss = 0.02361351
Iteration 21197, loss = 0.02361230
Iteration 21198, loss = 0.02361106
Iteration 21199, loss = 0.02360984
Iteration 21200, loss = 0.02360862
Iteration 21201, loss = 0.02360738
Iteration 21202, loss = 0.02360616
Iteration 21203, loss = 0.02360492
Iteration 21204, loss = 0.02360370
Iteration 21205, loss = 0.02360248
Iteration 21206, loss = 0.02360124
Iteration 21207, loss = 0.02360001
Iteration 21208, loss = 0.02359880
Iteration 21209, loss = 0.02359757
Iteration 21210, loss = 0.02359634
Iteration 21211, loss = 0.02359511
Iteration 21212, loss = 0.02359388
Iteration 21213, loss = 0.02359265
Iteration 21214, loss = 0.02359146
Iteration 21215, loss = 0.02359019
Iteration 21216, loss = 0.02358897
Iteration 21217, loss = 0.02358773
Iteration 21218, loss = 0.02358651
Iteration 21219, loss = 0.02358528
Iteration 21220, loss = 0.02358405
Iteration 21221, loss = 0.02358282
Iteration 21222, loss = 0.02358161
Iteration 21223, loss = 0.02358036
Iteration 21224, loss = 0.02357913
Iteration 21225, loss = 0.02357790
Iteration 21226, loss = 0.02357668
Iteration 21227, loss = 0.02357544
Iteration 21228, loss = 0.02357422
Iteration 21229, loss = 0.02357300
Iteration 21230, loss = 0.02357178
Iteration 21231, loss = 0.02357055
Iteration 21232, loss = 0.02356932
Iteration 21233, loss = 0.02356810
Iteration 21234, loss = 0.02356685
Iteration 21235, loss = 0.02356564
Iteration 21236, loss = 0.02356441
Iteration 21237, loss = 0.02356318
Iteration 21238, loss = 0.02356195
Iteration 21239, loss = 0.02356073
Iteration 21240, loss = 0.02355951
Iteration 21241, loss = 0.02355826
Iteration 21242, loss = 0.02355704
Iteration 21243, loss = 0.02355583
Iteration 21244, loss = 0.02355459
Iteration 21245, loss = 0.02355336
Iteration 21246, loss = 0.02355215
Iteration 21247, loss = 0.02355094
Iteration 21248, loss = 0.02354968
Iteration 21249, loss = 0.02354849
Iteration 21250, loss = 0.02354725
Iteration 21251, loss = 0.02354603
Iteration 21252, loss = 0.02354480
Iteration 21253, loss = 0.02354358
Iteration 21254, loss = 0.02354236
Iteration 21255, loss = 0.02354116
Iteration 21256, loss = 0.02353992
Iteration 21257, loss = 0.02353869
Iteration 21258, loss = 0.02353747
Iteration 21259, loss = 0.02353626
Iteration 21260, loss = 0.02353502
Iteration 21261, loss = 0.02353378
Iteration 21262, loss = 0.02353256
Iteration 21263, loss = 0.02353135
Iteration 21264, loss = 0.02353012
Iteration 21265, loss = 0.02352892
Iteration 21266, loss = 0.02352767
Iteration 21267, loss = 0.02352646
Iteration 21268, loss = 0.02352523
Iteration 21269, loss = 0.02352402
Iteration 21270, loss = 0.02352280
Iteration 21271, loss = 0.02352156
Iteration 21272, loss = 0.02352035
Iteration 21273, loss = 0.02351913
Iteration 21274, loss = 0.02351792
Iteration 21275, loss = 0.02351670
Iteration 21276, loss = 0.02351546
Iteration 21277, loss = 0.02351427
Iteration 21278, loss = 0.02351303
Iteration 21279, loss = 0.02351182
Iteration 21280, loss = 0.02351060
Iteration 21281, loss = 0.02350938
Iteration 21282, loss = 0.02350816
Iteration 21283, loss = 0.02350695
Iteration 21284, loss = 0.02350572
Iteration 21285, loss = 0.02350451
Iteration 21286, loss = 0.02350331
Iteration 21287, loss = 0.02350205
Iteration 21288, loss = 0.02350085
Iteration 21289, loss = 0.02349963
Iteration 21290, loss = 0.02349840
Iteration 21291, loss = 0.02349720
Iteration 21292, loss = 0.02349597
Iteration 21293, loss = 0.02349474
Iteration 21294, loss = 0.02349353
Iteration 21295, loss = 0.02349229
Iteration 21296, loss = 0.02349107
Iteration 21297, loss = 0.02348986
Iteration 21298, loss = 0.02348865
Iteration 21299, loss = 0.02348742
Iteration 21300, loss = 0.02348621
Iteration 21301, loss = 0.02348498
Iteration 21302, loss = 0.02348377
Iteration 21303, loss = 0.02348256
Iteration 21304, loss = 0.02348133
Iteration 21305, loss = 0.02348011
Iteration 21306, loss = 0.02347892
Iteration 21307, loss = 0.02347769
Iteration 21308, loss = 0.02347646
Iteration 21309, loss = 0.02347524
Iteration 21310, loss = 0.02347402
Iteration 21311, loss = 0.02347283
Iteration 21312, loss = 0.02347159
Iteration 21313, loss = 0.02347039
Iteration 21314, loss = 0.02346916
Iteration 21315, loss = 0.02346796
Iteration 21316, loss = 0.02346673
Iteration 21317, loss = 0.02346551
Iteration 21318, loss = 0.02346429
Iteration 21319, loss = 0.02346308
Iteration 21320, loss = 0.02346187
Iteration 21321, loss = 0.02346065
Iteration 21322, loss = 0.02345945
Iteration 21323, loss = 0.02345821
Iteration 21324, loss = 0.02345700
Iteration 21325, loss = 0.02345578
Iteration 21326, loss = 0.02345455
Iteration 21327, loss = 0.02345336
Iteration 21328, loss = 0.02345215
Iteration 21329, loss = 0.02345093
Iteration 21330, loss = 0.02344971
Iteration 21331, loss = 0.02344850
Iteration 21332, loss = 0.02344728
Iteration 21333, loss = 0.02344607
Iteration 21334, loss = 0.02344484
Iteration 21335, loss = 0.02344365
Iteration 21336, loss = 0.02344244
Iteration 21337, loss = 0.02344123
Iteration 21338, loss = 0.02344001
Iteration 21339, loss = 0.02343881
Iteration 21340, loss = 0.02343760
Iteration 21341, loss = 0.02343639
Iteration 21342, loss = 0.02343517
Iteration 21343, loss = 0.02343396
Iteration 21344, loss = 0.02343272
Iteration 21345, loss = 0.02343153
Iteration 21346, loss = 0.02343032
Iteration 21347, loss = 0.02342909
Iteration 21348, loss = 0.02342790
Iteration 21349, loss = 0.02342668
Iteration 21350, loss = 0.02342546
Iteration 21351, loss = 0.02342425
Iteration 21352, loss = 0.02342305
Iteration 21353, loss = 0.02342183
Iteration 21354, loss = 0.02342060
Iteration 21355, loss = 0.02341941
Iteration 21356, loss = 0.02341819
Iteration 21357, loss = 0.02341698
Iteration 21358, loss = 0.02341576
Iteration 21359, loss = 0.02341457
Iteration 21360, loss = 0.02341334
Iteration 21361, loss = 0.02341217
Iteration 21362, loss = 0.02341096
Iteration 21363, loss = 0.02340974
Iteration 21364, loss = 0.02340853
Iteration 21365, loss = 0.02340733
Iteration 21366, loss = 0.02340612
Iteration 21367, loss = 0.02340492
Iteration 21368, loss = 0.02340371
Iteration 21369, loss = 0.02340250
Iteration 21370, loss = 0.02340130
Iteration 21371, loss = 0.02340010
Iteration 21372, loss = 0.02339888
Iteration 21373, loss = 0.02339768
Iteration 21374, loss = 0.02339646
Iteration 21375, loss = 0.02339528
Iteration 21376, loss = 0.02339406
Iteration 21377, loss = 0.02339284
Iteration 21378, loss = 0.02339164
Iteration 21379, loss = 0.02339044
Iteration 21380, loss = 0.02338922
Iteration 21381, loss = 0.02338803
Iteration 21382, loss = 0.02338683
Iteration 21383, loss = 0.02338563
Iteration 21384, loss = 0.02338442
Iteration 21385, loss = 0.02338321
Iteration 21386, loss = 0.02338201
Iteration 21387, loss = 0.02338083
Iteration 21388, loss = 0.02337962
Iteration 21389, loss = 0.02337840
Iteration 21390, loss = 0.02337721
Iteration 21391, loss = 0.02337600
Iteration 21392, loss = 0.02337481
Iteration 21393, loss = 0.02337358
Iteration 21394, loss = 0.02337239
Iteration 21395, loss = 0.02337118
Iteration 21396, loss = 0.02336997
Iteration 21397, loss = 0.02336877
Iteration 21398, loss = 0.02336756
Iteration 21399, loss = 0.02336635
Iteration 21400, loss = 0.02336516
Iteration 21401, loss = 0.02336395
Iteration 21402, loss = 0.02336274
Iteration 21403, loss = 0.02336154
Iteration 21404, loss = 0.02336032
Iteration 21405, loss = 0.02335913
Iteration 21406, loss = 0.02335791
Iteration 21407, loss = 0.02335671
Iteration 21408, loss = 0.02335550
Iteration 21409, loss = 0.02335431
Iteration 21410, loss = 0.02335309
Iteration 21411, loss = 0.02335188
Iteration 21412, loss = 0.02335068
Iteration 21413, loss = 0.02334950
Iteration 21414, loss = 0.02334829
Iteration 21415, loss = 0.02334706
Iteration 21416, loss = 0.02334586
Iteration 21417, loss = 0.02334467
Iteration 21418, loss = 0.02334345
Iteration 21419, loss = 0.02334226
Iteration 21420, loss = 0.02334105
Iteration 21421, loss = 0.02333985
Iteration 21422, loss = 0.02333863
Iteration 21423, loss = 0.02333743
Iteration 21424, loss = 0.02333623
Iteration 21425, loss = 0.02333503
Iteration 21426, loss = 0.02333382
Iteration 21427, loss = 0.02333262
Iteration 21428, loss = 0.02333141
Iteration 21429, loss = 0.02333021
Iteration 21430, loss = 0.02332901
Iteration 21431, loss = 0.02332780
Iteration 21432, loss = 0.02332661
Iteration 21433, loss = 0.02332540
Iteration 21434, loss = 0.02332421
Iteration 21435, loss = 0.02332300
Iteration 21436, loss = 0.02332180
Iteration 21437, loss = 0.02332059
Iteration 21438, loss = 0.02331940
Iteration 21439, loss = 0.02331819
Iteration 21440, loss = 0.02331698
Iteration 21441, loss = 0.02331578
Iteration 21442, loss = 0.02331459
Iteration 21443, loss = 0.02331338
Iteration 21444, loss = 0.02331218
Iteration 21445, loss = 0.02331097
Iteration 21446, loss = 0.02330978
Iteration 21447, loss = 0.02330858
Iteration 21448, loss = 0.02330738
Iteration 21449, loss = 0.02330618
Iteration 21450, loss = 0.02330497
Iteration 21451, loss = 0.02330377
Iteration 21452, loss = 0.02330258
Iteration 21453, loss = 0.02330138
Iteration 21454, loss = 0.02330020
Iteration 21455, loss = 0.02329897
Iteration 21456, loss = 0.02329777
Iteration 21457, loss = 0.02329657
Iteration 21458, loss = 0.02329538
Iteration 21459, loss = 0.02329417
Iteration 21460, loss = 0.02329300
Iteration 21461, loss = 0.02329179
Iteration 21462, loss = 0.02329059
Iteration 21463, loss = 0.02328938
Iteration 21464, loss = 0.02328818
Iteration 21465, loss = 0.02328699
Iteration 21466, loss = 0.02328577
Iteration 21467, loss = 0.02328458
Iteration 21468, loss = 0.02328337
Iteration 21469, loss = 0.02328218
Iteration 21470, loss = 0.02328099
Iteration 21471, loss = 0.02327978
Iteration 21472, loss = 0.02327856
Iteration 21473, loss = 0.02327738
Iteration 21474, loss = 0.02327617
Iteration 21475, loss = 0.02327499
Iteration 21476, loss = 0.02327378
Iteration 21477, loss = 0.02327260
Iteration 21478, loss = 0.02327139
Iteration 21479, loss = 0.02327017
Iteration 21480, loss = 0.02326900
Iteration 21481, loss = 0.02326782
Iteration 21482, loss = 0.02326660
Iteration 21483, loss = 0.02326541
Iteration 21484, loss = 0.02326421
Iteration 21485, loss = 0.02326301
Iteration 21486, loss = 0.02326182
Iteration 21487, loss = 0.02326063
Iteration 21488, loss = 0.02325944
Iteration 21489, loss = 0.02325823
Iteration 21490, loss = 0.02325703
Iteration 21491, loss = 0.02325584
Iteration 21492, loss = 0.02325466
Iteration 21493, loss = 0.02325346
Iteration 21494, loss = 0.02325226
Iteration 21495, loss = 0.02325107
Iteration 21496, loss = 0.02324987
Iteration 21497, loss = 0.02324866
Iteration 21498, loss = 0.02324749
Iteration 21499, loss = 0.02324629
Iteration 21500, loss = 0.02324509
Iteration 21501, loss = 0.02324390
Iteration 21502, loss = 0.02324270
Iteration 21503, loss = 0.02324151
Iteration 21504, loss = 0.02324033
Iteration 21505, loss = 0.02323911
Iteration 21506, loss = 0.02323793
Iteration 21507, loss = 0.02323674
Iteration 21508, loss = 0.02323554
Iteration 21509, loss = 0.02323436
Iteration 21510, loss = 0.02323316
Iteration 21511, loss = 0.02323195
Iteration 21512, loss = 0.02323078
Iteration 21513, loss = 0.02322958
Iteration 21514, loss = 0.02322838
Iteration 21515, loss = 0.02322717
Iteration 21516, loss = 0.02322600
Iteration 21517, loss = 0.02322480
Iteration 21518, loss = 0.02322360
Iteration 21519, loss = 0.02322241
Iteration 21520, loss = 0.02322121
Iteration 21521, loss = 0.02322002
Iteration 21522, loss = 0.02321885
Iteration 21523, loss = 0.02321764
Iteration 21524, loss = 0.02321646
Iteration 21525, loss = 0.02321525
Iteration 21526, loss = 0.02321406
Iteration 21527, loss = 0.02321287
Iteration 21528, loss = 0.02321171
Iteration 21529, loss = 0.02321049
Iteration 21530, loss = 0.02320930
Iteration 21531, loss = 0.02320810
Iteration 21532, loss = 0.02320692
Iteration 21533, loss = 0.02320574
Iteration 21534, loss = 0.02320454
Iteration 21535, loss = 0.02320335
Iteration 21536, loss = 0.02320216
Iteration 21537, loss = 0.02320098
Iteration 21538, loss = 0.02319979
Iteration 21539, loss = 0.02319860
Iteration 21540, loss = 0.02319741
Iteration 21541, loss = 0.02319622
Iteration 21542, loss = 0.02319504
Iteration 21543, loss = 0.02319385
Iteration 21544, loss = 0.02319266
Iteration 21545, loss = 0.02319148
Iteration 21546, loss = 0.02319030
Iteration 21547, loss = 0.02318910
Iteration 21548, loss = 0.02318793
Iteration 21549, loss = 0.02318673
Iteration 21550, loss = 0.02318554
Iteration 21551, loss = 0.02318435
Iteration 21552, loss = 0.02318318
Iteration 21553, loss = 0.02318198
Iteration 21554, loss = 0.02318080
Iteration 21555, loss = 0.02317962
Iteration 21556, loss = 0.02317843
Iteration 21557, loss = 0.02317726
Iteration 21558, loss = 0.02317606
Iteration 21559, loss = 0.02317487
Iteration 21560, loss = 0.02317369
Iteration 21561, loss = 0.02317250
Iteration 21562, loss = 0.02317131
Iteration 21563, loss = 0.02317012
Iteration 21564, loss = 0.02316895
Iteration 21565, loss = 0.02316775
Iteration 21566, loss = 0.02316656
Iteration 21567, loss = 0.02316537
Iteration 21568, loss = 0.02316418
Iteration 21569, loss = 0.02316301
Iteration 21570, loss = 0.02316181
Iteration 21571, loss = 0.02316062
Iteration 21572, loss = 0.02315943
Iteration 21573, loss = 0.02315825
Iteration 21574, loss = 0.02315707
Iteration 21575, loss = 0.02315588
Iteration 21576, loss = 0.02315472
Iteration 21577, loss = 0.02315350
Iteration 21578, loss = 0.02315234
Iteration 21579, loss = 0.02315113
Iteration 21580, loss = 0.02314995
Iteration 21581, loss = 0.02314877
Iteration 21582, loss = 0.02314757
Iteration 21583, loss = 0.02314641
Iteration 21584, loss = 0.02314520
Iteration 21585, loss = 0.02314404
Iteration 21586, loss = 0.02314283
Iteration 21587, loss = 0.02314165
Iteration 21588, loss = 0.02314046
Iteration 21589, loss = 0.02313928
Iteration 21590, loss = 0.02313812
Iteration 21591, loss = 0.02313691
Iteration 21592, loss = 0.02313573
Iteration 21593, loss = 0.02313455
Iteration 21594, loss = 0.02313336
Iteration 21595, loss = 0.02313218
Iteration 21596, loss = 0.02313100
Iteration 21597, loss = 0.02312982
Iteration 21598, loss = 0.02312863
Iteration 21599, loss = 0.02312746
Iteration 21600, loss = 0.02312628
Iteration 21601, loss = 0.02312508
Iteration 21602, loss = 0.02312390
Iteration 21603, loss = 0.02312272
Iteration 21604, loss = 0.02312155
Iteration 21605, loss = 0.02312036
Iteration 21606, loss = 0.02311917
Iteration 21607, loss = 0.02311799
Iteration 21608, loss = 0.02311682
Iteration 21609, loss = 0.02311564
Iteration 21610, loss = 0.02311446
Iteration 21611, loss = 0.02311328
Iteration 21612, loss = 0.02311210
Iteration 21613, loss = 0.02311092
Iteration 21614, loss = 0.02310974
Iteration 21615, loss = 0.02310856
Iteration 21616, loss = 0.02310739
Iteration 21617, loss = 0.02310620
Iteration 21618, loss = 0.02310504
Iteration 21619, loss = 0.02310384
Iteration 21620, loss = 0.02310267
Iteration 21621, loss = 0.02310150
Iteration 21622, loss = 0.02310032
Iteration 21623, loss = 0.02309916
Iteration 21624, loss = 0.02309795
Iteration 21625, loss = 0.02309680
Iteration 21626, loss = 0.02309561
Iteration 21627, loss = 0.02309442
Iteration 21628, loss = 0.02309324
Iteration 21629, loss = 0.02309207
Iteration 21630, loss = 0.02309088
Iteration 21631, loss = 0.02308972
Iteration 21632, loss = 0.02308854
Iteration 21633, loss = 0.02308736
Iteration 21634, loss = 0.02308619
Iteration 21635, loss = 0.02308501
Iteration 21636, loss = 0.02308383
Iteration 21637, loss = 0.02308265
Iteration 21638, loss = 0.02308148
Iteration 21639, loss = 0.02308030
Iteration 21640, loss = 0.02307913
Iteration 21641, loss = 0.02307794
Iteration 21642, loss = 0.02307679
Iteration 21643, loss = 0.02307559
Iteration 21644, loss = 0.02307441
Iteration 21645, loss = 0.02307324
Iteration 21646, loss = 0.02307205
Iteration 21647, loss = 0.02307087
Iteration 21648, loss = 0.02306971
Iteration 21649, loss = 0.02306852
Iteration 21650, loss = 0.02306734
Iteration 21651, loss = 0.02306615
Iteration 21652, loss = 0.02306498
Iteration 21653, loss = 0.02306381
Iteration 21654, loss = 0.02306261
Iteration 21655, loss = 0.02306145
Iteration 21656, loss = 0.02306027
Iteration 21657, loss = 0.02305909
Iteration 21658, loss = 0.02305792
Iteration 21659, loss = 0.02305675
Iteration 21660, loss = 0.02305557
Iteration 21661, loss = 0.02305440
Iteration 21662, loss = 0.02305323
Iteration 21663, loss = 0.02305206
Iteration 21664, loss = 0.02305089
Iteration 21665, loss = 0.02304972
Iteration 21666, loss = 0.02304854
Iteration 21667, loss = 0.02304737
Iteration 21668, loss = 0.02304619
Iteration 21669, loss = 0.02304501
Iteration 21670, loss = 0.02304384
Iteration 21671, loss = 0.02304268
Iteration 21672, loss = 0.02304150
Iteration 21673, loss = 0.02304032
Iteration 21674, loss = 0.02303916
Iteration 21675, loss = 0.02303797
Iteration 21676, loss = 0.02303680
Iteration 21677, loss = 0.02303562
Iteration 21678, loss = 0.02303445
Iteration 21679, loss = 0.02303328
Iteration 21680, loss = 0.02303209
Iteration 21681, loss = 0.02303094
Iteration 21682, loss = 0.02302976
Iteration 21683, loss = 0.02302858
Iteration 21684, loss = 0.02302740
Iteration 21685, loss = 0.02302623
Iteration 21686, loss = 0.02302507
Iteration 21687, loss = 0.02302389
Iteration 21688, loss = 0.02302272
Iteration 21689, loss = 0.02302154
Iteration 21690, loss = 0.02302036
Iteration 21691, loss = 0.02301919
Iteration 21692, loss = 0.02301802
Iteration 21693, loss = 0.02301685
Iteration 21694, loss = 0.02301567
Iteration 21695, loss = 0.02301450
Iteration 21696, loss = 0.02301330
Iteration 21697, loss = 0.02301213
Iteration 21698, loss = 0.02301096
Iteration 21699, loss = 0.02300979
Iteration 21700, loss = 0.02300861
Iteration 21701, loss = 0.02300746
Iteration 21702, loss = 0.02300628
Iteration 21703, loss = 0.02300510
Iteration 21704, loss = 0.02300393
Iteration 21705, loss = 0.02300276
Iteration 21706, loss = 0.02300160
Iteration 21707, loss = 0.02300042
Iteration 21708, loss = 0.02299925
Iteration 21709, loss = 0.02299809
Iteration 21710, loss = 0.02299692
Iteration 21711, loss = 0.02299573
Iteration 21712, loss = 0.02299457
Iteration 21713, loss = 0.02299339
Iteration 21714, loss = 0.02299223
Iteration 21715, loss = 0.02299107
Iteration 21716, loss = 0.02298989
Iteration 21717, loss = 0.02298871
Iteration 21718, loss = 0.02298754
Iteration 21719, loss = 0.02298638
Iteration 21720, loss = 0.02298521
Iteration 21721, loss = 0.02298403
Iteration 21722, loss = 0.02298285
Iteration 21723, loss = 0.02298169
Iteration 21724, loss = 0.02298053
Iteration 21725, loss = 0.02297936
Iteration 21726, loss = 0.02297819
Iteration 21727, loss = 0.02297701
Iteration 21728, loss = 0.02297586
Iteration 21729, loss = 0.02297469
Iteration 21730, loss = 0.02297351
Iteration 21731, loss = 0.02297234
Iteration 21732, loss = 0.02297118
Iteration 21733, loss = 0.02297000
Iteration 21734, loss = 0.02296884
Iteration 21735, loss = 0.02296768
Iteration 21736, loss = 0.02296650
Iteration 21737, loss = 0.02296535
Iteration 21738, loss = 0.02296417
Iteration 21739, loss = 0.02296299
Iteration 21740, loss = 0.02296183
Iteration 21741, loss = 0.02296066
Iteration 21742, loss = 0.02295950
Iteration 21743, loss = 0.02295832
Iteration 21744, loss = 0.02295716
Iteration 21745, loss = 0.02295599
Iteration 21746, loss = 0.02295482
Iteration 21747, loss = 0.02295366
Iteration 21748, loss = 0.02295250
Iteration 21749, loss = 0.02295133
Iteration 21750, loss = 0.02295017
Iteration 21751, loss = 0.02294901
Iteration 21752, loss = 0.02294785
Iteration 21753, loss = 0.02294668
Iteration 21754, loss = 0.02294553
Iteration 21755, loss = 0.02294435
Iteration 21756, loss = 0.02294320
Iteration 21757, loss = 0.02294202
Iteration 21758, loss = 0.02294085
Iteration 21759, loss = 0.02293970
Iteration 21760, loss = 0.02293851
Iteration 21761, loss = 0.02293737
Iteration 21762, loss = 0.02293620
Iteration 21763, loss = 0.02293504
Iteration 21764, loss = 0.02293387
Iteration 21765, loss = 0.02293269
Iteration 21766, loss = 0.02293155
Iteration 21767, loss = 0.02293037
Iteration 21768, loss = 0.02292919
Iteration 21769, loss = 0.02292802
Iteration 21770, loss = 0.02292686
Iteration 21771, loss = 0.02292570
Iteration 21772, loss = 0.02292455
Iteration 21773, loss = 0.02292339
Iteration 21774, loss = 0.02292220
Iteration 21775, loss = 0.02292105
Iteration 21776, loss = 0.02291988
Iteration 21777, loss = 0.02291871
Iteration 21778, loss = 0.02291755
Iteration 21779, loss = 0.02291639
Iteration 21780, loss = 0.02291523
Iteration 21781, loss = 0.02291405
Iteration 21782, loss = 0.02291291
Iteration 21783, loss = 0.02291175
Iteration 21784, loss = 0.02291058
Iteration 21785, loss = 0.02290941
Iteration 21786, loss = 0.02290826
Iteration 21787, loss = 0.02290710
Iteration 21788, loss = 0.02290593
Iteration 21789, loss = 0.02290476
Iteration 21790, loss = 0.02290360
Iteration 21791, loss = 0.02290244
Iteration 21792, loss = 0.02290128
Iteration 21793, loss = 0.02290012
Iteration 21794, loss = 0.02289896
Iteration 21795, loss = 0.02289780
Iteration 21796, loss = 0.02289663
Iteration 21797, loss = 0.02289548
Iteration 21798, loss = 0.02289430
Iteration 21799, loss = 0.02289315
Iteration 21800, loss = 0.02289200
Iteration 21801, loss = 0.02289082
Iteration 21802, loss = 0.02288967
Iteration 21803, loss = 0.02288850
Iteration 21804, loss = 0.02288735
Iteration 21805, loss = 0.02288619
Iteration 21806, loss = 0.02288502
Iteration 21807, loss = 0.02288386
Iteration 21808, loss = 0.02288269
Iteration 21809, loss = 0.02288152
Iteration 21810, loss = 0.02288037
Iteration 21811, loss = 0.02287919
Iteration 21812, loss = 0.02287803
Iteration 21813, loss = 0.02287686
Iteration 21814, loss = 0.02287570
Iteration 21815, loss = 0.02287453
Iteration 21816, loss = 0.02287339
Iteration 21817, loss = 0.02287221
Iteration 21818, loss = 0.02287104
Iteration 21819, loss = 0.02286990
Iteration 21820, loss = 0.02286873
Iteration 21821, loss = 0.02286755
Iteration 21822, loss = 0.02286642
Iteration 21823, loss = 0.02286525
Iteration 21824, loss = 0.02286407
Iteration 21825, loss = 0.02286294
Iteration 21826, loss = 0.02286179
Iteration 21827, loss = 0.02286062
Iteration 21828, loss = 0.02285947
Iteration 21829, loss = 0.02285831
Iteration 21830, loss = 0.02285717
Iteration 21831, loss = 0.02285601
Iteration 21832, loss = 0.02285485
Iteration 21833, loss = 0.02285370
Iteration 21834, loss = 0.02285255
Iteration 21835, loss = 0.02285139
Iteration 21836, loss = 0.02285024
Iteration 21837, loss = 0.02284907
Iteration 21838, loss = 0.02284793
Iteration 21839, loss = 0.02284676
Iteration 21840, loss = 0.02284560
Iteration 21841, loss = 0.02284445
Iteration 21842, loss = 0.02284331
Iteration 21843, loss = 0.02284213
Iteration 21844, loss = 0.02284098
Iteration 21845, loss = 0.02283983
Iteration 21846, loss = 0.02283867
Iteration 21847, loss = 0.02283752
Iteration 21848, loss = 0.02283638
Iteration 21849, loss = 0.02283522
Iteration 21850, loss = 0.02283407
Iteration 21851, loss = 0.02283290
Iteration 21852, loss = 0.02283176
Iteration 21853, loss = 0.02283059
Iteration 21854, loss = 0.02282944
Iteration 21855, loss = 0.02282830
Iteration 21856, loss = 0.02282715
Iteration 21857, loss = 0.02282598
Iteration 21858, loss = 0.02282483
Iteration 21859, loss = 0.02282367
Iteration 21860, loss = 0.02282252
Iteration 21861, loss = 0.02282136
Iteration 21862, loss = 0.02282020
Iteration 21863, loss = 0.02281904
Iteration 21864, loss = 0.02281790
Iteration 21865, loss = 0.02281674
Iteration 21866, loss = 0.02281560
Iteration 21867, loss = 0.02281442
Iteration 21868, loss = 0.02281328
Iteration 21869, loss = 0.02281213
Iteration 21870, loss = 0.02281097
Iteration 21871, loss = 0.02280981
Iteration 21872, loss = 0.02280867
Iteration 21873, loss = 0.02280751
Iteration 21874, loss = 0.02280636
Iteration 21875, loss = 0.02280520
Iteration 21876, loss = 0.02280406
Iteration 21877, loss = 0.02280290
Iteration 21878, loss = 0.02280175
Iteration 21879, loss = 0.02280059
Iteration 21880, loss = 0.02279943
Iteration 21881, loss = 0.02279827
Iteration 21882, loss = 0.02279713
Iteration 21883, loss = 0.02279598
Iteration 21884, loss = 0.02279483
Iteration 21885, loss = 0.02279367
Iteration 21886, loss = 0.02279253
Iteration 21887, loss = 0.02279137
Iteration 21888, loss = 0.02279021
Iteration 21889, loss = 0.02278906
Iteration 21890, loss = 0.02278792
Iteration 21891, loss = 0.02278676
Iteration 21892, loss = 0.02278561
Iteration 21893, loss = 0.02278447
Iteration 21894, loss = 0.02278332
Iteration 21895, loss = 0.02278217
Iteration 21896, loss = 0.02278103
Iteration 21897, loss = 0.02277988
Iteration 21898, loss = 0.02277874
Iteration 21899, loss = 0.02277758
Iteration 21900, loss = 0.02277644
Iteration 21901, loss = 0.02277529
Iteration 21902, loss = 0.02277415
Iteration 21903, loss = 0.02277299
Iteration 21904, loss = 0.02277183
Iteration 21905, loss = 0.02277069
Iteration 21906, loss = 0.02276953
Iteration 21907, loss = 0.02276838
Iteration 21908, loss = 0.02276723
Iteration 21909, loss = 0.02276608
Iteration 21910, loss = 0.02276495
Iteration 21911, loss = 0.02276379
Iteration 21912, loss = 0.02276265
Iteration 21913, loss = 0.02276151
Iteration 21914, loss = 0.02276035
Iteration 21915, loss = 0.02275919
Iteration 21916, loss = 0.02275804
Iteration 21917, loss = 0.02275690
Iteration 21918, loss = 0.02275575
Iteration 21919, loss = 0.02275460
Iteration 21920, loss = 0.02275345
Iteration 21921, loss = 0.02275231
Iteration 21922, loss = 0.02275114
Iteration 21923, loss = 0.02275001
Iteration 21924, loss = 0.02274885
Iteration 21925, loss = 0.02274772
Iteration 21926, loss = 0.02274657
Iteration 21927, loss = 0.02274544
Iteration 21928, loss = 0.02274426
Iteration 21929, loss = 0.02274313
Iteration 21930, loss = 0.02274198
Iteration 21931, loss = 0.02274084
Iteration 21932, loss = 0.02273970
Iteration 21933, loss = 0.02273854
Iteration 21934, loss = 0.02273740
Iteration 21935, loss = 0.02273626
Iteration 21936, loss = 0.02273512
Iteration 21937, loss = 0.02273395
Iteration 21938, loss = 0.02273283
Iteration 21939, loss = 0.02273168
Iteration 21940, loss = 0.02273054
Iteration 21941, loss = 0.02272939
Iteration 21942, loss = 0.02272824
Iteration 21943, loss = 0.02272712
Iteration 21944, loss = 0.02272596
Iteration 21945, loss = 0.02272481
Iteration 21946, loss = 0.02272370
Iteration 21947, loss = 0.02272254
Iteration 21948, loss = 0.02272138
Iteration 21949, loss = 0.02272026
Iteration 21950, loss = 0.02271910
Iteration 21951, loss = 0.02271796
Iteration 21952, loss = 0.02271682
Iteration 21953, loss = 0.02271568
Iteration 21954, loss = 0.02271452
Iteration 21955, loss = 0.02271338
Iteration 21956, loss = 0.02271224
Iteration 21957, loss = 0.02271109
Iteration 21958, loss = 0.02270995
Iteration 21959, loss = 0.02270880
Iteration 21960, loss = 0.02270768
Iteration 21961, loss = 0.02270654
Iteration 21962, loss = 0.02270538
Iteration 21963, loss = 0.02270425
Iteration 21964, loss = 0.02270310
Iteration 21965, loss = 0.02270197
Iteration 21966, loss = 0.02270082
Iteration 21967, loss = 0.02269968
Iteration 21968, loss = 0.02269853
Iteration 21969, loss = 0.02269740
Iteration 21970, loss = 0.02269626
Iteration 21971, loss = 0.02269511
Iteration 21972, loss = 0.02269397
Iteration 21973, loss = 0.02269284
Iteration 21974, loss = 0.02269170
Iteration 21975, loss = 0.02269054
Iteration 21976, loss = 0.02268940
Iteration 21977, loss = 0.02268826
Iteration 21978, loss = 0.02268712
Iteration 21979, loss = 0.02268597
Iteration 21980, loss = 0.02268482
Iteration 21981, loss = 0.02268370
Iteration 21982, loss = 0.02268255
Iteration 21983, loss = 0.02268142
Iteration 21984, loss = 0.02268026
Iteration 21985, loss = 0.02267913
Iteration 21986, loss = 0.02267799
Iteration 21987, loss = 0.02267683
Iteration 21988, loss = 0.02267572
Iteration 21989, loss = 0.02267457
Iteration 21990, loss = 0.02267344
Iteration 21991, loss = 0.02267228
Iteration 21992, loss = 0.02267114
Iteration 21993, loss = 0.02267000
Iteration 21994, loss = 0.02266889
Iteration 21995, loss = 0.02266774
Iteration 21996, loss = 0.02266660
Iteration 21997, loss = 0.02266546
Iteration 21998, loss = 0.02266433
Iteration 21999, loss = 0.02266318
Iteration 22000, loss = 0.02266207
Iteration 22001, loss = 0.02266090
Iteration 22002, loss = 0.02265978
Iteration 22003, loss = 0.02265862
Iteration 22004, loss = 0.02265750
Iteration 22005, loss = 0.02265637
Iteration 22006, loss = 0.02265521
Iteration 22007, loss = 0.02265407
Iteration 22008, loss = 0.02265296
Iteration 22009, loss = 0.02265182
Iteration 22010, loss = 0.02265067
Iteration 22011, loss = 0.02264953
Iteration 22012, loss = 0.02264838
Iteration 22013, loss = 0.02264726
Iteration 22014, loss = 0.02264611
Iteration 22015, loss = 0.02264497
Iteration 22016, loss = 0.02264383
Iteration 22017, loss = 0.02264268
Iteration 22018, loss = 0.02264155
Iteration 22019, loss = 0.02264041
Iteration 22020, loss = 0.02263928
Iteration 22021, loss = 0.02263813
Iteration 22022, loss = 0.02263700
Iteration 22023, loss = 0.02263587
Iteration 22024, loss = 0.02263473
Iteration 22025, loss = 0.02263359
Iteration 22026, loss = 0.02263245
Iteration 22027, loss = 0.02263132
Iteration 22028, loss = 0.02263018
Iteration 22029, loss = 0.02262906
Iteration 22030, loss = 0.02262791
Iteration 22031, loss = 0.02262679
Iteration 22032, loss = 0.02262566
Iteration 22033, loss = 0.02262453
Iteration 22034, loss = 0.02262339
Iteration 22035, loss = 0.02262226
Iteration 22036, loss = 0.02262113
Iteration 22037, loss = 0.02262000
Iteration 22038, loss = 0.02261888
Iteration 22039, loss = 0.02261773
Iteration 22040, loss = 0.02261661
Iteration 22041, loss = 0.02261546
Iteration 22042, loss = 0.02261434
Iteration 22043, loss = 0.02261320
Iteration 22044, loss = 0.02261208
Iteration 22045, loss = 0.02261093
Iteration 22046, loss = 0.02260981
Iteration 22047, loss = 0.02260867
Iteration 22048, loss = 0.02260754
Iteration 22049, loss = 0.02260639
Iteration 22050, loss = 0.02260528
Iteration 22051, loss = 0.02260414
Iteration 22052, loss = 0.02260300
Iteration 22053, loss = 0.02260188
Iteration 22054, loss = 0.02260075
Iteration 22055, loss = 0.02259961
Iteration 22056, loss = 0.02259851
Iteration 22057, loss = 0.02259733
Iteration 22058, loss = 0.02259621
Iteration 22059, loss = 0.02259509
Iteration 22060, loss = 0.02259394
Iteration 22061, loss = 0.02259282
Iteration 22062, loss = 0.02259167
Iteration 22063, loss = 0.02259053
Iteration 22064, loss = 0.02258940
Iteration 22065, loss = 0.02258826
Iteration 22066, loss = 0.02258714
Iteration 22067, loss = 0.02258598
Iteration 22068, loss = 0.02258485
Iteration 22069, loss = 0.02258372
Iteration 22070, loss = 0.02258258
Iteration 22071, loss = 0.02258145
Iteration 22072, loss = 0.02258033
Iteration 22073, loss = 0.02257917
Iteration 22074, loss = 0.02257805
Iteration 22075, loss = 0.02257693
Iteration 22076, loss = 0.02257579
Iteration 22077, loss = 0.02257468
Iteration 22078, loss = 0.02257355
Iteration 22079, loss = 0.02257240
Iteration 22080, loss = 0.02257128
Iteration 22081, loss = 0.02257014
Iteration 22082, loss = 0.02256902
Iteration 22083, loss = 0.02256789
Iteration 22084, loss = 0.02256677
Iteration 22085, loss = 0.02256564
Iteration 22086, loss = 0.02256449
Iteration 22087, loss = 0.02256337
Iteration 22088, loss = 0.02256224
Iteration 22089, loss = 0.02256111
Iteration 22090, loss = 0.02255997
Iteration 22091, loss = 0.02255886
Iteration 22092, loss = 0.02255772
Iteration 22093, loss = 0.02255658
Iteration 22094, loss = 0.02255545
Iteration 22095, loss = 0.02255433
Iteration 22096, loss = 0.02255319
Iteration 22097, loss = 0.02255206
Iteration 22098, loss = 0.02255093
Iteration 22099, loss = 0.02254980
Iteration 22100, loss = 0.02254867
Iteration 22101, loss = 0.02254753
Iteration 22102, loss = 0.02254640
Iteration 22103, loss = 0.02254526
Iteration 22104, loss = 0.02254413
Iteration 22105, loss = 0.02254301
Iteration 22106, loss = 0.02254187
Iteration 22107, loss = 0.02254076
Iteration 22108, loss = 0.02253962
Iteration 22109, loss = 0.02253850
Iteration 22110, loss = 0.02253737
Iteration 22111, loss = 0.02253623
Iteration 22112, loss = 0.02253510
Iteration 22113, loss = 0.02253398
Iteration 22114, loss = 0.02253286
Iteration 22115, loss = 0.02253173
Iteration 22116, loss = 0.02253061
Iteration 22117, loss = 0.02252947
Iteration 22118, loss = 0.02252834
Iteration 22119, loss = 0.02252722
Iteration 22120, loss = 0.02252609
Iteration 22121, loss = 0.02252498
Iteration 22122, loss = 0.02252386
Iteration 22123, loss = 0.02252271
Iteration 22124, loss = 0.02252159
Iteration 22125, loss = 0.02252047
Iteration 22126, loss = 0.02251932
Iteration 22127, loss = 0.02251820
Iteration 22128, loss = 0.02251707
Iteration 22129, loss = 0.02251594
Iteration 22130, loss = 0.02251482
Iteration 22131, loss = 0.02251368
Iteration 22132, loss = 0.02251256
Iteration 22133, loss = 0.02251143
Iteration 22134, loss = 0.02251030
Iteration 22135, loss = 0.02250916
Iteration 22136, loss = 0.02250804
Iteration 22137, loss = 0.02250691
Iteration 22138, loss = 0.02250579
Iteration 22139, loss = 0.02250467
Iteration 22140, loss = 0.02250353
Iteration 22141, loss = 0.02250240
Iteration 22142, loss = 0.02250129
Iteration 22143, loss = 0.02250015
Iteration 22144, loss = 0.02249904
Iteration 22145, loss = 0.02249792
Iteration 22146, loss = 0.02249678
Iteration 22147, loss = 0.02249565
Iteration 22148, loss = 0.02249454
Iteration 22149, loss = 0.02249339
Iteration 22150, loss = 0.02249227
Iteration 22151, loss = 0.02249114
Iteration 22152, loss = 0.02249001
Iteration 22153, loss = 0.02248890
Iteration 22154, loss = 0.02248777
Iteration 22155, loss = 0.02248664
Iteration 22156, loss = 0.02248550
Iteration 22157, loss = 0.02248438
Iteration 22158, loss = 0.02248324
Iteration 22159, loss = 0.02248213
Iteration 22160, loss = 0.02248100
Iteration 22161, loss = 0.02247988
Iteration 22162, loss = 0.02247876
Iteration 22163, loss = 0.02247763
Iteration 22164, loss = 0.02247650
Iteration 22165, loss = 0.02247539
Iteration 22166, loss = 0.02247425
Iteration 22167, loss = 0.02247314
Iteration 22168, loss = 0.02247200
Iteration 22169, loss = 0.02247088
Iteration 22170, loss = 0.02246977
Iteration 22171, loss = 0.02246866
Iteration 22172, loss = 0.02246753
Iteration 22173, loss = 0.02246641
Iteration 22174, loss = 0.02246528
Iteration 22175, loss = 0.02246417
Iteration 22176, loss = 0.02246304
Iteration 22177, loss = 0.02246193
Iteration 22178, loss = 0.02246079
Iteration 22179, loss = 0.02245968
Iteration 22180, loss = 0.02245855
Iteration 22181, loss = 0.02245744
Iteration 22182, loss = 0.02245632
Iteration 22183, loss = 0.02245520
Iteration 22184, loss = 0.02245406
Iteration 22185, loss = 0.02245296
Iteration 22186, loss = 0.02245184
Iteration 22187, loss = 0.02245072
Iteration 22188, loss = 0.02244959
Iteration 22189, loss = 0.02244848
Iteration 22190, loss = 0.02244734
Iteration 22191, loss = 0.02244622
Iteration 22192, loss = 0.02244511
Iteration 22193, loss = 0.02244397
Iteration 22194, loss = 0.02244283
Iteration 22195, loss = 0.02244173
Iteration 22196, loss = 0.02244060
Iteration 22197, loss = 0.02243948
Iteration 22198, loss = 0.02243836
Iteration 22199, loss = 0.02243722
Iteration 22200, loss = 0.02243610
Iteration 22201, loss = 0.02243498
Iteration 22202, loss = 0.02243387
Iteration 22203, loss = 0.02243275
Iteration 22204, loss = 0.02243163
Iteration 22205, loss = 0.02243053
Iteration 22206, loss = 0.02242939
Iteration 22207, loss = 0.02242827
Iteration 22208, loss = 0.02242714
Iteration 22209, loss = 0.02242603
Iteration 22210, loss = 0.02242491
Iteration 22211, loss = 0.02242380
Iteration 22212, loss = 0.02242267
Iteration 22213, loss = 0.02242154
Iteration 22214, loss = 0.02242042
Iteration 22215, loss = 0.02241932
Iteration 22216, loss = 0.02241820
Iteration 22217, loss = 0.02241706
Iteration 22218, loss = 0.02241596
Iteration 22219, loss = 0.02241483
Iteration 22220, loss = 0.02241371
Iteration 22221, loss = 0.02241260
Iteration 22222, loss = 0.02241147
Iteration 22223, loss = 0.02241036
Iteration 22224, loss = 0.02240924
Iteration 22225, loss = 0.02240813
Iteration 22226, loss = 0.02240700
Iteration 22227, loss = 0.02240588
Iteration 22228, loss = 0.02240478
Iteration 22229, loss = 0.02240366
Iteration 22230, loss = 0.02240253
Iteration 22231, loss = 0.02240142
Iteration 22232, loss = 0.02240030
Iteration 22233, loss = 0.02239919
Iteration 22234, loss = 0.02239807
Iteration 22235, loss = 0.02239695
Iteration 22236, loss = 0.02239584
Iteration 22237, loss = 0.02239472
Iteration 22238, loss = 0.02239361
Iteration 22239, loss = 0.02239250
Iteration 22240, loss = 0.02239138
Iteration 22241, loss = 0.02239027
Iteration 22242, loss = 0.02238915
Iteration 22243, loss = 0.02238804
Iteration 22244, loss = 0.02238694
Iteration 22245, loss = 0.02238581
Iteration 22246, loss = 0.02238470
Iteration 22247, loss = 0.02238359
Iteration 22248, loss = 0.02238247
Iteration 22249, loss = 0.02238136
Iteration 22250, loss = 0.02238025
Iteration 22251, loss = 0.02237913
Iteration 22252, loss = 0.02237802
Iteration 22253, loss = 0.02237690
Iteration 22254, loss = 0.02237579
Iteration 22255, loss = 0.02237468
Iteration 22256, loss = 0.02237355
Iteration 22257, loss = 0.02237243
Iteration 22258, loss = 0.02237133
Iteration 22259, loss = 0.02237020
Iteration 22260, loss = 0.02236911
Iteration 22261, loss = 0.02236798
Iteration 22262, loss = 0.02236687
Iteration 22263, loss = 0.02236575
Iteration 22264, loss = 0.02236463
Iteration 22265, loss = 0.02236354
Iteration 22266, loss = 0.02236242
Iteration 22267, loss = 0.02236132
Iteration 22268, loss = 0.02236019
Iteration 22269, loss = 0.02235907
Iteration 22270, loss = 0.02235797
Iteration 22271, loss = 0.02235684
Iteration 22272, loss = 0.02235574
Iteration 22273, loss = 0.02235461
Iteration 22274, loss = 0.02235350
Iteration 22275, loss = 0.02235238
Iteration 22276, loss = 0.02235126
Iteration 22277, loss = 0.02235016
Iteration 22278, loss = 0.02234905
Iteration 22279, loss = 0.02234793
Iteration 22280, loss = 0.02234682
Iteration 22281, loss = 0.02234570
Iteration 22282, loss = 0.02234459
Iteration 22283, loss = 0.02234348
Iteration 22284, loss = 0.02234236
Iteration 22285, loss = 0.02234126
Iteration 22286, loss = 0.02234012
Iteration 22287, loss = 0.02233903
Iteration 22288, loss = 0.02233790
Iteration 22289, loss = 0.02233681
Iteration 22290, loss = 0.02233569
Iteration 22291, loss = 0.02233455
Iteration 22292, loss = 0.02233347
Iteration 22293, loss = 0.02233233
Iteration 22294, loss = 0.02233123
Iteration 22295, loss = 0.02233010
Iteration 22296, loss = 0.02232901
Iteration 22297, loss = 0.02232789
Iteration 22298, loss = 0.02232679
Iteration 22299, loss = 0.02232567
Iteration 22300, loss = 0.02232456
Iteration 22301, loss = 0.02232346
Iteration 22302, loss = 0.02232234
Iteration 22303, loss = 0.02232122
Iteration 22304, loss = 0.02232013
Iteration 22305, loss = 0.02231901
Iteration 22306, loss = 0.02231791
Iteration 22307, loss = 0.02231680
Iteration 22308, loss = 0.02231569
Iteration 22309, loss = 0.02231458
Iteration 22310, loss = 0.02231349
Iteration 22311, loss = 0.02231236
Iteration 22312, loss = 0.02231125
Iteration 22313, loss = 0.02231015
Iteration 22314, loss = 0.02230902
Iteration 22315, loss = 0.02230792
Iteration 22316, loss = 0.02230682
Iteration 22317, loss = 0.02230570
Iteration 22318, loss = 0.02230458
Iteration 22319, loss = 0.02230349
Iteration 22320, loss = 0.02230236
Iteration 22321, loss = 0.02230126
Iteration 22322, loss = 0.02230016
Iteration 22323, loss = 0.02229905
Iteration 22324, loss = 0.02229794
Iteration 22325, loss = 0.02229681
Iteration 22326, loss = 0.02229573
Iteration 22327, loss = 0.02229462
Iteration 22328, loss = 0.02229349
Iteration 22329, loss = 0.02229238
Iteration 22330, loss = 0.02229128
Iteration 22331, loss = 0.02229017
Iteration 22332, loss = 0.02228907
Iteration 22333, loss = 0.02228795
Iteration 22334, loss = 0.02228685
Iteration 22335, loss = 0.02228573
Iteration 22336, loss = 0.02228464
Iteration 22337, loss = 0.02228352
Iteration 22338, loss = 0.02228241
Iteration 22339, loss = 0.02228131
Iteration 22340, loss = 0.02228018
Iteration 22341, loss = 0.02227909
Iteration 22342, loss = 0.02227800
Iteration 22343, loss = 0.02227688
Iteration 22344, loss = 0.02227575
Iteration 22345, loss = 0.02227466
Iteration 22346, loss = 0.02227356
Iteration 22347, loss = 0.02227244
Iteration 22348, loss = 0.02227134
Iteration 22349, loss = 0.02227024
Iteration 22350, loss = 0.02226913
Iteration 22351, loss = 0.02226800
Iteration 22352, loss = 0.02226690
Iteration 22353, loss = 0.02226582
Iteration 22354, loss = 0.02226471
Iteration 22355, loss = 0.02226359
Iteration 22356, loss = 0.02226248
Iteration 22357, loss = 0.02226138
Iteration 22358, loss = 0.02226027
Iteration 22359, loss = 0.02225917
Iteration 22360, loss = 0.02225806
Iteration 22361, loss = 0.02225695
Iteration 22362, loss = 0.02225583
Iteration 22363, loss = 0.02225474
Iteration 22364, loss = 0.02225362
Iteration 22365, loss = 0.02225252
Iteration 22366, loss = 0.02225142
Iteration 22367, loss = 0.02225031
Iteration 22368, loss = 0.02224921
Iteration 22369, loss = 0.02224810
Iteration 22370, loss = 0.02224699
Iteration 22371, loss = 0.02224589
Iteration 22372, loss = 0.02224478
Iteration 22373, loss = 0.02224368
Iteration 22374, loss = 0.02224257
Iteration 22375, loss = 0.02224146
Iteration 22376, loss = 0.02224036
Iteration 22377, loss = 0.02223926
Iteration 22378, loss = 0.02223815
Iteration 22379, loss = 0.02223703
Iteration 22380, loss = 0.02223595
Iteration 22381, loss = 0.02223483
Iteration 22382, loss = 0.02223375
Iteration 22383, loss = 0.02223263
Iteration 22384, loss = 0.02223152
Iteration 22385, loss = 0.02223041
Iteration 22386, loss = 0.02222931
Iteration 22387, loss = 0.02222820
Iteration 22388, loss = 0.02222709
Iteration 22389, loss = 0.02222602
Iteration 22390, loss = 0.02222490
Iteration 22391, loss = 0.02222378
Iteration 22392, loss = 0.02222268
Iteration 22393, loss = 0.02222159
Iteration 22394, loss = 0.02222048
Iteration 22395, loss = 0.02221938
Iteration 22396, loss = 0.02221829
Iteration 22397, loss = 0.02221716
Iteration 22398, loss = 0.02221608
Iteration 22399, loss = 0.02221497
Iteration 22400, loss = 0.02221385
Iteration 22401, loss = 0.02221276
Iteration 22402, loss = 0.02221166
Iteration 22403, loss = 0.02221056
Iteration 22404, loss = 0.02220946
Iteration 22405, loss = 0.02220837
Iteration 22406, loss = 0.02220726
Iteration 22407, loss = 0.02220615
Iteration 22408, loss = 0.02220505
Iteration 22409, loss = 0.02220395
Iteration 22410, loss = 0.02220287
Iteration 22411, loss = 0.02220176
Iteration 22412, loss = 0.02220066
Iteration 22413, loss = 0.02219956
Iteration 22414, loss = 0.02219845
Iteration 22415, loss = 0.02219736
Iteration 22416, loss = 0.02219628
Iteration 22417, loss = 0.02219516
Iteration 22418, loss = 0.02219407
Iteration 22419, loss = 0.02219295
Iteration 22420, loss = 0.02219187
Iteration 22421, loss = 0.02219077
Iteration 22422, loss = 0.02218967
Iteration 22423, loss = 0.02218857
Iteration 22424, loss = 0.02218747
Iteration 22425, loss = 0.02218637
Iteration 22426, loss = 0.02218525
Iteration 22427, loss = 0.02218417
Iteration 22428, loss = 0.02218305
Iteration 22429, loss = 0.02218196
Iteration 22430, loss = 0.02218086
Iteration 22431, loss = 0.02217974
Iteration 22432, loss = 0.02217865
Iteration 22433, loss = 0.02217755
Iteration 22434, loss = 0.02217644
Iteration 22435, loss = 0.02217534
Iteration 22436, loss = 0.02217422
Iteration 22437, loss = 0.02217315
Iteration 22438, loss = 0.02217205
Iteration 22439, loss = 0.02217094
Iteration 22440, loss = 0.02216983
Iteration 22441, loss = 0.02216872
Iteration 22442, loss = 0.02216764
Iteration 22443, loss = 0.02216656
Iteration 22444, loss = 0.02216544
Iteration 22445, loss = 0.02216436
Iteration 22446, loss = 0.02216326
Iteration 22447, loss = 0.02216216
Iteration 22448, loss = 0.02216105
Iteration 22449, loss = 0.02215995
Iteration 22450, loss = 0.02215885
Iteration 22451, loss = 0.02215774
Iteration 22452, loss = 0.02215667
Iteration 22453, loss = 0.02215556
Iteration 22454, loss = 0.02215446
Iteration 22455, loss = 0.02215335
Iteration 22456, loss = 0.02215224
Iteration 22457, loss = 0.02215116
Iteration 22458, loss = 0.02215006
Iteration 22459, loss = 0.02214896
Iteration 22460, loss = 0.02214786
Iteration 22461, loss = 0.02214676
Iteration 22462, loss = 0.02214567
Iteration 22463, loss = 0.02214456
Iteration 22464, loss = 0.02214347
Iteration 22465, loss = 0.02214238
Iteration 22466, loss = 0.02214129
Iteration 22467, loss = 0.02214019
Iteration 22468, loss = 0.02213910
Iteration 22469, loss = 0.02213801
Iteration 22470, loss = 0.02213692
Iteration 22471, loss = 0.02213582
Iteration 22472, loss = 0.02213474
Iteration 22473, loss = 0.02213364
Iteration 22474, loss = 0.02213255
Iteration 22475, loss = 0.02213146
Iteration 22476, loss = 0.02213036
Iteration 22477, loss = 0.02212928
Iteration 22478, loss = 0.02212818
Iteration 22479, loss = 0.02212709
Iteration 22480, loss = 0.02212600
Iteration 22481, loss = 0.02212489
Iteration 22482, loss = 0.02212381
Iteration 22483, loss = 0.02212271
Iteration 22484, loss = 0.02212163
Iteration 22485, loss = 0.02212052
Iteration 22486, loss = 0.02211944
Iteration 22487, loss = 0.02211834
Iteration 22488, loss = 0.02211727
Iteration 22489, loss = 0.02211618
Iteration 22490, loss = 0.02211506
Iteration 22491, loss = 0.02211397
Iteration 22492, loss = 0.02211289
Iteration 22493, loss = 0.02211179
Iteration 22494, loss = 0.02211070
Iteration 22495, loss = 0.02210962
Iteration 22496, loss = 0.02210852
Iteration 22497, loss = 0.02210744
Iteration 22498, loss = 0.02210633
Iteration 22499, loss = 0.02210525
Iteration 22500, loss = 0.02210417
Iteration 22501, loss = 0.02210308
Iteration 22502, loss = 0.02210198
Iteration 22503, loss = 0.02210090
Iteration 22504, loss = 0.02209980
Iteration 22505, loss = 0.02209871
Iteration 22506, loss = 0.02209762
Iteration 22507, loss = 0.02209653
Iteration 22508, loss = 0.02209544
Iteration 22509, loss = 0.02209435
Iteration 22510, loss = 0.02209325
Iteration 22511, loss = 0.02209217
Iteration 22512, loss = 0.02209107
Iteration 22513, loss = 0.02208999
Iteration 22514, loss = 0.02208889
Iteration 22515, loss = 0.02208779
Iteration 22516, loss = 0.02208672
Iteration 22517, loss = 0.02208562
Iteration 22518, loss = 0.02208453
Iteration 22519, loss = 0.02208342
Iteration 22520, loss = 0.02208233
Iteration 22521, loss = 0.02208123
Iteration 22522, loss = 0.02208013
Iteration 22523, loss = 0.02207906
Iteration 22524, loss = 0.02207796
Iteration 22525, loss = 0.02207686
Iteration 22526, loss = 0.02207577
Iteration 22527, loss = 0.02207467
Iteration 22528, loss = 0.02207359
Iteration 22529, loss = 0.02207250
Iteration 22530, loss = 0.02207139
Iteration 22531, loss = 0.02207032
Iteration 22532, loss = 0.02206922
Iteration 22533, loss = 0.02206813
Iteration 22534, loss = 0.02206705
Iteration 22535, loss = 0.02206596
Iteration 22536, loss = 0.02206486
Iteration 22537, loss = 0.02206376
Iteration 22538, loss = 0.02206268
Iteration 22539, loss = 0.02206159
Iteration 22540, loss = 0.02206050
Iteration 22541, loss = 0.02205942
Iteration 22542, loss = 0.02205833
Iteration 22543, loss = 0.02205725
Iteration 22544, loss = 0.02205615
Iteration 22545, loss = 0.02205507
Iteration 22546, loss = 0.02205397
Iteration 22547, loss = 0.02205289
Iteration 22548, loss = 0.02205180
Iteration 22549, loss = 0.02205072
Iteration 22550, loss = 0.02204964
Iteration 22551, loss = 0.02204854
Iteration 22552, loss = 0.02204746
Iteration 22553, loss = 0.02204638
Iteration 22554, loss = 0.02204529
Iteration 22555, loss = 0.02204421
Iteration 22556, loss = 0.02204314
Iteration 22557, loss = 0.02204205
Iteration 22558, loss = 0.02204095
Iteration 22559, loss = 0.02203985
Iteration 22560, loss = 0.02203878
Iteration 22561, loss = 0.02203769
Iteration 22562, loss = 0.02203661
Iteration 22563, loss = 0.02203551
Iteration 22564, loss = 0.02203443
Iteration 22565, loss = 0.02203334
Iteration 22566, loss = 0.02203226
Iteration 22567, loss = 0.02203118
Iteration 22568, loss = 0.02203008
Iteration 22569, loss = 0.02202901
Iteration 22570, loss = 0.02202792
Iteration 22571, loss = 0.02202681
Iteration 22572, loss = 0.02202575
Iteration 22573, loss = 0.02202466
Iteration 22574, loss = 0.02202357
Iteration 22575, loss = 0.02202247
Iteration 22576, loss = 0.02202138
Iteration 22577, loss = 0.02202031
Iteration 22578, loss = 0.02201922
Iteration 22579, loss = 0.02201813
Iteration 22580, loss = 0.02201704
Iteration 22581, loss = 0.02201594
Iteration 22582, loss = 0.02201487
Iteration 22583, loss = 0.02201376
Iteration 22584, loss = 0.02201270
Iteration 22585, loss = 0.02201159
Iteration 22586, loss = 0.02201050
Iteration 22587, loss = 0.02200942
Iteration 22588, loss = 0.02200832
Iteration 22589, loss = 0.02200724
Iteration 22590, loss = 0.02200615
Iteration 22591, loss = 0.02200507
Iteration 22592, loss = 0.02200398
Iteration 22593, loss = 0.02200289
Iteration 22594, loss = 0.02200181
Iteration 22595, loss = 0.02200071
Iteration 22596, loss = 0.02199964
Iteration 22597, loss = 0.02199855
Iteration 22598, loss = 0.02199746
Iteration 22599, loss = 0.02199638
Iteration 22600, loss = 0.02199529
Iteration 22601, loss = 0.02199421
Iteration 22602, loss = 0.02199313
Iteration 22603, loss = 0.02199205
Iteration 22604, loss = 0.02199096
Iteration 22605, loss = 0.02198989
Iteration 22606, loss = 0.02198881
Iteration 22607, loss = 0.02198771
Iteration 22608, loss = 0.02198664
Iteration 22609, loss = 0.02198557
Iteration 22610, loss = 0.02198447
Iteration 22611, loss = 0.02198338
Iteration 22612, loss = 0.02198230
Iteration 22613, loss = 0.02198121
Iteration 22614, loss = 0.02198014
Iteration 22615, loss = 0.02197905
Iteration 22616, loss = 0.02197798
Iteration 22617, loss = 0.02197686
Iteration 22618, loss = 0.02197580
Iteration 22619, loss = 0.02197470
Iteration 22620, loss = 0.02197362
Iteration 22621, loss = 0.02197254
Iteration 22622, loss = 0.02197147
Iteration 22623, loss = 0.02197038
Iteration 22624, loss = 0.02196928
Iteration 22625, loss = 0.02196821
Iteration 22626, loss = 0.02196712
Iteration 22627, loss = 0.02196604
Iteration 22628, loss = 0.02196496
Iteration 22629, loss = 0.02196387
Iteration 22630, loss = 0.02196278
Iteration 22631, loss = 0.02196171
Iteration 22632, loss = 0.02196063
Iteration 22633, loss = 0.02195954
Iteration 22634, loss = 0.02195846
Iteration 22635, loss = 0.02195738
Iteration 22636, loss = 0.02195629
Iteration 22637, loss = 0.02195522
Iteration 22638, loss = 0.02195416
Iteration 22639, loss = 0.02195308
Iteration 22640, loss = 0.02195197
Iteration 22641, loss = 0.02195089
Iteration 22642, loss = 0.02194981
Iteration 22643, loss = 0.02194872
Iteration 22644, loss = 0.02194764
Iteration 22645, loss = 0.02194656
Iteration 22646, loss = 0.02194547
Iteration 22647, loss = 0.02194438
Iteration 22648, loss = 0.02194329
Iteration 22649, loss = 0.02194223
Iteration 22650, loss = 0.02194112
Iteration 22651, loss = 0.02194005
Iteration 22652, loss = 0.02193895
Iteration 22653, loss = 0.02193787
Iteration 22654, loss = 0.02193679
Iteration 22655, loss = 0.02193572
Iteration 22656, loss = 0.02193462
Iteration 22657, loss = 0.02193353
Iteration 22658, loss = 0.02193243
Iteration 22659, loss = 0.02193137
Iteration 22660, loss = 0.02193028
Iteration 22661, loss = 0.02192919
Iteration 22662, loss = 0.02192810
Iteration 22663, loss = 0.02192701
Iteration 22664, loss = 0.02192594
Iteration 22665, loss = 0.02192485
Iteration 22666, loss = 0.02192377
Iteration 22667, loss = 0.02192268
Iteration 22668, loss = 0.02192160
Iteration 22669, loss = 0.02192052
Iteration 22670, loss = 0.02191941
Iteration 22671, loss = 0.02191835
Iteration 22672, loss = 0.02191725
Iteration 22673, loss = 0.02191615
Iteration 22674, loss = 0.02191509
Iteration 22675, loss = 0.02191400
Iteration 22676, loss = 0.02191291
Iteration 22677, loss = 0.02191183
Iteration 22678, loss = 0.02191076
Iteration 22679, loss = 0.02190966
Iteration 22680, loss = 0.02190857
Iteration 22681, loss = 0.02190750
Iteration 22682, loss = 0.02190643
Iteration 22683, loss = 0.02190533
Iteration 22684, loss = 0.02190427
Iteration 22685, loss = 0.02190320
Iteration 22686, loss = 0.02190210
Iteration 22687, loss = 0.02190102
Iteration 22688, loss = 0.02189997
Iteration 22689, loss = 0.02189885
Iteration 22690, loss = 0.02189778
Iteration 22691, loss = 0.02189669
Iteration 22692, loss = 0.02189560
Iteration 22693, loss = 0.02189452
Iteration 22694, loss = 0.02189342
Iteration 22695, loss = 0.02189236
Iteration 22696, loss = 0.02189127
Iteration 22697, loss = 0.02189019
Iteration 22698, loss = 0.02188909
Iteration 22699, loss = 0.02188800
Iteration 22700, loss = 0.02188694
Iteration 22701, loss = 0.02188586
Iteration 22702, loss = 0.02188477
Iteration 22703, loss = 0.02188369
Iteration 22704, loss = 0.02188262
Iteration 22705, loss = 0.02188156
Iteration 22706, loss = 0.02188046
Iteration 22707, loss = 0.02187939
Iteration 22708, loss = 0.02187829
Iteration 22709, loss = 0.02187723
Iteration 22710, loss = 0.02187615
Iteration 22711, loss = 0.02187507
Iteration 22712, loss = 0.02187399
Iteration 22713, loss = 0.02187291
Iteration 22714, loss = 0.02187185
Iteration 22715, loss = 0.02187077
Iteration 22716, loss = 0.02186970
Iteration 22717, loss = 0.02186862
Iteration 22718, loss = 0.02186754
Iteration 22719, loss = 0.02186647
Iteration 22720, loss = 0.02186538
Iteration 22721, loss = 0.02186432
Iteration 22722, loss = 0.02186324
Iteration 22723, loss = 0.02186214
Iteration 22724, loss = 0.02186109
Iteration 22725, loss = 0.02185999
Iteration 22726, loss = 0.02185893
Iteration 22727, loss = 0.02185785
Iteration 22728, loss = 0.02185679
Iteration 22729, loss = 0.02185568
Iteration 22730, loss = 0.02185461
Iteration 22731, loss = 0.02185353
Iteration 22732, loss = 0.02185246
Iteration 22733, loss = 0.02185138
Iteration 22734, loss = 0.02185029
Iteration 22735, loss = 0.02184920
Iteration 22736, loss = 0.02184813
Iteration 22737, loss = 0.02184706
Iteration 22738, loss = 0.02184596
Iteration 22739, loss = 0.02184488
Iteration 22740, loss = 0.02184381
Iteration 22741, loss = 0.02184273
Iteration 22742, loss = 0.02184164
Iteration 22743, loss = 0.02184057
Iteration 22744, loss = 0.02183947
Iteration 22745, loss = 0.02183841
Iteration 22746, loss = 0.02183731
Iteration 22747, loss = 0.02183625
Iteration 22748, loss = 0.02183518
Iteration 22749, loss = 0.02183411
Iteration 22750, loss = 0.02183301
Iteration 22751, loss = 0.02183194
Iteration 22752, loss = 0.02183087
Iteration 22753, loss = 0.02182980
Iteration 22754, loss = 0.02182873
Iteration 22755, loss = 0.02182766
Iteration 22756, loss = 0.02182657
Iteration 22757, loss = 0.02182550
Iteration 22758, loss = 0.02182442
Iteration 22759, loss = 0.02182335
Iteration 22760, loss = 0.02182227
Iteration 22761, loss = 0.02182118
Iteration 22762, loss = 0.02182012
Iteration 22763, loss = 0.02181904
Iteration 22764, loss = 0.02181796
Iteration 22765, loss = 0.02181686
Iteration 22766, loss = 0.02181581
Iteration 22767, loss = 0.02181474
Iteration 22768, loss = 0.02181367
Iteration 22769, loss = 0.02181259
Iteration 22770, loss = 0.02181151
Iteration 22771, loss = 0.02181044
Iteration 22772, loss = 0.02180938
Iteration 22773, loss = 0.02180829
Iteration 22774, loss = 0.02180722
Iteration 22775, loss = 0.02180616
Iteration 22776, loss = 0.02180507
Iteration 22777, loss = 0.02180399
Iteration 22778, loss = 0.02180291
Iteration 22779, loss = 0.02180184
Iteration 22780, loss = 0.02180077
Iteration 22781, loss = 0.02179968
Iteration 22782, loss = 0.02179863
Iteration 22783, loss = 0.02179753
Iteration 22784, loss = 0.02179648
Iteration 22785, loss = 0.02179541
Iteration 22786, loss = 0.02179433
Iteration 22787, loss = 0.02179327
Iteration 22788, loss = 0.02179218
Iteration 22789, loss = 0.02179113
Iteration 22790, loss = 0.02179006
Iteration 22791, loss = 0.02178899
Iteration 22792, loss = 0.02178792
Iteration 22793, loss = 0.02178684
Iteration 22794, loss = 0.02178577
Iteration 22795, loss = 0.02178470
Iteration 22796, loss = 0.02178366
Iteration 22797, loss = 0.02178258
Iteration 22798, loss = 0.02178148
Iteration 22799, loss = 0.02178042
Iteration 22800, loss = 0.02177933
Iteration 22801, loss = 0.02177828
Iteration 22802, loss = 0.02177718
Iteration 22803, loss = 0.02177611
Iteration 22804, loss = 0.02177505
Iteration 22805, loss = 0.02177398
Iteration 22806, loss = 0.02177290
Iteration 22807, loss = 0.02177183
Iteration 22808, loss = 0.02177073
Iteration 22809, loss = 0.02176966
Iteration 22810, loss = 0.02176860
Iteration 22811, loss = 0.02176751
Iteration 22812, loss = 0.02176644
Iteration 22813, loss = 0.02176539
Iteration 22814, loss = 0.02176430
Iteration 22815, loss = 0.02176322
Iteration 22816, loss = 0.02176215
Iteration 22817, loss = 0.02176107
Iteration 22818, loss = 0.02176001
Iteration 22819, loss = 0.02175893
Iteration 22820, loss = 0.02175786
Iteration 22821, loss = 0.02175677
Iteration 22822, loss = 0.02175574
Iteration 22823, loss = 0.02175465
Iteration 22824, loss = 0.02175358
Iteration 22825, loss = 0.02175253
Iteration 22826, loss = 0.02175146
Iteration 22827, loss = 0.02175038
Iteration 22828, loss = 0.02174932
Iteration 22829, loss = 0.02174825
Iteration 22830, loss = 0.02174719
Iteration 22831, loss = 0.02174611
Iteration 22832, loss = 0.02174505
Iteration 22833, loss = 0.02174399
Iteration 22834, loss = 0.02174291
Iteration 22835, loss = 0.02174185
Iteration 22836, loss = 0.02174079
Iteration 22837, loss = 0.02173971
Iteration 22838, loss = 0.02173866
Iteration 22839, loss = 0.02173758
Iteration 22840, loss = 0.02173654
Iteration 22841, loss = 0.02173545
Iteration 22842, loss = 0.02173438
Iteration 22843, loss = 0.02173330
Iteration 22844, loss = 0.02173225
Iteration 22845, loss = 0.02173116
Iteration 22846, loss = 0.02173012
Iteration 22847, loss = 0.02172904
Iteration 22848, loss = 0.02172797
Iteration 22849, loss = 0.02172690
Iteration 22850, loss = 0.02172583
Iteration 22851, loss = 0.02172477
Iteration 22852, loss = 0.02172368
Iteration 22853, loss = 0.02172261
Iteration 22854, loss = 0.02172157
Iteration 22855, loss = 0.02172048
Iteration 22856, loss = 0.02171941
Iteration 22857, loss = 0.02171834
Iteration 22858, loss = 0.02171725
Iteration 22859, loss = 0.02171622
Iteration 22860, loss = 0.02171513
Iteration 22861, loss = 0.02171404
Iteration 22862, loss = 0.02171299
Iteration 22863, loss = 0.02171191
Iteration 22864, loss = 0.02171082
Iteration 22865, loss = 0.02170977
Iteration 22866, loss = 0.02170871
Iteration 22867, loss = 0.02170761
Iteration 22868, loss = 0.02170657
Iteration 22869, loss = 0.02170548
Iteration 22870, loss = 0.02170443
Iteration 22871, loss = 0.02170335
Iteration 22872, loss = 0.02170229
Iteration 22873, loss = 0.02170121
Iteration 22874, loss = 0.02170013
Iteration 22875, loss = 0.02169909
Iteration 22876, loss = 0.02169799
Iteration 22877, loss = 0.02169695
Iteration 22878, loss = 0.02169587
Iteration 22879, loss = 0.02169481
Iteration 22880, loss = 0.02169373
Iteration 22881, loss = 0.02169267
Iteration 22882, loss = 0.02169160
Iteration 22883, loss = 0.02169053
Iteration 22884, loss = 0.02168947
Iteration 22885, loss = 0.02168839
Iteration 22886, loss = 0.02168734
Iteration 22887, loss = 0.02168627
Iteration 22888, loss = 0.02168517
Iteration 22889, loss = 0.02168410
Iteration 22890, loss = 0.02168303
Iteration 22891, loss = 0.02168197
Iteration 22892, loss = 0.02168091
Iteration 22893, loss = 0.02167982
Iteration 22894, loss = 0.02167877
Iteration 22895, loss = 0.02167769
Iteration 22896, loss = 0.02167662
Iteration 22897, loss = 0.02167555
Iteration 22898, loss = 0.02167449
Iteration 22899, loss = 0.02167344
Iteration 22900, loss = 0.02167235
Iteration 22901, loss = 0.02167130
Iteration 22902, loss = 0.02167024
Iteration 22903, loss = 0.02166918
Iteration 22904, loss = 0.02166811
Iteration 22905, loss = 0.02166705
Iteration 22906, loss = 0.02166598
Iteration 22907, loss = 0.02166493
Iteration 22908, loss = 0.02166387
Iteration 22909, loss = 0.02166279
Iteration 22910, loss = 0.02166174
Iteration 22911, loss = 0.02166067
Iteration 22912, loss = 0.02165961
Iteration 22913, loss = 0.02165856
Iteration 22914, loss = 0.02165749
Iteration 22915, loss = 0.02165642
Iteration 22916, loss = 0.02165538
Iteration 22917, loss = 0.02165432
Iteration 22918, loss = 0.02165324
Iteration 22919, loss = 0.02165216
Iteration 22920, loss = 0.02165113
Iteration 22921, loss = 0.02165004
Iteration 22922, loss = 0.02164899
Iteration 22923, loss = 0.02164792
Iteration 22924, loss = 0.02164685
Iteration 22925, loss = 0.02164577
Iteration 22926, loss = 0.02164472
Iteration 22927, loss = 0.02164367
Iteration 22928, loss = 0.02164257
Iteration 22929, loss = 0.02164152
Iteration 22930, loss = 0.02164044
Iteration 22931, loss = 0.02163939
Iteration 22932, loss = 0.02163832
Iteration 22933, loss = 0.02163727
Iteration 22934, loss = 0.02163618
Iteration 22935, loss = 0.02163514
Iteration 22936, loss = 0.02163409
Iteration 22937, loss = 0.02163302
Iteration 22938, loss = 0.02163197
Iteration 22939, loss = 0.02163091
Iteration 22940, loss = 0.02162985
Iteration 22941, loss = 0.02162880
Iteration 22942, loss = 0.02162773
Iteration 22943, loss = 0.02162668
Iteration 22944, loss = 0.02162563
Iteration 22945, loss = 0.02162458
Iteration 22946, loss = 0.02162352
Iteration 22947, loss = 0.02162246
Iteration 22948, loss = 0.02162141
Iteration 22949, loss = 0.02162036
Iteration 22950, loss = 0.02161931
Iteration 22951, loss = 0.02161824
Iteration 22952, loss = 0.02161720
Iteration 22953, loss = 0.02161614
Iteration 22954, loss = 0.02161510
Iteration 22955, loss = 0.02161403
Iteration 22956, loss = 0.02161298
Iteration 22957, loss = 0.02161193
Iteration 22958, loss = 0.02161086
Iteration 22959, loss = 0.02160981
Iteration 22960, loss = 0.02160877
Iteration 22961, loss = 0.02160769
Iteration 22962, loss = 0.02160664
Iteration 22963, loss = 0.02160559
Iteration 22964, loss = 0.02160453
Iteration 22965, loss = 0.02160346
Iteration 22966, loss = 0.02160240
Iteration 22967, loss = 0.02160135
Iteration 22968, loss = 0.02160029
Iteration 22969, loss = 0.02159924
Iteration 22970, loss = 0.02159818
Iteration 22971, loss = 0.02159713
Iteration 22972, loss = 0.02159606
Iteration 22973, loss = 0.02159501
Iteration 22974, loss = 0.02159395
Iteration 22975, loss = 0.02159290
Iteration 22976, loss = 0.02159185
Iteration 22977, loss = 0.02159080
Iteration 22978, loss = 0.02158973
Iteration 22979, loss = 0.02158868
Iteration 22980, loss = 0.02158760
Iteration 22981, loss = 0.02158655
Iteration 22982, loss = 0.02158549
Iteration 22983, loss = 0.02158443
Iteration 22984, loss = 0.02158336
Iteration 22985, loss = 0.02158231
Iteration 22986, loss = 0.02158124
Iteration 22987, loss = 0.02158016
Iteration 22988, loss = 0.02157912
Iteration 22989, loss = 0.02157807
Iteration 22990, loss = 0.02157701
Iteration 22991, loss = 0.02157593
Iteration 22992, loss = 0.02157489
Iteration 22993, loss = 0.02157383
Iteration 22994, loss = 0.02157277
Iteration 22995, loss = 0.02157171
Iteration 22996, loss = 0.02157064
Iteration 22997, loss = 0.02156962
Iteration 22998, loss = 0.02156854
Iteration 22999, loss = 0.02156748
Iteration 23000, loss = 0.02156641
Iteration 23001, loss = 0.02156537
Iteration 23002, loss = 0.02156431
Iteration 23003, loss = 0.02156325
Iteration 23004, loss = 0.02156220
Iteration 23005, loss = 0.02156111
Iteration 23006, loss = 0.02156004
Iteration 23007, loss = 0.02155902
Iteration 23008, loss = 0.02155795
Iteration 23009, loss = 0.02155687
Iteration 23010, loss = 0.02155584
Iteration 23011, loss = 0.02155476
Iteration 23012, loss = 0.02155372
Iteration 23013, loss = 0.02155264
Iteration 23014, loss = 0.02155159
Iteration 23015, loss = 0.02155052
Iteration 23016, loss = 0.02154950
Iteration 23017, loss = 0.02154841
Iteration 23018, loss = 0.02154737
Iteration 23019, loss = 0.02154631
Iteration 23020, loss = 0.02154525
Iteration 23021, loss = 0.02154418
Iteration 23022, loss = 0.02154313
Iteration 23023, loss = 0.02154207
Iteration 23024, loss = 0.02154102
Iteration 23025, loss = 0.02153994
Iteration 23026, loss = 0.02153890
Iteration 23027, loss = 0.02153783
Iteration 23028, loss = 0.02153676
Iteration 23029, loss = 0.02153570
Iteration 23030, loss = 0.02153463
Iteration 23031, loss = 0.02153359
Iteration 23032, loss = 0.02153250
Iteration 23033, loss = 0.02153147
Iteration 23034, loss = 0.02153041
Iteration 23035, loss = 0.02152934
Iteration 23036, loss = 0.02152826
Iteration 23037, loss = 0.02152722
Iteration 23038, loss = 0.02152617
Iteration 23039, loss = 0.02152511
Iteration 23040, loss = 0.02152406
Iteration 23041, loss = 0.02152300
Iteration 23042, loss = 0.02152195
Iteration 23043, loss = 0.02152092
Iteration 23044, loss = 0.02151986
Iteration 23045, loss = 0.02151880
Iteration 23046, loss = 0.02151774
Iteration 23047, loss = 0.02151669
Iteration 23048, loss = 0.02151564
Iteration 23049, loss = 0.02151457
Iteration 23050, loss = 0.02151352
Iteration 23051, loss = 0.02151244
Iteration 23052, loss = 0.02151139
Iteration 23053, loss = 0.02151036
Iteration 23054, loss = 0.02150930
Iteration 23055, loss = 0.02150823
Iteration 23056, loss = 0.02150716
Iteration 23057, loss = 0.02150612
Iteration 23058, loss = 0.02150506
Iteration 23059, loss = 0.02150401
Iteration 23060, loss = 0.02150297
Iteration 23061, loss = 0.02150192
Iteration 23062, loss = 0.02150085
Iteration 23063, loss = 0.02149981
Iteration 23064, loss = 0.02149876
Iteration 23065, loss = 0.02149768
Iteration 23066, loss = 0.02149664
Iteration 23067, loss = 0.02149558
Iteration 23068, loss = 0.02149454
Iteration 23069, loss = 0.02149347
Iteration 23070, loss = 0.02149242
Iteration 23071, loss = 0.02149137
Iteration 23072, loss = 0.02149032
Iteration 23073, loss = 0.02148926
Iteration 23074, loss = 0.02148821
Iteration 23075, loss = 0.02148717
Iteration 23076, loss = 0.02148611
Iteration 23077, loss = 0.02148504
Iteration 23078, loss = 0.02148402
Iteration 23079, loss = 0.02148293
Iteration 23080, loss = 0.02148188
Iteration 23081, loss = 0.02148082
Iteration 23082, loss = 0.02147978
Iteration 23083, loss = 0.02147872
Iteration 23084, loss = 0.02147769
Iteration 23085, loss = 0.02147661
Iteration 23086, loss = 0.02147556
Iteration 23087, loss = 0.02147450
Iteration 23088, loss = 0.02147347
Iteration 23089, loss = 0.02147241
Iteration 23090, loss = 0.02147135
Iteration 23091, loss = 0.02147028
Iteration 23092, loss = 0.02146925
Iteration 23093, loss = 0.02146817
Iteration 23094, loss = 0.02146711
Iteration 23095, loss = 0.02146605
Iteration 23096, loss = 0.02146500
Iteration 23097, loss = 0.02146398
Iteration 23098, loss = 0.02146290
Iteration 23099, loss = 0.02146185
Iteration 23100, loss = 0.02146078
Iteration 23101, loss = 0.02145975
Iteration 23102, loss = 0.02145870
Iteration 23103, loss = 0.02145766
Iteration 23104, loss = 0.02145657
Iteration 23105, loss = 0.02145554
Iteration 23106, loss = 0.02145450
Iteration 23107, loss = 0.02145344
Iteration 23108, loss = 0.02145239
Iteration 23109, loss = 0.02145134
Iteration 23110, loss = 0.02145029
Iteration 23111, loss = 0.02144923
Iteration 23112, loss = 0.02144820
Iteration 23113, loss = 0.02144716
Iteration 23114, loss = 0.02144611
Iteration 23115, loss = 0.02144505
Iteration 23116, loss = 0.02144402
Iteration 23117, loss = 0.02144298
Iteration 23118, loss = 0.02144194
Iteration 23119, loss = 0.02144089
Iteration 23120, loss = 0.02143985
Iteration 23121, loss = 0.02143882
Iteration 23122, loss = 0.02143778
Iteration 23123, loss = 0.02143672
Iteration 23124, loss = 0.02143569
Iteration 23125, loss = 0.02143463
Iteration 23126, loss = 0.02143361
Iteration 23127, loss = 0.02143255
Iteration 23128, loss = 0.02143151
Iteration 23129, loss = 0.02143048
Iteration 23130, loss = 0.02142942
Iteration 23131, loss = 0.02142839
Iteration 23132, loss = 0.02142734
Iteration 23133, loss = 0.02142629
Iteration 23134, loss = 0.02142525
Iteration 23135, loss = 0.02142420
Iteration 23136, loss = 0.02142315
Iteration 23137, loss = 0.02142211
Iteration 23138, loss = 0.02142107
Iteration 23139, loss = 0.02142001
Iteration 23140, loss = 0.02141899
Iteration 23141, loss = 0.02141793
Iteration 23142, loss = 0.02141688
Iteration 23143, loss = 0.02141583
Iteration 23144, loss = 0.02141479
Iteration 23145, loss = 0.02141374
Iteration 23146, loss = 0.02141270
Iteration 23147, loss = 0.02141165
Iteration 23148, loss = 0.02141061
Iteration 23149, loss = 0.02140958
Iteration 23150, loss = 0.02140853
Iteration 23151, loss = 0.02140747
Iteration 23152, loss = 0.02140645
Iteration 23153, loss = 0.02140541
Iteration 23154, loss = 0.02140435
Iteration 23155, loss = 0.02140332
Iteration 23156, loss = 0.02140227
Iteration 23157, loss = 0.02140124
Iteration 23158, loss = 0.02140019
Iteration 23159, loss = 0.02139914
Iteration 23160, loss = 0.02139812
Iteration 23161, loss = 0.02139706
Iteration 23162, loss = 0.02139601
Iteration 23163, loss = 0.02139496
Iteration 23164, loss = 0.02139391
Iteration 23165, loss = 0.02139288
Iteration 23166, loss = 0.02139182
Iteration 23167, loss = 0.02139079
Iteration 23168, loss = 0.02138973
Iteration 23169, loss = 0.02138868
Iteration 23170, loss = 0.02138766
Iteration 23171, loss = 0.02138661
Iteration 23172, loss = 0.02138557
Iteration 23173, loss = 0.02138455
Iteration 23174, loss = 0.02138348
Iteration 23175, loss = 0.02138243
Iteration 23176, loss = 0.02138141
Iteration 23177, loss = 0.02138037
Iteration 23178, loss = 0.02137934
Iteration 23179, loss = 0.02137828
Iteration 23180, loss = 0.02137725
Iteration 23181, loss = 0.02137622
Iteration 23182, loss = 0.02137517
Iteration 23183, loss = 0.02137414
Iteration 23184, loss = 0.02137310
Iteration 23185, loss = 0.02137206
Iteration 23186, loss = 0.02137102
Iteration 23187, loss = 0.02136999
Iteration 23188, loss = 0.02136896
Iteration 23189, loss = 0.02136791
Iteration 23190, loss = 0.02136689
Iteration 23191, loss = 0.02136585
Iteration 23192, loss = 0.02136481
Iteration 23193, loss = 0.02136378
Iteration 23194, loss = 0.02136273
Iteration 23195, loss = 0.02136172
Iteration 23196, loss = 0.02136067
Iteration 23197, loss = 0.02135963
Iteration 23198, loss = 0.02135860
Iteration 23199, loss = 0.02135758
Iteration 23200, loss = 0.02135653
Iteration 23201, loss = 0.02135549
Iteration 23202, loss = 0.02135447
Iteration 23203, loss = 0.02135341
Iteration 23204, loss = 0.02135238
Iteration 23205, loss = 0.02135135
Iteration 23206, loss = 0.02135031
Iteration 23207, loss = 0.02134927
Iteration 23208, loss = 0.02134823
Iteration 23209, loss = 0.02134720
Iteration 23210, loss = 0.02134615
Iteration 23211, loss = 0.02134512
Iteration 23212, loss = 0.02134409
Iteration 23213, loss = 0.02134306
Iteration 23214, loss = 0.02134203
Iteration 23215, loss = 0.02134099
Iteration 23216, loss = 0.02133993
Iteration 23217, loss = 0.02133892
Iteration 23218, loss = 0.02133787
Iteration 23219, loss = 0.02133684
Iteration 23220, loss = 0.02133579
Iteration 23221, loss = 0.02133478
Iteration 23222, loss = 0.02133375
Iteration 23223, loss = 0.02133268
Iteration 23224, loss = 0.02133164
Iteration 23225, loss = 0.02133061
Iteration 23226, loss = 0.02132960
Iteration 23227, loss = 0.02132854
Iteration 23228, loss = 0.02132751
Iteration 23229, loss = 0.02132646
Iteration 23230, loss = 0.02132542
Iteration 23231, loss = 0.02132440
Iteration 23232, loss = 0.02132334
Iteration 23233, loss = 0.02132234
Iteration 23234, loss = 0.02132128
Iteration 23235, loss = 0.02132026
Iteration 23236, loss = 0.02131922
Iteration 23237, loss = 0.02131818
Iteration 23238, loss = 0.02131716
Iteration 23239, loss = 0.02131611
Iteration 23240, loss = 0.02131507
Iteration 23241, loss = 0.02131404
Iteration 23242, loss = 0.02131301
Iteration 23243, loss = 0.02131196
Iteration 23244, loss = 0.02131096
Iteration 23245, loss = 0.02130992
Iteration 23246, loss = 0.02130889
Iteration 23247, loss = 0.02130784
Iteration 23248, loss = 0.02130682
Iteration 23249, loss = 0.02130579
Iteration 23250, loss = 0.02130473
Iteration 23251, loss = 0.02130372
Iteration 23252, loss = 0.02130269
Iteration 23253, loss = 0.02130166
Iteration 23254, loss = 0.02130063
Iteration 23255, loss = 0.02129959
Iteration 23256, loss = 0.02129858
Iteration 23257, loss = 0.02129754
Iteration 23258, loss = 0.02129653
Iteration 23259, loss = 0.02129549
Iteration 23260, loss = 0.02129447
Iteration 23261, loss = 0.02129345
Iteration 23262, loss = 0.02129242
Iteration 23263, loss = 0.02129138
Iteration 23264, loss = 0.02129036
Iteration 23265, loss = 0.02128935
Iteration 23266, loss = 0.02128830
Iteration 23267, loss = 0.02128729
Iteration 23268, loss = 0.02128626
Iteration 23269, loss = 0.02128522
Iteration 23270, loss = 0.02128421
Iteration 23271, loss = 0.02128317
Iteration 23272, loss = 0.02128213
Iteration 23273, loss = 0.02128111
Iteration 23274, loss = 0.02128009
Iteration 23275, loss = 0.02127904
Iteration 23276, loss = 0.02127803
Iteration 23277, loss = 0.02127700
Iteration 23278, loss = 0.02127596
Iteration 23279, loss = 0.02127492
Iteration 23280, loss = 0.02127391
Iteration 23281, loss = 0.02127287
Iteration 23282, loss = 0.02127182
Iteration 23283, loss = 0.02127082
Iteration 23284, loss = 0.02126978
Iteration 23285, loss = 0.02126873
Iteration 23286, loss = 0.02126770
Iteration 23287, loss = 0.02126668
Iteration 23288, loss = 0.02126565
Iteration 23289, loss = 0.02126462
Iteration 23290, loss = 0.02126358
Iteration 23291, loss = 0.02126254
Iteration 23292, loss = 0.02126151
Iteration 23293, loss = 0.02126050
Iteration 23294, loss = 0.02125948
Iteration 23295, loss = 0.02125844
Iteration 23296, loss = 0.02125740
Iteration 23297, loss = 0.02125638
Iteration 23298, loss = 0.02125533
Iteration 23299, loss = 0.02125432
Iteration 23300, loss = 0.02125328
Iteration 23301, loss = 0.02125227
Iteration 23302, loss = 0.02125122
Iteration 23303, loss = 0.02125021
Iteration 23304, loss = 0.02124918
Iteration 23305, loss = 0.02124813
Iteration 23306, loss = 0.02124713
Iteration 23307, loss = 0.02124606
Iteration 23308, loss = 0.02124506
Iteration 23309, loss = 0.02124401
Iteration 23310, loss = 0.02124301
Iteration 23311, loss = 0.02124196
Iteration 23312, loss = 0.02124095
Iteration 23313, loss = 0.02123993
Iteration 23314, loss = 0.02123889
Iteration 23315, loss = 0.02123788
Iteration 23316, loss = 0.02123684
Iteration 23317, loss = 0.02123585
Iteration 23318, loss = 0.02123481
Iteration 23319, loss = 0.02123378
Iteration 23320, loss = 0.02123274
Iteration 23321, loss = 0.02123172
Iteration 23322, loss = 0.02123072
Iteration 23323, loss = 0.02122968
Iteration 23324, loss = 0.02122865
Iteration 23325, loss = 0.02122761
Iteration 23326, loss = 0.02122660
Iteration 23327, loss = 0.02122557
Iteration 23328, loss = 0.02122453
Iteration 23329, loss = 0.02122350
Iteration 23330, loss = 0.02122249
Iteration 23331, loss = 0.02122148
Iteration 23332, loss = 0.02122041
Iteration 23333, loss = 0.02121940
Iteration 23334, loss = 0.02121839
Iteration 23335, loss = 0.02121736
Iteration 23336, loss = 0.02121632
Iteration 23337, loss = 0.02121532
Iteration 23338, loss = 0.02121428
Iteration 23339, loss = 0.02121326
Iteration 23340, loss = 0.02121223
Iteration 23341, loss = 0.02121122
Iteration 23342, loss = 0.02121019
Iteration 23343, loss = 0.02120916
Iteration 23344, loss = 0.02120814
Iteration 23345, loss = 0.02120710
Iteration 23346, loss = 0.02120610
Iteration 23347, loss = 0.02120504
Iteration 23348, loss = 0.02120403
Iteration 23349, loss = 0.02120302
Iteration 23350, loss = 0.02120198
Iteration 23351, loss = 0.02120098
Iteration 23352, loss = 0.02119993
Iteration 23353, loss = 0.02119891
Iteration 23354, loss = 0.02119790
Iteration 23355, loss = 0.02119685
Iteration 23356, loss = 0.02119586
Iteration 23357, loss = 0.02119481
Iteration 23358, loss = 0.02119383
Iteration 23359, loss = 0.02119279
Iteration 23360, loss = 0.02119176
Iteration 23361, loss = 0.02119075
Iteration 23362, loss = 0.02118971
Iteration 23363, loss = 0.02118871
Iteration 23364, loss = 0.02118769
Iteration 23365, loss = 0.02118667
Iteration 23366, loss = 0.02118567
Iteration 23367, loss = 0.02118464
Iteration 23368, loss = 0.02118362
Iteration 23369, loss = 0.02118260
Iteration 23370, loss = 0.02118160
Iteration 23371, loss = 0.02118058
Iteration 23372, loss = 0.02117957
Iteration 23373, loss = 0.02117855
Iteration 23374, loss = 0.02117754
Iteration 23375, loss = 0.02117651
Iteration 23376, loss = 0.02117551
Iteration 23377, loss = 0.02117449
Iteration 23378, loss = 0.02117346
Iteration 23379, loss = 0.02117246
Iteration 23380, loss = 0.02117144
Iteration 23381, loss = 0.02117041
Iteration 23382, loss = 0.02116940
Iteration 23383, loss = 0.02116839
Iteration 23384, loss = 0.02116737
Iteration 23385, loss = 0.02116636
Iteration 23386, loss = 0.02116531
Iteration 23387, loss = 0.02116432
Iteration 23388, loss = 0.02116330
Iteration 23389, loss = 0.02116228
Iteration 23390, loss = 0.02116125
Iteration 23391, loss = 0.02116025
Iteration 23392, loss = 0.02115923
Iteration 23393, loss = 0.02115820
Iteration 23394, loss = 0.02115718
Iteration 23395, loss = 0.02115619
Iteration 23396, loss = 0.02115515
Iteration 23397, loss = 0.02115415
Iteration 23398, loss = 0.02115313
Iteration 23399, loss = 0.02115210
Iteration 23400, loss = 0.02115110
Iteration 23401, loss = 0.02115009
Iteration 23402, loss = 0.02114907
Iteration 23403, loss = 0.02114804
Iteration 23404, loss = 0.02114704
Iteration 23405, loss = 0.02114601
Iteration 23406, loss = 0.02114502
Iteration 23407, loss = 0.02114399
Iteration 23408, loss = 0.02114296
Iteration 23409, loss = 0.02114196
Iteration 23410, loss = 0.02114095
Iteration 23411, loss = 0.02113993
Iteration 23412, loss = 0.02113893
Iteration 23413, loss = 0.02113791
Iteration 23414, loss = 0.02113691
Iteration 23415, loss = 0.02113588
Iteration 23416, loss = 0.02113488
Iteration 23417, loss = 0.02113384
Iteration 23418, loss = 0.02113286
Iteration 23419, loss = 0.02113183
Iteration 23420, loss = 0.02113082
Iteration 23421, loss = 0.02112981
Iteration 23422, loss = 0.02112879
Iteration 23423, loss = 0.02112777
Iteration 23424, loss = 0.02112677
Iteration 23425, loss = 0.02112576
Iteration 23426, loss = 0.02112474
Iteration 23427, loss = 0.02112372
Iteration 23428, loss = 0.02112270
Iteration 23429, loss = 0.02112171
Iteration 23430, loss = 0.02112068
Iteration 23431, loss = 0.02111968
Iteration 23432, loss = 0.02111863
Iteration 23433, loss = 0.02111765
Iteration 23434, loss = 0.02111661
Iteration 23435, loss = 0.02111562
Iteration 23436, loss = 0.02111460
Iteration 23437, loss = 0.02111356
Iteration 23438, loss = 0.02111256
Iteration 23439, loss = 0.02111154
Iteration 23440, loss = 0.02111054
Iteration 23441, loss = 0.02110951
Iteration 23442, loss = 0.02110852
Iteration 23443, loss = 0.02110749
Iteration 23444, loss = 0.02110648
Iteration 23445, loss = 0.02110548
Iteration 23446, loss = 0.02110446
Iteration 23447, loss = 0.02110345
Iteration 23448, loss = 0.02110244
Iteration 23449, loss = 0.02110144
Iteration 23450, loss = 0.02110042
Iteration 23451, loss = 0.02109943
Iteration 23452, loss = 0.02109840
Iteration 23453, loss = 0.02109739
Iteration 23454, loss = 0.02109639
Iteration 23455, loss = 0.02109536
Iteration 23456, loss = 0.02109437
Iteration 23457, loss = 0.02109335
Iteration 23458, loss = 0.02109232
Iteration 23459, loss = 0.02109132
Iteration 23460, loss = 0.02109031
Iteration 23461, loss = 0.02108930
Iteration 23462, loss = 0.02108831
Iteration 23463, loss = 0.02108730
Iteration 23464, loss = 0.02108629
Iteration 23465, loss = 0.02108529
Iteration 23466, loss = 0.02108429
Iteration 23467, loss = 0.02108330
Iteration 23468, loss = 0.02108228
Iteration 23469, loss = 0.02108128
Iteration 23470, loss = 0.02108029
Iteration 23471, loss = 0.02107928
Iteration 23472, loss = 0.02107827
Iteration 23473, loss = 0.02107727
Iteration 23474, loss = 0.02107628
Iteration 23475, loss = 0.02107526
Iteration 23476, loss = 0.02107425
Iteration 23477, loss = 0.02107328
Iteration 23478, loss = 0.02107225
Iteration 23479, loss = 0.02107124
Iteration 23480, loss = 0.02107023
Iteration 23481, loss = 0.02106924
Iteration 23482, loss = 0.02106823
Iteration 23483, loss = 0.02106724
Iteration 23484, loss = 0.02106622
Iteration 23485, loss = 0.02106521
Iteration 23486, loss = 0.02106421
Iteration 23487, loss = 0.02106320
Iteration 23488, loss = 0.02106221
Iteration 23489, loss = 0.02106120
Iteration 23490, loss = 0.02106020
Iteration 23491, loss = 0.02105918
Iteration 23492, loss = 0.02105818
Iteration 23493, loss = 0.02105718
Iteration 23494, loss = 0.02105619
Iteration 23495, loss = 0.02105518
Iteration 23496, loss = 0.02105418
Iteration 23497, loss = 0.02105319
Iteration 23498, loss = 0.02105217
Iteration 23499, loss = 0.02105118
Iteration 23500, loss = 0.02105018
Iteration 23501, loss = 0.02104918
Iteration 23502, loss = 0.02104817
Iteration 23503, loss = 0.02104717
Iteration 23504, loss = 0.02104617
Iteration 23505, loss = 0.02104516
Iteration 23506, loss = 0.02104415
Iteration 23507, loss = 0.02104315
Iteration 23508, loss = 0.02104216
Iteration 23509, loss = 0.02104114
Iteration 23510, loss = 0.02104014
Iteration 23511, loss = 0.02103913
Iteration 23512, loss = 0.02103814
Iteration 23513, loss = 0.02103712
Iteration 23514, loss = 0.02103615
Iteration 23515, loss = 0.02103512
Iteration 23516, loss = 0.02103413
Iteration 23517, loss = 0.02103312
Iteration 23518, loss = 0.02103213
Iteration 23519, loss = 0.02103110
Iteration 23520, loss = 0.02103014
Iteration 23521, loss = 0.02102911
Iteration 23522, loss = 0.02102812
Iteration 23523, loss = 0.02102712
Iteration 23524, loss = 0.02102612
Iteration 23525, loss = 0.02102513
Iteration 23526, loss = 0.02102412
Iteration 23527, loss = 0.02102312
Iteration 23528, loss = 0.02102213
Iteration 23529, loss = 0.02102113
Iteration 23530, loss = 0.02102012
Iteration 23531, loss = 0.02101912
Iteration 23532, loss = 0.02101813
Iteration 23533, loss = 0.02101714
Iteration 23534, loss = 0.02101615
Iteration 23535, loss = 0.02101516
Iteration 23536, loss = 0.02101416
Iteration 23537, loss = 0.02101315
Iteration 23538, loss = 0.02101218
Iteration 23539, loss = 0.02101118
Iteration 23540, loss = 0.02101020
Iteration 23541, loss = 0.02100920
Iteration 23542, loss = 0.02100821
Iteration 23543, loss = 0.02100721
Iteration 23544, loss = 0.02100623
Iteration 23545, loss = 0.02100525
Iteration 23546, loss = 0.02100424
Iteration 23547, loss = 0.02100327
Iteration 23548, loss = 0.02100226
Iteration 23549, loss = 0.02100128
Iteration 23550, loss = 0.02100028
Iteration 23551, loss = 0.02099929
Iteration 23552, loss = 0.02099829
Iteration 23553, loss = 0.02099729
Iteration 23554, loss = 0.02099629
Iteration 23555, loss = 0.02099530
Iteration 23556, loss = 0.02099429
Iteration 23557, loss = 0.02099330
Iteration 23558, loss = 0.02099232
Iteration 23559, loss = 0.02099131
Iteration 23560, loss = 0.02099032
Iteration 23561, loss = 0.02098930
Iteration 23562, loss = 0.02098833
Iteration 23563, loss = 0.02098732
Iteration 23564, loss = 0.02098634
Iteration 23565, loss = 0.02098533
Iteration 23566, loss = 0.02098435
Iteration 23567, loss = 0.02098335
Iteration 23568, loss = 0.02098237
Iteration 23569, loss = 0.02098137
Iteration 23570, loss = 0.02098038
Iteration 23571, loss = 0.02097940
Iteration 23572, loss = 0.02097841
Iteration 23573, loss = 0.02097742
Iteration 23574, loss = 0.02097643
Iteration 23575, loss = 0.02097543
Iteration 23576, loss = 0.02097445
Iteration 23577, loss = 0.02097346
Iteration 23578, loss = 0.02097248
Iteration 23579, loss = 0.02097150
Iteration 23580, loss = 0.02097050
Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.
Best parameters found:  {&#39;activation&#39;: &#39;logistic&#39;, &#39;hidden_layer_sizes&#39;: (30,), &#39;solver&#39;: &#39;sgd&#39;}
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[80]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">nn_mlp_eva_best</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nn_mlp_best</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">nn_mlp_df_best</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">nn_mlp_eva_best</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                         <span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;standard&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;NN_MLP&quot;</span><span class="p">,</span> <span class="s2">&quot;NN_MLP&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">nn_mlp_df_best</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[80]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">standard</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">NN_MLP</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.95</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.97</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.95</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.95</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="NN--MLP-conclusion">NN--MLP conclusion<a class="anchor-link" href="#NN--MLP-conclusion">&#182;</a></h3><p>可以發現 best 的結果就已經是最好的 <br>
因此使用 best 為最佳模型 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="NN--SEQ">NN--SEQ<a class="anchor-link" href="#NN--SEQ">&#182;</a></h2><p>神經網路 (Neural Network) -- 簡單的線性堆疊模型 <br>
一層一層的建立神經網路 <br>
接著列出現在構建的網路長甚麼樣子 <br>
建模之後以 測試集 進行評估 <br>
並且列出 4 種指標以供觀察 <br></p>

</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[26]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># change 1~3 to 0~2</span>
<span class="n">y_train_nn</span> <span class="o">=</span> <span class="n">y_train</span>
<span class="n">y_test_nn</span> <span class="o">=</span> <span class="n">y_test</span>

<span class="c1"># model</span>
<span class="n">nn_seq_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">nn_seq_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                 <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],)))</span>
<span class="n">nn_seq_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="c1"># avoid overfitting</span>
<span class="n">nn_seq_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
<span class="n">nn_seq_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

<span class="c1"># compile</span>
<span class="c1"># set learning rate</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">nn_seq_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                     <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span>
                     <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="c1"># check model</span>
<span class="n">nn_seq_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>WARNING:tensorflow:From C:\Users\ACER\anaconda3\lib\site-packages\keras\src\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

Model: &#34;sequential&#34;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 64)                262208    
                                                                 
 dense_1 (Dense)             (None, 32)                2080      
                                                                 
 dropout (Dropout)           (None, 32)                0         
                                                                 
 dense_2 (Dense)             (None, 40)                1320      
                                                                 
=================================================================
Total params: 265608 (1.01 MB)
Trainable params: 265608 (1.01 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[27]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># fit model</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">nn_seq_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train_nn</span><span class="p">,</span>
                           <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>WARNING:tensorflow:From C:\Users\ACER\anaconda3\lib\site-packages\keras\src\utils\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.

WARNING:tensorflow:From C:\Users\ACER\anaconda3\lib\site-packages\keras\src\engine\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.

</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[28]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">nn_seq_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test_nn</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_pred_prob</span> <span class="o">=</span> <span class="n">nn_seq_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_pred_nn_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred_prob</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loss :&quot;</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy :&quot;</span><span class="p">,</span> <span class="n">test_accuracy</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>loss : 0.2628282904624939
accuracy : 0.9125000238418579
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[82]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">nn_seq_eva</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test_nn</span><span class="p">,</span> <span class="n">y_pred_nn_seq</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">nn_seq_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">nn_seq_eva</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                         <span class="p">[</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;standard&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;NN_SEQ&quot;</span><span class="p">,</span> <span class="s2">&quot;NN_SEQ&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">nn_seq_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[82]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">standard</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">NN_SEQ</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.91</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.94</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.91</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.91</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="compare-evaluation-of-standardize-data">compare evaluation of standardize data<a class="anchor-link" href="#compare-evaluation-of-standardize-data">&#182;</a></h2><p>把上述所有的評估指標組合一起觀察 <br></p>

</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[83]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># combine df together</span>
<span class="n">mlr_df_copy</span> <span class="o">=</span> <span class="n">mlr_df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">svm_df_copy</span> <span class="o">=</span> <span class="n">svm_df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">nn_mlp_df_best_copy</span> <span class="o">=</span> <span class="n">nn_mlp_df_best</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">nn_seq_df_copy</span> <span class="o">=</span> <span class="n">nn_seq_df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="c1"># del same col</span>
<span class="n">svm_df_copy</span><span class="o">.</span><span class="n">drop</span><span class="p">((</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;SVM&quot;</span><span class="p">,</span> <span class="s2">&quot;metric&quot;</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">nn_mlp_df_best_copy</span><span class="o">.</span><span class="n">drop</span><span class="p">((</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;NN_MLP&quot;</span><span class="p">,</span> <span class="s2">&quot;metric&quot;</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">nn_seq_df_copy</span><span class="o">.</span><span class="n">drop</span><span class="p">((</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;NN_SEQ&quot;</span><span class="p">,</span> <span class="s2">&quot;metric&quot;</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># combine</span>
<span class="n">final_df_standard</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
    <span class="p">[</span><span class="n">mlr_df_copy</span><span class="p">,</span> <span class="n">svm_df_copy</span><span class="p">,</span> <span class="n">nn_mlp_df_best_copy</span><span class="p">,</span> <span class="n">nn_seq_df_copy</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># set &quot;metric&quot; to index</span>
<span class="n">final_df_standard</span><span class="o">.</span><span class="n">set_index</span><span class="p">((</span><span class="s2">&quot;standard&quot;</span><span class="p">,</span> <span class="s2">&quot;MLR&quot;</span><span class="p">,</span> <span class="s2">&quot;metric&quot;</span><span class="p">),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">final_df_standard</span> <span class="o">=</span> <span class="n">final_df_standard</span><span class="o">.</span><span class="n">rename_axis</span><span class="p">(</span><span class="s2">&quot;metric&quot;</span><span class="p">)</span>

<span class="c1"># show</span>
<span class="n">final_df_standard</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[83]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="4" halign="left">standard</th>
    </tr>
    <tr>
      <th></th>
      <th>MLR</th>
      <th>SVM</th>
      <th>NN_MLP</th>
      <th>NN_SEQ</th>
    </tr>
    <tr>
      <th></th>
      <th>value</th>
      <th>value</th>
      <th>value</th>
      <th>value</th>
    </tr>
    <tr>
      <th>metric</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>accuracy</th>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.95</td>
      <td>0.91</td>
    </tr>
    <tr>
      <th>precision</th>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.97</td>
      <td>0.94</td>
    </tr>
    <tr>
      <th>recall</th>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.95</td>
      <td>0.91</td>
    </tr>
    <tr>
      <th>F1-score</th>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.95</td>
      <td>0.91</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="model-of-standardize-data-conclusion">model of standardize data conclusion<a class="anchor-link" href="#model-of-standardize-data-conclusion">&#182;</a></h2><p>可以看出總體來說 MLR、NN_MLP 模型的表現是最好的 <br>
不過考慮到運行時間 <br>
MLR 似乎會是比較好的選項 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="modeling-pca-data">modeling pca data<a class="anchor-link" href="#modeling-pca-data">&#182;</a></h1><p>把 PCA 後的資料建模 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="MLR">MLR<a class="anchor-link" href="#MLR">&#182;</a></h2><p>多元羅吉斯回歸 (Multinomial Logistic Regression) <br>
建模之後以 測試集 進行評估 <br>
並且列出 4 種指標以供觀察 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="default">default<a class="anchor-link" href="#default">&#182;</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[38]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># MLR model &amp; hyperparametersmodel</span>
<span class="n">mlr_opts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mlr_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="o">**</span><span class="n">mlr_opts</span><span class="p">)</span>
<span class="n">mlr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># predict</span>
<span class="n">y_pred_mlr</span> <span class="o">=</span> <span class="n">mlr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[62]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">mlr_eva</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlr</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">mlr_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">mlr_eva</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                      <span class="p">[</span><span class="s2">&quot;PCA&quot;</span><span class="p">,</span> <span class="s2">&quot;PCA&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;MLR&quot;</span><span class="p">,</span> <span class="s2">&quot;MLR&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">mlr_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[62]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">PCA</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">MLR</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.99</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="grid-search">grid search<a class="anchor-link" href="#grid-search">&#182;</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[40]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># search best parameter</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="s1">&#39;solver&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;newton-cg&#39;</span><span class="p">,</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">],</span>
    <span class="s1">&#39;penalty&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># model</span>
<span class="n">mlr_opts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mlr_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="o">**</span><span class="n">mlr_opts</span><span class="p">)</span>

<span class="c1"># grid search</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">mlr_model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># print</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters found: &quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="n">y_pred_mlr_best</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Fitting 5 folds for each of 12 candidates, totalling 60 fits
Best parameters found:  {&#39;C&#39;: 1, &#39;penalty&#39;: &#39;l2&#39;, &#39;solver&#39;: &#39;newton-cg&#39;}
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[63]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">mlr_eva_best</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlr_best</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">mlr_df_best</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">mlr_eva_best</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                      <span class="p">[</span><span class="s2">&quot;PCA&quot;</span><span class="p">,</span> <span class="s2">&quot;PCA&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;MLR&quot;</span><span class="p">,</span> <span class="s2">&quot;MLR&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">mlr_df_best</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[63]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">PCA</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">MLR</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.99</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="MLR-conclusion">MLR conclusion<a class="anchor-link" href="#MLR-conclusion">&#182;</a></h3><p>可以發現 2個的結果相同 <br>
為了避免麻煩 <br>
因此使用 default 為最佳模型 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="SVM">SVM<a class="anchor-link" href="#SVM">&#182;</a></h2><p>支援向量機 (Support Vector Machine) <br>
建模之後以 測試集 進行評估 <br>
並且列出 4 種指標以供觀察 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="default">default<a class="anchor-link" href="#default">&#182;</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[42]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># SVM model &amp; hyperparameter</span>
<span class="n">svm_opts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">))</span>
<span class="n">svm_model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">svm_opts</span><span class="p">)</span>
<span class="n">svm_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># predict</span>
<span class="n">y_pred_svm</span> <span class="o">=</span> <span class="n">svm_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[65]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">svm_eva</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">svm_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">svm_eva</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                      <span class="p">[</span><span class="s2">&quot;PCA&quot;</span><span class="p">,</span> <span class="s2">&quot;PCA&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;SVM&quot;</span><span class="p">,</span> <span class="s2">&quot;SVM&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">svm_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[65]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">PCA</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">SVM</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.99</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="grid-search">grid search<a class="anchor-link" href="#grid-search">&#182;</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[44]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># search best parameter</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">],</span>
    <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;auto&#39;</span><span class="p">],</span>
<span class="p">}</span>

<span class="c1"># model</span>
<span class="n">svm_opts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">))</span>
<span class="n">svm_model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="o">**</span><span class="n">svm_opts</span><span class="p">)</span>

<span class="c1"># grid search</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">svm_model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># print</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters found: &quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="n">y_pred_svm_best</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Fitting 5 folds for each of 32 candidates, totalling 160 fits
Best parameters found:  {&#39;C&#39;: 0.08, &#39;gamma&#39;: &#39;scale&#39;, &#39;kernel&#39;: &#39;linear&#39;}
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[45]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">svm_eva_best</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm_best</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">svm_df_best</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">svm_eva_best</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                      <span class="p">[</span><span class="s2">&quot;PCA&quot;</span><span class="p">,</span> <span class="s2">&quot;PCA&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;SVM&quot;</span><span class="p">,</span> <span class="s2">&quot;SVM&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">svm_df_best</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[45]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">PCA</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">SVM</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.99</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.99</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="SVM-conclusion">SVM conclusion<a class="anchor-link" href="#SVM-conclusion">&#182;</a></h3><p>可以發現 2個的結果相同 <br>
為了避免麻煩 <br>
因此使用 default 為最佳模型 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="NN--MLP">NN--MLP<a class="anchor-link" href="#NN--MLP">&#182;</a></h2><p>神經網路 (Neural Network) -- 多層感知機 (Multilayer perceptron) <br>
建模之後以 測試集 進行評估 <br>
並且列出 4 種指標以供觀察 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="default">default<a class="anchor-link" href="#default">&#182;</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[46]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># NN-MLP model &amp; hyperparameter</span>
<span class="n">hidden_layers</span> <span class="o">=</span> <span class="p">(</span><span class="mi">30</span><span class="p">,)</span>
<span class="n">opts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">))</span>
<span class="n">nn_mlp_model</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">opts</span><span class="p">)</span>
<span class="n">nn_mlp_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># predict</span>
<span class="n">y_pred_nn_mlp</span> <span class="o">=</span> <span class="n">nn_mlp_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[47]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">nn_mlp_eva</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nn_mlp</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">nn_mlp_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">nn_mlp_eva</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                         <span class="p">[</span><span class="s2">&quot;PCA&quot;</span><span class="p">,</span> <span class="s2">&quot;PCA&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;NN_MLP&quot;</span><span class="p">,</span> <span class="s2">&quot;NN_MLP&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">nn_mlp_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[47]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">NN_MLP</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.90</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.96</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.90</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.91</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="grid-search">grid search<a class="anchor-link" href="#grid-search">&#182;</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[48]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># search best parameter</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;hidden_layer_sizes&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mi">30</span><span class="p">,)],</span>
    <span class="s1">&#39;activation&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;logistic&#39;</span><span class="p">,</span> <span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">],</span>
    <span class="s1">&#39;solver&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="s1">&#39;sgd&#39;</span><span class="p">,</span> <span class="s1">&#39;adam&#39;</span><span class="p">],</span>
<span class="p">}</span>

<span class="c1"># model</span>
<span class="n">opts</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">))</span>
<span class="n">nn_mlp_model</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">opts</span><span class="p">)</span>

<span class="c1"># grid search</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">nn_mlp_model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># print</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters found: &quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="n">y_pred_nn_mlp_best</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Fitting 5 folds for each of 9 candidates, totalling 45 fits
Best parameters found:  {&#39;activation&#39;: &#39;relu&#39;, &#39;hidden_layer_sizes&#39;: (30,), &#39;solver&#39;: &#39;lbfgs&#39;}
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[49]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">nn_mlp_eva_best</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nn_mlp_best</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">nn_mlp_df_best</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">nn_mlp_eva_best</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                         <span class="p">[</span><span class="s2">&quot;PCA&quot;</span><span class="p">,</span> <span class="s2">&quot;PCA&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;NN_MLP&quot;</span><span class="p">,</span> <span class="s2">&quot;NN_MLP&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">nn_mlp_df_best</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[49]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">PCA</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">NN_MLP</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.95</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.97</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.95</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.95</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="NN--MLP-conclusion">NN--MLP conclusion<a class="anchor-link" href="#NN--MLP-conclusion">&#182;</a></h3><p>可以發現 best 的結果就已經是最好的 <br>
因此使用 best 為最佳模型 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="NN--SEQ">NN--SEQ<a class="anchor-link" href="#NN--SEQ">&#182;</a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[55]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># change 1~3 to 0~2</span>
<span class="n">y_train_nn</span> <span class="o">=</span> <span class="n">y_train</span>
<span class="n">y_test_nn</span> <span class="o">=</span> <span class="n">y_test</span>

<span class="c1"># model</span>
<span class="n">nn_seq_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">nn_seq_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                 <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],)))</span>
<span class="n">nn_seq_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="c1"># avoid overfitting</span>
<span class="n">nn_seq_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
<span class="n">nn_seq_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

<span class="c1"># compile</span>
<span class="c1"># set learning rate</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">nn_seq_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                     <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span>
                     <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="c1"># check model</span>
<span class="n">nn_seq_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Model: &#34;sequential_2&#34;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_6 (Dense)             (None, 64)                1728      
                                                                 
 dense_7 (Dense)             (None, 32)                2080      
                                                                 
 dropout_2 (Dropout)         (None, 32)                0         
                                                                 
 dense_8 (Dense)             (None, 40)                1320      
                                                                 
=================================================================
Total params: 5128 (20.03 KB)
Trainable params: 5128 (20.03 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[58]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># fit model</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">nn_seq_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">,</span> <span class="n">y_train_nn</span><span class="p">,</span>
                           <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Epoch 1/200
8/8 [==============================] - 0s 8ms/step - loss: 0.0949 - accuracy: 0.9648 - val_loss: 0.5425 - val_accuracy: 0.8750
Epoch 2/200
8/8 [==============================] - 0s 5ms/step - loss: 0.1008 - accuracy: 0.9570 - val_loss: 0.5467 - val_accuracy: 0.8906
Epoch 3/200
8/8 [==============================] - 0s 4ms/step - loss: 0.1108 - accuracy: 0.9648 - val_loss: 0.5488 - val_accuracy: 0.8906
Epoch 4/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0675 - accuracy: 0.9844 - val_loss: 0.5563 - val_accuracy: 0.9062
Epoch 5/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0642 - accuracy: 0.9844 - val_loss: 0.5565 - val_accuracy: 0.9062
Epoch 6/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0903 - accuracy: 0.9805 - val_loss: 0.5522 - val_accuracy: 0.9062
Epoch 7/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0781 - accuracy: 0.9805 - val_loss: 0.5611 - val_accuracy: 0.8750
Epoch 8/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0852 - accuracy: 0.9805 - val_loss: 0.5621 - val_accuracy: 0.8750
Epoch 9/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0810 - accuracy: 0.9727 - val_loss: 0.5668 - val_accuracy: 0.8750
Epoch 10/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0868 - accuracy: 0.9688 - val_loss: 0.5667 - val_accuracy: 0.8750
Epoch 11/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0626 - accuracy: 0.9883 - val_loss: 0.5578 - val_accuracy: 0.8750
Epoch 12/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0578 - accuracy: 0.9844 - val_loss: 0.5621 - val_accuracy: 0.8906
Epoch 13/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0763 - accuracy: 0.9844 - val_loss: 0.5712 - val_accuracy: 0.8750
Epoch 14/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0686 - accuracy: 0.9844 - val_loss: 0.5828 - val_accuracy: 0.8594
Epoch 15/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0641 - accuracy: 0.9805 - val_loss: 0.5944 - val_accuracy: 0.8594
Epoch 16/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0656 - accuracy: 0.9883 - val_loss: 0.5854 - val_accuracy: 0.8594
Epoch 17/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0736 - accuracy: 0.9844 - val_loss: 0.5763 - val_accuracy: 0.8594
Epoch 18/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0565 - accuracy: 0.9805 - val_loss: 0.5744 - val_accuracy: 0.8594
Epoch 19/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0526 - accuracy: 0.9883 - val_loss: 0.5708 - val_accuracy: 0.8750
Epoch 20/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9883 - val_loss: 0.5605 - val_accuracy: 0.8906
Epoch 21/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0453 - accuracy: 0.9883 - val_loss: 0.5561 - val_accuracy: 0.8906
Epoch 22/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0414 - accuracy: 0.9922 - val_loss: 0.5368 - val_accuracy: 0.8906
Epoch 23/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0458 - accuracy: 0.9961 - val_loss: 0.5302 - val_accuracy: 0.8750
Epoch 24/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0669 - accuracy: 0.9805 - val_loss: 0.5541 - val_accuracy: 0.8750
Epoch 25/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0537 - accuracy: 0.9883 - val_loss: 0.5637 - val_accuracy: 0.8594
Epoch 26/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.9766 - val_loss: 0.5632 - val_accuracy: 0.8906
Epoch 27/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0822 - accuracy: 0.9727 - val_loss: 0.5637 - val_accuracy: 0.9062
Epoch 28/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0679 - accuracy: 0.9805 - val_loss: 0.5570 - val_accuracy: 0.9062
Epoch 29/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0581 - accuracy: 0.9844 - val_loss: 0.5489 - val_accuracy: 0.8750
Epoch 30/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0703 - accuracy: 0.9883 - val_loss: 0.5517 - val_accuracy: 0.8750
Epoch 31/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0538 - accuracy: 0.9883 - val_loss: 0.5554 - val_accuracy: 0.8750
Epoch 32/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9922 - val_loss: 0.5519 - val_accuracy: 0.8750
Epoch 33/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0886 - accuracy: 0.9766 - val_loss: 0.5447 - val_accuracy: 0.8750
Epoch 34/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0414 - accuracy: 0.9961 - val_loss: 0.5456 - val_accuracy: 0.8750
Epoch 35/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.9766 - val_loss: 0.5265 - val_accuracy: 0.8750
Epoch 36/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.9844 - val_loss: 0.5114 - val_accuracy: 0.8750
Epoch 37/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0430 - accuracy: 0.9922 - val_loss: 0.4977 - val_accuracy: 0.8750
Epoch 38/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0510 - accuracy: 0.9805 - val_loss: 0.5058 - val_accuracy: 0.8750
Epoch 39/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9766 - val_loss: 0.5123 - val_accuracy: 0.8750
Epoch 40/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0460 - accuracy: 0.9883 - val_loss: 0.5079 - val_accuracy: 0.8750
Epoch 41/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0679 - accuracy: 0.9766 - val_loss: 0.5175 - val_accuracy: 0.8750
Epoch 42/200
8/8 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9727 - val_loss: 0.5381 - val_accuracy: 0.8750
Epoch 43/200
8/8 [==============================] - 0s 6ms/step - loss: 0.0458 - accuracy: 0.9766 - val_loss: 0.5533 - val_accuracy: 0.8750
Epoch 44/200
8/8 [==============================] - 0s 6ms/step - loss: 0.0696 - accuracy: 0.9688 - val_loss: 0.5579 - val_accuracy: 0.8750
Epoch 45/200
8/8 [==============================] - 0s 6ms/step - loss: 0.0516 - accuracy: 0.9844 - val_loss: 0.5646 - val_accuracy: 0.8750
Epoch 46/200
8/8 [==============================] - 0s 6ms/step - loss: 0.0583 - accuracy: 0.9922 - val_loss: 0.5614 - val_accuracy: 0.8750
Epoch 47/200
8/8 [==============================] - 0s 6ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 0.5511 - val_accuracy: 0.8750
Epoch 48/200
8/8 [==============================] - 0s 6ms/step - loss: 0.0566 - accuracy: 0.9844 - val_loss: 0.5530 - val_accuracy: 0.8750
Epoch 49/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 0.9805 - val_loss: 0.5406 - val_accuracy: 0.8750
Epoch 50/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0434 - accuracy: 0.9883 - val_loss: 0.5234 - val_accuracy: 0.8906
Epoch 51/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 0.9883 - val_loss: 0.5103 - val_accuracy: 0.8906
Epoch 52/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0618 - accuracy: 0.9766 - val_loss: 0.5073 - val_accuracy: 0.8906
Epoch 53/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0329 - accuracy: 0.9922 - val_loss: 0.5048 - val_accuracy: 0.8906
Epoch 54/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0435 - accuracy: 0.9883 - val_loss: 0.5107 - val_accuracy: 0.8906
Epoch 55/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0622 - accuracy: 0.9883 - val_loss: 0.5211 - val_accuracy: 0.8750
Epoch 56/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 0.9922 - val_loss: 0.5321 - val_accuracy: 0.8750
Epoch 57/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0544 - accuracy: 0.9727 - val_loss: 0.5363 - val_accuracy: 0.8750
Epoch 58/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0415 - accuracy: 0.9883 - val_loss: 0.5390 - val_accuracy: 0.8750
Epoch 59/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0568 - accuracy: 0.9805 - val_loss: 0.5366 - val_accuracy: 0.8750
Epoch 60/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0329 - accuracy: 0.9922 - val_loss: 0.5307 - val_accuracy: 0.8750
Epoch 61/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9922 - val_loss: 0.5189 - val_accuracy: 0.8750
Epoch 62/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0598 - accuracy: 0.9727 - val_loss: 0.5020 - val_accuracy: 0.8750
Epoch 63/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0580 - accuracy: 0.9844 - val_loss: 0.5065 - val_accuracy: 0.8906
Epoch 64/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.9805 - val_loss: 0.5033 - val_accuracy: 0.9062
Epoch 65/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0605 - accuracy: 0.9844 - val_loss: 0.5132 - val_accuracy: 0.9062
Epoch 66/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0432 - accuracy: 0.9883 - val_loss: 0.5281 - val_accuracy: 0.9062
Epoch 67/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.9922 - val_loss: 0.5353 - val_accuracy: 0.8906
Epoch 68/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0324 - accuracy: 0.9883 - val_loss: 0.5392 - val_accuracy: 0.8906
Epoch 69/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0476 - accuracy: 0.9922 - val_loss: 0.5342 - val_accuracy: 0.8906
Epoch 70/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0382 - accuracy: 0.9922 - val_loss: 0.5405 - val_accuracy: 0.8906
Epoch 71/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.9922 - val_loss: 0.5402 - val_accuracy: 0.8906
Epoch 72/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0505 - accuracy: 0.9883 - val_loss: 0.5314 - val_accuracy: 0.8906
Epoch 73/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0538 - accuracy: 0.9844 - val_loss: 0.5319 - val_accuracy: 0.8906
Epoch 74/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0287 - accuracy: 0.9922 - val_loss: 0.5331 - val_accuracy: 0.8906
Epoch 75/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.5381 - val_accuracy: 0.8906
Epoch 76/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.9883 - val_loss: 0.5074 - val_accuracy: 0.8906
Epoch 77/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0224 - accuracy: 0.9961 - val_loss: 0.4933 - val_accuracy: 0.8906
Epoch 78/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0436 - accuracy: 0.9922 - val_loss: 0.4881 - val_accuracy: 0.8906
Epoch 79/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9844 - val_loss: 0.4803 - val_accuracy: 0.8906
Epoch 80/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0452 - accuracy: 0.9844 - val_loss: 0.4614 - val_accuracy: 0.8906
Epoch 81/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0493 - accuracy: 0.9688 - val_loss: 0.4742 - val_accuracy: 0.8906
Epoch 82/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0444 - accuracy: 0.9883 - val_loss: 0.5107 - val_accuracy: 0.8906
Epoch 83/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9961 - val_loss: 0.5348 - val_accuracy: 0.8906
Epoch 84/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.9922 - val_loss: 0.5474 - val_accuracy: 0.8906
Epoch 85/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0530 - accuracy: 0.9805 - val_loss: 0.5308 - val_accuracy: 0.8906
Epoch 86/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0463 - accuracy: 0.9883 - val_loss: 0.5195 - val_accuracy: 0.8906
Epoch 87/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.9961 - val_loss: 0.5065 - val_accuracy: 0.8906
Epoch 88/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0323 - accuracy: 0.9961 - val_loss: 0.4978 - val_accuracy: 0.8906
Epoch 89/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0330 - accuracy: 0.9922 - val_loss: 0.5005 - val_accuracy: 0.8906
Epoch 90/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0298 - accuracy: 0.9883 - val_loss: 0.4886 - val_accuracy: 0.8906
Epoch 91/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.9883 - val_loss: 0.4778 - val_accuracy: 0.8906
Epoch 92/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9922 - val_loss: 0.4684 - val_accuracy: 0.8906
Epoch 93/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.9922 - val_loss: 0.4739 - val_accuracy: 0.8906
Epoch 94/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.9883 - val_loss: 0.4815 - val_accuracy: 0.8906
Epoch 95/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0460 - accuracy: 0.9844 - val_loss: 0.5021 - val_accuracy: 0.8906
Epoch 96/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0544 - accuracy: 0.9844 - val_loss: 0.5132 - val_accuracy: 0.8906
Epoch 97/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0289 - accuracy: 0.9922 - val_loss: 0.5165 - val_accuracy: 0.8906
Epoch 98/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 0.9883 - val_loss: 0.5290 - val_accuracy: 0.8906
Epoch 99/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0427 - accuracy: 0.9844 - val_loss: 0.5329 - val_accuracy: 0.8906
Epoch 100/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0313 - accuracy: 0.9883 - val_loss: 0.5223 - val_accuracy: 0.8906
Epoch 101/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0506 - accuracy: 0.9844 - val_loss: 0.5053 - val_accuracy: 0.8906
Epoch 102/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0790 - accuracy: 0.9727 - val_loss: 0.4891 - val_accuracy: 0.8906
Epoch 103/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9883 - val_loss: 0.4816 - val_accuracy: 0.8906
Epoch 104/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.9883 - val_loss: 0.4857 - val_accuracy: 0.8906
Epoch 105/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0497 - accuracy: 0.9805 - val_loss: 0.4863 - val_accuracy: 0.8906
Epoch 106/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0374 - accuracy: 0.9922 - val_loss: 0.4831 - val_accuracy: 0.8906
Epoch 107/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0367 - accuracy: 0.9883 - val_loss: 0.4753 - val_accuracy: 0.8906
Epoch 108/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.9961 - val_loss: 0.4749 - val_accuracy: 0.8906
Epoch 109/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0443 - accuracy: 0.9883 - val_loss: 0.4737 - val_accuracy: 0.8906
Epoch 110/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0339 - accuracy: 0.9922 - val_loss: 0.4789 - val_accuracy: 0.8906
Epoch 111/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.9883 - val_loss: 0.4907 - val_accuracy: 0.8906
Epoch 112/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.4992 - val_accuracy: 0.8906
Epoch 113/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.9961 - val_loss: 0.4975 - val_accuracy: 0.8906
Epoch 114/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0437 - accuracy: 0.9844 - val_loss: 0.5065 - val_accuracy: 0.8906
Epoch 115/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0235 - accuracy: 0.9961 - val_loss: 0.5187 - val_accuracy: 0.8906
Epoch 116/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0310 - accuracy: 0.9922 - val_loss: 0.5304 - val_accuracy: 0.8906
Epoch 117/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9922 - val_loss: 0.5333 - val_accuracy: 0.8906
Epoch 118/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9883 - val_loss: 0.5316 - val_accuracy: 0.8906
Epoch 119/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9727 - val_loss: 0.5146 - val_accuracy: 0.8906
Epoch 120/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0366 - accuracy: 0.9922 - val_loss: 0.4866 - val_accuracy: 0.8906
Epoch 121/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.9922 - val_loss: 0.4862 - val_accuracy: 0.8906
Epoch 122/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 0.9922 - val_loss: 0.4911 - val_accuracy: 0.8906
Epoch 123/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.9961 - val_loss: 0.4922 - val_accuracy: 0.8906
Epoch 124/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9844 - val_loss: 0.4895 - val_accuracy: 0.8906
Epoch 125/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0268 - accuracy: 0.9922 - val_loss: 0.4659 - val_accuracy: 0.8906
Epoch 126/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.9922 - val_loss: 0.4527 - val_accuracy: 0.9062
Epoch 127/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0476 - accuracy: 0.9883 - val_loss: 0.4562 - val_accuracy: 0.9062
Epoch 128/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 0.9844 - val_loss: 0.4459 - val_accuracy: 0.9062
Epoch 129/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0377 - accuracy: 0.9961 - val_loss: 0.4339 - val_accuracy: 0.9062
Epoch 130/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0419 - accuracy: 0.9883 - val_loss: 0.4320 - val_accuracy: 0.9062
Epoch 131/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0521 - accuracy: 0.9844 - val_loss: 0.4325 - val_accuracy: 0.9062
Epoch 132/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.9805 - val_loss: 0.4278 - val_accuracy: 0.9062
Epoch 133/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0273 - accuracy: 0.9922 - val_loss: 0.4234 - val_accuracy: 0.9219
Epoch 134/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.9922 - val_loss: 0.4203 - val_accuracy: 0.9062
Epoch 135/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 0.9766 - val_loss: 0.4357 - val_accuracy: 0.9062
Epoch 136/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0494 - accuracy: 0.9805 - val_loss: 0.4431 - val_accuracy: 0.9062
Epoch 137/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9844 - val_loss: 0.4431 - val_accuracy: 0.9062
Epoch 138/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9961 - val_loss: 0.4528 - val_accuracy: 0.9062
Epoch 139/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.4592 - val_accuracy: 0.9062
Epoch 140/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.4648 - val_accuracy: 0.9062
Epoch 141/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0524 - accuracy: 0.9766 - val_loss: 0.4560 - val_accuracy: 0.9062
Epoch 142/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.9844 - val_loss: 0.4480 - val_accuracy: 0.9062
Epoch 143/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9727 - val_loss: 0.4238 - val_accuracy: 0.8906
Epoch 144/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9922 - val_loss: 0.4315 - val_accuracy: 0.8906
Epoch 145/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.9961 - val_loss: 0.4367 - val_accuracy: 0.8906
Epoch 146/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9844 - val_loss: 0.4488 - val_accuracy: 0.8906
Epoch 147/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0290 - accuracy: 0.9961 - val_loss: 0.4686 - val_accuracy: 0.8906
Epoch 148/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.9922 - val_loss: 0.4754 - val_accuracy: 0.8906
Epoch 149/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0382 - accuracy: 0.9883 - val_loss: 0.4837 - val_accuracy: 0.8906
Epoch 150/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9961 - val_loss: 0.4952 - val_accuracy: 0.8906
Epoch 151/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.9922 - val_loss: 0.5127 - val_accuracy: 0.8906
Epoch 152/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0382 - accuracy: 0.9883 - val_loss: 0.5154 - val_accuracy: 0.8906
Epoch 153/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.9961 - val_loss: 0.5184 - val_accuracy: 0.9062
Epoch 154/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.9961 - val_loss: 0.5199 - val_accuracy: 0.9062
Epoch 155/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0465 - accuracy: 0.9805 - val_loss: 0.5120 - val_accuracy: 0.9062
Epoch 156/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0239 - accuracy: 0.9961 - val_loss: 0.5106 - val_accuracy: 0.8750
Epoch 157/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9961 - val_loss: 0.5042 - val_accuracy: 0.8750
Epoch 158/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.9961 - val_loss: 0.4939 - val_accuracy: 0.8906
Epoch 159/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.9961 - val_loss: 0.4891 - val_accuracy: 0.8906
Epoch 160/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.9961 - val_loss: 0.4685 - val_accuracy: 0.8906
Epoch 161/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.9844 - val_loss: 0.4535 - val_accuracy: 0.8906
Epoch 162/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0272 - accuracy: 0.9922 - val_loss: 0.4485 - val_accuracy: 0.8906
Epoch 163/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.4581 - val_accuracy: 0.9062
Epoch 164/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0449 - accuracy: 0.9844 - val_loss: 0.4580 - val_accuracy: 0.9062
Epoch 165/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0208 - accuracy: 0.9961 - val_loss: 0.4604 - val_accuracy: 0.9062
Epoch 166/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.9961 - val_loss: 0.4707 - val_accuracy: 0.9062
Epoch 167/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.9922 - val_loss: 0.4747 - val_accuracy: 0.9219
Epoch 168/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.4704 - val_accuracy: 0.9219
Epoch 169/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.4744 - val_accuracy: 0.9219
Epoch 170/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.9922 - val_loss: 0.4792 - val_accuracy: 0.9219
Epoch 171/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.4827 - val_accuracy: 0.9062
Epoch 172/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.9844 - val_loss: 0.4922 - val_accuracy: 0.9062
Epoch 173/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.9883 - val_loss: 0.4867 - val_accuracy: 0.8906
Epoch 174/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.9922 - val_loss: 0.5036 - val_accuracy: 0.8906
Epoch 175/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 0.9922 - val_loss: 0.5059 - val_accuracy: 0.8750
Epoch 176/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0196 - accuracy: 0.9961 - val_loss: 0.5005 - val_accuracy: 0.8750
Epoch 177/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9961 - val_loss: 0.5030 - val_accuracy: 0.8750
Epoch 178/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9961 - val_loss: 0.5075 - val_accuracy: 0.8750
Epoch 179/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.5104 - val_accuracy: 0.8750
Epoch 180/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9961 - val_loss: 0.5134 - val_accuracy: 0.8750
Epoch 181/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9883 - val_loss: 0.5167 - val_accuracy: 0.8750
Epoch 182/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.4976 - val_accuracy: 0.8906
Epoch 183/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9961 - val_loss: 0.4849 - val_accuracy: 0.8906
Epoch 184/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.9961 - val_loss: 0.4773 - val_accuracy: 0.8906
Epoch 185/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.4811 - val_accuracy: 0.8906
Epoch 186/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.9922 - val_loss: 0.4735 - val_accuracy: 0.9062
Epoch 187/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.9922 - val_loss: 0.4778 - val_accuracy: 0.9062
Epoch 188/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9922 - val_loss: 0.4734 - val_accuracy: 0.9062
Epoch 189/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0320 - accuracy: 0.9883 - val_loss: 0.4719 - val_accuracy: 0.9062
Epoch 190/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.4650 - val_accuracy: 0.9062
Epoch 191/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0611 - accuracy: 0.9844 - val_loss: 0.4573 - val_accuracy: 0.9062
Epoch 192/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0263 - accuracy: 0.9922 - val_loss: 0.4473 - val_accuracy: 0.9062
Epoch 193/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0229 - accuracy: 0.9922 - val_loss: 0.4721 - val_accuracy: 0.9062
Epoch 194/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0156 - accuracy: 0.9961 - val_loss: 0.4855 - val_accuracy: 0.9062
Epoch 195/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0240 - accuracy: 0.9883 - val_loss: 0.4881 - val_accuracy: 0.9062
Epoch 196/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.9883 - val_loss: 0.4735 - val_accuracy: 0.9062
Epoch 197/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9922 - val_loss: 0.4495 - val_accuracy: 0.9062
Epoch 198/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9961 - val_loss: 0.4413 - val_accuracy: 0.9062
Epoch 199/200
8/8 [==============================] - 0s 5ms/step - loss: 0.0205 - accuracy: 0.9922 - val_loss: 0.4369 - val_accuracy: 0.9062
Epoch 200/200
8/8 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.9961 - val_loss: 0.4488 - val_accuracy: 0.9062
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[59]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">nn_seq_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
    <span class="n">X_test_pca</span><span class="p">,</span> <span class="n">y_test_nn</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_pred_prob</span> <span class="o">=</span> <span class="n">nn_seq_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_pca</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_pred_nn_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred_prob</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loss :&quot;</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy :&quot;</span><span class="p">,</span> <span class="n">test_accuracy</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>loss : 0.2821381986141205
accuracy : 0.9125000238418579
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[60]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># evaluation</span>
<span class="n">nn_seq_eva</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">(</span><span class="n">y_test_nn</span><span class="p">,</span> <span class="n">y_pred_nn_seq</span><span class="p">)</span>
<span class="c1"># change to df</span>
<span class="n">nn_seq_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">nn_seq_eva</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
                         <span class="p">[</span><span class="s2">&quot;PCA&quot;</span><span class="p">,</span> <span class="s2">&quot;PCA&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;NN_SEQ&quot;</span><span class="p">,</span> <span class="s2">&quot;NN_SEQ&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]])</span>
<span class="n">nn_seq_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[60]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="2" halign="left">PCA</th>
    </tr>
    <tr>
      <th></th>
      <th colspan="2" halign="left">NN_SEQ</th>
    </tr>
    <tr>
      <th></th>
      <th>metric</th>
      <th>value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>accuracy</td>
      <td>0.91</td>
    </tr>
    <tr>
      <th>1</th>
      <td>precision</td>
      <td>0.94</td>
    </tr>
    <tr>
      <th>2</th>
      <td>recall</td>
      <td>0.91</td>
    </tr>
    <tr>
      <th>3</th>
      <td>F1-score</td>
      <td>0.91</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="compare-evaluation-of-pca-data">compare evaluation of pca data<a class="anchor-link" href="#compare-evaluation-of-pca-data">&#182;</a></h2>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[70]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># combine df together</span>
<span class="n">mlr_df_copy</span> <span class="o">=</span> <span class="n">mlr_df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">svm_df_copy</span> <span class="o">=</span> <span class="n">svm_df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">nn_mlp_df_best_copy</span> <span class="o">=</span> <span class="n">nn_mlp_df_best</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">nn_seq_df_copy</span> <span class="o">=</span> <span class="n">nn_seq_df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="c1"># del same col</span>
<span class="n">svm_df_copy</span><span class="o">.</span><span class="n">drop</span><span class="p">((</span><span class="s2">&quot;PCA&quot;</span><span class="p">,</span> <span class="s2">&quot;SVM&quot;</span><span class="p">,</span> <span class="s2">&quot;metric&quot;</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">nn_mlp_df_best_copy</span><span class="o">.</span><span class="n">drop</span><span class="p">((</span><span class="s2">&quot;PCA&quot;</span><span class="p">,</span> <span class="s2">&quot;NN_MLP&quot;</span><span class="p">,</span> <span class="s2">&quot;metric&quot;</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">nn_seq_df_copy</span><span class="o">.</span><span class="n">drop</span><span class="p">((</span><span class="s2">&quot;PCA&quot;</span><span class="p">,</span> <span class="s2">&quot;NN_SEQ&quot;</span><span class="p">,</span> <span class="s2">&quot;metric&quot;</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># combine</span>
<span class="n">final_df_pca</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
    <span class="p">[</span><span class="n">mlr_df_copy</span><span class="p">,</span> <span class="n">svm_df_copy</span><span class="p">,</span> <span class="n">nn_mlp_df_best_copy</span><span class="p">,</span> <span class="n">nn_seq_df_copy</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># set &quot;metric&quot; to index</span>
<span class="n">final_df_pca</span><span class="o">.</span><span class="n">set_index</span><span class="p">((</span><span class="s2">&quot;PCA&quot;</span><span class="p">,</span> <span class="s2">&quot;MLR&quot;</span><span class="p">,</span> <span class="s2">&quot;metric&quot;</span><span class="p">),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">final_df_pca</span> <span class="o">=</span> <span class="n">final_df</span><span class="o">.</span><span class="n">rename_axis</span><span class="p">(</span><span class="s2">&quot;metric&quot;</span><span class="p">)</span>

<span class="c1"># show</span>
<span class="n">final_df_pca</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[70]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="4" halign="left">PCA</th>
    </tr>
    <tr>
      <th></th>
      <th>MLR</th>
      <th>SVM</th>
      <th>NN_MLP</th>
      <th>NN_SEQ</th>
    </tr>
    <tr>
      <th></th>
      <th>value</th>
      <th>value</th>
      <th>value</th>
      <th>value</th>
    </tr>
    <tr>
      <th>metric</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>accuracy</th>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.95</td>
      <td>0.91</td>
    </tr>
    <tr>
      <th>precision</th>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.97</td>
      <td>0.94</td>
    </tr>
    <tr>
      <th>recall</th>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.95</td>
      <td>0.91</td>
    </tr>
    <tr>
      <th>F1-score</th>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.95</td>
      <td>0.91</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="model-of-pca-data-conclusion">model of pca data conclusion<a class="anchor-link" href="#model-of-pca-data-conclusion">&#182;</a></h2><p>可以看出總體來說 MLR、SVM 模型的表現是最好的 <br>
這 2 者相較於神經網路 <br>
不僅可以得到非常高的評估指標分數 <br>
同時運行時間也比較短 <br></p>

</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="final-conclusion">final conclusion<a class="anchor-link" href="#final-conclusion">&#182;</a></h1><p>把上述所有的評估指標組合一起觀察 <br></p>

</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[84]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># final conclusion</span>
<span class="n">final_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
    <span class="p">[</span><span class="n">final_df_standard</span><span class="p">,</span> <span class="n">final_df_pca</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">final_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[84]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }

    .dataframe thead tr:last-of-type th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="4" halign="left">standard</th>
      <th colspan="4" halign="left">PCA</th>
    </tr>
    <tr>
      <th></th>
      <th>MLR</th>
      <th>SVM</th>
      <th>NN_MLP</th>
      <th>NN_SEQ</th>
      <th>MLR</th>
      <th>SVM</th>
      <th>NN_MLP</th>
      <th>NN_SEQ</th>
    </tr>
    <tr>
      <th></th>
      <th>value</th>
      <th>value</th>
      <th>value</th>
      <th>value</th>
      <th>value</th>
      <th>value</th>
      <th>value</th>
      <th>value</th>
    </tr>
    <tr>
      <th>metric</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>accuracy</th>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.95</td>
      <td>0.91</td>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.95</td>
      <td>0.91</td>
    </tr>
    <tr>
      <th>precision</th>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.97</td>
      <td>0.94</td>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.97</td>
      <td>0.94</td>
    </tr>
    <tr>
      <th>recall</th>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.95</td>
      <td>0.91</td>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.95</td>
      <td>0.91</td>
    </tr>
    <tr>
      <th>F1-score</th>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.95</td>
      <td>0.91</td>
      <td>0.99</td>
      <td>0.99</td>
      <td>0.95</td>
      <td>0.91</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>可以發現到其實資料集進行標準化與加上 PCA 之後並沒有很大的差別 <br>
猜測有可能是因為資料集不夠大所以造成沒有太大的差異 <br>
另外也只需要用到比較基礎的模型就能夠得到還不錯的效果 <br></p>

</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[&nbsp;]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

     </div>
</div>
</div>
</div>

</div>
</body>







</html>
