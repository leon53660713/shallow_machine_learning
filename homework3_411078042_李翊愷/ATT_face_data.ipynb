{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed174151",
   "metadata": {},
   "source": [
    "# AT&T_face_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7b1db",
   "metadata": {},
   "source": [
    "# import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7848d4f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:39:32.995457Z",
     "start_time": "2024-05-11T18:39:32.983433Z"
    }
   },
   "outputs": [],
   "source": [
    "# import package\n",
    "# warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# visulaize\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "# adjust data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# ML\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30677833",
   "metadata": {},
   "source": [
    "# import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97ede27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:17:05.659998Z",
     "start_time": "2024-05-11T18:17:05.641196Z"
    }
   },
   "outputs": [],
   "source": [
    "# import ML_func.py\n",
    "from ML_func import ml_func\n",
    "func = ml_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e719be80",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d9de24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:17:07.387688Z",
     "start_time": "2024-05-11T18:17:07.030123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4087</th>\n",
       "      <th>4088</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.309917</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.417355</td>\n",
       "      <td>0.442149</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.607438</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669422</td>\n",
       "      <td>0.652893</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.475207</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.557851</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.710744</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.400826</td>\n",
       "      <td>0.491736</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.586777</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.698347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.128099</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.140496</td>\n",
       "      <td>0.148760</td>\n",
       "      <td>0.152893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.194215</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.190083</td>\n",
       "      <td>0.243802</td>\n",
       "      <td>0.404959</td>\n",
       "      <td>0.483471</td>\n",
       "      <td>0.516529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.764463</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.739669</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.582645</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0.648760</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>0.714876</td>\n",
       "      <td>0.723140</td>\n",
       "      <td>0.731405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161157</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.177686</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0.173554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4097 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.309917  0.367769  0.417355  0.442149  0.528926  0.607438  0.657025   \n",
       "1  0.454545  0.471074  0.512397  0.557851  0.595041  0.640496  0.681818   \n",
       "2  0.318182  0.400826  0.491736  0.528926  0.586777  0.657025  0.681818   \n",
       "3  0.198347  0.194215  0.194215  0.194215  0.190083  0.190083  0.243802   \n",
       "4  0.500000  0.545455  0.582645  0.623967  0.648760  0.690083  0.694215   \n",
       "\n",
       "          7         8         9  ...      4087      4088      4089      4090  \\\n",
       "0  0.677686  0.690083  0.685950  ...  0.669422  0.652893  0.661157  0.475207   \n",
       "1  0.702479  0.710744  0.702479  ...  0.157025  0.136364  0.148760  0.152893   \n",
       "2  0.685950  0.702479  0.698347  ...  0.132231  0.181818  0.136364  0.128099   \n",
       "3  0.404959  0.483471  0.516529  ...  0.636364  0.657025  0.685950  0.727273   \n",
       "4  0.714876  0.723140  0.731405  ...  0.161157  0.177686  0.173554  0.177686   \n",
       "\n",
       "       4091      4092      4093      4094      4095  target  \n",
       "0  0.132231  0.148760  0.152893  0.161157  0.157025       0  \n",
       "1  0.152893  0.152893  0.152893  0.152893  0.152893       0  \n",
       "2  0.148760  0.144628  0.140496  0.148760  0.152893       0  \n",
       "3  0.743802  0.764463  0.752066  0.752066  0.739669       0  \n",
       "4  0.177686  0.177686  0.177686  0.173554  0.173554       0  \n",
       "\n",
       "[5 rows x 4097 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_data = pd.read_csv(\"face_data.csv\")\n",
    "att_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b97550",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T15:19:40.357898Z",
     "start_time": "2024-05-08T15:19:40.342851Z"
    }
   },
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e81932a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:17:30.459606Z",
     "start_time": "2024-05-11T18:17:30.439651Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# take out X, y\n",
    "X = att_data.drop(columns=['target'])\n",
    "y = att_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bdde6a",
   "metadata": {},
   "source": [
    "## original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b404723f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:17:35.203045Z",
     "start_time": "2024-05-11T18:17:35.184681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data : 320\n",
      "testing data : 80\n"
     ]
    }
   ],
   "source": [
    "X_train_original, X_test_original, y_train, y_test = func.preprocess_data(\n",
    "    X, y, 65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de9eca",
   "metadata": {},
   "source": [
    "## standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18aaa7c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:17:38.165744Z",
     "start_time": "2024-05-11T18:17:38.081411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardlize training data : 320\n",
      "standardlize testing data : 80\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test = func.preprocess_data(\n",
    "    X, y, 65536, standard=\"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05289c4",
   "metadata": {},
   "source": [
    "## pca data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "530fc797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:17:41.537012Z",
     "start_time": "2024-05-11T18:17:40.956278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardlize training data : 320\n",
      "standardlize testing data : 80\n",
      "finish doing pca\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_pca, X_test_pca, y_train, y_test = func.preprocess_data(\n",
    "    X, y, 65536, standard=\"True\", pca=\"True\", n_pc=0.8, show_plot=\"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db26b0",
   "metadata": {},
   "source": [
    "上述得到的檔案共有 3 個，分別是以下 :  <br>\n",
    "* 原始的資料 (original) <br>\n",
    "* 標準化後的資料 (scaled) <br>\n",
    "* 先標準化過後，再進行 PCA 的資料，取累積解釋力到 0.8 的 (pca) <br>\n",
    "\n",
    "之後藉由這些資料各自拆分成不同的 train & test <br>\n",
    "共得到 3 種類型的資料 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe82d3",
   "metadata": {},
   "source": [
    "# modeling standardize data\n",
    "把標準化後的資料建模 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77ef8e",
   "metadata": {},
   "source": [
    "## MLR\n",
    "多元羅吉斯回歸 (Multinomial Logistic Regression) <br>\n",
    "建模之後以 測試集 進行評估 <br>\n",
    "並且列出 4 種指標以供觀察 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02479d1",
   "metadata": {},
   "source": [
    "### default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1597a8c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:19:02.456649Z",
     "start_time": "2024-05-11T18:18:49.150553Z"
    }
   },
   "outputs": [],
   "source": [
    "# MLR model & hyperparametersmodel\n",
    "mlr_opts = dict(multi_class='auto', tol=1e-6, max_iter=int(1e6), verbose=1)\n",
    "mlr_model = LogisticRegression(**mlr_opts)\n",
    "mlr_model.fit(X_train_scaled, y_train)\n",
    "# predict\n",
    "y_pred_mlr = mlr_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a1bfc212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:37:40.866443Z",
     "start_time": "2024-05-11T20:37:40.838093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">MLR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    standard      \n",
       "         MLR      \n",
       "      metric value\n",
       "0   accuracy  0.99\n",
       "1  precision  0.99\n",
       "2     recall  0.99\n",
       "3   F1-score  0.99"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "mlr_eva = func.evaluate_model(y_test, y_pred_mlr)\n",
    "# change to df\n",
    "mlr_df = pd.DataFrame(list(mlr_eva.items()), columns=[\n",
    "                      [\"standard\", \"standard\"], [\"MLR\", \"MLR\"], ['metric', 'value']])\n",
    "mlr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5544298",
   "metadata": {},
   "source": [
    "### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd0a75d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:55:37.103176Z",
     "start_time": "2024-05-11T18:51:30.027808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters found:  {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "# search best parameter\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1],\n",
    "    'solver': ['newton-cg', 'lbfgs'],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# model\n",
    "mlr_opts = dict(multi_class='auto', tol=1e-6, max_iter=int(1e6), verbose=1)\n",
    "mlr_model = LogisticRegression(**mlr_opts)\n",
    "\n",
    "# grid search\n",
    "grid_search = GridSearchCV(mlr_model, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# predict\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_mlr_best = best_model.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3d35203a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:37:43.373718Z",
     "start_time": "2024-05-11T20:37:43.342326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">MLR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    standard      \n",
       "         MLR      \n",
       "      metric value\n",
       "0   accuracy  0.99\n",
       "1  precision  0.99\n",
       "2     recall  0.99\n",
       "3   F1-score  0.99"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "mlr_eva_best = func.evaluate_model(y_test, y_pred_mlr_best)\n",
    "# change to df\n",
    "mlr_df_best = pd.DataFrame(list(mlr_eva_best.items()), columns=[\n",
    "                      [\"standard\", \"standard\"], [\"MLR\", \"MLR\"], ['metric', 'value']])\n",
    "mlr_df_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d274e320",
   "metadata": {},
   "source": [
    "### MLR conclusion\n",
    "可以發現 2個的結果相同 <br>\n",
    "為了避免麻煩 <br>\n",
    "因此使用 default 為最佳模型 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e833aa4",
   "metadata": {},
   "source": [
    "## SVM\n",
    "支援向量機 (Support Vector Machine) <br>\n",
    "建模之後以 測試集 進行評估 <br>\n",
    "並且列出 4 種指標以供觀察 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4ff02",
   "metadata": {},
   "source": [
    "### default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2161f040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T19:11:38.924816Z",
     "start_time": "2024-05-11T19:11:38.791217Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVM model & hyperparameter\n",
    "svm_opts = dict(C=1, tol=1e-6, max_iter=int(1e6))\n",
    "svm_model = SVC(kernel='linear', **svm_opts)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "# predict\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "717b53c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:38:05.684312Z",
     "start_time": "2024-05-11T20:38:05.663738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">SVM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    standard      \n",
       "         SVM      \n",
       "      metric value\n",
       "0   accuracy  0.99\n",
       "1  precision  0.99\n",
       "2     recall  0.99\n",
       "3   F1-score  0.99"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "svm_eva = func.evaluate_model(y_test, y_pred_svm)\n",
    "# change to df\n",
    "svm_df = pd.DataFrame(list(svm_eva.items()), columns=[\n",
    "                      [\"standard\", \"standard\"], [\"SVM\", \"SVM\"], ['metric', 'value']])\n",
    "svm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34deed",
   "metadata": {},
   "source": [
    "### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36d66be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T19:13:05.183292Z",
     "start_time": "2024-05-11T19:12:51.947266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best parameters found:  {'C': 0.08, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# search best parameter\n",
    "param_grid = {\n",
    "    'C': [0.08, 0.1, 1, 5],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "}\n",
    "\n",
    "# model\n",
    "svm_opts = dict(tol=1e-6, max_iter=int(1e6))\n",
    "svm_model = SVC(**svm_opts)\n",
    "\n",
    "# grid search\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# predict\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_svm_best = best_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26af7db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T19:13:07.524843Z",
     "start_time": "2024-05-11T19:13:07.493853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">SVM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    standard      \n",
       "         SVM      \n",
       "      metric value\n",
       "0   accuracy  0.96\n",
       "1  precision  0.98\n",
       "2     recall  0.96\n",
       "3   F1-score  0.97"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "svm_eva_best = func.evaluate_model(y_test, y_pred_svm_best)\n",
    "# change to df\n",
    "svm_df_best = pd.DataFrame(list(svm_eva_best.items()), columns=[\n",
    "                      [\"standard\", \"standard\"], [\"SVM\", \"SVM\"], ['metric', 'value']])\n",
    "svm_df_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c5d843",
   "metadata": {},
   "source": [
    "### SVM conclusion\n",
    "可以發現 2個的結果相同 <br>\n",
    "為了避免麻煩 <br>\n",
    "因此使用 default 為最佳模型 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28812e9b",
   "metadata": {},
   "source": [
    "## NN--MLP\n",
    "神經網路 (Neural Network) -- 多層感知機 (Multilayer perceptron) <br>\n",
    "建模之後以 測試集 進行評估 <br>\n",
    "並且列出 4 種指標以供觀察 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502ed30",
   "metadata": {},
   "source": [
    "### default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9593a9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T19:14:52.905491Z",
     "start_time": "2024-05-11T19:14:42.923742Z"
    }
   },
   "outputs": [],
   "source": [
    "# NN-MLP model & hyperparameter\n",
    "hidden_layers = (30,)\n",
    "opts = dict(hidden_layer_sizes=hidden_layers, verbose=0,\n",
    "            activation='relu', tol=1e-6, max_iter=int(1e6))\n",
    "nn_mlp_model = MLPClassifier(solver='adam', **opts)\n",
    "nn_mlp_model.fit(X_train_scaled, y_train)\n",
    "# predict\n",
    "y_pred_nn_mlp = nn_mlp_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64eee9b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:22:55.621140Z",
     "start_time": "2024-05-11T20:22:55.600231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">NN_MLP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    standard      \n",
       "      NN_MLP      \n",
       "      metric value\n",
       "0   accuracy  0.89\n",
       "1  precision  0.94\n",
       "2     recall  0.89\n",
       "3   F1-score  0.89"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "nn_mlp_eva = func.evaluate_model(y_test, y_pred_nn_mlp)\n",
    "# change to df\n",
    "nn_mlp_df = pd.DataFrame(list(nn_mlp_eva.items()), columns=[\n",
    "                         [\"standard\", \"standard\"], [\"NN_MLP\", \"NN_MLP\"], ['metric', 'value']])\n",
    "nn_mlp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9350489",
   "metadata": {},
   "source": [
    "### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "891f5ed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:20:33.470953Z",
     "start_time": "2024-05-11T19:42:50.416350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Iteration 1, loss = 3.74697672\n",
      "Iteration 2, loss = 3.74202818\n",
      "Iteration 3, loss = 3.73408647\n",
      "Iteration 4, loss = 3.72429764\n",
      "Iteration 5, loss = 3.71252714\n",
      "Iteration 6, loss = 3.69983980\n",
      "Iteration 7, loss = 3.68623623\n",
      "Iteration 8, loss = 3.67252057\n",
      "Iteration 9, loss = 3.65841552\n",
      "Iteration 10, loss = 3.64447281\n",
      "Iteration 11, loss = 3.63094157\n",
      "Iteration 12, loss = 3.61783022\n",
      "Iteration 13, loss = 3.60511916\n",
      "Iteration 14, loss = 3.59271689\n",
      "Iteration 15, loss = 3.58076370\n",
      "Iteration 16, loss = 3.56936486\n",
      "Iteration 17, loss = 3.55823431\n",
      "Iteration 18, loss = 3.54765610\n",
      "Iteration 19, loss = 3.53738687\n",
      "Iteration 20, loss = 3.52743680\n",
      "Iteration 21, loss = 3.51790555\n",
      "Iteration 22, loss = 3.50870775\n",
      "Iteration 23, loss = 3.49973912\n",
      "Iteration 24, loss = 3.49100037\n",
      "Iteration 25, loss = 3.48268464\n",
      "Iteration 26, loss = 3.47450938\n",
      "Iteration 27, loss = 3.46666637\n",
      "Iteration 28, loss = 3.45892082\n",
      "Iteration 29, loss = 3.45144703\n",
      "Iteration 30, loss = 3.44416208\n",
      "Iteration 31, loss = 3.43710296\n",
      "Iteration 32, loss = 3.43013712\n",
      "Iteration 33, loss = 3.42338096\n",
      "Iteration 34, loss = 3.41679905\n",
      "Iteration 35, loss = 3.41040235\n",
      "Iteration 36, loss = 3.40401083\n",
      "Iteration 37, loss = 3.39790445\n",
      "Iteration 38, loss = 3.39179723\n",
      "Iteration 39, loss = 3.38579636\n",
      "Iteration 40, loss = 3.37992199\n",
      "Iteration 41, loss = 3.37419671\n",
      "Iteration 42, loss = 3.36856697\n",
      "Iteration 43, loss = 3.36296793\n",
      "Iteration 44, loss = 3.35755146\n",
      "Iteration 45, loss = 3.35212180\n",
      "Iteration 46, loss = 3.34677286\n",
      "Iteration 47, loss = 3.34154075\n",
      "Iteration 48, loss = 3.33647176\n",
      "Iteration 49, loss = 3.33135471\n",
      "Iteration 50, loss = 3.32633870\n",
      "Iteration 51, loss = 3.32135353\n",
      "Iteration 52, loss = 3.31646785\n",
      "Iteration 53, loss = 3.31160245\n",
      "Iteration 54, loss = 3.30683829\n",
      "Iteration 55, loss = 3.30213072\n",
      "Iteration 56, loss = 3.29745145\n",
      "Iteration 57, loss = 3.29282666\n",
      "Iteration 58, loss = 3.28825390\n",
      "Iteration 59, loss = 3.28365878\n",
      "Iteration 60, loss = 3.27918521\n",
      "Iteration 61, loss = 3.27476499\n",
      "Iteration 62, loss = 3.27030734\n",
      "Iteration 63, loss = 3.26600127\n",
      "Iteration 64, loss = 3.26161161\n",
      "Iteration 65, loss = 3.25736597\n",
      "Iteration 66, loss = 3.25308695\n",
      "Iteration 67, loss = 3.24885958\n",
      "Iteration 68, loss = 3.24469412\n",
      "Iteration 69, loss = 3.24060442\n",
      "Iteration 70, loss = 3.23643266\n",
      "Iteration 71, loss = 3.23238947\n",
      "Iteration 72, loss = 3.22833460\n",
      "Iteration 73, loss = 3.22430763\n",
      "Iteration 74, loss = 3.22035003\n",
      "Iteration 75, loss = 3.21642257\n",
      "Iteration 76, loss = 3.21250161\n",
      "Iteration 77, loss = 3.20858761\n",
      "Iteration 78, loss = 3.20472181\n",
      "Iteration 79, loss = 3.20090130\n",
      "Iteration 80, loss = 3.19709997\n",
      "Iteration 81, loss = 3.19328342\n",
      "Iteration 82, loss = 3.18952816\n",
      "Iteration 83, loss = 3.18585333\n",
      "Iteration 84, loss = 3.18206187\n",
      "Iteration 85, loss = 3.17838564\n",
      "Iteration 86, loss = 3.17472345\n",
      "Iteration 87, loss = 3.17109695\n",
      "Iteration 88, loss = 3.16746441\n",
      "Iteration 89, loss = 3.16389226\n",
      "Iteration 90, loss = 3.16031215\n",
      "Iteration 91, loss = 3.15675729\n",
      "Iteration 92, loss = 3.15322831\n",
      "Iteration 93, loss = 3.14972620\n",
      "Iteration 94, loss = 3.14623803\n",
      "Iteration 95, loss = 3.14274836\n",
      "Iteration 96, loss = 3.13932788\n",
      "Iteration 97, loss = 3.13583642\n",
      "Iteration 98, loss = 3.13243916\n",
      "Iteration 99, loss = 3.12909301\n",
      "Iteration 100, loss = 3.12567156\n",
      "Iteration 101, loss = 3.12226614\n",
      "Iteration 102, loss = 3.11896675\n",
      "Iteration 103, loss = 3.11564343\n",
      "Iteration 104, loss = 3.11231958\n",
      "Iteration 105, loss = 3.10902619\n",
      "Iteration 106, loss = 3.10575171\n",
      "Iteration 107, loss = 3.10249751\n",
      "Iteration 108, loss = 3.09924204\n",
      "Iteration 109, loss = 3.09598430\n",
      "Iteration 110, loss = 3.09276771\n",
      "Iteration 111, loss = 3.08955161\n",
      "Iteration 112, loss = 3.08640665\n",
      "Iteration 113, loss = 3.08324408\n",
      "Iteration 114, loss = 3.08005859\n",
      "Iteration 115, loss = 3.07692231\n",
      "Iteration 116, loss = 3.07379240\n",
      "Iteration 117, loss = 3.07066068\n",
      "Iteration 118, loss = 3.06755058\n",
      "Iteration 119, loss = 3.06446797\n",
      "Iteration 120, loss = 3.06140555\n",
      "Iteration 121, loss = 3.05831603\n",
      "Iteration 122, loss = 3.05528239\n",
      "Iteration 123, loss = 3.05222406\n",
      "Iteration 124, loss = 3.04918592\n",
      "Iteration 125, loss = 3.04618264\n",
      "Iteration 126, loss = 3.04318663\n",
      "Iteration 127, loss = 3.04017937\n",
      "Iteration 128, loss = 3.03720994\n",
      "Iteration 129, loss = 3.03426220\n",
      "Iteration 130, loss = 3.03128998\n",
      "Iteration 131, loss = 3.02834098\n",
      "Iteration 132, loss = 3.02538299\n",
      "Iteration 133, loss = 3.02250560\n",
      "Iteration 134, loss = 3.01957376\n",
      "Iteration 135, loss = 3.01666256\n",
      "Iteration 136, loss = 3.01375652\n",
      "Iteration 137, loss = 3.01086941\n",
      "Iteration 138, loss = 3.00798416\n",
      "Iteration 139, loss = 3.00511987\n",
      "Iteration 140, loss = 3.00226303\n",
      "Iteration 141, loss = 2.99936079\n",
      "Iteration 142, loss = 2.99653463\n",
      "Iteration 143, loss = 2.99370852\n",
      "Iteration 144, loss = 2.99087240\n",
      "Iteration 145, loss = 2.98804003\n",
      "Iteration 146, loss = 2.98521763\n",
      "Iteration 147, loss = 2.98242912\n",
      "Iteration 148, loss = 2.97962749\n",
      "Iteration 149, loss = 2.97683557\n",
      "Iteration 150, loss = 2.97403434\n",
      "Iteration 151, loss = 2.97134086\n",
      "Iteration 152, loss = 2.96853842\n",
      "Iteration 153, loss = 2.96579950\n",
      "Iteration 154, loss = 2.96304623\n",
      "Iteration 155, loss = 2.96031157\n",
      "Iteration 156, loss = 2.95759755\n",
      "Iteration 157, loss = 2.95487707\n",
      "Iteration 158, loss = 2.95217482\n",
      "Iteration 159, loss = 2.94947243\n",
      "Iteration 160, loss = 2.94674453\n",
      "Iteration 161, loss = 2.94407270\n",
      "Iteration 162, loss = 2.94137701\n",
      "Iteration 163, loss = 2.93870387\n",
      "Iteration 164, loss = 2.93603673\n",
      "Iteration 165, loss = 2.93334584\n",
      "Iteration 166, loss = 2.93069377\n",
      "Iteration 167, loss = 2.92807672\n",
      "Iteration 168, loss = 2.92538553\n",
      "Iteration 169, loss = 2.92272501\n",
      "Iteration 170, loss = 2.92009206\n",
      "Iteration 171, loss = 2.91749621\n",
      "Iteration 172, loss = 2.91484472\n",
      "Iteration 173, loss = 2.91223224\n",
      "Iteration 174, loss = 2.90959320\n",
      "Iteration 175, loss = 2.90700071\n",
      "Iteration 176, loss = 2.90442101\n",
      "Iteration 177, loss = 2.90178484\n",
      "Iteration 178, loss = 2.89921499\n",
      "Iteration 179, loss = 2.89667257\n",
      "Iteration 180, loss = 2.89404772\n",
      "Iteration 181, loss = 2.89148056\n",
      "Iteration 182, loss = 2.88889888\n",
      "Iteration 183, loss = 2.88635992\n",
      "Iteration 184, loss = 2.88377643\n",
      "Iteration 185, loss = 2.88121660\n",
      "Iteration 186, loss = 2.87865994\n",
      "Iteration 187, loss = 2.87613655\n",
      "Iteration 188, loss = 2.87359744\n",
      "Iteration 189, loss = 2.87104827\n",
      "Iteration 190, loss = 2.86853114\n",
      "Iteration 191, loss = 2.86600530\n",
      "Iteration 192, loss = 2.86350432\n",
      "Iteration 193, loss = 2.86096994\n",
      "Iteration 194, loss = 2.85848066\n",
      "Iteration 195, loss = 2.85601541\n",
      "Iteration 196, loss = 2.85347978\n",
      "Iteration 197, loss = 2.85100757\n",
      "Iteration 198, loss = 2.84855840\n",
      "Iteration 199, loss = 2.84604404\n",
      "Iteration 200, loss = 2.84360272\n",
      "Iteration 201, loss = 2.84107457\n",
      "Iteration 202, loss = 2.83863074\n",
      "Iteration 203, loss = 2.83617706\n",
      "Iteration 204, loss = 2.83372925\n",
      "Iteration 205, loss = 2.83125071\n",
      "Iteration 206, loss = 2.82881896\n",
      "Iteration 207, loss = 2.82639372\n",
      "Iteration 208, loss = 2.82393389\n",
      "Iteration 209, loss = 2.82149191\n",
      "Iteration 210, loss = 2.81906092\n",
      "Iteration 211, loss = 2.81662566\n",
      "Iteration 212, loss = 2.81419936\n",
      "Iteration 213, loss = 2.81178079\n",
      "Iteration 214, loss = 2.80938320\n",
      "Iteration 215, loss = 2.80695800\n",
      "Iteration 216, loss = 2.80456713\n",
      "Iteration 217, loss = 2.80214241\n",
      "Iteration 218, loss = 2.79975976\n",
      "Iteration 219, loss = 2.79735098\n",
      "Iteration 220, loss = 2.79495757\n",
      "Iteration 221, loss = 2.79256453\n",
      "Iteration 222, loss = 2.79020263\n",
      "Iteration 223, loss = 2.78781146\n",
      "Iteration 224, loss = 2.78543672\n",
      "Iteration 225, loss = 2.78303992\n",
      "Iteration 226, loss = 2.78065589\n",
      "Iteration 227, loss = 2.77829342\n",
      "Iteration 228, loss = 2.77593138\n",
      "Iteration 229, loss = 2.77356219\n",
      "Iteration 230, loss = 2.77120585\n",
      "Iteration 231, loss = 2.76885466\n",
      "Iteration 232, loss = 2.76647766\n",
      "Iteration 233, loss = 2.76411202\n",
      "Iteration 234, loss = 2.76179178\n",
      "Iteration 235, loss = 2.75943980\n",
      "Iteration 236, loss = 2.75708377\n",
      "Iteration 237, loss = 2.75472355\n",
      "Iteration 238, loss = 2.75238882\n",
      "Iteration 239, loss = 2.75003767\n",
      "Iteration 240, loss = 2.74772916\n",
      "Iteration 241, loss = 2.74537044\n",
      "Iteration 242, loss = 2.74304655\n",
      "Iteration 243, loss = 2.74073147\n",
      "Iteration 244, loss = 2.73840163\n",
      "Iteration 245, loss = 2.73608434\n",
      "Iteration 246, loss = 2.73376675\n",
      "Iteration 247, loss = 2.73145670\n",
      "Iteration 248, loss = 2.72912591\n",
      "Iteration 249, loss = 2.72680321\n",
      "Iteration 250, loss = 2.72452507\n",
      "Iteration 251, loss = 2.72222550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 252, loss = 2.71989465\n",
      "Iteration 253, loss = 2.71760500\n",
      "Iteration 254, loss = 2.71532434\n",
      "Iteration 255, loss = 2.71300876\n",
      "Iteration 256, loss = 2.71073341\n",
      "Iteration 257, loss = 2.70844251\n",
      "Iteration 258, loss = 2.70615017\n",
      "Iteration 259, loss = 2.70386832\n",
      "Iteration 260, loss = 2.70157651\n",
      "Iteration 261, loss = 2.69931231\n",
      "Iteration 262, loss = 2.69701053\n",
      "Iteration 263, loss = 2.69475545\n",
      "Iteration 264, loss = 2.69247821\n",
      "Iteration 265, loss = 2.69017773\n",
      "Iteration 266, loss = 2.68793112\n",
      "Iteration 267, loss = 2.68567450\n",
      "Iteration 268, loss = 2.68339823\n",
      "Iteration 269, loss = 2.68113014\n",
      "Iteration 270, loss = 2.67885372\n",
      "Iteration 271, loss = 2.67661297\n",
      "Iteration 272, loss = 2.67433781\n",
      "Iteration 273, loss = 2.67210459\n",
      "Iteration 274, loss = 2.66982854\n",
      "Iteration 275, loss = 2.66758241\n",
      "Iteration 276, loss = 2.66534291\n",
      "Iteration 277, loss = 2.66308725\n",
      "Iteration 278, loss = 2.66088784\n",
      "Iteration 279, loss = 2.65861537\n",
      "Iteration 280, loss = 2.65637173\n",
      "Iteration 281, loss = 2.65416677\n",
      "Iteration 282, loss = 2.65190942\n",
      "Iteration 283, loss = 2.64968395\n",
      "Iteration 284, loss = 2.64745189\n",
      "Iteration 285, loss = 2.64522549\n",
      "Iteration 286, loss = 2.64299350\n",
      "Iteration 287, loss = 2.64079480\n",
      "Iteration 288, loss = 2.63857068\n",
      "Iteration 289, loss = 2.63632773\n",
      "Iteration 290, loss = 2.63413328\n",
      "Iteration 291, loss = 2.63190663\n",
      "Iteration 292, loss = 2.62968628\n",
      "Iteration 293, loss = 2.62748187\n",
      "Iteration 294, loss = 2.62525272\n",
      "Iteration 295, loss = 2.62304543\n",
      "Iteration 296, loss = 2.62086428\n",
      "Iteration 297, loss = 2.61863371\n",
      "Iteration 298, loss = 2.61645860\n",
      "Iteration 299, loss = 2.61423411\n",
      "Iteration 300, loss = 2.61206076\n",
      "Iteration 301, loss = 2.60982117\n",
      "Iteration 302, loss = 2.60764385\n",
      "Iteration 303, loss = 2.60543942\n",
      "Iteration 304, loss = 2.60326866\n",
      "Iteration 305, loss = 2.60109076\n",
      "Iteration 306, loss = 2.59888052\n",
      "Iteration 307, loss = 2.59669723\n",
      "Iteration 308, loss = 2.59450334\n",
      "Iteration 309, loss = 2.59231879\n",
      "Iteration 310, loss = 2.59011844\n",
      "Iteration 311, loss = 2.58795379\n",
      "Iteration 312, loss = 2.58576065\n",
      "Iteration 313, loss = 2.58357359\n",
      "Iteration 314, loss = 2.58138633\n",
      "Iteration 315, loss = 2.57920474\n",
      "Iteration 316, loss = 2.57704129\n",
      "Iteration 317, loss = 2.57487567\n",
      "Iteration 318, loss = 2.57268588\n",
      "Iteration 319, loss = 2.57050572\n",
      "Iteration 320, loss = 2.56838881\n",
      "Iteration 321, loss = 2.56617873\n",
      "Iteration 322, loss = 2.56402055\n",
      "Iteration 323, loss = 2.56185102\n",
      "Iteration 324, loss = 2.55969392\n",
      "Iteration 325, loss = 2.55753912\n",
      "Iteration 326, loss = 2.55536222\n",
      "Iteration 327, loss = 2.55323624\n",
      "Iteration 328, loss = 2.55106475\n",
      "Iteration 329, loss = 2.54891690\n",
      "Iteration 330, loss = 2.54676690\n",
      "Iteration 331, loss = 2.54460904\n",
      "Iteration 332, loss = 2.54245883\n",
      "Iteration 333, loss = 2.54033298\n",
      "Iteration 334, loss = 2.53817246\n",
      "Iteration 335, loss = 2.53603039\n",
      "Iteration 336, loss = 2.53391831\n",
      "Iteration 337, loss = 2.53178044\n",
      "Iteration 338, loss = 2.52961828\n",
      "Iteration 339, loss = 2.52750715\n",
      "Iteration 340, loss = 2.52535070\n",
      "Iteration 341, loss = 2.52320976\n",
      "Iteration 342, loss = 2.52109728\n",
      "Iteration 343, loss = 2.51895922\n",
      "Iteration 344, loss = 2.51683614\n",
      "Iteration 345, loss = 2.51470728\n",
      "Iteration 346, loss = 2.51257539\n",
      "Iteration 347, loss = 2.51045629\n",
      "Iteration 348, loss = 2.50834052\n",
      "Iteration 349, loss = 2.50620893\n",
      "Iteration 350, loss = 2.50410387\n",
      "Iteration 351, loss = 2.50196683\n",
      "Iteration 352, loss = 2.49985309\n",
      "Iteration 353, loss = 2.49772847\n",
      "Iteration 354, loss = 2.49561849\n",
      "Iteration 355, loss = 2.49351616\n",
      "Iteration 356, loss = 2.49138970\n",
      "Iteration 357, loss = 2.48929192\n",
      "Iteration 358, loss = 2.48719376\n",
      "Iteration 359, loss = 2.48506622\n",
      "Iteration 360, loss = 2.48298217\n",
      "Iteration 361, loss = 2.48089034\n",
      "Iteration 362, loss = 2.47875612\n",
      "Iteration 363, loss = 2.47667331\n",
      "Iteration 364, loss = 2.47460574\n",
      "Iteration 365, loss = 2.47248807\n",
      "Iteration 366, loss = 2.47038468\n",
      "Iteration 367, loss = 2.46829547\n",
      "Iteration 368, loss = 2.46621739\n",
      "Iteration 369, loss = 2.46410566\n",
      "Iteration 370, loss = 2.46203352\n",
      "Iteration 371, loss = 2.45993724\n",
      "Iteration 372, loss = 2.45786316\n",
      "Iteration 373, loss = 2.45576166\n",
      "Iteration 374, loss = 2.45368734\n",
      "Iteration 375, loss = 2.45160082\n",
      "Iteration 376, loss = 2.44954408\n",
      "Iteration 377, loss = 2.44743396\n",
      "Iteration 378, loss = 2.44536335\n",
      "Iteration 379, loss = 2.44328295\n",
      "Iteration 380, loss = 2.44120858\n",
      "Iteration 381, loss = 2.43914871\n",
      "Iteration 382, loss = 2.43704341\n",
      "Iteration 383, loss = 2.43497951\n",
      "Iteration 384, loss = 2.43290464\n",
      "Iteration 385, loss = 2.43082949\n",
      "Iteration 386, loss = 2.42875887\n",
      "Iteration 387, loss = 2.42668262\n",
      "Iteration 388, loss = 2.42463037\n",
      "Iteration 389, loss = 2.42255040\n",
      "Iteration 390, loss = 2.42050713\n",
      "Iteration 391, loss = 2.41843526\n",
      "Iteration 392, loss = 2.41636954\n",
      "Iteration 393, loss = 2.41429923\n",
      "Iteration 394, loss = 2.41224963\n",
      "Iteration 395, loss = 2.41021996\n",
      "Iteration 396, loss = 2.40813472\n",
      "Iteration 397, loss = 2.40609565\n",
      "Iteration 398, loss = 2.40403496\n",
      "Iteration 399, loss = 2.40194474\n",
      "Iteration 400, loss = 2.39991103\n",
      "Iteration 401, loss = 2.39785937\n",
      "Iteration 402, loss = 2.39578672\n",
      "Iteration 403, loss = 2.39374658\n",
      "Iteration 404, loss = 2.39169156\n",
      "Iteration 405, loss = 2.38964012\n",
      "Iteration 406, loss = 2.38760096\n",
      "Iteration 407, loss = 2.38554789\n",
      "Iteration 408, loss = 2.38350956\n",
      "Iteration 409, loss = 2.38145616\n",
      "Iteration 410, loss = 2.37942479\n",
      "Iteration 411, loss = 2.37738321\n",
      "Iteration 412, loss = 2.37535473\n",
      "Iteration 413, loss = 2.37330932\n",
      "Iteration 414, loss = 2.37129270\n",
      "Iteration 415, loss = 2.36925249\n",
      "Iteration 416, loss = 2.36719373\n",
      "Iteration 417, loss = 2.36517874\n",
      "Iteration 418, loss = 2.36313176\n",
      "Iteration 419, loss = 2.36111302\n",
      "Iteration 420, loss = 2.35908341\n",
      "Iteration 421, loss = 2.35702449\n",
      "Iteration 422, loss = 2.35502227\n",
      "Iteration 423, loss = 2.35297665\n",
      "Iteration 424, loss = 2.35095228\n",
      "Iteration 425, loss = 2.34893661\n",
      "Iteration 426, loss = 2.34689225\n",
      "Iteration 427, loss = 2.34487368\n",
      "Iteration 428, loss = 2.34285566\n",
      "Iteration 429, loss = 2.34080687\n",
      "Iteration 430, loss = 2.33879103\n",
      "Iteration 431, loss = 2.33678665\n",
      "Iteration 432, loss = 2.33474205\n",
      "Iteration 433, loss = 2.33276044\n",
      "Iteration 434, loss = 2.33071849\n",
      "Iteration 435, loss = 2.32871749\n",
      "Iteration 436, loss = 2.32669814\n",
      "Iteration 437, loss = 2.32468625\n",
      "Iteration 438, loss = 2.32266971\n",
      "Iteration 439, loss = 2.32065508\n",
      "Iteration 440, loss = 2.31865331\n",
      "Iteration 441, loss = 2.31664414\n",
      "Iteration 442, loss = 2.31464106\n",
      "Iteration 443, loss = 2.31265761\n",
      "Iteration 444, loss = 2.31063005\n",
      "Iteration 445, loss = 2.30861490\n",
      "Iteration 446, loss = 2.30661503\n",
      "Iteration 447, loss = 2.30461697\n",
      "Iteration 448, loss = 2.30261996\n",
      "Iteration 449, loss = 2.30061147\n",
      "Iteration 450, loss = 2.29863094\n",
      "Iteration 451, loss = 2.29661666\n",
      "Iteration 452, loss = 2.29463427\n",
      "Iteration 453, loss = 2.29262077\n",
      "Iteration 454, loss = 2.29063658\n",
      "Iteration 455, loss = 2.28865194\n",
      "Iteration 456, loss = 2.28665648\n",
      "Iteration 457, loss = 2.28469331\n",
      "Iteration 458, loss = 2.28268522\n",
      "Iteration 459, loss = 2.28070123\n",
      "Iteration 460, loss = 2.27869616\n",
      "Iteration 461, loss = 2.27673424\n",
      "Iteration 462, loss = 2.27473402\n",
      "Iteration 463, loss = 2.27276335\n",
      "Iteration 464, loss = 2.27076749\n",
      "Iteration 465, loss = 2.26882709\n",
      "Iteration 466, loss = 2.26681113\n",
      "Iteration 467, loss = 2.26485731\n",
      "Iteration 468, loss = 2.26289171\n",
      "Iteration 469, loss = 2.26089321\n",
      "Iteration 470, loss = 2.25891698\n",
      "Iteration 471, loss = 2.25693073\n",
      "Iteration 472, loss = 2.25497852\n",
      "Iteration 473, loss = 2.25302371\n",
      "Iteration 474, loss = 2.25102386\n",
      "Iteration 475, loss = 2.24907294\n",
      "Iteration 476, loss = 2.24708890\n",
      "Iteration 477, loss = 2.24515491\n",
      "Iteration 478, loss = 2.24318014\n",
      "Iteration 479, loss = 2.24120778\n",
      "Iteration 480, loss = 2.23924266\n",
      "Iteration 481, loss = 2.23727813\n",
      "Iteration 482, loss = 2.23531759\n",
      "Iteration 483, loss = 2.23336912\n",
      "Iteration 484, loss = 2.23140919\n",
      "Iteration 485, loss = 2.22946041\n",
      "Iteration 486, loss = 2.22748864\n",
      "Iteration 487, loss = 2.22553168\n",
      "Iteration 488, loss = 2.22358731\n",
      "Iteration 489, loss = 2.22164242\n",
      "Iteration 490, loss = 2.21969505\n",
      "Iteration 491, loss = 2.21773656\n",
      "Iteration 492, loss = 2.21580553\n",
      "Iteration 493, loss = 2.21384525\n",
      "Iteration 494, loss = 2.21188794\n",
      "Iteration 495, loss = 2.20995947\n",
      "Iteration 496, loss = 2.20799493\n",
      "Iteration 497, loss = 2.20604761\n",
      "Iteration 498, loss = 2.20411363\n",
      "Iteration 499, loss = 2.20216611\n",
      "Iteration 500, loss = 2.20021835\n",
      "Iteration 501, loss = 2.19830364\n",
      "Iteration 502, loss = 2.19635360\n",
      "Iteration 503, loss = 2.19441564\n",
      "Iteration 504, loss = 2.19247701\n",
      "Iteration 505, loss = 2.19053096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 506, loss = 2.18859596\n",
      "Iteration 507, loss = 2.18666183\n",
      "Iteration 508, loss = 2.18472958\n",
      "Iteration 509, loss = 2.18280601\n",
      "Iteration 510, loss = 2.18084746\n",
      "Iteration 511, loss = 2.17893984\n",
      "Iteration 512, loss = 2.17701487\n",
      "Iteration 513, loss = 2.17508122\n",
      "Iteration 514, loss = 2.17316037\n",
      "Iteration 515, loss = 2.17122309\n",
      "Iteration 516, loss = 2.16930938\n",
      "Iteration 517, loss = 2.16737781\n",
      "Iteration 518, loss = 2.16545942\n",
      "Iteration 519, loss = 2.16353674\n",
      "Iteration 520, loss = 2.16161925\n",
      "Iteration 521, loss = 2.15970319\n",
      "Iteration 522, loss = 2.15777416\n",
      "Iteration 523, loss = 2.15586712\n",
      "Iteration 524, loss = 2.15394071\n",
      "Iteration 525, loss = 2.15203118\n",
      "Iteration 526, loss = 2.15010419\n",
      "Iteration 527, loss = 2.14820256\n",
      "Iteration 528, loss = 2.14629175\n",
      "Iteration 529, loss = 2.14437534\n",
      "Iteration 530, loss = 2.14245891\n",
      "Iteration 531, loss = 2.14056248\n",
      "Iteration 532, loss = 2.13865560\n",
      "Iteration 533, loss = 2.13674829\n",
      "Iteration 534, loss = 2.13484392\n",
      "Iteration 535, loss = 2.13295110\n",
      "Iteration 536, loss = 2.13103175\n",
      "Iteration 537, loss = 2.12913754\n",
      "Iteration 538, loss = 2.12724337\n",
      "Iteration 539, loss = 2.12533542\n",
      "Iteration 540, loss = 2.12343361\n",
      "Iteration 541, loss = 2.12153184\n",
      "Iteration 542, loss = 2.11964641\n",
      "Iteration 543, loss = 2.11773432\n",
      "Iteration 544, loss = 2.11583982\n",
      "Iteration 545, loss = 2.11394925\n",
      "Iteration 546, loss = 2.11207503\n",
      "Iteration 547, loss = 2.11016944\n",
      "Iteration 548, loss = 2.10827891\n",
      "Iteration 549, loss = 2.10639863\n",
      "Iteration 550, loss = 2.10453151\n",
      "Iteration 551, loss = 2.10262989\n",
      "Iteration 552, loss = 2.10073713\n",
      "Iteration 553, loss = 2.09885383\n",
      "Iteration 554, loss = 2.09697053\n",
      "Iteration 555, loss = 2.09509188\n",
      "Iteration 556, loss = 2.09319438\n",
      "Iteration 557, loss = 2.09132485\n",
      "Iteration 558, loss = 2.08945750\n",
      "Iteration 559, loss = 2.08756626\n",
      "Iteration 560, loss = 2.08571036\n",
      "Iteration 561, loss = 2.08381410\n",
      "Iteration 562, loss = 2.08193460\n",
      "Iteration 563, loss = 2.08006055\n",
      "Iteration 564, loss = 2.07819097\n",
      "Iteration 565, loss = 2.07632526\n",
      "Iteration 566, loss = 2.07443306\n",
      "Iteration 567, loss = 2.07258387\n",
      "Iteration 568, loss = 2.07071922\n",
      "Iteration 569, loss = 2.06885096\n",
      "Iteration 570, loss = 2.06699046\n",
      "Iteration 571, loss = 2.06511746\n",
      "Iteration 572, loss = 2.06326261\n",
      "Iteration 573, loss = 2.06140211\n",
      "Iteration 574, loss = 2.05954923\n",
      "Iteration 575, loss = 2.05768167\n",
      "Iteration 576, loss = 2.05584287\n",
      "Iteration 577, loss = 2.05395930\n",
      "Iteration 578, loss = 2.05211379\n",
      "Iteration 579, loss = 2.05024760\n",
      "Iteration 580, loss = 2.04839166\n",
      "Iteration 581, loss = 2.04654331\n",
      "Iteration 582, loss = 2.04469111\n",
      "Iteration 583, loss = 2.04283628\n",
      "Iteration 584, loss = 2.04098144\n",
      "Iteration 585, loss = 2.03913125\n",
      "Iteration 586, loss = 2.03729750\n",
      "Iteration 587, loss = 2.03543699\n",
      "Iteration 588, loss = 2.03359813\n",
      "Iteration 589, loss = 2.03174887\n",
      "Iteration 590, loss = 2.02989498\n",
      "Iteration 591, loss = 2.02806736\n",
      "Iteration 592, loss = 2.02621890\n",
      "Iteration 593, loss = 2.02438693\n",
      "Iteration 594, loss = 2.02253275\n",
      "Iteration 595, loss = 2.02070866\n",
      "Iteration 596, loss = 2.01889241\n",
      "Iteration 597, loss = 2.01704426\n",
      "Iteration 598, loss = 2.01520519\n",
      "Iteration 599, loss = 2.01337627\n",
      "Iteration 600, loss = 2.01153965\n",
      "Iteration 601, loss = 2.00970664\n",
      "Iteration 602, loss = 2.00787193\n",
      "Iteration 603, loss = 2.00605696\n",
      "Iteration 604, loss = 2.00422315\n",
      "Iteration 605, loss = 2.00240615\n",
      "Iteration 606, loss = 2.00058100\n",
      "Iteration 607, loss = 1.99876292\n",
      "Iteration 608, loss = 1.99693107\n",
      "Iteration 609, loss = 1.99511847\n",
      "Iteration 610, loss = 1.99329355\n",
      "Iteration 611, loss = 1.99148541\n",
      "Iteration 612, loss = 1.98967953\n",
      "Iteration 613, loss = 1.98784382\n",
      "Iteration 614, loss = 1.98601377\n",
      "Iteration 615, loss = 1.98419546\n",
      "Iteration 616, loss = 1.98237715\n",
      "Iteration 617, loss = 1.98056773\n",
      "Iteration 618, loss = 1.97875380\n",
      "Iteration 619, loss = 1.97695017\n",
      "Iteration 620, loss = 1.97513423\n",
      "Iteration 621, loss = 1.97333235\n",
      "Iteration 622, loss = 1.97152947\n",
      "Iteration 623, loss = 1.96973619\n",
      "Iteration 624, loss = 1.96791761\n",
      "Iteration 625, loss = 1.96611157\n",
      "Iteration 626, loss = 1.96430806\n",
      "Iteration 627, loss = 1.96251095\n",
      "Iteration 628, loss = 1.96072461\n",
      "Iteration 629, loss = 1.95891623\n",
      "Iteration 630, loss = 1.95713486\n",
      "Iteration 631, loss = 1.95533348\n",
      "Iteration 632, loss = 1.95353618\n",
      "Iteration 633, loss = 1.95175003\n",
      "Iteration 634, loss = 1.94993994\n",
      "Iteration 635, loss = 1.94815335\n",
      "Iteration 636, loss = 1.94636654\n",
      "Iteration 637, loss = 1.94457611\n",
      "Iteration 638, loss = 1.94279424\n",
      "Iteration 639, loss = 1.94100185\n",
      "Iteration 640, loss = 1.93923130\n",
      "Iteration 641, loss = 1.93743349\n",
      "Iteration 642, loss = 1.93565738\n",
      "Iteration 643, loss = 1.93387617\n",
      "Iteration 644, loss = 1.93209811\n",
      "Iteration 645, loss = 1.93032061\n",
      "Iteration 646, loss = 1.92854523\n",
      "Iteration 647, loss = 1.92677114\n",
      "Iteration 648, loss = 1.92499753\n",
      "Iteration 649, loss = 1.92320725\n",
      "Iteration 650, loss = 1.92144154\n",
      "Iteration 651, loss = 1.91965687\n",
      "Iteration 652, loss = 1.91789146\n",
      "Iteration 653, loss = 1.91610401\n",
      "Iteration 654, loss = 1.91435760\n",
      "Iteration 655, loss = 1.91259865\n",
      "Iteration 656, loss = 1.91082091\n",
      "Iteration 657, loss = 1.90906848\n",
      "Iteration 658, loss = 1.90729614\n",
      "Iteration 659, loss = 1.90554336\n",
      "Iteration 660, loss = 1.90377187\n",
      "Iteration 661, loss = 1.90202534\n",
      "Iteration 662, loss = 1.90027433\n",
      "Iteration 663, loss = 1.89849916\n",
      "Iteration 664, loss = 1.89676162\n",
      "Iteration 665, loss = 1.89499853\n",
      "Iteration 666, loss = 1.89324623\n",
      "Iteration 667, loss = 1.89148602\n",
      "Iteration 668, loss = 1.88973564\n",
      "Iteration 669, loss = 1.88799644\n",
      "Iteration 670, loss = 1.88625170\n",
      "Iteration 671, loss = 1.88450283\n",
      "Iteration 672, loss = 1.88274355\n",
      "Iteration 673, loss = 1.88101814\n",
      "Iteration 674, loss = 1.87926695\n",
      "Iteration 675, loss = 1.87750232\n",
      "Iteration 676, loss = 1.87577193\n",
      "Iteration 677, loss = 1.87404549\n",
      "Iteration 678, loss = 1.87230467\n",
      "Iteration 679, loss = 1.87055837\n",
      "Iteration 680, loss = 1.86882496\n",
      "Iteration 681, loss = 1.86710021\n",
      "Iteration 682, loss = 1.86536512\n",
      "Iteration 683, loss = 1.86362469\n",
      "Iteration 684, loss = 1.86190854\n",
      "Iteration 685, loss = 1.86017893\n",
      "Iteration 686, loss = 1.85843606\n",
      "Iteration 687, loss = 1.85671123\n",
      "Iteration 688, loss = 1.85499626\n",
      "Iteration 689, loss = 1.85327920\n",
      "Iteration 690, loss = 1.85154376\n",
      "Iteration 691, loss = 1.84982671\n",
      "Iteration 692, loss = 1.84811942\n",
      "Iteration 693, loss = 1.84637949\n",
      "Iteration 694, loss = 1.84465969\n",
      "Iteration 695, loss = 1.84293058\n",
      "Iteration 696, loss = 1.84123501\n",
      "Iteration 697, loss = 1.83951235\n",
      "Iteration 698, loss = 1.83780511\n",
      "Iteration 699, loss = 1.83609305\n",
      "Iteration 700, loss = 1.83438996\n",
      "Iteration 701, loss = 1.83267075\n",
      "Iteration 702, loss = 1.83096131\n",
      "Iteration 703, loss = 1.82926134\n",
      "Iteration 704, loss = 1.82756506\n",
      "Iteration 705, loss = 1.82585886\n",
      "Iteration 706, loss = 1.82417469\n",
      "Iteration 707, loss = 1.82245942\n",
      "Iteration 708, loss = 1.82075252\n",
      "Iteration 709, loss = 1.81904875\n",
      "Iteration 710, loss = 1.81735615\n",
      "Iteration 711, loss = 1.81566433\n",
      "Iteration 712, loss = 1.81396346\n",
      "Iteration 713, loss = 1.81227168\n",
      "Iteration 714, loss = 1.81058398\n",
      "Iteration 715, loss = 1.80888102\n",
      "Iteration 716, loss = 1.80719433\n",
      "Iteration 717, loss = 1.80550099\n",
      "Iteration 718, loss = 1.80382051\n",
      "Iteration 719, loss = 1.80213315\n",
      "Iteration 720, loss = 1.80044055\n",
      "Iteration 721, loss = 1.79876444\n",
      "Iteration 722, loss = 1.79708222\n",
      "Iteration 723, loss = 1.79540365\n",
      "Iteration 724, loss = 1.79371300\n",
      "Iteration 725, loss = 1.79204043\n",
      "Iteration 726, loss = 1.79036444\n",
      "Iteration 727, loss = 1.78869656\n",
      "Iteration 728, loss = 1.78701865\n",
      "Iteration 729, loss = 1.78533926\n",
      "Iteration 730, loss = 1.78368938\n",
      "Iteration 731, loss = 1.78200759\n",
      "Iteration 732, loss = 1.78034190\n",
      "Iteration 733, loss = 1.77866397\n",
      "Iteration 734, loss = 1.77700023\n",
      "Iteration 735, loss = 1.77533453\n",
      "Iteration 736, loss = 1.77366266\n",
      "Iteration 737, loss = 1.77202257\n",
      "Iteration 738, loss = 1.77035273\n",
      "Iteration 739, loss = 1.76868794\n",
      "Iteration 740, loss = 1.76703356\n",
      "Iteration 741, loss = 1.76536845\n",
      "Iteration 742, loss = 1.76370606\n",
      "Iteration 743, loss = 1.76205966\n",
      "Iteration 744, loss = 1.76040062\n",
      "Iteration 745, loss = 1.75874926\n",
      "Iteration 746, loss = 1.75710817\n",
      "Iteration 747, loss = 1.75544822\n",
      "Iteration 748, loss = 1.75379842\n",
      "Iteration 749, loss = 1.75215626\n",
      "Iteration 750, loss = 1.75050123\n",
      "Iteration 751, loss = 1.74886564\n",
      "Iteration 752, loss = 1.74721315\n",
      "Iteration 753, loss = 1.74555418\n",
      "Iteration 754, loss = 1.74392516\n",
      "Iteration 755, loss = 1.74227842\n",
      "Iteration 756, loss = 1.74065023\n",
      "Iteration 757, loss = 1.73901099\n",
      "Iteration 758, loss = 1.73736582\n",
      "Iteration 759, loss = 1.73571689\n",
      "Iteration 760, loss = 1.73408680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 761, loss = 1.73245534\n",
      "Iteration 762, loss = 1.73082764\n",
      "Iteration 763, loss = 1.72919806\n",
      "Iteration 764, loss = 1.72756412\n",
      "Iteration 765, loss = 1.72592861\n",
      "Iteration 766, loss = 1.72430445\n",
      "Iteration 767, loss = 1.72268774\n",
      "Iteration 768, loss = 1.72105108\n",
      "Iteration 769, loss = 1.71944733\n",
      "Iteration 770, loss = 1.71782717\n",
      "Iteration 771, loss = 1.71620990\n",
      "Iteration 772, loss = 1.71458429\n",
      "Iteration 773, loss = 1.71296180\n",
      "Iteration 774, loss = 1.71134632\n",
      "Iteration 775, loss = 1.70973176\n",
      "Iteration 776, loss = 1.70811903\n",
      "Iteration 777, loss = 1.70650675\n",
      "Iteration 778, loss = 1.70491345\n",
      "Iteration 779, loss = 1.70329273\n",
      "Iteration 780, loss = 1.70167717\n",
      "Iteration 781, loss = 1.70009064\n",
      "Iteration 782, loss = 1.69846047\n",
      "Iteration 783, loss = 1.69687566\n",
      "Iteration 784, loss = 1.69525202\n",
      "Iteration 785, loss = 1.69366148\n",
      "Iteration 786, loss = 1.69206975\n",
      "Iteration 787, loss = 1.69046111\n",
      "Iteration 788, loss = 1.68886262\n",
      "Iteration 789, loss = 1.68727081\n",
      "Iteration 790, loss = 1.68568167\n",
      "Iteration 791, loss = 1.68408058\n",
      "Iteration 792, loss = 1.68249881\n",
      "Iteration 793, loss = 1.68091526\n",
      "Iteration 794, loss = 1.67932628\n",
      "Iteration 795, loss = 1.67772701\n",
      "Iteration 796, loss = 1.67614617\n",
      "Iteration 797, loss = 1.67455082\n",
      "Iteration 798, loss = 1.67298133\n",
      "Iteration 799, loss = 1.67138800\n",
      "Iteration 800, loss = 1.66982965\n",
      "Iteration 801, loss = 1.66822369\n",
      "Iteration 802, loss = 1.66663921\n",
      "Iteration 803, loss = 1.66506292\n",
      "Iteration 804, loss = 1.66349122\n",
      "Iteration 805, loss = 1.66190161\n",
      "Iteration 806, loss = 1.66033701\n",
      "Iteration 807, loss = 1.65876354\n",
      "Iteration 808, loss = 1.65718456\n",
      "Iteration 809, loss = 1.65562625\n",
      "Iteration 810, loss = 1.65406036\n",
      "Iteration 811, loss = 1.65248689\n",
      "Iteration 812, loss = 1.65092135\n",
      "Iteration 813, loss = 1.64936893\n",
      "Iteration 814, loss = 1.64778037\n",
      "Iteration 815, loss = 1.64623037\n",
      "Iteration 816, loss = 1.64466225\n",
      "Iteration 817, loss = 1.64310748\n",
      "Iteration 818, loss = 1.64154781\n",
      "Iteration 819, loss = 1.63997682\n",
      "Iteration 820, loss = 1.63843230\n",
      "Iteration 821, loss = 1.63687293\n",
      "Iteration 822, loss = 1.63533122\n",
      "Iteration 823, loss = 1.63377309\n",
      "Iteration 824, loss = 1.63222393\n",
      "Iteration 825, loss = 1.63067943\n",
      "Iteration 826, loss = 1.62913212\n",
      "Iteration 827, loss = 1.62758548\n",
      "Iteration 828, loss = 1.62603389\n",
      "Iteration 829, loss = 1.62448909\n",
      "Iteration 830, loss = 1.62295851\n",
      "Iteration 831, loss = 1.62140695\n",
      "Iteration 832, loss = 1.61986925\n",
      "Iteration 833, loss = 1.61834416\n",
      "Iteration 834, loss = 1.61679731\n",
      "Iteration 835, loss = 1.61526530\n",
      "Iteration 836, loss = 1.61373263\n",
      "Iteration 837, loss = 1.61218742\n",
      "Iteration 838, loss = 1.61065657\n",
      "Iteration 839, loss = 1.60912706\n",
      "Iteration 840, loss = 1.60759702\n",
      "Iteration 841, loss = 1.60607656\n",
      "Iteration 842, loss = 1.60455318\n",
      "Iteration 843, loss = 1.60303511\n",
      "Iteration 844, loss = 1.60149252\n",
      "Iteration 845, loss = 1.59997823\n",
      "Iteration 846, loss = 1.59844384\n",
      "Iteration 847, loss = 1.59693495\n",
      "Iteration 848, loss = 1.59541497\n",
      "Iteration 849, loss = 1.59388424\n",
      "Iteration 850, loss = 1.59238091\n",
      "Iteration 851, loss = 1.59086116\n",
      "Iteration 852, loss = 1.58933933\n",
      "Iteration 853, loss = 1.58781301\n",
      "Iteration 854, loss = 1.58630524\n",
      "Iteration 855, loss = 1.58479341\n",
      "Iteration 856, loss = 1.58327626\n",
      "Iteration 857, loss = 1.58177178\n",
      "Iteration 858, loss = 1.58026292\n",
      "Iteration 859, loss = 1.57874087\n",
      "Iteration 860, loss = 1.57724572\n",
      "Iteration 861, loss = 1.57574489\n",
      "Iteration 862, loss = 1.57423477\n",
      "Iteration 863, loss = 1.57273827\n",
      "Iteration 864, loss = 1.57124058\n",
      "Iteration 865, loss = 1.56973940\n",
      "Iteration 866, loss = 1.56823679\n",
      "Iteration 867, loss = 1.56675226\n",
      "Iteration 868, loss = 1.56526264\n",
      "Iteration 869, loss = 1.56377050\n",
      "Iteration 870, loss = 1.56227981\n",
      "Iteration 871, loss = 1.56078681\n",
      "Iteration 872, loss = 1.55929259\n",
      "Iteration 873, loss = 1.55780154\n",
      "Iteration 874, loss = 1.55631445\n",
      "Iteration 875, loss = 1.55483432\n",
      "Iteration 876, loss = 1.55336158\n",
      "Iteration 877, loss = 1.55187463\n",
      "Iteration 878, loss = 1.55038347\n",
      "Iteration 879, loss = 1.54890611\n",
      "Iteration 880, loss = 1.54742996\n",
      "Iteration 881, loss = 1.54596359\n",
      "Iteration 882, loss = 1.54447367\n",
      "Iteration 883, loss = 1.54300250\n",
      "Iteration 884, loss = 1.54153516\n",
      "Iteration 885, loss = 1.54005917\n",
      "Iteration 886, loss = 1.53858554\n",
      "Iteration 887, loss = 1.53711695\n",
      "Iteration 888, loss = 1.53565340\n",
      "Iteration 889, loss = 1.53417859\n",
      "Iteration 890, loss = 1.53270965\n",
      "Iteration 891, loss = 1.53125509\n",
      "Iteration 892, loss = 1.52979619\n",
      "Iteration 893, loss = 1.52833140\n",
      "Iteration 894, loss = 1.52687441\n",
      "Iteration 895, loss = 1.52541832\n",
      "Iteration 896, loss = 1.52396017\n",
      "Iteration 897, loss = 1.52249039\n",
      "Iteration 898, loss = 1.52103823\n",
      "Iteration 899, loss = 1.51959478\n",
      "Iteration 900, loss = 1.51814127\n",
      "Iteration 901, loss = 1.51668988\n",
      "Iteration 902, loss = 1.51524388\n",
      "Iteration 903, loss = 1.51378077\n",
      "Iteration 904, loss = 1.51236285\n",
      "Iteration 905, loss = 1.51090372\n",
      "Iteration 906, loss = 1.50945525\n",
      "Iteration 907, loss = 1.50802136\n",
      "Iteration 908, loss = 1.50658153\n",
      "Iteration 909, loss = 1.50514162\n",
      "Iteration 910, loss = 1.50369651\n",
      "Iteration 911, loss = 1.50226219\n",
      "Iteration 912, loss = 1.50082533\n",
      "Iteration 913, loss = 1.49940247\n",
      "Iteration 914, loss = 1.49796874\n",
      "Iteration 915, loss = 1.49652607\n",
      "Iteration 916, loss = 1.49509907\n",
      "Iteration 917, loss = 1.49367456\n",
      "Iteration 918, loss = 1.49223943\n",
      "Iteration 919, loss = 1.49081698\n",
      "Iteration 920, loss = 1.48938015\n",
      "Iteration 921, loss = 1.48795991\n",
      "Iteration 922, loss = 1.48652930\n",
      "Iteration 923, loss = 1.48511482\n",
      "Iteration 924, loss = 1.48369544\n",
      "Iteration 925, loss = 1.48226938\n",
      "Iteration 926, loss = 1.48086407\n",
      "Iteration 927, loss = 1.47945029\n",
      "Iteration 928, loss = 1.47802611\n",
      "Iteration 929, loss = 1.47661798\n",
      "Iteration 930, loss = 1.47519365\n",
      "Iteration 931, loss = 1.47378816\n",
      "Iteration 932, loss = 1.47239536\n",
      "Iteration 933, loss = 1.47097081\n",
      "Iteration 934, loss = 1.46956455\n",
      "Iteration 935, loss = 1.46815137\n",
      "Iteration 936, loss = 1.46674970\n",
      "Iteration 937, loss = 1.46533928\n",
      "Iteration 938, loss = 1.46394676\n",
      "Iteration 939, loss = 1.46254767\n",
      "Iteration 940, loss = 1.46114520\n",
      "Iteration 941, loss = 1.45975301\n",
      "Iteration 942, loss = 1.45835152\n",
      "Iteration 943, loss = 1.45695909\n",
      "Iteration 944, loss = 1.45555615\n",
      "Iteration 945, loss = 1.45416099\n",
      "Iteration 946, loss = 1.45278474\n",
      "Iteration 947, loss = 1.45137863\n",
      "Iteration 948, loss = 1.45000613\n",
      "Iteration 949, loss = 1.44861398\n",
      "Iteration 950, loss = 1.44722691\n",
      "Iteration 951, loss = 1.44583265\n",
      "Iteration 952, loss = 1.44445514\n",
      "Iteration 953, loss = 1.44306867\n",
      "Iteration 954, loss = 1.44168329\n",
      "Iteration 955, loss = 1.44029877\n",
      "Iteration 956, loss = 1.43892019\n",
      "Iteration 957, loss = 1.43754074\n",
      "Iteration 958, loss = 1.43616490\n",
      "Iteration 959, loss = 1.43478738\n",
      "Iteration 960, loss = 1.43341613\n",
      "Iteration 961, loss = 1.43203018\n",
      "Iteration 962, loss = 1.43065694\n",
      "Iteration 963, loss = 1.42929432\n",
      "Iteration 964, loss = 1.42790893\n",
      "Iteration 965, loss = 1.42654853\n",
      "Iteration 966, loss = 1.42517885\n",
      "Iteration 967, loss = 1.42381517\n",
      "Iteration 968, loss = 1.42245877\n",
      "Iteration 969, loss = 1.42107650\n",
      "Iteration 970, loss = 1.41973094\n",
      "Iteration 971, loss = 1.41836002\n",
      "Iteration 972, loss = 1.41700628\n",
      "Iteration 973, loss = 1.41564478\n",
      "Iteration 974, loss = 1.41430065\n",
      "Iteration 975, loss = 1.41292670\n",
      "Iteration 976, loss = 1.41158988\n",
      "Iteration 977, loss = 1.41023094\n",
      "Iteration 978, loss = 1.40887273\n",
      "Iteration 979, loss = 1.40753173\n",
      "Iteration 980, loss = 1.40619063\n",
      "Iteration 981, loss = 1.40482610\n",
      "Iteration 982, loss = 1.40349615\n",
      "Iteration 983, loss = 1.40214195\n",
      "Iteration 984, loss = 1.40080955\n",
      "Iteration 985, loss = 1.39947055\n",
      "Iteration 986, loss = 1.39811141\n",
      "Iteration 987, loss = 1.39677837\n",
      "Iteration 988, loss = 1.39545526\n",
      "Iteration 989, loss = 1.39411504\n",
      "Iteration 990, loss = 1.39278327\n",
      "Iteration 991, loss = 1.39144558\n",
      "Iteration 992, loss = 1.39010945\n",
      "Iteration 993, loss = 1.38878039\n",
      "Iteration 994, loss = 1.38744682\n",
      "Iteration 995, loss = 1.38611114\n",
      "Iteration 996, loss = 1.38480020\n",
      "Iteration 997, loss = 1.38346614\n",
      "Iteration 998, loss = 1.38213625\n",
      "Iteration 999, loss = 1.38081006\n",
      "Iteration 1000, loss = 1.37949575\n",
      "Iteration 1001, loss = 1.37817968\n",
      "Iteration 1002, loss = 1.37685449\n",
      "Iteration 1003, loss = 1.37553102\n",
      "Iteration 1004, loss = 1.37421845\n",
      "Iteration 1005, loss = 1.37289780\n",
      "Iteration 1006, loss = 1.37159354\n",
      "Iteration 1007, loss = 1.37028265\n",
      "Iteration 1008, loss = 1.36895903\n",
      "Iteration 1009, loss = 1.36764829\n",
      "Iteration 1010, loss = 1.36634116\n",
      "Iteration 1011, loss = 1.36502817\n",
      "Iteration 1012, loss = 1.36372730\n",
      "Iteration 1013, loss = 1.36242007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1014, loss = 1.36111986\n",
      "Iteration 1015, loss = 1.35981547\n",
      "Iteration 1016, loss = 1.35849349\n",
      "Iteration 1017, loss = 1.35721246\n",
      "Iteration 1018, loss = 1.35589919\n",
      "Iteration 1019, loss = 1.35461048\n",
      "Iteration 1020, loss = 1.35332189\n",
      "Iteration 1021, loss = 1.35200477\n",
      "Iteration 1022, loss = 1.35072072\n",
      "Iteration 1023, loss = 1.34942777\n",
      "Iteration 1024, loss = 1.34813318\n",
      "Iteration 1025, loss = 1.34683614\n",
      "Iteration 1026, loss = 1.34554398\n",
      "Iteration 1027, loss = 1.34426466\n",
      "Iteration 1028, loss = 1.34298154\n",
      "Iteration 1029, loss = 1.34168626\n",
      "Iteration 1030, loss = 1.34040637\n",
      "Iteration 1031, loss = 1.33914123\n",
      "Iteration 1032, loss = 1.33783887\n",
      "Iteration 1033, loss = 1.33655281\n",
      "Iteration 1034, loss = 1.33528462\n",
      "Iteration 1035, loss = 1.33399834\n",
      "Iteration 1036, loss = 1.33271233\n",
      "Iteration 1037, loss = 1.33144103\n",
      "Iteration 1038, loss = 1.33015385\n",
      "Iteration 1039, loss = 1.32888654\n",
      "Iteration 1040, loss = 1.32761255\n",
      "Iteration 1041, loss = 1.32634392\n",
      "Iteration 1042, loss = 1.32506512\n",
      "Iteration 1043, loss = 1.32379292\n",
      "Iteration 1044, loss = 1.32253822\n",
      "Iteration 1045, loss = 1.32125161\n",
      "Iteration 1046, loss = 1.32000024\n",
      "Iteration 1047, loss = 1.31872346\n",
      "Iteration 1048, loss = 1.31746169\n",
      "Iteration 1049, loss = 1.31620470\n",
      "Iteration 1050, loss = 1.31493390\n",
      "Iteration 1051, loss = 1.31369184\n",
      "Iteration 1052, loss = 1.31242486\n",
      "Iteration 1053, loss = 1.31116650\n",
      "Iteration 1054, loss = 1.30990852\n",
      "Iteration 1055, loss = 1.30866212\n",
      "Iteration 1056, loss = 1.30739142\n",
      "Iteration 1057, loss = 1.30614482\n",
      "Iteration 1058, loss = 1.30489665\n",
      "Iteration 1059, loss = 1.30365630\n",
      "Iteration 1060, loss = 1.30239018\n",
      "Iteration 1061, loss = 1.30115229\n",
      "Iteration 1062, loss = 1.29990556\n",
      "Iteration 1063, loss = 1.29867103\n",
      "Iteration 1064, loss = 1.29742002\n",
      "Iteration 1065, loss = 1.29617534\n",
      "Iteration 1066, loss = 1.29493841\n",
      "Iteration 1067, loss = 1.29371085\n",
      "Iteration 1068, loss = 1.29246027\n",
      "Iteration 1069, loss = 1.29122632\n",
      "Iteration 1070, loss = 1.28998767\n",
      "Iteration 1071, loss = 1.28875423\n",
      "Iteration 1072, loss = 1.28753117\n",
      "Iteration 1073, loss = 1.28629064\n",
      "Iteration 1074, loss = 1.28506595\n",
      "Iteration 1075, loss = 1.28384539\n",
      "Iteration 1076, loss = 1.28260137\n",
      "Iteration 1077, loss = 1.28137476\n",
      "Iteration 1078, loss = 1.28014455\n",
      "Iteration 1079, loss = 1.27892597\n",
      "Iteration 1080, loss = 1.27769103\n",
      "Iteration 1081, loss = 1.27648664\n",
      "Iteration 1082, loss = 1.27525403\n",
      "Iteration 1083, loss = 1.27402669\n",
      "Iteration 1084, loss = 1.27281031\n",
      "Iteration 1085, loss = 1.27158846\n",
      "Iteration 1086, loss = 1.27037183\n",
      "Iteration 1087, loss = 1.26917265\n",
      "Iteration 1088, loss = 1.26794044\n",
      "Iteration 1089, loss = 1.26673470\n",
      "Iteration 1090, loss = 1.26551329\n",
      "Iteration 1091, loss = 1.26430687\n",
      "Iteration 1092, loss = 1.26309235\n",
      "Iteration 1093, loss = 1.26189969\n",
      "Iteration 1094, loss = 1.26067930\n",
      "Iteration 1095, loss = 1.25948106\n",
      "Iteration 1096, loss = 1.25827575\n",
      "Iteration 1097, loss = 1.25708607\n",
      "Iteration 1098, loss = 1.25587231\n",
      "Iteration 1099, loss = 1.25467396\n",
      "Iteration 1100, loss = 1.25347198\n",
      "Iteration 1101, loss = 1.25229207\n",
      "Iteration 1102, loss = 1.25108279\n",
      "Iteration 1103, loss = 1.24989675\n",
      "Iteration 1104, loss = 1.24869785\n",
      "Iteration 1105, loss = 1.24749838\n",
      "Iteration 1106, loss = 1.24632033\n",
      "Iteration 1107, loss = 1.24512550\n",
      "Iteration 1108, loss = 1.24392840\n",
      "Iteration 1109, loss = 1.24273685\n",
      "Iteration 1110, loss = 1.24156242\n",
      "Iteration 1111, loss = 1.24037413\n",
      "Iteration 1112, loss = 1.23918214\n",
      "Iteration 1113, loss = 1.23799578\n",
      "Iteration 1114, loss = 1.23681252\n",
      "Iteration 1115, loss = 1.23562309\n",
      "Iteration 1116, loss = 1.23445814\n",
      "Iteration 1117, loss = 1.23326458\n",
      "Iteration 1118, loss = 1.23208657\n",
      "Iteration 1119, loss = 1.23091549\n",
      "Iteration 1120, loss = 1.22974692\n",
      "Iteration 1121, loss = 1.22856914\n",
      "Iteration 1122, loss = 1.22740057\n",
      "Iteration 1123, loss = 1.22621744\n",
      "Iteration 1124, loss = 1.22505954\n",
      "Iteration 1125, loss = 1.22387594\n",
      "Iteration 1126, loss = 1.22271607\n",
      "Iteration 1127, loss = 1.22154039\n",
      "Iteration 1128, loss = 1.22037965\n",
      "Iteration 1129, loss = 1.21920634\n",
      "Iteration 1130, loss = 1.21804439\n",
      "Iteration 1131, loss = 1.21689578\n",
      "Iteration 1132, loss = 1.21572742\n",
      "Iteration 1133, loss = 1.21456670\n",
      "Iteration 1134, loss = 1.21341770\n",
      "Iteration 1135, loss = 1.21226088\n",
      "Iteration 1136, loss = 1.21110283\n",
      "Iteration 1137, loss = 1.20995516\n",
      "Iteration 1138, loss = 1.20879902\n",
      "Iteration 1139, loss = 1.20764598\n",
      "Iteration 1140, loss = 1.20650399\n",
      "Iteration 1141, loss = 1.20535341\n",
      "Iteration 1142, loss = 1.20421039\n",
      "Iteration 1143, loss = 1.20306700\n",
      "Iteration 1144, loss = 1.20191791\n",
      "Iteration 1145, loss = 1.20077559\n",
      "Iteration 1146, loss = 1.19963830\n",
      "Iteration 1147, loss = 1.19849121\n",
      "Iteration 1148, loss = 1.19735233\n",
      "Iteration 1149, loss = 1.19621159\n",
      "Iteration 1150, loss = 1.19507174\n",
      "Iteration 1151, loss = 1.19394059\n",
      "Iteration 1152, loss = 1.19281083\n",
      "Iteration 1153, loss = 1.19166042\n",
      "Iteration 1154, loss = 1.19053201\n",
      "Iteration 1155, loss = 1.18939229\n",
      "Iteration 1156, loss = 1.18826931\n",
      "Iteration 1157, loss = 1.18713339\n",
      "Iteration 1158, loss = 1.18599025\n",
      "Iteration 1159, loss = 1.18487978\n",
      "Iteration 1160, loss = 1.18374611\n",
      "Iteration 1161, loss = 1.18261095\n",
      "Iteration 1162, loss = 1.18149905\n",
      "Iteration 1163, loss = 1.18035903\n",
      "Iteration 1164, loss = 1.17923819\n",
      "Iteration 1165, loss = 1.17811576\n",
      "Iteration 1166, loss = 1.17700237\n",
      "Iteration 1167, loss = 1.17586682\n",
      "Iteration 1168, loss = 1.17474548\n",
      "Iteration 1169, loss = 1.17363529\n",
      "Iteration 1170, loss = 1.17252519\n",
      "Iteration 1171, loss = 1.17140633\n",
      "Iteration 1172, loss = 1.17029179\n",
      "Iteration 1173, loss = 1.16917626\n",
      "Iteration 1174, loss = 1.16806643\n",
      "Iteration 1175, loss = 1.16695226\n",
      "Iteration 1176, loss = 1.16584065\n",
      "Iteration 1177, loss = 1.16473477\n",
      "Iteration 1178, loss = 1.16362603\n",
      "Iteration 1179, loss = 1.16251392\n",
      "Iteration 1180, loss = 1.16141794\n",
      "Iteration 1181, loss = 1.16031163\n",
      "Iteration 1182, loss = 1.15921220\n",
      "Iteration 1183, loss = 1.15810764\n",
      "Iteration 1184, loss = 1.15700238\n",
      "Iteration 1185, loss = 1.15591031\n",
      "Iteration 1186, loss = 1.15481685\n",
      "Iteration 1187, loss = 1.15372571\n",
      "Iteration 1188, loss = 1.15263062\n",
      "Iteration 1189, loss = 1.15153414\n",
      "Iteration 1190, loss = 1.15044687\n",
      "Iteration 1191, loss = 1.14934846\n",
      "Iteration 1192, loss = 1.14826387\n",
      "Iteration 1193, loss = 1.14716494\n",
      "Iteration 1194, loss = 1.14608189\n",
      "Iteration 1195, loss = 1.14500897\n",
      "Iteration 1196, loss = 1.14391177\n",
      "Iteration 1197, loss = 1.14282528\n",
      "Iteration 1198, loss = 1.14174732\n",
      "Iteration 1199, loss = 1.14066473\n",
      "Iteration 1200, loss = 1.13958752\n",
      "Iteration 1201, loss = 1.13849462\n",
      "Iteration 1202, loss = 1.13743496\n",
      "Iteration 1203, loss = 1.13634917\n",
      "Iteration 1204, loss = 1.13527407\n",
      "Iteration 1205, loss = 1.13419128\n",
      "Iteration 1206, loss = 1.13312253\n",
      "Iteration 1207, loss = 1.13204566\n",
      "Iteration 1208, loss = 1.13097254\n",
      "Iteration 1209, loss = 1.12990357\n",
      "Iteration 1210, loss = 1.12883118\n",
      "Iteration 1211, loss = 1.12778020\n",
      "Iteration 1212, loss = 1.12670006\n",
      "Iteration 1213, loss = 1.12564079\n",
      "Iteration 1214, loss = 1.12456855\n",
      "Iteration 1215, loss = 1.12351371\n",
      "Iteration 1216, loss = 1.12244330\n",
      "Iteration 1217, loss = 1.12138102\n",
      "Iteration 1218, loss = 1.12031270\n",
      "Iteration 1219, loss = 1.11926289\n",
      "Iteration 1220, loss = 1.11819215\n",
      "Iteration 1221, loss = 1.11714369\n",
      "Iteration 1222, loss = 1.11608127\n",
      "Iteration 1223, loss = 1.11502997\n",
      "Iteration 1224, loss = 1.11397399\n",
      "Iteration 1225, loss = 1.11292053\n",
      "Iteration 1226, loss = 1.11186390\n",
      "Iteration 1227, loss = 1.11081951\n",
      "Iteration 1228, loss = 1.10976780\n",
      "Iteration 1229, loss = 1.10870849\n",
      "Iteration 1230, loss = 1.10765872\n",
      "Iteration 1231, loss = 1.10662505\n",
      "Iteration 1232, loss = 1.10556967\n",
      "Iteration 1233, loss = 1.10453389\n",
      "Iteration 1234, loss = 1.10348662\n",
      "Iteration 1235, loss = 1.10244451\n",
      "Iteration 1236, loss = 1.10140322\n",
      "Iteration 1237, loss = 1.10035790\n",
      "Iteration 1238, loss = 1.09932238\n",
      "Iteration 1239, loss = 1.09829316\n",
      "Iteration 1240, loss = 1.09725503\n",
      "Iteration 1241, loss = 1.09622269\n",
      "Iteration 1242, loss = 1.09517410\n",
      "Iteration 1243, loss = 1.09414371\n",
      "Iteration 1244, loss = 1.09311533\n",
      "Iteration 1245, loss = 1.09209032\n",
      "Iteration 1246, loss = 1.09104606\n",
      "Iteration 1247, loss = 1.09002646\n",
      "Iteration 1248, loss = 1.08900598\n",
      "Iteration 1249, loss = 1.08797599\n",
      "Iteration 1250, loss = 1.08695723\n",
      "Iteration 1251, loss = 1.08592420\n",
      "Iteration 1252, loss = 1.08490388\n",
      "Iteration 1253, loss = 1.08389025\n",
      "Iteration 1254, loss = 1.08286241\n",
      "Iteration 1255, loss = 1.08184653\n",
      "Iteration 1256, loss = 1.08082467\n",
      "Iteration 1257, loss = 1.07980596\n",
      "Iteration 1258, loss = 1.07881761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1259, loss = 1.07777057\n",
      "Iteration 1260, loss = 1.07677761\n",
      "Iteration 1261, loss = 1.07575812\n",
      "Iteration 1262, loss = 1.07473429\n",
      "Iteration 1263, loss = 1.07371840\n",
      "Iteration 1264, loss = 1.07272254\n",
      "Iteration 1265, loss = 1.07171076\n",
      "Iteration 1266, loss = 1.07069734\n",
      "Iteration 1267, loss = 1.06969689\n",
      "Iteration 1268, loss = 1.06869206\n",
      "Iteration 1269, loss = 1.06768653\n",
      "Iteration 1270, loss = 1.06668231\n",
      "Iteration 1271, loss = 1.06567693\n",
      "Iteration 1272, loss = 1.06467197\n",
      "Iteration 1273, loss = 1.06368160\n",
      "Iteration 1274, loss = 1.06266757\n",
      "Iteration 1275, loss = 1.06167469\n",
      "Iteration 1276, loss = 1.06067574\n",
      "Iteration 1277, loss = 1.05967892\n",
      "Iteration 1278, loss = 1.05869168\n",
      "Iteration 1279, loss = 1.05769068\n",
      "Iteration 1280, loss = 1.05669837\n",
      "Iteration 1281, loss = 1.05570832\n",
      "Iteration 1282, loss = 1.05470864\n",
      "Iteration 1283, loss = 1.05372744\n",
      "Iteration 1284, loss = 1.05273589\n",
      "Iteration 1285, loss = 1.05176794\n",
      "Iteration 1286, loss = 1.05077126\n",
      "Iteration 1287, loss = 1.04978168\n",
      "Iteration 1288, loss = 1.04879649\n",
      "Iteration 1289, loss = 1.04781603\n",
      "Iteration 1290, loss = 1.04684096\n",
      "Iteration 1291, loss = 1.04586283\n",
      "Iteration 1292, loss = 1.04489187\n",
      "Iteration 1293, loss = 1.04390946\n",
      "Iteration 1294, loss = 1.04293071\n",
      "Iteration 1295, loss = 1.04195657\n",
      "Iteration 1296, loss = 1.04097768\n",
      "Iteration 1297, loss = 1.04001393\n",
      "Iteration 1298, loss = 1.03903215\n",
      "Iteration 1299, loss = 1.03806001\n",
      "Iteration 1300, loss = 1.03709076\n",
      "Iteration 1301, loss = 1.03612537\n",
      "Iteration 1302, loss = 1.03515210\n",
      "Iteration 1303, loss = 1.03419067\n",
      "Iteration 1304, loss = 1.03323797\n",
      "Iteration 1305, loss = 1.03226053\n",
      "Iteration 1306, loss = 1.03130554\n",
      "Iteration 1307, loss = 1.03033859\n",
      "Iteration 1308, loss = 1.02937900\n",
      "Iteration 1309, loss = 1.02840680\n",
      "Iteration 1310, loss = 1.02745926\n",
      "Iteration 1311, loss = 1.02650399\n",
      "Iteration 1312, loss = 1.02555041\n",
      "Iteration 1313, loss = 1.02458693\n",
      "Iteration 1314, loss = 1.02363086\n",
      "Iteration 1315, loss = 1.02267651\n",
      "Iteration 1316, loss = 1.02172303\n",
      "Iteration 1317, loss = 1.02077433\n",
      "Iteration 1318, loss = 1.01982001\n",
      "Iteration 1319, loss = 1.01887704\n",
      "Iteration 1320, loss = 1.01792956\n",
      "Iteration 1321, loss = 1.01697503\n",
      "Iteration 1322, loss = 1.01603336\n",
      "Iteration 1323, loss = 1.01509222\n",
      "Iteration 1324, loss = 1.01415576\n",
      "Iteration 1325, loss = 1.01320836\n",
      "Iteration 1326, loss = 1.01226613\n",
      "Iteration 1327, loss = 1.01131483\n",
      "Iteration 1328, loss = 1.01037846\n",
      "Iteration 1329, loss = 1.00943301\n",
      "Iteration 1330, loss = 1.00849690\n",
      "Iteration 1331, loss = 1.00755447\n",
      "Iteration 1332, loss = 1.00662072\n",
      "Iteration 1333, loss = 1.00569354\n",
      "Iteration 1334, loss = 1.00475748\n",
      "Iteration 1335, loss = 1.00381690\n",
      "Iteration 1336, loss = 1.00287896\n",
      "Iteration 1337, loss = 1.00194797\n",
      "Iteration 1338, loss = 1.00102419\n",
      "Iteration 1339, loss = 1.00007881\n",
      "Iteration 1340, loss = 0.99916228\n",
      "Iteration 1341, loss = 0.99823352\n",
      "Iteration 1342, loss = 0.99731768\n",
      "Iteration 1343, loss = 0.99638619\n",
      "Iteration 1344, loss = 0.99546412\n",
      "Iteration 1345, loss = 0.99454183\n",
      "Iteration 1346, loss = 0.99362343\n",
      "Iteration 1347, loss = 0.99269961\n",
      "Iteration 1348, loss = 0.99177738\n",
      "Iteration 1349, loss = 0.99086059\n",
      "Iteration 1350, loss = 0.98994633\n",
      "Iteration 1351, loss = 0.98903220\n",
      "Iteration 1352, loss = 0.98811080\n",
      "Iteration 1353, loss = 0.98720042\n",
      "Iteration 1354, loss = 0.98628277\n",
      "Iteration 1355, loss = 0.98537326\n",
      "Iteration 1356, loss = 0.98446232\n",
      "Iteration 1357, loss = 0.98354919\n",
      "Iteration 1358, loss = 0.98264336\n",
      "Iteration 1359, loss = 0.98173739\n",
      "Iteration 1360, loss = 0.98083102\n",
      "Iteration 1361, loss = 0.97992127\n",
      "Iteration 1362, loss = 0.97903187\n",
      "Iteration 1363, loss = 0.97811273\n",
      "Iteration 1364, loss = 0.97721731\n",
      "Iteration 1365, loss = 0.97631450\n",
      "Iteration 1366, loss = 0.97541788\n",
      "Iteration 1367, loss = 0.97451380\n",
      "Iteration 1368, loss = 0.97362882\n",
      "Iteration 1369, loss = 0.97271626\n",
      "Iteration 1370, loss = 0.97182600\n",
      "Iteration 1371, loss = 0.97092670\n",
      "Iteration 1372, loss = 0.97004067\n",
      "Iteration 1373, loss = 0.96913623\n",
      "Iteration 1374, loss = 0.96824528\n",
      "Iteration 1375, loss = 0.96735258\n",
      "Iteration 1376, loss = 0.96645851\n",
      "Iteration 1377, loss = 0.96557085\n",
      "Iteration 1378, loss = 0.96467517\n",
      "Iteration 1379, loss = 0.96378630\n",
      "Iteration 1380, loss = 0.96289759\n",
      "Iteration 1381, loss = 0.96201542\n",
      "Iteration 1382, loss = 0.96111848\n",
      "Iteration 1383, loss = 0.96023879\n",
      "Iteration 1384, loss = 0.95935527\n",
      "Iteration 1385, loss = 0.95847711\n",
      "Iteration 1386, loss = 0.95759344\n",
      "Iteration 1387, loss = 0.95670701\n",
      "Iteration 1388, loss = 0.95583988\n",
      "Iteration 1389, loss = 0.95496748\n",
      "Iteration 1390, loss = 0.95408004\n",
      "Iteration 1391, loss = 0.95321116\n",
      "Iteration 1392, loss = 0.95234067\n",
      "Iteration 1393, loss = 0.95147113\n",
      "Iteration 1394, loss = 0.95058160\n",
      "Iteration 1395, loss = 0.94972189\n",
      "Iteration 1396, loss = 0.94885161\n",
      "Iteration 1397, loss = 0.94798250\n",
      "Iteration 1398, loss = 0.94711191\n",
      "Iteration 1399, loss = 0.94624720\n",
      "Iteration 1400, loss = 0.94537757\n",
      "Iteration 1401, loss = 0.94451466\n",
      "Iteration 1402, loss = 0.94364441\n",
      "Iteration 1403, loss = 0.94278822\n",
      "Iteration 1404, loss = 0.94191259\n",
      "Iteration 1405, loss = 0.94105221\n",
      "Iteration 1406, loss = 0.94019402\n",
      "Iteration 1407, loss = 0.93933958\n",
      "Iteration 1408, loss = 0.93848294\n",
      "Iteration 1409, loss = 0.93762606\n",
      "Iteration 1410, loss = 0.93676887\n",
      "Iteration 1411, loss = 0.93591808\n",
      "Iteration 1412, loss = 0.93506601\n",
      "Iteration 1413, loss = 0.93421547\n",
      "Iteration 1414, loss = 0.93336636\n",
      "Iteration 1415, loss = 0.93251245\n",
      "Iteration 1416, loss = 0.93165932\n",
      "Iteration 1417, loss = 0.93080873\n",
      "Iteration 1418, loss = 0.92996572\n",
      "Iteration 1419, loss = 0.92912503\n",
      "Iteration 1420, loss = 0.92826842\n",
      "Iteration 1421, loss = 0.92742496\n",
      "Iteration 1422, loss = 0.92658065\n",
      "Iteration 1423, loss = 0.92572729\n",
      "Iteration 1424, loss = 0.92489975\n",
      "Iteration 1425, loss = 0.92404147\n",
      "Iteration 1426, loss = 0.92321265\n",
      "Iteration 1427, loss = 0.92236552\n",
      "Iteration 1428, loss = 0.92152096\n",
      "Iteration 1429, loss = 0.92068291\n",
      "Iteration 1430, loss = 0.91984239\n",
      "Iteration 1431, loss = 0.91901076\n",
      "Iteration 1432, loss = 0.91817874\n",
      "Iteration 1433, loss = 0.91734249\n",
      "Iteration 1434, loss = 0.91651143\n",
      "Iteration 1435, loss = 0.91567765\n",
      "Iteration 1436, loss = 0.91483001\n",
      "Iteration 1437, loss = 0.91400542\n",
      "Iteration 1438, loss = 0.91318380\n",
      "Iteration 1439, loss = 0.91235156\n",
      "Iteration 1440, loss = 0.91152686\n",
      "Iteration 1441, loss = 0.91069378\n",
      "Iteration 1442, loss = 0.90986896\n",
      "Iteration 1443, loss = 0.90904468\n",
      "Iteration 1444, loss = 0.90822135\n",
      "Iteration 1445, loss = 0.90740658\n",
      "Iteration 1446, loss = 0.90657692\n",
      "Iteration 1447, loss = 0.90576185\n",
      "Iteration 1448, loss = 0.90494315\n",
      "Iteration 1449, loss = 0.90411327\n",
      "Iteration 1450, loss = 0.90330233\n",
      "Iteration 1451, loss = 0.90247762\n",
      "Iteration 1452, loss = 0.90166073\n",
      "Iteration 1453, loss = 0.90085227\n",
      "Iteration 1454, loss = 0.90003599\n",
      "Iteration 1455, loss = 0.89921666\n",
      "Iteration 1456, loss = 0.89840871\n",
      "Iteration 1457, loss = 0.89759324\n",
      "Iteration 1458, loss = 0.89678548\n",
      "Iteration 1459, loss = 0.89597380\n",
      "Iteration 1460, loss = 0.89515832\n",
      "Iteration 1461, loss = 0.89434456\n",
      "Iteration 1462, loss = 0.89354435\n",
      "Iteration 1463, loss = 0.89273752\n",
      "Iteration 1464, loss = 0.89193804\n",
      "Iteration 1465, loss = 0.89112145\n",
      "Iteration 1466, loss = 0.89031921\n",
      "Iteration 1467, loss = 0.88951408\n",
      "Iteration 1468, loss = 0.88871695\n",
      "Iteration 1469, loss = 0.88791186\n",
      "Iteration 1470, loss = 0.88711747\n",
      "Iteration 1471, loss = 0.88631830\n",
      "Iteration 1472, loss = 0.88551915\n",
      "Iteration 1473, loss = 0.88472523\n",
      "Iteration 1474, loss = 0.88393191\n",
      "Iteration 1475, loss = 0.88314688\n",
      "Iteration 1476, loss = 0.88233996\n",
      "Iteration 1477, loss = 0.88154625\n",
      "Iteration 1478, loss = 0.88075357\n",
      "Iteration 1479, loss = 0.87996514\n",
      "Iteration 1480, loss = 0.87917152\n",
      "Iteration 1481, loss = 0.87838294\n",
      "Iteration 1482, loss = 0.87759898\n",
      "Iteration 1483, loss = 0.87681051\n",
      "Iteration 1484, loss = 0.87601760\n",
      "Iteration 1485, loss = 0.87523949\n",
      "Iteration 1486, loss = 0.87445581\n",
      "Iteration 1487, loss = 0.87366222\n",
      "Iteration 1488, loss = 0.87288609\n",
      "Iteration 1489, loss = 0.87210297\n",
      "Iteration 1490, loss = 0.87131935\n",
      "Iteration 1491, loss = 0.87054091\n",
      "Iteration 1492, loss = 0.86976688\n",
      "Iteration 1493, loss = 0.86898473\n",
      "Iteration 1494, loss = 0.86820155\n",
      "Iteration 1495, loss = 0.86743332\n",
      "Iteration 1496, loss = 0.86665060\n",
      "Iteration 1497, loss = 0.86588728\n",
      "Iteration 1498, loss = 0.86510838\n",
      "Iteration 1499, loss = 0.86433451\n",
      "Iteration 1500, loss = 0.86355912\n",
      "Iteration 1501, loss = 0.86278506\n",
      "Iteration 1502, loss = 0.86202323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1503, loss = 0.86125318\n",
      "Iteration 1504, loss = 0.86048277\n",
      "Iteration 1505, loss = 0.85971031\n",
      "Iteration 1506, loss = 0.85894612\n",
      "Iteration 1507, loss = 0.85818272\n",
      "Iteration 1508, loss = 0.85741823\n",
      "Iteration 1509, loss = 0.85665307\n",
      "Iteration 1510, loss = 0.85589337\n",
      "Iteration 1511, loss = 0.85512722\n",
      "Iteration 1512, loss = 0.85436646\n",
      "Iteration 1513, loss = 0.85360765\n",
      "Iteration 1514, loss = 0.85284609\n",
      "Iteration 1515, loss = 0.85207671\n",
      "Iteration 1516, loss = 0.85133045\n",
      "Iteration 1517, loss = 0.85056741\n",
      "Iteration 1518, loss = 0.84981706\n",
      "Iteration 1519, loss = 0.84905865\n",
      "Iteration 1520, loss = 0.84829642\n",
      "Iteration 1521, loss = 0.84754598\n",
      "Iteration 1522, loss = 0.84679219\n",
      "Iteration 1523, loss = 0.84604533\n",
      "Iteration 1524, loss = 0.84529040\n",
      "Iteration 1525, loss = 0.84454534\n",
      "Iteration 1526, loss = 0.84379017\n",
      "Iteration 1527, loss = 0.84304763\n",
      "Iteration 1528, loss = 0.84229788\n",
      "Iteration 1529, loss = 0.84156156\n",
      "Iteration 1530, loss = 0.84081101\n",
      "Iteration 1531, loss = 0.84006030\n",
      "Iteration 1532, loss = 0.83931523\n",
      "Iteration 1533, loss = 0.83858280\n",
      "Iteration 1534, loss = 0.83783461\n",
      "Iteration 1535, loss = 0.83709392\n",
      "Iteration 1536, loss = 0.83635154\n",
      "Iteration 1537, loss = 0.83560532\n",
      "Iteration 1538, loss = 0.83486670\n",
      "Iteration 1539, loss = 0.83413159\n",
      "Iteration 1540, loss = 0.83339653\n",
      "Iteration 1541, loss = 0.83266267\n",
      "Iteration 1542, loss = 0.83192801\n",
      "Iteration 1543, loss = 0.83119350\n",
      "Iteration 1544, loss = 0.83046288\n",
      "Iteration 1545, loss = 0.82971948\n",
      "Iteration 1546, loss = 0.82898696\n",
      "Iteration 1547, loss = 0.82825333\n",
      "Iteration 1548, loss = 0.82752564\n",
      "Iteration 1549, loss = 0.82679122\n",
      "Iteration 1550, loss = 0.82606659\n",
      "Iteration 1551, loss = 0.82534126\n",
      "Iteration 1552, loss = 0.82461050\n",
      "Iteration 1553, loss = 0.82388668\n",
      "Iteration 1554, loss = 0.82315747\n",
      "Iteration 1555, loss = 0.82242732\n",
      "Iteration 1556, loss = 0.82170779\n",
      "Iteration 1557, loss = 0.82098609\n",
      "Iteration 1558, loss = 0.82026186\n",
      "Iteration 1559, loss = 0.81954101\n",
      "Iteration 1560, loss = 0.81881732\n",
      "Iteration 1561, loss = 0.81809876\n",
      "Iteration 1562, loss = 0.81738008\n",
      "Iteration 1563, loss = 0.81666145\n",
      "Iteration 1564, loss = 0.81594488\n",
      "Iteration 1565, loss = 0.81522832\n",
      "Iteration 1566, loss = 0.81451163\n",
      "Iteration 1567, loss = 0.81380028\n",
      "Iteration 1568, loss = 0.81308207\n",
      "Iteration 1569, loss = 0.81237450\n",
      "Iteration 1570, loss = 0.81165547\n",
      "Iteration 1571, loss = 0.81094913\n",
      "Iteration 1572, loss = 0.81023682\n",
      "Iteration 1573, loss = 0.80953255\n",
      "Iteration 1574, loss = 0.80882547\n",
      "Iteration 1575, loss = 0.80811741\n",
      "Iteration 1576, loss = 0.80741099\n",
      "Iteration 1577, loss = 0.80671243\n",
      "Iteration 1578, loss = 0.80600214\n",
      "Iteration 1579, loss = 0.80529344\n",
      "Iteration 1580, loss = 0.80459378\n",
      "Iteration 1581, loss = 0.80389046\n",
      "Iteration 1582, loss = 0.80318274\n",
      "Iteration 1583, loss = 0.80248625\n",
      "Iteration 1584, loss = 0.80178260\n",
      "Iteration 1585, loss = 0.80109033\n",
      "Iteration 1586, loss = 0.80038657\n",
      "Iteration 1587, loss = 0.79968214\n",
      "Iteration 1588, loss = 0.79898168\n",
      "Iteration 1589, loss = 0.79828237\n",
      "Iteration 1590, loss = 0.79758974\n",
      "Iteration 1591, loss = 0.79689394\n",
      "Iteration 1592, loss = 0.79620063\n",
      "Iteration 1593, loss = 0.79551117\n",
      "Iteration 1594, loss = 0.79481415\n",
      "Iteration 1595, loss = 0.79412285\n",
      "Iteration 1596, loss = 0.79342838\n",
      "Iteration 1597, loss = 0.79273903\n",
      "Iteration 1598, loss = 0.79205948\n",
      "Iteration 1599, loss = 0.79136849\n",
      "Iteration 1600, loss = 0.79067944\n",
      "Iteration 1601, loss = 0.78999186\n",
      "Iteration 1602, loss = 0.78931578\n",
      "Iteration 1603, loss = 0.78862386\n",
      "Iteration 1604, loss = 0.78793923\n",
      "Iteration 1605, loss = 0.78725887\n",
      "Iteration 1606, loss = 0.78658502\n",
      "Iteration 1607, loss = 0.78589527\n",
      "Iteration 1608, loss = 0.78521496\n",
      "Iteration 1609, loss = 0.78453677\n",
      "Iteration 1610, loss = 0.78385904\n",
      "Iteration 1611, loss = 0.78318617\n",
      "Iteration 1612, loss = 0.78249421\n",
      "Iteration 1613, loss = 0.78182729\n",
      "Iteration 1614, loss = 0.78115077\n",
      "Iteration 1615, loss = 0.78046597\n",
      "Iteration 1616, loss = 0.77978991\n",
      "Iteration 1617, loss = 0.77911966\n",
      "Iteration 1618, loss = 0.77845896\n",
      "Iteration 1619, loss = 0.77776858\n",
      "Iteration 1620, loss = 0.77710309\n",
      "Iteration 1621, loss = 0.77642295\n",
      "Iteration 1622, loss = 0.77575943\n",
      "Iteration 1623, loss = 0.77508420\n",
      "Iteration 1624, loss = 0.77442064\n",
      "Iteration 1625, loss = 0.77374979\n",
      "Iteration 1626, loss = 0.77308998\n",
      "Iteration 1627, loss = 0.77242061\n",
      "Iteration 1628, loss = 0.77175771\n",
      "Iteration 1629, loss = 0.77109294\n",
      "Iteration 1630, loss = 0.77042949\n",
      "Iteration 1631, loss = 0.76976171\n",
      "Iteration 1632, loss = 0.76910443\n",
      "Iteration 1633, loss = 0.76844361\n",
      "Iteration 1634, loss = 0.76777705\n",
      "Iteration 1635, loss = 0.76711849\n",
      "Iteration 1636, loss = 0.76645716\n",
      "Iteration 1637, loss = 0.76580927\n",
      "Iteration 1638, loss = 0.76514008\n",
      "Iteration 1639, loss = 0.76448720\n",
      "Iteration 1640, loss = 0.76383151\n",
      "Iteration 1641, loss = 0.76317142\n",
      "Iteration 1642, loss = 0.76251373\n",
      "Iteration 1643, loss = 0.76185093\n",
      "Iteration 1644, loss = 0.76119273\n",
      "Iteration 1645, loss = 0.76054771\n",
      "Iteration 1646, loss = 0.75989784\n",
      "Iteration 1647, loss = 0.75924102\n",
      "Iteration 1648, loss = 0.75858704\n",
      "Iteration 1649, loss = 0.75794810\n",
      "Iteration 1650, loss = 0.75728881\n",
      "Iteration 1651, loss = 0.75664166\n",
      "Iteration 1652, loss = 0.75599040\n",
      "Iteration 1653, loss = 0.75535028\n",
      "Iteration 1654, loss = 0.75470147\n",
      "Iteration 1655, loss = 0.75405468\n",
      "Iteration 1656, loss = 0.75341782\n",
      "Iteration 1657, loss = 0.75277525\n",
      "Iteration 1658, loss = 0.75213782\n",
      "Iteration 1659, loss = 0.75148775\n",
      "Iteration 1660, loss = 0.75085027\n",
      "Iteration 1661, loss = 0.75021401\n",
      "Iteration 1662, loss = 0.74957643\n",
      "Iteration 1663, loss = 0.74893438\n",
      "Iteration 1664, loss = 0.74829971\n",
      "Iteration 1665, loss = 0.74766349\n",
      "Iteration 1666, loss = 0.74702415\n",
      "Iteration 1667, loss = 0.74638812\n",
      "Iteration 1668, loss = 0.74576122\n",
      "Iteration 1669, loss = 0.74511491\n",
      "Iteration 1670, loss = 0.74448300\n",
      "Iteration 1671, loss = 0.74384623\n",
      "Iteration 1672, loss = 0.74321784\n",
      "Iteration 1673, loss = 0.74258603\n",
      "Iteration 1674, loss = 0.74196060\n",
      "Iteration 1675, loss = 0.74132638\n",
      "Iteration 1676, loss = 0.74069446\n",
      "Iteration 1677, loss = 0.74006291\n",
      "Iteration 1678, loss = 0.73944477\n",
      "Iteration 1679, loss = 0.73881675\n",
      "Iteration 1680, loss = 0.73818163\n",
      "Iteration 1681, loss = 0.73756046\n",
      "Iteration 1682, loss = 0.73693363\n",
      "Iteration 1683, loss = 0.73631186\n",
      "Iteration 1684, loss = 0.73568309\n",
      "Iteration 1685, loss = 0.73506318\n",
      "Iteration 1686, loss = 0.73444515\n",
      "Iteration 1687, loss = 0.73382374\n",
      "Iteration 1688, loss = 0.73320143\n",
      "Iteration 1689, loss = 0.73258826\n",
      "Iteration 1690, loss = 0.73197179\n",
      "Iteration 1691, loss = 0.73134991\n",
      "Iteration 1692, loss = 0.73072628\n",
      "Iteration 1693, loss = 0.73010847\n",
      "Iteration 1694, loss = 0.72949759\n",
      "Iteration 1695, loss = 0.72887586\n",
      "Iteration 1696, loss = 0.72826023\n",
      "Iteration 1697, loss = 0.72765246\n",
      "Iteration 1698, loss = 0.72702935\n",
      "Iteration 1699, loss = 0.72641809\n",
      "Iteration 1700, loss = 0.72580315\n",
      "Iteration 1701, loss = 0.72520062\n",
      "Iteration 1702, loss = 0.72458205\n",
      "Iteration 1703, loss = 0.72396933\n",
      "Iteration 1704, loss = 0.72337327\n",
      "Iteration 1705, loss = 0.72275757\n",
      "Iteration 1706, loss = 0.72214280\n",
      "Iteration 1707, loss = 0.72153887\n",
      "Iteration 1708, loss = 0.72093860\n",
      "Iteration 1709, loss = 0.72033094\n",
      "Iteration 1710, loss = 0.71972996\n",
      "Iteration 1711, loss = 0.71912252\n",
      "Iteration 1712, loss = 0.71851624\n",
      "Iteration 1713, loss = 0.71791395\n",
      "Iteration 1714, loss = 0.71731243\n",
      "Iteration 1715, loss = 0.71671947\n",
      "Iteration 1716, loss = 0.71611553\n",
      "Iteration 1717, loss = 0.71551662\n",
      "Iteration 1718, loss = 0.71492248\n",
      "Iteration 1719, loss = 0.71431523\n",
      "Iteration 1720, loss = 0.71372336\n",
      "Iteration 1721, loss = 0.71312516\n",
      "Iteration 1722, loss = 0.71252496\n",
      "Iteration 1723, loss = 0.71193764\n",
      "Iteration 1724, loss = 0.71133868\n",
      "Iteration 1725, loss = 0.71074471\n",
      "Iteration 1726, loss = 0.71015107\n",
      "Iteration 1727, loss = 0.70955762\n",
      "Iteration 1728, loss = 0.70896360\n",
      "Iteration 1729, loss = 0.70838130\n",
      "Iteration 1730, loss = 0.70778395\n",
      "Iteration 1731, loss = 0.70718673\n",
      "Iteration 1732, loss = 0.70660098\n",
      "Iteration 1733, loss = 0.70601048\n",
      "Iteration 1734, loss = 0.70542298\n",
      "Iteration 1735, loss = 0.70483524\n",
      "Iteration 1736, loss = 0.70425425\n",
      "Iteration 1737, loss = 0.70366752\n",
      "Iteration 1738, loss = 0.70307036\n",
      "Iteration 1739, loss = 0.70249519\n",
      "Iteration 1740, loss = 0.70191605\n",
      "Iteration 1741, loss = 0.70133547\n",
      "Iteration 1742, loss = 0.70073983\n",
      "Iteration 1743, loss = 0.70015936\n",
      "Iteration 1744, loss = 0.69958361\n",
      "Iteration 1745, loss = 0.69899452\n",
      "Iteration 1746, loss = 0.69842336\n",
      "Iteration 1747, loss = 0.69784548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1748, loss = 0.69726239\n",
      "Iteration 1749, loss = 0.69667716\n",
      "Iteration 1750, loss = 0.69609884\n",
      "Iteration 1751, loss = 0.69553031\n",
      "Iteration 1752, loss = 0.69495151\n",
      "Iteration 1753, loss = 0.69437349\n",
      "Iteration 1754, loss = 0.69379744\n",
      "Iteration 1755, loss = 0.69322258\n",
      "Iteration 1756, loss = 0.69264953\n",
      "Iteration 1757, loss = 0.69207646\n",
      "Iteration 1758, loss = 0.69150430\n",
      "Iteration 1759, loss = 0.69093234\n",
      "Iteration 1760, loss = 0.69036467\n",
      "Iteration 1761, loss = 0.68979108\n",
      "Iteration 1762, loss = 0.68921982\n",
      "Iteration 1763, loss = 0.68864827\n",
      "Iteration 1764, loss = 0.68808337\n",
      "Iteration 1765, loss = 0.68751374\n",
      "Iteration 1766, loss = 0.68695600\n",
      "Iteration 1767, loss = 0.68637814\n",
      "Iteration 1768, loss = 0.68582322\n",
      "Iteration 1769, loss = 0.68525446\n",
      "Iteration 1770, loss = 0.68469401\n",
      "Iteration 1771, loss = 0.68413351\n",
      "Iteration 1772, loss = 0.68356987\n",
      "Iteration 1773, loss = 0.68300576\n",
      "Iteration 1774, loss = 0.68244627\n",
      "Iteration 1775, loss = 0.68188135\n",
      "Iteration 1776, loss = 0.68132764\n",
      "Iteration 1777, loss = 0.68076206\n",
      "Iteration 1778, loss = 0.68020543\n",
      "Iteration 1779, loss = 0.67964469\n",
      "Iteration 1780, loss = 0.67908984\n",
      "Iteration 1781, loss = 0.67852936\n",
      "Iteration 1782, loss = 0.67796711\n",
      "Iteration 1783, loss = 0.67741613\n",
      "Iteration 1784, loss = 0.67685757\n",
      "Iteration 1785, loss = 0.67630877\n",
      "Iteration 1786, loss = 0.67575277\n",
      "Iteration 1787, loss = 0.67519503\n",
      "Iteration 1788, loss = 0.67464780\n",
      "Iteration 1789, loss = 0.67409165\n",
      "Iteration 1790, loss = 0.67354235\n",
      "Iteration 1791, loss = 0.67299273\n",
      "Iteration 1792, loss = 0.67244445\n",
      "Iteration 1793, loss = 0.67189825\n",
      "Iteration 1794, loss = 0.67135215\n",
      "Iteration 1795, loss = 0.67080383\n",
      "Iteration 1796, loss = 0.67025878\n",
      "Iteration 1797, loss = 0.66971692\n",
      "Iteration 1798, loss = 0.66916585\n",
      "Iteration 1799, loss = 0.66862131\n",
      "Iteration 1800, loss = 0.66807793\n",
      "Iteration 1801, loss = 0.66753746\n",
      "Iteration 1802, loss = 0.66699070\n",
      "Iteration 1803, loss = 0.66645146\n",
      "Iteration 1804, loss = 0.66590553\n",
      "Iteration 1805, loss = 0.66536541\n",
      "Iteration 1806, loss = 0.66482172\n",
      "Iteration 1807, loss = 0.66428296\n",
      "Iteration 1808, loss = 0.66374107\n",
      "Iteration 1809, loss = 0.66320599\n",
      "Iteration 1810, loss = 0.66266347\n",
      "Iteration 1811, loss = 0.66212288\n",
      "Iteration 1812, loss = 0.66158376\n",
      "Iteration 1813, loss = 0.66104801\n",
      "Iteration 1814, loss = 0.66051102\n",
      "Iteration 1815, loss = 0.65997320\n",
      "Iteration 1816, loss = 0.65943682\n",
      "Iteration 1817, loss = 0.65890427\n",
      "Iteration 1818, loss = 0.65837012\n",
      "Iteration 1819, loss = 0.65783266\n",
      "Iteration 1820, loss = 0.65730757\n",
      "Iteration 1821, loss = 0.65677262\n",
      "Iteration 1822, loss = 0.65623934\n",
      "Iteration 1823, loss = 0.65570666\n",
      "Iteration 1824, loss = 0.65517889\n",
      "Iteration 1825, loss = 0.65464639\n",
      "Iteration 1826, loss = 0.65411960\n",
      "Iteration 1827, loss = 0.65358889\n",
      "Iteration 1828, loss = 0.65306164\n",
      "Iteration 1829, loss = 0.65253236\n",
      "Iteration 1830, loss = 0.65200759\n",
      "Iteration 1831, loss = 0.65147779\n",
      "Iteration 1832, loss = 0.65095569\n",
      "Iteration 1833, loss = 0.65043143\n",
      "Iteration 1834, loss = 0.64989971\n",
      "Iteration 1835, loss = 0.64937577\n",
      "Iteration 1836, loss = 0.64885350\n",
      "Iteration 1837, loss = 0.64833014\n",
      "Iteration 1838, loss = 0.64780646\n",
      "Iteration 1839, loss = 0.64728724\n",
      "Iteration 1840, loss = 0.64676246\n",
      "Iteration 1841, loss = 0.64625099\n",
      "Iteration 1842, loss = 0.64572508\n",
      "Iteration 1843, loss = 0.64520141\n",
      "Iteration 1844, loss = 0.64468938\n",
      "Iteration 1845, loss = 0.64416893\n",
      "Iteration 1846, loss = 0.64364665\n",
      "Iteration 1847, loss = 0.64313580\n",
      "Iteration 1848, loss = 0.64261850\n",
      "Iteration 1849, loss = 0.64209767\n",
      "Iteration 1850, loss = 0.64159487\n",
      "Iteration 1851, loss = 0.64107002\n",
      "Iteration 1852, loss = 0.64056014\n",
      "Iteration 1853, loss = 0.64004407\n",
      "Iteration 1854, loss = 0.63953043\n",
      "Iteration 1855, loss = 0.63902920\n",
      "Iteration 1856, loss = 0.63851075\n",
      "Iteration 1857, loss = 0.63800306\n",
      "Iteration 1858, loss = 0.63749139\n",
      "Iteration 1859, loss = 0.63698380\n",
      "Iteration 1860, loss = 0.63647852\n",
      "Iteration 1861, loss = 0.63596943\n",
      "Iteration 1862, loss = 0.63546372\n",
      "Iteration 1863, loss = 0.63495102\n",
      "Iteration 1864, loss = 0.63445181\n",
      "Iteration 1865, loss = 0.63394237\n",
      "Iteration 1866, loss = 0.63343865\n",
      "Iteration 1867, loss = 0.63293740\n",
      "Iteration 1868, loss = 0.63243058\n",
      "Iteration 1869, loss = 0.63192244\n",
      "Iteration 1870, loss = 0.63142095\n",
      "Iteration 1871, loss = 0.63092206\n",
      "Iteration 1872, loss = 0.63041776\n",
      "Iteration 1873, loss = 0.62991970\n",
      "Iteration 1874, loss = 0.62942314\n",
      "Iteration 1875, loss = 0.62892246\n",
      "Iteration 1876, loss = 0.62842855\n",
      "Iteration 1877, loss = 0.62791889\n",
      "Iteration 1878, loss = 0.62741973\n",
      "Iteration 1879, loss = 0.62692660\n",
      "Iteration 1880, loss = 0.62642993\n",
      "Iteration 1881, loss = 0.62593351\n",
      "Iteration 1882, loss = 0.62544044\n",
      "Iteration 1883, loss = 0.62493512\n",
      "Iteration 1884, loss = 0.62444545\n",
      "Iteration 1885, loss = 0.62394882\n",
      "Iteration 1886, loss = 0.62345363\n",
      "Iteration 1887, loss = 0.62296241\n",
      "Iteration 1888, loss = 0.62246799\n",
      "Iteration 1889, loss = 0.62197757\n",
      "Iteration 1890, loss = 0.62148116\n",
      "Iteration 1891, loss = 0.62099295\n",
      "Iteration 1892, loss = 0.62050706\n",
      "Iteration 1893, loss = 0.62001223\n",
      "Iteration 1894, loss = 0.61952878\n",
      "Iteration 1895, loss = 0.61903682\n",
      "Iteration 1896, loss = 0.61855022\n",
      "Iteration 1897, loss = 0.61805752\n",
      "Iteration 1898, loss = 0.61757159\n",
      "Iteration 1899, loss = 0.61708682\n",
      "Iteration 1900, loss = 0.61659592\n",
      "Iteration 1901, loss = 0.61611220\n",
      "Iteration 1902, loss = 0.61563049\n",
      "Iteration 1903, loss = 0.61514131\n",
      "Iteration 1904, loss = 0.61465736\n",
      "Iteration 1905, loss = 0.61417433\n",
      "Iteration 1906, loss = 0.61368972\n",
      "Iteration 1907, loss = 0.61320956\n",
      "Iteration 1908, loss = 0.61272171\n",
      "Iteration 1909, loss = 0.61224382\n",
      "Iteration 1910, loss = 0.61176590\n",
      "Iteration 1911, loss = 0.61128111\n",
      "Iteration 1912, loss = 0.61080309\n",
      "Iteration 1913, loss = 0.61032687\n",
      "Iteration 1914, loss = 0.60984249\n",
      "Iteration 1915, loss = 0.60936602\n",
      "Iteration 1916, loss = 0.60889329\n",
      "Iteration 1917, loss = 0.60842018\n",
      "Iteration 1918, loss = 0.60793959\n",
      "Iteration 1919, loss = 0.60746066\n",
      "Iteration 1920, loss = 0.60698510\n",
      "Iteration 1921, loss = 0.60651496\n",
      "Iteration 1922, loss = 0.60604126\n",
      "Iteration 1923, loss = 0.60556293\n",
      "Iteration 1924, loss = 0.60509144\n",
      "Iteration 1925, loss = 0.60461948\n",
      "Iteration 1926, loss = 0.60414565\n",
      "Iteration 1927, loss = 0.60367368\n",
      "Iteration 1928, loss = 0.60320072\n",
      "Iteration 1929, loss = 0.60273639\n",
      "Iteration 1930, loss = 0.60225991\n",
      "Iteration 1931, loss = 0.60179130\n",
      "Iteration 1932, loss = 0.60132073\n",
      "Iteration 1933, loss = 0.60085221\n",
      "Iteration 1934, loss = 0.60038239\n",
      "Iteration 1935, loss = 0.59991366\n",
      "Iteration 1936, loss = 0.59945365\n",
      "Iteration 1937, loss = 0.59898013\n",
      "Iteration 1938, loss = 0.59851125\n",
      "Iteration 1939, loss = 0.59805681\n",
      "Iteration 1940, loss = 0.59758920\n",
      "Iteration 1941, loss = 0.59712526\n",
      "Iteration 1942, loss = 0.59665748\n",
      "Iteration 1943, loss = 0.59619538\n",
      "Iteration 1944, loss = 0.59573289\n",
      "Iteration 1945, loss = 0.59527155\n",
      "Iteration 1946, loss = 0.59481181\n",
      "Iteration 1947, loss = 0.59434789\n",
      "Iteration 1948, loss = 0.59389335\n",
      "Iteration 1949, loss = 0.59342685\n",
      "Iteration 1950, loss = 0.59296967\n",
      "Iteration 1951, loss = 0.59251284\n",
      "Iteration 1952, loss = 0.59205069\n",
      "Iteration 1953, loss = 0.59159340\n",
      "Iteration 1954, loss = 0.59113725\n",
      "Iteration 1955, loss = 0.59068300\n",
      "Iteration 1956, loss = 0.59022443\n",
      "Iteration 1957, loss = 0.58976610\n",
      "Iteration 1958, loss = 0.58930782\n",
      "Iteration 1959, loss = 0.58885334\n",
      "Iteration 1960, loss = 0.58839667\n",
      "Iteration 1961, loss = 0.58795089\n",
      "Iteration 1962, loss = 0.58748813\n",
      "Iteration 1963, loss = 0.58703884\n",
      "Iteration 1964, loss = 0.58658418\n",
      "Iteration 1965, loss = 0.58613653\n",
      "Iteration 1966, loss = 0.58568290\n",
      "Iteration 1967, loss = 0.58523486\n",
      "Iteration 1968, loss = 0.58478495\n",
      "Iteration 1969, loss = 0.58433599\n",
      "Iteration 1970, loss = 0.58388159\n",
      "Iteration 1971, loss = 0.58343568\n",
      "Iteration 1972, loss = 0.58298496\n",
      "Iteration 1973, loss = 0.58253629\n",
      "Iteration 1974, loss = 0.58209805\n",
      "Iteration 1975, loss = 0.58164928\n",
      "Iteration 1976, loss = 0.58120004\n",
      "Iteration 1977, loss = 0.58075554\n",
      "Iteration 1978, loss = 0.58031137\n",
      "Iteration 1979, loss = 0.57987007\n",
      "Iteration 1980, loss = 0.57942091\n",
      "Iteration 1981, loss = 0.57897494\n",
      "Iteration 1982, loss = 0.57853547\n",
      "Iteration 1983, loss = 0.57809618\n",
      "Iteration 1984, loss = 0.57765267\n",
      "Iteration 1985, loss = 0.57720949\n",
      "Iteration 1986, loss = 0.57676580\n",
      "Iteration 1987, loss = 0.57633510\n",
      "Iteration 1988, loss = 0.57588937\n",
      "Iteration 1989, loss = 0.57544840\n",
      "Iteration 1990, loss = 0.57501025\n",
      "Iteration 1991, loss = 0.57457309\n",
      "Iteration 1992, loss = 0.57413987\n",
      "Iteration 1993, loss = 0.57369459\n",
      "Iteration 1994, loss = 0.57325995\n",
      "Iteration 1995, loss = 0.57282614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1996, loss = 0.57238602\n",
      "Iteration 1997, loss = 0.57195257\n",
      "Iteration 1998, loss = 0.57151322\n",
      "Iteration 1999, loss = 0.57107982\n",
      "Iteration 2000, loss = 0.57065117\n",
      "Iteration 2001, loss = 0.57020893\n",
      "Iteration 2002, loss = 0.56977971\n",
      "Iteration 2003, loss = 0.56934339\n",
      "Iteration 2004, loss = 0.56891028\n",
      "Iteration 2005, loss = 0.56847648\n",
      "Iteration 2006, loss = 0.56804248\n",
      "Iteration 2007, loss = 0.56761140\n",
      "Iteration 2008, loss = 0.56718473\n",
      "Iteration 2009, loss = 0.56675033\n",
      "Iteration 2010, loss = 0.56632293\n",
      "Iteration 2011, loss = 0.56589608\n",
      "Iteration 2012, loss = 0.56546516\n",
      "Iteration 2013, loss = 0.56503481\n",
      "Iteration 2014, loss = 0.56460539\n",
      "Iteration 2015, loss = 0.56418136\n",
      "Iteration 2016, loss = 0.56375511\n",
      "Iteration 2017, loss = 0.56332772\n",
      "Iteration 2018, loss = 0.56290209\n",
      "Iteration 2019, loss = 0.56247638\n",
      "Iteration 2020, loss = 0.56205053\n",
      "Iteration 2021, loss = 0.56163191\n",
      "Iteration 2022, loss = 0.56120814\n",
      "Iteration 2023, loss = 0.56078400\n",
      "Iteration 2024, loss = 0.56035614\n",
      "Iteration 2025, loss = 0.55993709\n",
      "Iteration 2026, loss = 0.55951943\n",
      "Iteration 2027, loss = 0.55909701\n",
      "Iteration 2028, loss = 0.55867579\n",
      "Iteration 2029, loss = 0.55826120\n",
      "Iteration 2030, loss = 0.55783763\n",
      "Iteration 2031, loss = 0.55741515\n",
      "Iteration 2032, loss = 0.55700082\n",
      "Iteration 2033, loss = 0.55658078\n",
      "Iteration 2034, loss = 0.55616627\n",
      "Iteration 2035, loss = 0.55573986\n",
      "Iteration 2036, loss = 0.55532620\n",
      "Iteration 2037, loss = 0.55490530\n",
      "Iteration 2038, loss = 0.55449441\n",
      "Iteration 2039, loss = 0.55407566\n",
      "Iteration 2040, loss = 0.55366840\n",
      "Iteration 2041, loss = 0.55324732\n",
      "Iteration 2042, loss = 0.55283397\n",
      "Iteration 2043, loss = 0.55241823\n",
      "Iteration 2044, loss = 0.55200216\n",
      "Iteration 2045, loss = 0.55159163\n",
      "Iteration 2046, loss = 0.55118026\n",
      "Iteration 2047, loss = 0.55077196\n",
      "Iteration 2048, loss = 0.55035123\n",
      "Iteration 2049, loss = 0.54994198\n",
      "Iteration 2050, loss = 0.54953106\n",
      "Iteration 2051, loss = 0.54911912\n",
      "Iteration 2052, loss = 0.54871946\n",
      "Iteration 2053, loss = 0.54830510\n",
      "Iteration 2054, loss = 0.54789863\n",
      "Iteration 2055, loss = 0.54748775\n",
      "Iteration 2056, loss = 0.54708147\n",
      "Iteration 2057, loss = 0.54667636\n",
      "Iteration 2058, loss = 0.54627379\n",
      "Iteration 2059, loss = 0.54586312\n",
      "Iteration 2060, loss = 0.54546284\n",
      "Iteration 2061, loss = 0.54505731\n",
      "Iteration 2062, loss = 0.54465444\n",
      "Iteration 2063, loss = 0.54424553\n",
      "Iteration 2064, loss = 0.54384421\n",
      "Iteration 2065, loss = 0.54344684\n",
      "Iteration 2066, loss = 0.54304005\n",
      "Iteration 2067, loss = 0.54263837\n",
      "Iteration 2068, loss = 0.54223531\n",
      "Iteration 2069, loss = 0.54183797\n",
      "Iteration 2070, loss = 0.54143666\n",
      "Iteration 2071, loss = 0.54102873\n",
      "Iteration 2072, loss = 0.54062986\n",
      "Iteration 2073, loss = 0.54023172\n",
      "Iteration 2074, loss = 0.53982948\n",
      "Iteration 2075, loss = 0.53943250\n",
      "Iteration 2076, loss = 0.53903131\n",
      "Iteration 2077, loss = 0.53863385\n",
      "Iteration 2078, loss = 0.53823342\n",
      "Iteration 2079, loss = 0.53783931\n",
      "Iteration 2080, loss = 0.53744372\n",
      "Iteration 2081, loss = 0.53704464\n",
      "Iteration 2082, loss = 0.53665141\n",
      "Iteration 2083, loss = 0.53625756\n",
      "Iteration 2084, loss = 0.53586343\n",
      "Iteration 2085, loss = 0.53546851\n",
      "Iteration 2086, loss = 0.53507382\n",
      "Iteration 2087, loss = 0.53467955\n",
      "Iteration 2088, loss = 0.53428826\n",
      "Iteration 2089, loss = 0.53389563\n",
      "Iteration 2090, loss = 0.53350308\n",
      "Iteration 2091, loss = 0.53310878\n",
      "Iteration 2092, loss = 0.53272181\n",
      "Iteration 2093, loss = 0.53232838\n",
      "Iteration 2094, loss = 0.53193698\n",
      "Iteration 2095, loss = 0.53154619\n",
      "Iteration 2096, loss = 0.53115685\n",
      "Iteration 2097, loss = 0.53076918\n",
      "Iteration 2098, loss = 0.53038088\n",
      "Iteration 2099, loss = 0.52999061\n",
      "Iteration 2100, loss = 0.52960521\n",
      "Iteration 2101, loss = 0.52921728\n",
      "Iteration 2102, loss = 0.52882784\n",
      "Iteration 2103, loss = 0.52844332\n",
      "Iteration 2104, loss = 0.52805648\n",
      "Iteration 2105, loss = 0.52767132\n",
      "Iteration 2106, loss = 0.52728455\n",
      "Iteration 2107, loss = 0.52690059\n",
      "Iteration 2108, loss = 0.52651520\n",
      "Iteration 2109, loss = 0.52612973\n",
      "Iteration 2110, loss = 0.52574856\n",
      "Iteration 2111, loss = 0.52536256\n",
      "Iteration 2112, loss = 0.52497925\n",
      "Iteration 2113, loss = 0.52459601\n",
      "Iteration 2114, loss = 0.52421464\n",
      "Iteration 2115, loss = 0.52383402\n",
      "Iteration 2116, loss = 0.52345396\n",
      "Iteration 2117, loss = 0.52307781\n",
      "Iteration 2118, loss = 0.52268784\n",
      "Iteration 2119, loss = 0.52231639\n",
      "Iteration 2120, loss = 0.52193298\n",
      "Iteration 2121, loss = 0.52155242\n",
      "Iteration 2122, loss = 0.52117443\n",
      "Iteration 2123, loss = 0.52079505\n",
      "Iteration 2124, loss = 0.52042223\n",
      "Iteration 2125, loss = 0.52004098\n",
      "Iteration 2126, loss = 0.51966485\n",
      "Iteration 2127, loss = 0.51928783\n",
      "Iteration 2128, loss = 0.51891069\n",
      "Iteration 2129, loss = 0.51853362\n",
      "Iteration 2130, loss = 0.51816069\n",
      "Iteration 2131, loss = 0.51778481\n",
      "Iteration 2132, loss = 0.51740863\n",
      "Iteration 2133, loss = 0.51703583\n",
      "Iteration 2134, loss = 0.51666216\n",
      "Iteration 2135, loss = 0.51628577\n",
      "Iteration 2136, loss = 0.51591494\n",
      "Iteration 2137, loss = 0.51554259\n",
      "Iteration 2138, loss = 0.51516498\n",
      "Iteration 2139, loss = 0.51479574\n",
      "Iteration 2140, loss = 0.51442306\n",
      "Iteration 2141, loss = 0.51405327\n",
      "Iteration 2142, loss = 0.51368025\n",
      "Iteration 2143, loss = 0.51330778\n",
      "Iteration 2144, loss = 0.51293652\n",
      "Iteration 2145, loss = 0.51256569\n",
      "Iteration 2146, loss = 0.51219860\n",
      "Iteration 2147, loss = 0.51182629\n",
      "Iteration 2148, loss = 0.51146131\n",
      "Iteration 2149, loss = 0.51108915\n",
      "Iteration 2150, loss = 0.51072155\n",
      "Iteration 2151, loss = 0.51035912\n",
      "Iteration 2152, loss = 0.50998898\n",
      "Iteration 2153, loss = 0.50962479\n",
      "Iteration 2154, loss = 0.50925575\n",
      "Iteration 2155, loss = 0.50889094\n",
      "Iteration 2156, loss = 0.50852989\n",
      "Iteration 2157, loss = 0.50816552\n",
      "Iteration 2158, loss = 0.50780121\n",
      "Iteration 2159, loss = 0.50743963\n",
      "Iteration 2160, loss = 0.50707790\n",
      "Iteration 2161, loss = 0.50671323\n",
      "Iteration 2162, loss = 0.50635391\n",
      "Iteration 2163, loss = 0.50599265\n",
      "Iteration 2164, loss = 0.50562915\n",
      "Iteration 2165, loss = 0.50526767\n",
      "Iteration 2166, loss = 0.50490627\n",
      "Iteration 2167, loss = 0.50455194\n",
      "Iteration 2168, loss = 0.50418654\n",
      "Iteration 2169, loss = 0.50382782\n",
      "Iteration 2170, loss = 0.50346989\n",
      "Iteration 2171, loss = 0.50310773\n",
      "Iteration 2172, loss = 0.50275082\n",
      "Iteration 2173, loss = 0.50239096\n",
      "Iteration 2174, loss = 0.50203679\n",
      "Iteration 2175, loss = 0.50167578\n",
      "Iteration 2176, loss = 0.50131613\n",
      "Iteration 2177, loss = 0.50096285\n",
      "Iteration 2178, loss = 0.50060698\n",
      "Iteration 2179, loss = 0.50024620\n",
      "Iteration 2180, loss = 0.49989270\n",
      "Iteration 2181, loss = 0.49953617\n",
      "Iteration 2182, loss = 0.49918647\n",
      "Iteration 2183, loss = 0.49882752\n",
      "Iteration 2184, loss = 0.49847406\n",
      "Iteration 2185, loss = 0.49812125\n",
      "Iteration 2186, loss = 0.49776860\n",
      "Iteration 2187, loss = 0.49741393\n",
      "Iteration 2188, loss = 0.49706363\n",
      "Iteration 2189, loss = 0.49671571\n",
      "Iteration 2190, loss = 0.49636418\n",
      "Iteration 2191, loss = 0.49601297\n",
      "Iteration 2192, loss = 0.49566140\n",
      "Iteration 2193, loss = 0.49531164\n",
      "Iteration 2194, loss = 0.49495872\n",
      "Iteration 2195, loss = 0.49461652\n",
      "Iteration 2196, loss = 0.49426243\n",
      "Iteration 2197, loss = 0.49391411\n",
      "Iteration 2198, loss = 0.49356624\n",
      "Iteration 2199, loss = 0.49322088\n",
      "Iteration 2200, loss = 0.49287270\n",
      "Iteration 2201, loss = 0.49252282\n",
      "Iteration 2202, loss = 0.49217888\n",
      "Iteration 2203, loss = 0.49183273\n",
      "Iteration 2204, loss = 0.49148716\n",
      "Iteration 2205, loss = 0.49114220\n",
      "Iteration 2206, loss = 0.49080024\n",
      "Iteration 2207, loss = 0.49045140\n",
      "Iteration 2208, loss = 0.49010709\n",
      "Iteration 2209, loss = 0.48976075\n",
      "Iteration 2210, loss = 0.48941922\n",
      "Iteration 2211, loss = 0.48907664\n",
      "Iteration 2212, loss = 0.48873292\n",
      "Iteration 2213, loss = 0.48838890\n",
      "Iteration 2214, loss = 0.48804511\n",
      "Iteration 2215, loss = 0.48770576\n",
      "Iteration 2216, loss = 0.48736341\n",
      "Iteration 2217, loss = 0.48702541\n",
      "Iteration 2218, loss = 0.48667588\n",
      "Iteration 2219, loss = 0.48633617\n",
      "Iteration 2220, loss = 0.48599185\n",
      "Iteration 2221, loss = 0.48565581\n",
      "Iteration 2222, loss = 0.48531261\n",
      "Iteration 2223, loss = 0.48497503\n",
      "Iteration 2224, loss = 0.48463336\n",
      "Iteration 2225, loss = 0.48429283\n",
      "Iteration 2226, loss = 0.48395546\n",
      "Iteration 2227, loss = 0.48362089\n",
      "Iteration 2228, loss = 0.48327924\n",
      "Iteration 2229, loss = 0.48294335\n",
      "Iteration 2230, loss = 0.48260863\n",
      "Iteration 2231, loss = 0.48227376\n",
      "Iteration 2232, loss = 0.48193743\n",
      "Iteration 2233, loss = 0.48159900\n",
      "Iteration 2234, loss = 0.48126502\n",
      "Iteration 2235, loss = 0.48092976\n",
      "Iteration 2236, loss = 0.48059441\n",
      "Iteration 2237, loss = 0.48026129\n",
      "Iteration 2238, loss = 0.47992832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2239, loss = 0.47959437\n",
      "Iteration 2240, loss = 0.47925775\n",
      "Iteration 2241, loss = 0.47892470\n",
      "Iteration 2242, loss = 0.47859220\n",
      "Iteration 2243, loss = 0.47826028\n",
      "Iteration 2244, loss = 0.47793066\n",
      "Iteration 2245, loss = 0.47760028\n",
      "Iteration 2246, loss = 0.47726999\n",
      "Iteration 2247, loss = 0.47693753\n",
      "Iteration 2248, loss = 0.47660734\n",
      "Iteration 2249, loss = 0.47628123\n",
      "Iteration 2250, loss = 0.47594779\n",
      "Iteration 2251, loss = 0.47561834\n",
      "Iteration 2252, loss = 0.47529043\n",
      "Iteration 2253, loss = 0.47495926\n",
      "Iteration 2254, loss = 0.47463316\n",
      "Iteration 2255, loss = 0.47430198\n",
      "Iteration 2256, loss = 0.47397775\n",
      "Iteration 2257, loss = 0.47365531\n",
      "Iteration 2258, loss = 0.47332334\n",
      "Iteration 2259, loss = 0.47299538\n",
      "Iteration 2260, loss = 0.47267335\n",
      "Iteration 2261, loss = 0.47234562\n",
      "Iteration 2262, loss = 0.47202126\n",
      "Iteration 2263, loss = 0.47169738\n",
      "Iteration 2264, loss = 0.47137266\n",
      "Iteration 2265, loss = 0.47105162\n",
      "Iteration 2266, loss = 0.47072364\n",
      "Iteration 2267, loss = 0.47039984\n",
      "Iteration 2268, loss = 0.47008182\n",
      "Iteration 2269, loss = 0.46976151\n",
      "Iteration 2270, loss = 0.46943767\n",
      "Iteration 2271, loss = 0.46911439\n",
      "Iteration 2272, loss = 0.46879031\n",
      "Iteration 2273, loss = 0.46847162\n",
      "Iteration 2274, loss = 0.46815429\n",
      "Iteration 2275, loss = 0.46783070\n",
      "Iteration 2276, loss = 0.46750962\n",
      "Iteration 2277, loss = 0.46719142\n",
      "Iteration 2278, loss = 0.46687470\n",
      "Iteration 2279, loss = 0.46655163\n",
      "Iteration 2280, loss = 0.46623498\n",
      "Iteration 2281, loss = 0.46591429\n",
      "Iteration 2282, loss = 0.46559594\n",
      "Iteration 2283, loss = 0.46527833\n",
      "Iteration 2284, loss = 0.46496399\n",
      "Iteration 2285, loss = 0.46464427\n",
      "Iteration 2286, loss = 0.46432704\n",
      "Iteration 2287, loss = 0.46401695\n",
      "Iteration 2288, loss = 0.46369603\n",
      "Iteration 2289, loss = 0.46337957\n",
      "Iteration 2290, loss = 0.46306613\n",
      "Iteration 2291, loss = 0.46274933\n",
      "Iteration 2292, loss = 0.46243587\n",
      "Iteration 2293, loss = 0.46212277\n",
      "Iteration 2294, loss = 0.46180867\n",
      "Iteration 2295, loss = 0.46149572\n",
      "Iteration 2296, loss = 0.46118002\n",
      "Iteration 2297, loss = 0.46087050\n",
      "Iteration 2298, loss = 0.46055952\n",
      "Iteration 2299, loss = 0.46024361\n",
      "Iteration 2300, loss = 0.45993629\n",
      "Iteration 2301, loss = 0.45962077\n",
      "Iteration 2302, loss = 0.45930752\n",
      "Iteration 2303, loss = 0.45899879\n",
      "Iteration 2304, loss = 0.45868569\n",
      "Iteration 2305, loss = 0.45837701\n",
      "Iteration 2306, loss = 0.45807206\n",
      "Iteration 2307, loss = 0.45775901\n",
      "Iteration 2308, loss = 0.45744999\n",
      "Iteration 2309, loss = 0.45714195\n",
      "Iteration 2310, loss = 0.45683115\n",
      "Iteration 2311, loss = 0.45652692\n",
      "Iteration 2312, loss = 0.45621728\n",
      "Iteration 2313, loss = 0.45591001\n",
      "Iteration 2314, loss = 0.45560038\n",
      "Iteration 2315, loss = 0.45529848\n",
      "Iteration 2316, loss = 0.45498836\n",
      "Iteration 2317, loss = 0.45468065\n",
      "Iteration 2318, loss = 0.45437975\n",
      "Iteration 2319, loss = 0.45407411\n",
      "Iteration 2320, loss = 0.45376817\n",
      "Iteration 2321, loss = 0.45346089\n",
      "Iteration 2322, loss = 0.45315493\n",
      "Iteration 2323, loss = 0.45285575\n",
      "Iteration 2324, loss = 0.45255315\n",
      "Iteration 2325, loss = 0.45224610\n",
      "Iteration 2326, loss = 0.45194342\n",
      "Iteration 2327, loss = 0.45164153\n",
      "Iteration 2328, loss = 0.45134138\n",
      "Iteration 2329, loss = 0.45103966\n",
      "Iteration 2330, loss = 0.45073740\n",
      "Iteration 2331, loss = 0.45043663\n",
      "Iteration 2332, loss = 0.45013775\n",
      "Iteration 2333, loss = 0.44983711\n",
      "Iteration 2334, loss = 0.44953776\n",
      "Iteration 2335, loss = 0.44923441\n",
      "Iteration 2336, loss = 0.44893563\n",
      "Iteration 2337, loss = 0.44863521\n",
      "Iteration 2338, loss = 0.44833865\n",
      "Iteration 2339, loss = 0.44803882\n",
      "Iteration 2340, loss = 0.44774277\n",
      "Iteration 2341, loss = 0.44743956\n",
      "Iteration 2342, loss = 0.44714227\n",
      "Iteration 2343, loss = 0.44684323\n",
      "Iteration 2344, loss = 0.44654733\n",
      "Iteration 2345, loss = 0.44625038\n",
      "Iteration 2346, loss = 0.44595440\n",
      "Iteration 2347, loss = 0.44565818\n",
      "Iteration 2348, loss = 0.44536520\n",
      "Iteration 2349, loss = 0.44506298\n",
      "Iteration 2350, loss = 0.44476806\n",
      "Iteration 2351, loss = 0.44446981\n",
      "Iteration 2352, loss = 0.44417663\n",
      "Iteration 2353, loss = 0.44388315\n",
      "Iteration 2354, loss = 0.44359144\n",
      "Iteration 2355, loss = 0.44329389\n",
      "Iteration 2356, loss = 0.44299944\n",
      "Iteration 2357, loss = 0.44270333\n",
      "Iteration 2358, loss = 0.44241231\n",
      "Iteration 2359, loss = 0.44212012\n",
      "Iteration 2360, loss = 0.44182856\n",
      "Iteration 2361, loss = 0.44153712\n",
      "Iteration 2362, loss = 0.44124489\n",
      "Iteration 2363, loss = 0.44095067\n",
      "Iteration 2364, loss = 0.44065970\n",
      "Iteration 2365, loss = 0.44036814\n",
      "Iteration 2366, loss = 0.44007748\n",
      "Iteration 2367, loss = 0.43978513\n",
      "Iteration 2368, loss = 0.43949513\n",
      "Iteration 2369, loss = 0.43920753\n",
      "Iteration 2370, loss = 0.43891779\n",
      "Iteration 2371, loss = 0.43862758\n",
      "Iteration 2372, loss = 0.43833895\n",
      "Iteration 2373, loss = 0.43805054\n",
      "Iteration 2374, loss = 0.43776368\n",
      "Iteration 2375, loss = 0.43747767\n",
      "Iteration 2376, loss = 0.43718426\n",
      "Iteration 2377, loss = 0.43689988\n",
      "Iteration 2378, loss = 0.43661161\n",
      "Iteration 2379, loss = 0.43632572\n",
      "Iteration 2380, loss = 0.43603685\n",
      "Iteration 2381, loss = 0.43575306\n",
      "Iteration 2382, loss = 0.43546805\n",
      "Iteration 2383, loss = 0.43517767\n",
      "Iteration 2384, loss = 0.43489350\n",
      "Iteration 2385, loss = 0.43460927\n",
      "Iteration 2386, loss = 0.43432258\n",
      "Iteration 2387, loss = 0.43403924\n",
      "Iteration 2388, loss = 0.43375324\n",
      "Iteration 2389, loss = 0.43346833\n",
      "Iteration 2390, loss = 0.43318525\n",
      "Iteration 2391, loss = 0.43290063\n",
      "Iteration 2392, loss = 0.43261777\n",
      "Iteration 2393, loss = 0.43233831\n",
      "Iteration 2394, loss = 0.43205704\n",
      "Iteration 2395, loss = 0.43177160\n",
      "Iteration 2396, loss = 0.43148969\n",
      "Iteration 2397, loss = 0.43120732\n",
      "Iteration 2398, loss = 0.43092520\n",
      "Iteration 2399, loss = 0.43064423\n",
      "Iteration 2400, loss = 0.43036559\n",
      "Iteration 2401, loss = 0.43008530\n",
      "Iteration 2402, loss = 0.42980608\n",
      "Iteration 2403, loss = 0.42952941\n",
      "Iteration 2404, loss = 0.42924713\n",
      "Iteration 2405, loss = 0.42896841\n",
      "Iteration 2406, loss = 0.42869081\n",
      "Iteration 2407, loss = 0.42840730\n",
      "Iteration 2408, loss = 0.42813182\n",
      "Iteration 2409, loss = 0.42785345\n",
      "Iteration 2410, loss = 0.42757830\n",
      "Iteration 2411, loss = 0.42730497\n",
      "Iteration 2412, loss = 0.42702368\n",
      "Iteration 2413, loss = 0.42674552\n",
      "Iteration 2414, loss = 0.42647235\n",
      "Iteration 2415, loss = 0.42619377\n",
      "Iteration 2416, loss = 0.42591752\n",
      "Iteration 2417, loss = 0.42564682\n",
      "Iteration 2418, loss = 0.42536948\n",
      "Iteration 2419, loss = 0.42509310\n",
      "Iteration 2420, loss = 0.42482139\n",
      "Iteration 2421, loss = 0.42454747\n",
      "Iteration 2422, loss = 0.42427109\n",
      "Iteration 2423, loss = 0.42399793\n",
      "Iteration 2424, loss = 0.42372413\n",
      "Iteration 2425, loss = 0.42345371\n",
      "Iteration 2426, loss = 0.42317834\n",
      "Iteration 2427, loss = 0.42290556\n",
      "Iteration 2428, loss = 0.42263133\n",
      "Iteration 2429, loss = 0.42236225\n",
      "Iteration 2430, loss = 0.42208724\n",
      "Iteration 2431, loss = 0.42181322\n",
      "Iteration 2432, loss = 0.42154228\n",
      "Iteration 2433, loss = 0.42126830\n",
      "Iteration 2434, loss = 0.42100201\n",
      "Iteration 2435, loss = 0.42072243\n",
      "Iteration 2436, loss = 0.42045569\n",
      "Iteration 2437, loss = 0.42018077\n",
      "Iteration 2438, loss = 0.41991391\n",
      "Iteration 2439, loss = 0.41964087\n",
      "Iteration 2440, loss = 0.41937694\n",
      "Iteration 2441, loss = 0.41910388\n",
      "Iteration 2442, loss = 0.41883271\n",
      "Iteration 2443, loss = 0.41856630\n",
      "Iteration 2444, loss = 0.41829826\n",
      "Iteration 2445, loss = 0.41803199\n",
      "Iteration 2446, loss = 0.41776522\n",
      "Iteration 2447, loss = 0.41749601\n",
      "Iteration 2448, loss = 0.41722778\n",
      "Iteration 2449, loss = 0.41696455\n",
      "Iteration 2450, loss = 0.41669999\n",
      "Iteration 2451, loss = 0.41643636\n",
      "Iteration 2452, loss = 0.41616791\n",
      "Iteration 2453, loss = 0.41590126\n",
      "Iteration 2454, loss = 0.41563403\n",
      "Iteration 2455, loss = 0.41536990\n",
      "Iteration 2456, loss = 0.41510070\n",
      "Iteration 2457, loss = 0.41483827\n",
      "Iteration 2458, loss = 0.41457052\n",
      "Iteration 2459, loss = 0.41430783\n",
      "Iteration 2460, loss = 0.41404040\n",
      "Iteration 2461, loss = 0.41377994\n",
      "Iteration 2462, loss = 0.41351386\n",
      "Iteration 2463, loss = 0.41325139\n",
      "Iteration 2464, loss = 0.41298601\n",
      "Iteration 2465, loss = 0.41272796\n",
      "Iteration 2466, loss = 0.41246440\n",
      "Iteration 2467, loss = 0.41220214\n",
      "Iteration 2468, loss = 0.41194224\n",
      "Iteration 2469, loss = 0.41168177\n",
      "Iteration 2470, loss = 0.41141978\n",
      "Iteration 2471, loss = 0.41115844\n",
      "Iteration 2472, loss = 0.41089751\n",
      "Iteration 2473, loss = 0.41063989\n",
      "Iteration 2474, loss = 0.41037805\n",
      "Iteration 2475, loss = 0.41012504\n",
      "Iteration 2476, loss = 0.40985910\n",
      "Iteration 2477, loss = 0.40959789\n",
      "Iteration 2478, loss = 0.40934016\n",
      "Iteration 2479, loss = 0.40908223\n",
      "Iteration 2480, loss = 0.40882370\n",
      "Iteration 2481, loss = 0.40856756\n",
      "Iteration 2482, loss = 0.40830842\n",
      "Iteration 2483, loss = 0.40804930\n",
      "Iteration 2484, loss = 0.40779474\n",
      "Iteration 2485, loss = 0.40753800\n",
      "Iteration 2486, loss = 0.40727874\n",
      "Iteration 2487, loss = 0.40701986\n",
      "Iteration 2488, loss = 0.40676498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2489, loss = 0.40650813\n",
      "Iteration 2490, loss = 0.40625325\n",
      "Iteration 2491, loss = 0.40599778\n",
      "Iteration 2492, loss = 0.40574015\n",
      "Iteration 2493, loss = 0.40548499\n",
      "Iteration 2494, loss = 0.40523486\n",
      "Iteration 2495, loss = 0.40497733\n",
      "Iteration 2496, loss = 0.40472148\n",
      "Iteration 2497, loss = 0.40446739\n",
      "Iteration 2498, loss = 0.40421458\n",
      "Iteration 2499, loss = 0.40396199\n",
      "Iteration 2500, loss = 0.40370791\n",
      "Iteration 2501, loss = 0.40345435\n",
      "Iteration 2502, loss = 0.40320417\n",
      "Iteration 2503, loss = 0.40294903\n",
      "Iteration 2504, loss = 0.40269626\n",
      "Iteration 2505, loss = 0.40244478\n",
      "Iteration 2506, loss = 0.40219157\n",
      "Iteration 2507, loss = 0.40194301\n",
      "Iteration 2508, loss = 0.40168861\n",
      "Iteration 2509, loss = 0.40143869\n",
      "Iteration 2510, loss = 0.40118737\n",
      "Iteration 2511, loss = 0.40093861\n",
      "Iteration 2512, loss = 0.40068663\n",
      "Iteration 2513, loss = 0.40043749\n",
      "Iteration 2514, loss = 0.40018742\n",
      "Iteration 2515, loss = 0.39994203\n",
      "Iteration 2516, loss = 0.39969291\n",
      "Iteration 2517, loss = 0.39944220\n",
      "Iteration 2518, loss = 0.39919423\n",
      "Iteration 2519, loss = 0.39894489\n",
      "Iteration 2520, loss = 0.39870064\n",
      "Iteration 2521, loss = 0.39845005\n",
      "Iteration 2522, loss = 0.39820144\n",
      "Iteration 2523, loss = 0.39795233\n",
      "Iteration 2524, loss = 0.39770960\n",
      "Iteration 2525, loss = 0.39745885\n",
      "Iteration 2526, loss = 0.39721841\n",
      "Iteration 2527, loss = 0.39697009\n",
      "Iteration 2528, loss = 0.39672264\n",
      "Iteration 2529, loss = 0.39647652\n",
      "Iteration 2530, loss = 0.39623423\n",
      "Iteration 2531, loss = 0.39598799\n",
      "Iteration 2532, loss = 0.39574375\n",
      "Iteration 2533, loss = 0.39550131\n",
      "Iteration 2534, loss = 0.39525304\n",
      "Iteration 2535, loss = 0.39501088\n",
      "Iteration 2536, loss = 0.39476913\n",
      "Iteration 2537, loss = 0.39452225\n",
      "Iteration 2538, loss = 0.39428474\n",
      "Iteration 2539, loss = 0.39403772\n",
      "Iteration 2540, loss = 0.39379471\n",
      "Iteration 2541, loss = 0.39355344\n",
      "Iteration 2542, loss = 0.39330875\n",
      "Iteration 2543, loss = 0.39306625\n",
      "Iteration 2544, loss = 0.39282222\n",
      "Iteration 2545, loss = 0.39257896\n",
      "Iteration 2546, loss = 0.39234274\n",
      "Iteration 2547, loss = 0.39209653\n",
      "Iteration 2548, loss = 0.39185685\n",
      "Iteration 2549, loss = 0.39161792\n",
      "Iteration 2550, loss = 0.39137592\n",
      "Iteration 2551, loss = 0.39113986\n",
      "Iteration 2552, loss = 0.39089762\n",
      "Iteration 2553, loss = 0.39065837\n",
      "Iteration 2554, loss = 0.39041995\n",
      "Iteration 2555, loss = 0.39018047\n",
      "Iteration 2556, loss = 0.38994127\n",
      "Iteration 2557, loss = 0.38970241\n",
      "Iteration 2558, loss = 0.38946618\n",
      "Iteration 2559, loss = 0.38922696\n",
      "Iteration 2560, loss = 0.38898946\n",
      "Iteration 2561, loss = 0.38875163\n",
      "Iteration 2562, loss = 0.38851835\n",
      "Iteration 2563, loss = 0.38827902\n",
      "Iteration 2564, loss = 0.38803907\n",
      "Iteration 2565, loss = 0.38780404\n",
      "Iteration 2566, loss = 0.38756308\n",
      "Iteration 2567, loss = 0.38732774\n",
      "Iteration 2568, loss = 0.38709669\n",
      "Iteration 2569, loss = 0.38685727\n",
      "Iteration 2570, loss = 0.38662051\n",
      "Iteration 2571, loss = 0.38638550\n",
      "Iteration 2572, loss = 0.38614698\n",
      "Iteration 2573, loss = 0.38591082\n",
      "Iteration 2574, loss = 0.38567767\n",
      "Iteration 2575, loss = 0.38544040\n",
      "Iteration 2576, loss = 0.38520507\n",
      "Iteration 2577, loss = 0.38497143\n",
      "Iteration 2578, loss = 0.38473848\n",
      "Iteration 2579, loss = 0.38450583\n",
      "Iteration 2580, loss = 0.38427005\n",
      "Iteration 2581, loss = 0.38403444\n",
      "Iteration 2582, loss = 0.38380032\n",
      "Iteration 2583, loss = 0.38356890\n",
      "Iteration 2584, loss = 0.38333468\n",
      "Iteration 2585, loss = 0.38310230\n",
      "Iteration 2586, loss = 0.38286979\n",
      "Iteration 2587, loss = 0.38264048\n",
      "Iteration 2588, loss = 0.38240401\n",
      "Iteration 2589, loss = 0.38217363\n",
      "Iteration 2590, loss = 0.38194207\n",
      "Iteration 2591, loss = 0.38171387\n",
      "Iteration 2592, loss = 0.38148172\n",
      "Iteration 2593, loss = 0.38124924\n",
      "Iteration 2594, loss = 0.38102163\n",
      "Iteration 2595, loss = 0.38078954\n",
      "Iteration 2596, loss = 0.38056247\n",
      "Iteration 2597, loss = 0.38033107\n",
      "Iteration 2598, loss = 0.38010012\n",
      "Iteration 2599, loss = 0.37987088\n",
      "Iteration 2600, loss = 0.37964142\n",
      "Iteration 2601, loss = 0.37941370\n",
      "Iteration 2602, loss = 0.37918362\n",
      "Iteration 2603, loss = 0.37895535\n",
      "Iteration 2604, loss = 0.37872938\n",
      "Iteration 2605, loss = 0.37849755\n",
      "Iteration 2606, loss = 0.37827429\n",
      "Iteration 2607, loss = 0.37804472\n",
      "Iteration 2608, loss = 0.37781872\n",
      "Iteration 2609, loss = 0.37759302\n",
      "Iteration 2610, loss = 0.37736569\n",
      "Iteration 2611, loss = 0.37713866\n",
      "Iteration 2612, loss = 0.37691433\n",
      "Iteration 2613, loss = 0.37668807\n",
      "Iteration 2614, loss = 0.37646042\n",
      "Iteration 2615, loss = 0.37623788\n",
      "Iteration 2616, loss = 0.37600794\n",
      "Iteration 2617, loss = 0.37578519\n",
      "Iteration 2618, loss = 0.37555750\n",
      "Iteration 2619, loss = 0.37533564\n",
      "Iteration 2620, loss = 0.37510799\n",
      "Iteration 2621, loss = 0.37488544\n",
      "Iteration 2622, loss = 0.37466129\n",
      "Iteration 2623, loss = 0.37443730\n",
      "Iteration 2624, loss = 0.37421565\n",
      "Iteration 2625, loss = 0.37398888\n",
      "Iteration 2626, loss = 0.37376841\n",
      "Iteration 2627, loss = 0.37354794\n",
      "Iteration 2628, loss = 0.37332159\n",
      "Iteration 2629, loss = 0.37309858\n",
      "Iteration 2630, loss = 0.37288008\n",
      "Iteration 2631, loss = 0.37265933\n",
      "Iteration 2632, loss = 0.37243561\n",
      "Iteration 2633, loss = 0.37221476\n",
      "Iteration 2634, loss = 0.37199463\n",
      "Iteration 2635, loss = 0.37177092\n",
      "Iteration 2636, loss = 0.37155271\n",
      "Iteration 2637, loss = 0.37133381\n",
      "Iteration 2638, loss = 0.37110866\n",
      "Iteration 2639, loss = 0.37088968\n",
      "Iteration 2640, loss = 0.37066811\n",
      "Iteration 2641, loss = 0.37045164\n",
      "Iteration 2642, loss = 0.37022822\n",
      "Iteration 2643, loss = 0.37001045\n",
      "Iteration 2644, loss = 0.36978802\n",
      "Iteration 2645, loss = 0.36957273\n",
      "Iteration 2646, loss = 0.36935111\n",
      "Iteration 2647, loss = 0.36913532\n",
      "Iteration 2648, loss = 0.36891661\n",
      "Iteration 2649, loss = 0.36869664\n",
      "Iteration 2650, loss = 0.36847822\n",
      "Iteration 2651, loss = 0.36826434\n",
      "Iteration 2652, loss = 0.36804370\n",
      "Iteration 2653, loss = 0.36782581\n",
      "Iteration 2654, loss = 0.36761176\n",
      "Iteration 2655, loss = 0.36739573\n",
      "Iteration 2656, loss = 0.36717906\n",
      "Iteration 2657, loss = 0.36696063\n",
      "Iteration 2658, loss = 0.36674454\n",
      "Iteration 2659, loss = 0.36652921\n",
      "Iteration 2660, loss = 0.36631182\n",
      "Iteration 2661, loss = 0.36610167\n",
      "Iteration 2662, loss = 0.36588472\n",
      "Iteration 2663, loss = 0.36566902\n",
      "Iteration 2664, loss = 0.36545236\n",
      "Iteration 2665, loss = 0.36523850\n",
      "Iteration 2666, loss = 0.36502543\n",
      "Iteration 2667, loss = 0.36480946\n",
      "Iteration 2668, loss = 0.36459625\n",
      "Iteration 2669, loss = 0.36438380\n",
      "Iteration 2670, loss = 0.36416842\n",
      "Iteration 2671, loss = 0.36395476\n",
      "Iteration 2672, loss = 0.36374267\n",
      "Iteration 2673, loss = 0.36352807\n",
      "Iteration 2674, loss = 0.36331506\n",
      "Iteration 2675, loss = 0.36310327\n",
      "Iteration 2676, loss = 0.36288789\n",
      "Iteration 2677, loss = 0.36267857\n",
      "Iteration 2678, loss = 0.36246247\n",
      "Iteration 2679, loss = 0.36225248\n",
      "Iteration 2680, loss = 0.36203888\n",
      "Iteration 2681, loss = 0.36182762\n",
      "Iteration 2682, loss = 0.36161566\n",
      "Iteration 2683, loss = 0.36140234\n",
      "Iteration 2684, loss = 0.36119514\n",
      "Iteration 2685, loss = 0.36098262\n",
      "Iteration 2686, loss = 0.36077260\n",
      "Iteration 2687, loss = 0.36056134\n",
      "Iteration 2688, loss = 0.36034828\n",
      "Iteration 2689, loss = 0.36013926\n",
      "Iteration 2690, loss = 0.35992668\n",
      "Iteration 2691, loss = 0.35971875\n",
      "Iteration 2692, loss = 0.35950886\n",
      "Iteration 2693, loss = 0.35930050\n",
      "Iteration 2694, loss = 0.35909225\n",
      "Iteration 2695, loss = 0.35888320\n",
      "Iteration 2696, loss = 0.35867487\n",
      "Iteration 2697, loss = 0.35846722\n",
      "Iteration 2698, loss = 0.35825524\n",
      "Iteration 2699, loss = 0.35805005\n",
      "Iteration 2700, loss = 0.35784077\n",
      "Iteration 2701, loss = 0.35763415\n",
      "Iteration 2702, loss = 0.35742624\n",
      "Iteration 2703, loss = 0.35721928\n",
      "Iteration 2704, loss = 0.35700963\n",
      "Iteration 2705, loss = 0.35680532\n",
      "Iteration 2706, loss = 0.35659759\n",
      "Iteration 2707, loss = 0.35638975\n",
      "Iteration 2708, loss = 0.35618753\n",
      "Iteration 2709, loss = 0.35597821\n",
      "Iteration 2710, loss = 0.35577391\n",
      "Iteration 2711, loss = 0.35556758\n",
      "Iteration 2712, loss = 0.35536409\n",
      "Iteration 2713, loss = 0.35515578\n",
      "Iteration 2714, loss = 0.35494832\n",
      "Iteration 2715, loss = 0.35474729\n",
      "Iteration 2716, loss = 0.35454236\n",
      "Iteration 2717, loss = 0.35433518\n",
      "Iteration 2718, loss = 0.35413054\n",
      "Iteration 2719, loss = 0.35392784\n",
      "Iteration 2720, loss = 0.35372540\n",
      "Iteration 2721, loss = 0.35351991\n",
      "Iteration 2722, loss = 0.35331790\n",
      "Iteration 2723, loss = 0.35311094\n",
      "Iteration 2724, loss = 0.35291337\n",
      "Iteration 2725, loss = 0.35270598\n",
      "Iteration 2726, loss = 0.35250554\n",
      "Iteration 2727, loss = 0.35230286\n",
      "Iteration 2728, loss = 0.35210234\n",
      "Iteration 2729, loss = 0.35189894\n",
      "Iteration 2730, loss = 0.35169747\n",
      "Iteration 2731, loss = 0.35149608\n",
      "Iteration 2732, loss = 0.35129758\n",
      "Iteration 2733, loss = 0.35109420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2734, loss = 0.35089213\n",
      "Iteration 2735, loss = 0.35069253\n",
      "Iteration 2736, loss = 0.35049392\n",
      "Iteration 2737, loss = 0.35029233\n",
      "Iteration 2738, loss = 0.35009240\n",
      "Iteration 2739, loss = 0.34989199\n",
      "Iteration 2740, loss = 0.34969363\n",
      "Iteration 2741, loss = 0.34949331\n",
      "Iteration 2742, loss = 0.34929542\n",
      "Iteration 2743, loss = 0.34909445\n",
      "Iteration 2744, loss = 0.34889388\n",
      "Iteration 2745, loss = 0.34869527\n",
      "Iteration 2746, loss = 0.34849970\n",
      "Iteration 2747, loss = 0.34829870\n",
      "Iteration 2748, loss = 0.34809792\n",
      "Iteration 2749, loss = 0.34790257\n",
      "Iteration 2750, loss = 0.34770540\n",
      "Iteration 2751, loss = 0.34750633\n",
      "Iteration 2752, loss = 0.34730932\n",
      "Iteration 2753, loss = 0.34711223\n",
      "Iteration 2754, loss = 0.34691555\n",
      "Iteration 2755, loss = 0.34672104\n",
      "Iteration 2756, loss = 0.34652039\n",
      "Iteration 2757, loss = 0.34632401\n",
      "Iteration 2758, loss = 0.34612728\n",
      "Iteration 2759, loss = 0.34593186\n",
      "Iteration 2760, loss = 0.34573823\n",
      "Iteration 2761, loss = 0.34554120\n",
      "Iteration 2762, loss = 0.34534389\n",
      "Iteration 2763, loss = 0.34514814\n",
      "Iteration 2764, loss = 0.34495210\n",
      "Iteration 2765, loss = 0.34475971\n",
      "Iteration 2766, loss = 0.34456185\n",
      "Iteration 2767, loss = 0.34436740\n",
      "Iteration 2768, loss = 0.34417179\n",
      "Iteration 2769, loss = 0.34397924\n",
      "Iteration 2770, loss = 0.34378061\n",
      "Iteration 2771, loss = 0.34358802\n",
      "Iteration 2772, loss = 0.34339331\n",
      "Iteration 2773, loss = 0.34320007\n",
      "Iteration 2774, loss = 0.34300506\n",
      "Iteration 2775, loss = 0.34281049\n",
      "Iteration 2776, loss = 0.34261778\n",
      "Iteration 2777, loss = 0.34242617\n",
      "Iteration 2778, loss = 0.34223191\n",
      "Iteration 2779, loss = 0.34203832\n",
      "Iteration 2780, loss = 0.34184719\n",
      "Iteration 2781, loss = 0.34165358\n",
      "Iteration 2782, loss = 0.34146225\n",
      "Iteration 2783, loss = 0.34126892\n",
      "Iteration 2784, loss = 0.34107852\n",
      "Iteration 2785, loss = 0.34088699\n",
      "Iteration 2786, loss = 0.34069666\n",
      "Iteration 2787, loss = 0.34050586\n",
      "Iteration 2788, loss = 0.34031384\n",
      "Iteration 2789, loss = 0.34012234\n",
      "Iteration 2790, loss = 0.33993139\n",
      "Iteration 2791, loss = 0.33974133\n",
      "Iteration 2792, loss = 0.33955138\n",
      "Iteration 2793, loss = 0.33936374\n",
      "Iteration 2794, loss = 0.33916972\n",
      "Iteration 2795, loss = 0.33898085\n",
      "Iteration 2796, loss = 0.33879183\n",
      "Iteration 2797, loss = 0.33860378\n",
      "Iteration 2798, loss = 0.33841122\n",
      "Iteration 2799, loss = 0.33822239\n",
      "Iteration 2800, loss = 0.33803099\n",
      "Iteration 2801, loss = 0.33784462\n",
      "Iteration 2802, loss = 0.33765217\n",
      "Iteration 2803, loss = 0.33746020\n",
      "Iteration 2804, loss = 0.33727623\n",
      "Iteration 2805, loss = 0.33708483\n",
      "Iteration 2806, loss = 0.33689654\n",
      "Iteration 2807, loss = 0.33671045\n",
      "Iteration 2808, loss = 0.33652105\n",
      "Iteration 2809, loss = 0.33633283\n",
      "Iteration 2810, loss = 0.33614450\n",
      "Iteration 2811, loss = 0.33595695\n",
      "Iteration 2812, loss = 0.33577299\n",
      "Iteration 2813, loss = 0.33558489\n",
      "Iteration 2814, loss = 0.33539840\n",
      "Iteration 2815, loss = 0.33521142\n",
      "Iteration 2816, loss = 0.33502525\n",
      "Iteration 2817, loss = 0.33483794\n",
      "Iteration 2818, loss = 0.33465161\n",
      "Iteration 2819, loss = 0.33446784\n",
      "Iteration 2820, loss = 0.33428249\n",
      "Iteration 2821, loss = 0.33409921\n",
      "Iteration 2822, loss = 0.33391106\n",
      "Iteration 2823, loss = 0.33372624\n",
      "Iteration 2824, loss = 0.33354061\n",
      "Iteration 2825, loss = 0.33335798\n",
      "Iteration 2826, loss = 0.33316939\n",
      "Iteration 2827, loss = 0.33298647\n",
      "Iteration 2828, loss = 0.33280074\n",
      "Iteration 2829, loss = 0.33261741\n",
      "Iteration 2830, loss = 0.33243264\n",
      "Iteration 2831, loss = 0.33224948\n",
      "Iteration 2832, loss = 0.33206746\n",
      "Iteration 2833, loss = 0.33188191\n",
      "Iteration 2834, loss = 0.33169719\n",
      "Iteration 2835, loss = 0.33151646\n",
      "Iteration 2836, loss = 0.33133084\n",
      "Iteration 2837, loss = 0.33114922\n",
      "Iteration 2838, loss = 0.33096856\n",
      "Iteration 2839, loss = 0.33078696\n",
      "Iteration 2840, loss = 0.33060450\n",
      "Iteration 2841, loss = 0.33042065\n",
      "Iteration 2842, loss = 0.33023890\n",
      "Iteration 2843, loss = 0.33005674\n",
      "Iteration 2844, loss = 0.32987674\n",
      "Iteration 2845, loss = 0.32969356\n",
      "Iteration 2846, loss = 0.32951473\n",
      "Iteration 2847, loss = 0.32933428\n",
      "Iteration 2848, loss = 0.32915106\n",
      "Iteration 2849, loss = 0.32897155\n",
      "Iteration 2850, loss = 0.32879169\n",
      "Iteration 2851, loss = 0.32861169\n",
      "Iteration 2852, loss = 0.32843265\n",
      "Iteration 2853, loss = 0.32825197\n",
      "Iteration 2854, loss = 0.32807068\n",
      "Iteration 2855, loss = 0.32789383\n",
      "Iteration 2856, loss = 0.32771301\n",
      "Iteration 2857, loss = 0.32753408\n",
      "Iteration 2858, loss = 0.32735331\n",
      "Iteration 2859, loss = 0.32717838\n",
      "Iteration 2860, loss = 0.32699778\n",
      "Iteration 2861, loss = 0.32681868\n",
      "Iteration 2862, loss = 0.32664023\n",
      "Iteration 2863, loss = 0.32646366\n",
      "Iteration 2864, loss = 0.32628524\n",
      "Iteration 2865, loss = 0.32610713\n",
      "Iteration 2866, loss = 0.32593033\n",
      "Iteration 2867, loss = 0.32575272\n",
      "Iteration 2868, loss = 0.32557428\n",
      "Iteration 2869, loss = 0.32539575\n",
      "Iteration 2870, loss = 0.32522147\n",
      "Iteration 2871, loss = 0.32504260\n",
      "Iteration 2872, loss = 0.32486591\n",
      "Iteration 2873, loss = 0.32468687\n",
      "Iteration 2874, loss = 0.32451157\n",
      "Iteration 2875, loss = 0.32433520\n",
      "Iteration 2876, loss = 0.32416091\n",
      "Iteration 2877, loss = 0.32398354\n",
      "Iteration 2878, loss = 0.32380730\n",
      "Iteration 2879, loss = 0.32362934\n",
      "Iteration 2880, loss = 0.32345596\n",
      "Iteration 2881, loss = 0.32328274\n",
      "Iteration 2882, loss = 0.32310532\n",
      "Iteration 2883, loss = 0.32293040\n",
      "Iteration 2884, loss = 0.32275461\n",
      "Iteration 2885, loss = 0.32258121\n",
      "Iteration 2886, loss = 0.32240533\n",
      "Iteration 2887, loss = 0.32223179\n",
      "Iteration 2888, loss = 0.32205675\n",
      "Iteration 2889, loss = 0.32188382\n",
      "Iteration 2890, loss = 0.32170906\n",
      "Iteration 2891, loss = 0.32153650\n",
      "Iteration 2892, loss = 0.32136239\n",
      "Iteration 2893, loss = 0.32119039\n",
      "Iteration 2894, loss = 0.32101504\n",
      "Iteration 2895, loss = 0.32084143\n",
      "Iteration 2896, loss = 0.32066976\n",
      "Iteration 2897, loss = 0.32049613\n",
      "Iteration 2898, loss = 0.32032483\n",
      "Iteration 2899, loss = 0.32015262\n",
      "Iteration 2900, loss = 0.31997986\n",
      "Iteration 2901, loss = 0.31980730\n",
      "Iteration 2902, loss = 0.31963383\n",
      "Iteration 2903, loss = 0.31946143\n",
      "Iteration 2904, loss = 0.31929154\n",
      "Iteration 2905, loss = 0.31911740\n",
      "Iteration 2906, loss = 0.31894861\n",
      "Iteration 2907, loss = 0.31877509\n",
      "Iteration 2908, loss = 0.31860443\n",
      "Iteration 2909, loss = 0.31843259\n",
      "Iteration 2910, loss = 0.31826224\n",
      "Iteration 2911, loss = 0.31809135\n",
      "Iteration 2912, loss = 0.31791872\n",
      "Iteration 2913, loss = 0.31774873\n",
      "Iteration 2914, loss = 0.31757938\n",
      "Iteration 2915, loss = 0.31740739\n",
      "Iteration 2916, loss = 0.31723890\n",
      "Iteration 2917, loss = 0.31706950\n",
      "Iteration 2918, loss = 0.31689804\n",
      "Iteration 2919, loss = 0.31672827\n",
      "Iteration 2920, loss = 0.31656000\n",
      "Iteration 2921, loss = 0.31639035\n",
      "Iteration 2922, loss = 0.31622071\n",
      "Iteration 2923, loss = 0.31605097\n",
      "Iteration 2924, loss = 0.31588224\n",
      "Iteration 2925, loss = 0.31571453\n",
      "Iteration 2926, loss = 0.31554679\n",
      "Iteration 2927, loss = 0.31537776\n",
      "Iteration 2928, loss = 0.31521094\n",
      "Iteration 2929, loss = 0.31504332\n",
      "Iteration 2930, loss = 0.31487631\n",
      "Iteration 2931, loss = 0.31470617\n",
      "Iteration 2932, loss = 0.31454033\n",
      "Iteration 2933, loss = 0.31437433\n",
      "Iteration 2934, loss = 0.31420603\n",
      "Iteration 2935, loss = 0.31403928\n",
      "Iteration 2936, loss = 0.31387256\n",
      "Iteration 2937, loss = 0.31370727\n",
      "Iteration 2938, loss = 0.31354032\n",
      "Iteration 2939, loss = 0.31337426\n",
      "Iteration 2940, loss = 0.31320767\n",
      "Iteration 2941, loss = 0.31304072\n",
      "Iteration 2942, loss = 0.31287697\n",
      "Iteration 2943, loss = 0.31270949\n",
      "Iteration 2944, loss = 0.31254509\n",
      "Iteration 2945, loss = 0.31238105\n",
      "Iteration 2946, loss = 0.31221494\n",
      "Iteration 2947, loss = 0.31205017\n",
      "Iteration 2948, loss = 0.31188693\n",
      "Iteration 2949, loss = 0.31172187\n",
      "Iteration 2950, loss = 0.31155643\n",
      "Iteration 2951, loss = 0.31139297\n",
      "Iteration 2952, loss = 0.31122763\n",
      "Iteration 2953, loss = 0.31106409\n",
      "Iteration 2954, loss = 0.31090125\n",
      "Iteration 2955, loss = 0.31073646\n",
      "Iteration 2956, loss = 0.31057215\n",
      "Iteration 2957, loss = 0.31041026\n",
      "Iteration 2958, loss = 0.31024551\n",
      "Iteration 2959, loss = 0.31008179\n",
      "Iteration 2960, loss = 0.30991916\n",
      "Iteration 2961, loss = 0.30975674\n",
      "Iteration 2962, loss = 0.30959217\n",
      "Iteration 2963, loss = 0.30943065\n",
      "Iteration 2964, loss = 0.30926745\n",
      "Iteration 2965, loss = 0.30910569\n",
      "Iteration 2966, loss = 0.30894254\n",
      "Iteration 2967, loss = 0.30878183\n",
      "Iteration 2968, loss = 0.30862049\n",
      "Iteration 2969, loss = 0.30845637\n",
      "Iteration 2970, loss = 0.30829436\n",
      "Iteration 2971, loss = 0.30813541\n",
      "Iteration 2972, loss = 0.30797168\n",
      "Iteration 2973, loss = 0.30781186\n",
      "Iteration 2974, loss = 0.30764943\n",
      "Iteration 2975, loss = 0.30748988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2976, loss = 0.30732865\n",
      "Iteration 2977, loss = 0.30716747\n",
      "Iteration 2978, loss = 0.30700671\n",
      "Iteration 2979, loss = 0.30684697\n",
      "Iteration 2980, loss = 0.30668692\n",
      "Iteration 2981, loss = 0.30652566\n",
      "Iteration 2982, loss = 0.30636773\n",
      "Iteration 2983, loss = 0.30620674\n",
      "Iteration 2984, loss = 0.30604667\n",
      "Iteration 2985, loss = 0.30588645\n",
      "Iteration 2986, loss = 0.30572906\n",
      "Iteration 2987, loss = 0.30556804\n",
      "Iteration 2988, loss = 0.30540898\n",
      "Iteration 2989, loss = 0.30524979\n",
      "Iteration 2990, loss = 0.30509040\n",
      "Iteration 2991, loss = 0.30493027\n",
      "Iteration 2992, loss = 0.30477019\n",
      "Iteration 2993, loss = 0.30461294\n",
      "Iteration 2994, loss = 0.30445264\n",
      "Iteration 2995, loss = 0.30429554\n",
      "Iteration 2996, loss = 0.30413673\n",
      "Iteration 2997, loss = 0.30397720\n",
      "Iteration 2998, loss = 0.30381969\n",
      "Iteration 2999, loss = 0.30366238\n",
      "Iteration 3000, loss = 0.30350609\n",
      "Iteration 3001, loss = 0.30334781\n",
      "Iteration 3002, loss = 0.30318965\n",
      "Iteration 3003, loss = 0.30303342\n",
      "Iteration 3004, loss = 0.30287561\n",
      "Iteration 3005, loss = 0.30271903\n",
      "Iteration 3006, loss = 0.30256125\n",
      "Iteration 3007, loss = 0.30240544\n",
      "Iteration 3008, loss = 0.30224827\n",
      "Iteration 3009, loss = 0.30209228\n",
      "Iteration 3010, loss = 0.30193668\n",
      "Iteration 3011, loss = 0.30177960\n",
      "Iteration 3012, loss = 0.30162451\n",
      "Iteration 3013, loss = 0.30146761\n",
      "Iteration 3014, loss = 0.30131336\n",
      "Iteration 3015, loss = 0.30115760\n",
      "Iteration 3016, loss = 0.30100326\n",
      "Iteration 3017, loss = 0.30084804\n",
      "Iteration 3018, loss = 0.30069263\n",
      "Iteration 3019, loss = 0.30054265\n",
      "Iteration 3020, loss = 0.30038412\n",
      "Iteration 3021, loss = 0.30023156\n",
      "Iteration 3022, loss = 0.30007582\n",
      "Iteration 3023, loss = 0.29992314\n",
      "Iteration 3024, loss = 0.29976954\n",
      "Iteration 3025, loss = 0.29961559\n",
      "Iteration 3026, loss = 0.29946025\n",
      "Iteration 3027, loss = 0.29931053\n",
      "Iteration 3028, loss = 0.29915556\n",
      "Iteration 3029, loss = 0.29900206\n",
      "Iteration 3030, loss = 0.29884818\n",
      "Iteration 3031, loss = 0.29869493\n",
      "Iteration 3032, loss = 0.29854140\n",
      "Iteration 3033, loss = 0.29838866\n",
      "Iteration 3034, loss = 0.29823675\n",
      "Iteration 3035, loss = 0.29808222\n",
      "Iteration 3036, loss = 0.29792799\n",
      "Iteration 3037, loss = 0.29778058\n",
      "Iteration 3038, loss = 0.29762263\n",
      "Iteration 3039, loss = 0.29747227\n",
      "Iteration 3040, loss = 0.29732013\n",
      "Iteration 3041, loss = 0.29716755\n",
      "Iteration 3042, loss = 0.29701642\n",
      "Iteration 3043, loss = 0.29686443\n",
      "Iteration 3044, loss = 0.29671142\n",
      "Iteration 3045, loss = 0.29656178\n",
      "Iteration 3046, loss = 0.29640914\n",
      "Iteration 3047, loss = 0.29625858\n",
      "Iteration 3048, loss = 0.29610692\n",
      "Iteration 3049, loss = 0.29595885\n",
      "Iteration 3050, loss = 0.29580788\n",
      "Iteration 3051, loss = 0.29565336\n",
      "Iteration 3052, loss = 0.29550500\n",
      "Iteration 3053, loss = 0.29535360\n",
      "Iteration 3054, loss = 0.29520231\n",
      "Iteration 3055, loss = 0.29505207\n",
      "Iteration 3056, loss = 0.29490282\n",
      "Iteration 3057, loss = 0.29475186\n",
      "Iteration 3058, loss = 0.29460236\n",
      "Iteration 3059, loss = 0.29445473\n",
      "Iteration 3060, loss = 0.29430202\n",
      "Iteration 3061, loss = 0.29415392\n",
      "Iteration 3062, loss = 0.29400386\n",
      "Iteration 3063, loss = 0.29385699\n",
      "Iteration 3064, loss = 0.29370680\n",
      "Iteration 3065, loss = 0.29355788\n",
      "Iteration 3066, loss = 0.29341126\n",
      "Iteration 3067, loss = 0.29326195\n",
      "Iteration 3068, loss = 0.29311399\n",
      "Iteration 3069, loss = 0.29296567\n",
      "Iteration 3070, loss = 0.29281874\n",
      "Iteration 3071, loss = 0.29267185\n",
      "Iteration 3072, loss = 0.29252239\n",
      "Iteration 3073, loss = 0.29237407\n",
      "Iteration 3074, loss = 0.29222693\n",
      "Iteration 3075, loss = 0.29207883\n",
      "Iteration 3076, loss = 0.29193224\n",
      "Iteration 3077, loss = 0.29178628\n",
      "Iteration 3078, loss = 0.29163893\n",
      "Iteration 3079, loss = 0.29149103\n",
      "Iteration 3080, loss = 0.29134274\n",
      "Iteration 3081, loss = 0.29119814\n",
      "Iteration 3082, loss = 0.29104821\n",
      "Iteration 3083, loss = 0.29090349\n",
      "Iteration 3084, loss = 0.29075656\n",
      "Iteration 3085, loss = 0.29060928\n",
      "Iteration 3086, loss = 0.29046237\n",
      "Iteration 3087, loss = 0.29031401\n",
      "Iteration 3088, loss = 0.29017272\n",
      "Iteration 3089, loss = 0.29002399\n",
      "Iteration 3090, loss = 0.28987767\n",
      "Iteration 3091, loss = 0.28973348\n",
      "Iteration 3092, loss = 0.28958648\n",
      "Iteration 3093, loss = 0.28944254\n",
      "Iteration 3094, loss = 0.28929430\n",
      "Iteration 3095, loss = 0.28914944\n",
      "Iteration 3096, loss = 0.28900347\n",
      "Iteration 3097, loss = 0.28885848\n",
      "Iteration 3098, loss = 0.28871266\n",
      "Iteration 3099, loss = 0.28856834\n",
      "Iteration 3100, loss = 0.28842342\n",
      "Iteration 3101, loss = 0.28827996\n",
      "Iteration 3102, loss = 0.28813578\n",
      "Iteration 3103, loss = 0.28799047\n",
      "Iteration 3104, loss = 0.28784608\n",
      "Iteration 3105, loss = 0.28770110\n",
      "Iteration 3106, loss = 0.28755917\n",
      "Iteration 3107, loss = 0.28741677\n",
      "Iteration 3108, loss = 0.28727051\n",
      "Iteration 3109, loss = 0.28712630\n",
      "Iteration 3110, loss = 0.28698577\n",
      "Iteration 3111, loss = 0.28684166\n",
      "Iteration 3112, loss = 0.28669949\n",
      "Iteration 3113, loss = 0.28655620\n",
      "Iteration 3114, loss = 0.28641488\n",
      "Iteration 3115, loss = 0.28627237\n",
      "Iteration 3116, loss = 0.28612885\n",
      "Iteration 3117, loss = 0.28598742\n",
      "Iteration 3118, loss = 0.28584711\n",
      "Iteration 3119, loss = 0.28570451\n",
      "Iteration 3120, loss = 0.28556218\n",
      "Iteration 3121, loss = 0.28542176\n",
      "Iteration 3122, loss = 0.28527893\n",
      "Iteration 3123, loss = 0.28513783\n",
      "Iteration 3124, loss = 0.28499509\n",
      "Iteration 3125, loss = 0.28485455\n",
      "Iteration 3126, loss = 0.28471329\n",
      "Iteration 3127, loss = 0.28457257\n",
      "Iteration 3128, loss = 0.28443158\n",
      "Iteration 3129, loss = 0.28429052\n",
      "Iteration 3130, loss = 0.28414884\n",
      "Iteration 3131, loss = 0.28401106\n",
      "Iteration 3132, loss = 0.28386770\n",
      "Iteration 3133, loss = 0.28372807\n",
      "Iteration 3134, loss = 0.28358874\n",
      "Iteration 3135, loss = 0.28344766\n",
      "Iteration 3136, loss = 0.28330670\n",
      "Iteration 3137, loss = 0.28316834\n",
      "Iteration 3138, loss = 0.28302866\n",
      "Iteration 3139, loss = 0.28288871\n",
      "Iteration 3140, loss = 0.28274980\n",
      "Iteration 3141, loss = 0.28261021\n",
      "Iteration 3142, loss = 0.28246846\n",
      "Iteration 3143, loss = 0.28233195\n",
      "Iteration 3144, loss = 0.28218938\n",
      "Iteration 3145, loss = 0.28205197\n",
      "Iteration 3146, loss = 0.28191425\n",
      "Iteration 3147, loss = 0.28177397\n",
      "Iteration 3148, loss = 0.28163521\n",
      "Iteration 3149, loss = 0.28149842\n",
      "Iteration 3150, loss = 0.28136013\n",
      "Iteration 3151, loss = 0.28122021\n",
      "Iteration 3152, loss = 0.28108245\n",
      "Iteration 3153, loss = 0.28094546\n",
      "Iteration 3154, loss = 0.28080859\n",
      "Iteration 3155, loss = 0.28067225\n",
      "Iteration 3156, loss = 0.28053280\n",
      "Iteration 3157, loss = 0.28039642\n",
      "Iteration 3158, loss = 0.28025808\n",
      "Iteration 3159, loss = 0.28012361\n",
      "Iteration 3160, loss = 0.27998384\n",
      "Iteration 3161, loss = 0.27984771\n",
      "Iteration 3162, loss = 0.27970964\n",
      "Iteration 3163, loss = 0.27957408\n",
      "Iteration 3164, loss = 0.27943956\n",
      "Iteration 3165, loss = 0.27930128\n",
      "Iteration 3166, loss = 0.27916453\n",
      "Iteration 3167, loss = 0.27902915\n",
      "Iteration 3168, loss = 0.27889347\n",
      "Iteration 3169, loss = 0.27875858\n",
      "Iteration 3170, loss = 0.27862060\n",
      "Iteration 3171, loss = 0.27848506\n",
      "Iteration 3172, loss = 0.27835009\n",
      "Iteration 3173, loss = 0.27821347\n",
      "Iteration 3174, loss = 0.27807799\n",
      "Iteration 3175, loss = 0.27794268\n",
      "Iteration 3176, loss = 0.27780838\n",
      "Iteration 3177, loss = 0.27767275\n",
      "Iteration 3178, loss = 0.27753647\n",
      "Iteration 3179, loss = 0.27740369\n",
      "Iteration 3180, loss = 0.27726718\n",
      "Iteration 3181, loss = 0.27713163\n",
      "Iteration 3182, loss = 0.27699927\n",
      "Iteration 3183, loss = 0.27686156\n",
      "Iteration 3184, loss = 0.27672877\n",
      "Iteration 3185, loss = 0.27659471\n",
      "Iteration 3186, loss = 0.27645943\n",
      "Iteration 3187, loss = 0.27632567\n",
      "Iteration 3188, loss = 0.27619240\n",
      "Iteration 3189, loss = 0.27605665\n",
      "Iteration 3190, loss = 0.27592289\n",
      "Iteration 3191, loss = 0.27579123\n",
      "Iteration 3192, loss = 0.27565622\n",
      "Iteration 3193, loss = 0.27552332\n",
      "Iteration 3194, loss = 0.27538928\n",
      "Iteration 3195, loss = 0.27525714\n",
      "Iteration 3196, loss = 0.27512459\n",
      "Iteration 3197, loss = 0.27499230\n",
      "Iteration 3198, loss = 0.27485968\n",
      "Iteration 3199, loss = 0.27472454\n",
      "Iteration 3200, loss = 0.27459383\n",
      "Iteration 3201, loss = 0.27446192\n",
      "Iteration 3202, loss = 0.27433002\n",
      "Iteration 3203, loss = 0.27419796\n",
      "Iteration 3204, loss = 0.27406642\n",
      "Iteration 3205, loss = 0.27393328\n",
      "Iteration 3206, loss = 0.27380190\n",
      "Iteration 3207, loss = 0.27367001\n",
      "Iteration 3208, loss = 0.27353842\n",
      "Iteration 3209, loss = 0.27340702\n",
      "Iteration 3210, loss = 0.27327666\n",
      "Iteration 3211, loss = 0.27314504\n",
      "Iteration 3212, loss = 0.27301429\n",
      "Iteration 3213, loss = 0.27288183\n",
      "Iteration 3214, loss = 0.27274940\n",
      "Iteration 3215, loss = 0.27262068\n",
      "Iteration 3216, loss = 0.27248760\n",
      "Iteration 3217, loss = 0.27235913\n",
      "Iteration 3218, loss = 0.27222688\n",
      "Iteration 3219, loss = 0.27209645\n",
      "Iteration 3220, loss = 0.27196667\n",
      "Iteration 3221, loss = 0.27183655\n",
      "Iteration 3222, loss = 0.27170699\n",
      "Iteration 3223, loss = 0.27157755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3224, loss = 0.27144666\n",
      "Iteration 3225, loss = 0.27131588\n",
      "Iteration 3226, loss = 0.27118910\n",
      "Iteration 3227, loss = 0.27105615\n",
      "Iteration 3228, loss = 0.27092879\n",
      "Iteration 3229, loss = 0.27079974\n",
      "Iteration 3230, loss = 0.27066798\n",
      "Iteration 3231, loss = 0.27054057\n",
      "Iteration 3232, loss = 0.27041151\n",
      "Iteration 3233, loss = 0.27028358\n",
      "Iteration 3234, loss = 0.27015375\n",
      "Iteration 3235, loss = 0.27002571\n",
      "Iteration 3236, loss = 0.26989652\n",
      "Iteration 3237, loss = 0.26976992\n",
      "Iteration 3238, loss = 0.26964040\n",
      "Iteration 3239, loss = 0.26951157\n",
      "Iteration 3240, loss = 0.26938378\n",
      "Iteration 3241, loss = 0.26925663\n",
      "Iteration 3242, loss = 0.26912873\n",
      "Iteration 3243, loss = 0.26899897\n",
      "Iteration 3244, loss = 0.26887184\n",
      "Iteration 3245, loss = 0.26874472\n",
      "Iteration 3246, loss = 0.26861675\n",
      "Iteration 3247, loss = 0.26849056\n",
      "Iteration 3248, loss = 0.26836263\n",
      "Iteration 3249, loss = 0.26823537\n",
      "Iteration 3250, loss = 0.26810797\n",
      "Iteration 3251, loss = 0.26798107\n",
      "Iteration 3252, loss = 0.26785528\n",
      "Iteration 3253, loss = 0.26772758\n",
      "Iteration 3254, loss = 0.26760165\n",
      "Iteration 3255, loss = 0.26747538\n",
      "Iteration 3256, loss = 0.26734843\n",
      "Iteration 3257, loss = 0.26722190\n",
      "Iteration 3258, loss = 0.26709649\n",
      "Iteration 3259, loss = 0.26696818\n",
      "Iteration 3260, loss = 0.26684233\n",
      "Iteration 3261, loss = 0.26671772\n",
      "Iteration 3262, loss = 0.26659080\n",
      "Iteration 3263, loss = 0.26646449\n",
      "Iteration 3264, loss = 0.26633931\n",
      "Iteration 3265, loss = 0.26621438\n",
      "Iteration 3266, loss = 0.26608765\n",
      "Iteration 3267, loss = 0.26596204\n",
      "Iteration 3268, loss = 0.26583760\n",
      "Iteration 3269, loss = 0.26571193\n",
      "Iteration 3270, loss = 0.26558566\n",
      "Iteration 3271, loss = 0.26545995\n",
      "Iteration 3272, loss = 0.26533530\n",
      "Iteration 3273, loss = 0.26520921\n",
      "Iteration 3274, loss = 0.26508563\n",
      "Iteration 3275, loss = 0.26496021\n",
      "Iteration 3276, loss = 0.26483529\n",
      "Iteration 3277, loss = 0.26470931\n",
      "Iteration 3278, loss = 0.26458609\n",
      "Iteration 3279, loss = 0.26446221\n",
      "Iteration 3280, loss = 0.26433961\n",
      "Iteration 3281, loss = 0.26421265\n",
      "Iteration 3282, loss = 0.26408854\n",
      "Iteration 3283, loss = 0.26396405\n",
      "Iteration 3284, loss = 0.26384116\n",
      "Iteration 3285, loss = 0.26371913\n",
      "Iteration 3286, loss = 0.26359314\n",
      "Iteration 3287, loss = 0.26347070\n",
      "Iteration 3288, loss = 0.26334693\n",
      "Iteration 3289, loss = 0.26322364\n",
      "Iteration 3290, loss = 0.26310061\n",
      "Iteration 3291, loss = 0.26297841\n",
      "Iteration 3292, loss = 0.26285500\n",
      "Iteration 3293, loss = 0.26273232\n",
      "Iteration 3294, loss = 0.26260998\n",
      "Iteration 3295, loss = 0.26248574\n",
      "Iteration 3296, loss = 0.26236462\n",
      "Iteration 3297, loss = 0.26224121\n",
      "Iteration 3298, loss = 0.26211883\n",
      "Iteration 3299, loss = 0.26199827\n",
      "Iteration 3300, loss = 0.26187624\n",
      "Iteration 3301, loss = 0.26175295\n",
      "Iteration 3302, loss = 0.26163103\n",
      "Iteration 3303, loss = 0.26151088\n",
      "Iteration 3304, loss = 0.26138935\n",
      "Iteration 3305, loss = 0.26126560\n",
      "Iteration 3306, loss = 0.26114511\n",
      "Iteration 3307, loss = 0.26102302\n",
      "Iteration 3308, loss = 0.26090236\n",
      "Iteration 3309, loss = 0.26078101\n",
      "Iteration 3310, loss = 0.26066048\n",
      "Iteration 3311, loss = 0.26054039\n",
      "Iteration 3312, loss = 0.26041966\n",
      "Iteration 3313, loss = 0.26029856\n",
      "Iteration 3314, loss = 0.26017791\n",
      "Iteration 3315, loss = 0.26005713\n",
      "Iteration 3316, loss = 0.25993710\n",
      "Iteration 3317, loss = 0.25981534\n",
      "Iteration 3318, loss = 0.25969557\n",
      "Iteration 3319, loss = 0.25957597\n",
      "Iteration 3320, loss = 0.25945516\n",
      "Iteration 3321, loss = 0.25933374\n",
      "Iteration 3322, loss = 0.25921373\n",
      "Iteration 3323, loss = 0.25909414\n",
      "Iteration 3324, loss = 0.25897436\n",
      "Iteration 3325, loss = 0.25885396\n",
      "Iteration 3326, loss = 0.25873335\n",
      "Iteration 3327, loss = 0.25861397\n",
      "Iteration 3328, loss = 0.25849408\n",
      "Iteration 3329, loss = 0.25837486\n",
      "Iteration 3330, loss = 0.25825461\n",
      "Iteration 3331, loss = 0.25813613\n",
      "Iteration 3332, loss = 0.25801628\n",
      "Iteration 3333, loss = 0.25789704\n",
      "Iteration 3334, loss = 0.25777695\n",
      "Iteration 3335, loss = 0.25765829\n",
      "Iteration 3336, loss = 0.25754138\n",
      "Iteration 3337, loss = 0.25742136\n",
      "Iteration 3338, loss = 0.25730231\n",
      "Iteration 3339, loss = 0.25718523\n",
      "Iteration 3340, loss = 0.25706763\n",
      "Iteration 3341, loss = 0.25694836\n",
      "Iteration 3342, loss = 0.25683041\n",
      "Iteration 3343, loss = 0.25671081\n",
      "Iteration 3344, loss = 0.25659457\n",
      "Iteration 3345, loss = 0.25647793\n",
      "Iteration 3346, loss = 0.25635995\n",
      "Iteration 3347, loss = 0.25624235\n",
      "Iteration 3348, loss = 0.25612307\n",
      "Iteration 3349, loss = 0.25600723\n",
      "Iteration 3350, loss = 0.25588923\n",
      "Iteration 3351, loss = 0.25577219\n",
      "Iteration 3352, loss = 0.25565590\n",
      "Iteration 3353, loss = 0.25553756\n",
      "Iteration 3354, loss = 0.25542173\n",
      "Iteration 3355, loss = 0.25530392\n",
      "Iteration 3356, loss = 0.25518739\n",
      "Iteration 3357, loss = 0.25507046\n",
      "Iteration 3358, loss = 0.25495420\n",
      "Iteration 3359, loss = 0.25483770\n",
      "Iteration 3360, loss = 0.25472144\n",
      "Iteration 3361, loss = 0.25460331\n",
      "Iteration 3362, loss = 0.25448801\n",
      "Iteration 3363, loss = 0.25437162\n",
      "Iteration 3364, loss = 0.25425520\n",
      "Iteration 3365, loss = 0.25413918\n",
      "Iteration 3366, loss = 0.25402158\n",
      "Iteration 3367, loss = 0.25390912\n",
      "Iteration 3368, loss = 0.25379116\n",
      "Iteration 3369, loss = 0.25367701\n",
      "Iteration 3370, loss = 0.25356092\n",
      "Iteration 3371, loss = 0.25344440\n",
      "Iteration 3372, loss = 0.25332960\n",
      "Iteration 3373, loss = 0.25321301\n",
      "Iteration 3374, loss = 0.25309950\n",
      "Iteration 3375, loss = 0.25298333\n",
      "Iteration 3376, loss = 0.25286959\n",
      "Iteration 3377, loss = 0.25275510\n",
      "Iteration 3378, loss = 0.25264016\n",
      "Iteration 3379, loss = 0.25252355\n",
      "Iteration 3380, loss = 0.25241047\n",
      "Iteration 3381, loss = 0.25229617\n",
      "Iteration 3382, loss = 0.25218146\n",
      "Iteration 3383, loss = 0.25206774\n",
      "Iteration 3384, loss = 0.25195366\n",
      "Iteration 3385, loss = 0.25183785\n",
      "Iteration 3386, loss = 0.25172373\n",
      "Iteration 3387, loss = 0.25160871\n",
      "Iteration 3388, loss = 0.25149711\n",
      "Iteration 3389, loss = 0.25138116\n",
      "Iteration 3390, loss = 0.25126871\n",
      "Iteration 3391, loss = 0.25115475\n",
      "Iteration 3392, loss = 0.25104070\n",
      "Iteration 3393, loss = 0.25092803\n",
      "Iteration 3394, loss = 0.25081421\n",
      "Iteration 3395, loss = 0.25070128\n",
      "Iteration 3396, loss = 0.25058633\n",
      "Iteration 3397, loss = 0.25047357\n",
      "Iteration 3398, loss = 0.25035977\n",
      "Iteration 3399, loss = 0.25024723\n",
      "Iteration 3400, loss = 0.25013444\n",
      "Iteration 3401, loss = 0.25002390\n",
      "Iteration 3402, loss = 0.24990831\n",
      "Iteration 3403, loss = 0.24979800\n",
      "Iteration 3404, loss = 0.24968349\n",
      "Iteration 3405, loss = 0.24957194\n",
      "Iteration 3406, loss = 0.24945997\n",
      "Iteration 3407, loss = 0.24934769\n",
      "Iteration 3408, loss = 0.24923717\n",
      "Iteration 3409, loss = 0.24912371\n",
      "Iteration 3410, loss = 0.24901318\n",
      "Iteration 3411, loss = 0.24890184\n",
      "Iteration 3412, loss = 0.24878906\n",
      "Iteration 3413, loss = 0.24867719\n",
      "Iteration 3414, loss = 0.24856544\n",
      "Iteration 3415, loss = 0.24845349\n",
      "Iteration 3416, loss = 0.24834206\n",
      "Iteration 3417, loss = 0.24823035\n",
      "Iteration 3418, loss = 0.24811848\n",
      "Iteration 3419, loss = 0.24800753\n",
      "Iteration 3420, loss = 0.24789622\n",
      "Iteration 3421, loss = 0.24778374\n",
      "Iteration 3422, loss = 0.24767414\n",
      "Iteration 3423, loss = 0.24756363\n",
      "Iteration 3424, loss = 0.24745136\n",
      "Iteration 3425, loss = 0.24734030\n",
      "Iteration 3426, loss = 0.24723238\n",
      "Iteration 3427, loss = 0.24712120\n",
      "Iteration 3428, loss = 0.24700914\n",
      "Iteration 3429, loss = 0.24689947\n",
      "Iteration 3430, loss = 0.24678930\n",
      "Iteration 3431, loss = 0.24667887\n",
      "Iteration 3432, loss = 0.24657014\n",
      "Iteration 3433, loss = 0.24645863\n",
      "Iteration 3434, loss = 0.24634949\n",
      "Iteration 3435, loss = 0.24624085\n",
      "Iteration 3436, loss = 0.24613136\n",
      "Iteration 3437, loss = 0.24602033\n",
      "Iteration 3438, loss = 0.24591091\n",
      "Iteration 3439, loss = 0.24580167\n",
      "Iteration 3440, loss = 0.24569160\n",
      "Iteration 3441, loss = 0.24558323\n",
      "Iteration 3442, loss = 0.24547503\n",
      "Iteration 3443, loss = 0.24536525\n",
      "Iteration 3444, loss = 0.24525653\n",
      "Iteration 3445, loss = 0.24514762\n",
      "Iteration 3446, loss = 0.24503920\n",
      "Iteration 3447, loss = 0.24493106\n",
      "Iteration 3448, loss = 0.24482124\n",
      "Iteration 3449, loss = 0.24471374\n",
      "Iteration 3450, loss = 0.24460513\n",
      "Iteration 3451, loss = 0.24449694\n",
      "Iteration 3452, loss = 0.24438916\n",
      "Iteration 3453, loss = 0.24428099\n",
      "Iteration 3454, loss = 0.24417088\n",
      "Iteration 3455, loss = 0.24406471\n",
      "Iteration 3456, loss = 0.24395620\n",
      "Iteration 3457, loss = 0.24384855\n",
      "Iteration 3458, loss = 0.24374092\n",
      "Iteration 3459, loss = 0.24363282\n",
      "Iteration 3460, loss = 0.24352348\n",
      "Iteration 3461, loss = 0.24341833\n",
      "Iteration 3462, loss = 0.24331005\n",
      "Iteration 3463, loss = 0.24320232\n",
      "Iteration 3464, loss = 0.24309461\n",
      "Iteration 3465, loss = 0.24298633\n",
      "Iteration 3466, loss = 0.24288258\n",
      "Iteration 3467, loss = 0.24277258\n",
      "Iteration 3468, loss = 0.24266671\n",
      "Iteration 3469, loss = 0.24255950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3470, loss = 0.24245236\n",
      "Iteration 3471, loss = 0.24234554\n",
      "Iteration 3472, loss = 0.24223933\n",
      "Iteration 3473, loss = 0.24213094\n",
      "Iteration 3474, loss = 0.24202435\n",
      "Iteration 3475, loss = 0.24191839\n",
      "Iteration 3476, loss = 0.24181225\n",
      "Iteration 3477, loss = 0.24170630\n",
      "Iteration 3478, loss = 0.24159900\n",
      "Iteration 3479, loss = 0.24149352\n",
      "Iteration 3480, loss = 0.24138687\n",
      "Iteration 3481, loss = 0.24128005\n",
      "Iteration 3482, loss = 0.24117452\n",
      "Iteration 3483, loss = 0.24106696\n",
      "Iteration 3484, loss = 0.24096302\n",
      "Iteration 3485, loss = 0.24085647\n",
      "Iteration 3486, loss = 0.24075113\n",
      "Iteration 3487, loss = 0.24064492\n",
      "Iteration 3488, loss = 0.24053983\n",
      "Iteration 3489, loss = 0.24043530\n",
      "Iteration 3490, loss = 0.24032987\n",
      "Iteration 3491, loss = 0.24022417\n",
      "Iteration 3492, loss = 0.24011995\n",
      "Iteration 3493, loss = 0.24001466\n",
      "Iteration 3494, loss = 0.23991057\n",
      "Iteration 3495, loss = 0.23980528\n",
      "Iteration 3496, loss = 0.23970127\n",
      "Iteration 3497, loss = 0.23959569\n",
      "Iteration 3498, loss = 0.23949235\n",
      "Iteration 3499, loss = 0.23938911\n",
      "Iteration 3500, loss = 0.23928328\n",
      "Iteration 3501, loss = 0.23918013\n",
      "Iteration 3502, loss = 0.23907485\n",
      "Iteration 3503, loss = 0.23897257\n",
      "Iteration 3504, loss = 0.23886805\n",
      "Iteration 3505, loss = 0.23876423\n",
      "Iteration 3506, loss = 0.23866135\n",
      "Iteration 3507, loss = 0.23855638\n",
      "Iteration 3508, loss = 0.23845393\n",
      "Iteration 3509, loss = 0.23834888\n",
      "Iteration 3510, loss = 0.23824634\n",
      "Iteration 3511, loss = 0.23814311\n",
      "Iteration 3512, loss = 0.23803908\n",
      "Iteration 3513, loss = 0.23793852\n",
      "Iteration 3514, loss = 0.23783301\n",
      "Iteration 3515, loss = 0.23772965\n",
      "Iteration 3516, loss = 0.23762669\n",
      "Iteration 3517, loss = 0.23752366\n",
      "Iteration 3518, loss = 0.23742136\n",
      "Iteration 3519, loss = 0.23731893\n",
      "Iteration 3520, loss = 0.23721546\n",
      "Iteration 3521, loss = 0.23711389\n",
      "Iteration 3522, loss = 0.23701147\n",
      "Iteration 3523, loss = 0.23690754\n",
      "Iteration 3524, loss = 0.23680602\n",
      "Iteration 3525, loss = 0.23670311\n",
      "Iteration 3526, loss = 0.23660110\n",
      "Iteration 3527, loss = 0.23649992\n",
      "Iteration 3528, loss = 0.23639739\n",
      "Iteration 3529, loss = 0.23629464\n",
      "Iteration 3530, loss = 0.23619350\n",
      "Iteration 3531, loss = 0.23609120\n",
      "Iteration 3532, loss = 0.23598879\n",
      "Iteration 3533, loss = 0.23588792\n",
      "Iteration 3534, loss = 0.23578518\n",
      "Iteration 3535, loss = 0.23568434\n",
      "Iteration 3536, loss = 0.23558301\n",
      "Iteration 3537, loss = 0.23548094\n",
      "Iteration 3538, loss = 0.23537933\n",
      "Iteration 3539, loss = 0.23527784\n",
      "Iteration 3540, loss = 0.23517582\n",
      "Iteration 3541, loss = 0.23507543\n",
      "Iteration 3542, loss = 0.23497420\n",
      "Iteration 3543, loss = 0.23487226\n",
      "Iteration 3544, loss = 0.23477229\n",
      "Iteration 3545, loss = 0.23467197\n",
      "Iteration 3546, loss = 0.23457036\n",
      "Iteration 3547, loss = 0.23446854\n",
      "Iteration 3548, loss = 0.23436784\n",
      "Iteration 3549, loss = 0.23426854\n",
      "Iteration 3550, loss = 0.23416712\n",
      "Iteration 3551, loss = 0.23406731\n",
      "Iteration 3552, loss = 0.23396589\n",
      "Iteration 3553, loss = 0.23386631\n",
      "Iteration 3554, loss = 0.23376588\n",
      "Iteration 3555, loss = 0.23366691\n",
      "Iteration 3556, loss = 0.23356589\n",
      "Iteration 3557, loss = 0.23346599\n",
      "Iteration 3558, loss = 0.23336602\n",
      "Iteration 3559, loss = 0.23326527\n",
      "Iteration 3560, loss = 0.23316643\n",
      "Iteration 3561, loss = 0.23306662\n",
      "Iteration 3562, loss = 0.23296636\n",
      "Iteration 3563, loss = 0.23286716\n",
      "Iteration 3564, loss = 0.23276785\n",
      "Iteration 3565, loss = 0.23266974\n",
      "Iteration 3566, loss = 0.23257022\n",
      "Iteration 3567, loss = 0.23247114\n",
      "Iteration 3568, loss = 0.23237163\n",
      "Iteration 3569, loss = 0.23227368\n",
      "Iteration 3570, loss = 0.23217374\n",
      "Iteration 3571, loss = 0.23207501\n",
      "Iteration 3572, loss = 0.23197555\n",
      "Iteration 3573, loss = 0.23187749\n",
      "Iteration 3574, loss = 0.23177851\n",
      "Iteration 3575, loss = 0.23168019\n",
      "Iteration 3576, loss = 0.23158163\n",
      "Iteration 3577, loss = 0.23148184\n",
      "Iteration 3578, loss = 0.23138341\n",
      "Iteration 3579, loss = 0.23128496\n",
      "Iteration 3580, loss = 0.23118574\n",
      "Iteration 3581, loss = 0.23108808\n",
      "Iteration 3582, loss = 0.23098891\n",
      "Iteration 3583, loss = 0.23089205\n",
      "Iteration 3584, loss = 0.23079224\n",
      "Iteration 3585, loss = 0.23069531\n",
      "Iteration 3586, loss = 0.23059835\n",
      "Iteration 3587, loss = 0.23049877\n",
      "Iteration 3588, loss = 0.23040205\n",
      "Iteration 3589, loss = 0.23030365\n",
      "Iteration 3590, loss = 0.23020680\n",
      "Iteration 3591, loss = 0.23010872\n",
      "Iteration 3592, loss = 0.23001184\n",
      "Iteration 3593, loss = 0.22991438\n",
      "Iteration 3594, loss = 0.22981572\n",
      "Iteration 3595, loss = 0.22971803\n",
      "Iteration 3596, loss = 0.22962158\n",
      "Iteration 3597, loss = 0.22952513\n",
      "Iteration 3598, loss = 0.22942697\n",
      "Iteration 3599, loss = 0.22933023\n",
      "Iteration 3600, loss = 0.22923305\n",
      "Iteration 3601, loss = 0.22913575\n",
      "Iteration 3602, loss = 0.22903904\n",
      "Iteration 3603, loss = 0.22894345\n",
      "Iteration 3604, loss = 0.22884641\n",
      "Iteration 3605, loss = 0.22875037\n",
      "Iteration 3606, loss = 0.22865423\n",
      "Iteration 3607, loss = 0.22855716\n",
      "Iteration 3608, loss = 0.22846082\n",
      "Iteration 3609, loss = 0.22836484\n",
      "Iteration 3610, loss = 0.22826861\n",
      "Iteration 3611, loss = 0.22817406\n",
      "Iteration 3612, loss = 0.22807708\n",
      "Iteration 3613, loss = 0.22798138\n",
      "Iteration 3614, loss = 0.22788693\n",
      "Iteration 3615, loss = 0.22778978\n",
      "Iteration 3616, loss = 0.22769468\n",
      "Iteration 3617, loss = 0.22759928\n",
      "Iteration 3618, loss = 0.22750398\n",
      "Iteration 3619, loss = 0.22740754\n",
      "Iteration 3620, loss = 0.22731227\n",
      "Iteration 3621, loss = 0.22721745\n",
      "Iteration 3622, loss = 0.22712260\n",
      "Iteration 3623, loss = 0.22702702\n",
      "Iteration 3624, loss = 0.22693154\n",
      "Iteration 3625, loss = 0.22683723\n",
      "Iteration 3626, loss = 0.22674212\n",
      "Iteration 3627, loss = 0.22664634\n",
      "Iteration 3628, loss = 0.22655164\n",
      "Iteration 3629, loss = 0.22645721\n",
      "Iteration 3630, loss = 0.22636393\n",
      "Iteration 3631, loss = 0.22626706\n",
      "Iteration 3632, loss = 0.22617194\n",
      "Iteration 3633, loss = 0.22607854\n",
      "Iteration 3634, loss = 0.22598395\n",
      "Iteration 3635, loss = 0.22588870\n",
      "Iteration 3636, loss = 0.22579543\n",
      "Iteration 3637, loss = 0.22569968\n",
      "Iteration 3638, loss = 0.22560665\n",
      "Iteration 3639, loss = 0.22551177\n",
      "Iteration 3640, loss = 0.22541865\n",
      "Iteration 3641, loss = 0.22532335\n",
      "Iteration 3642, loss = 0.22523040\n",
      "Iteration 3643, loss = 0.22513663\n",
      "Iteration 3644, loss = 0.22504176\n",
      "Iteration 3645, loss = 0.22494748\n",
      "Iteration 3646, loss = 0.22485416\n",
      "Iteration 3647, loss = 0.22476164\n",
      "Iteration 3648, loss = 0.22466817\n",
      "Iteration 3649, loss = 0.22457340\n",
      "Iteration 3650, loss = 0.22448036\n",
      "Iteration 3651, loss = 0.22438796\n",
      "Iteration 3652, loss = 0.22429281\n",
      "Iteration 3653, loss = 0.22420134\n",
      "Iteration 3654, loss = 0.22410874\n",
      "Iteration 3655, loss = 0.22401375\n",
      "Iteration 3656, loss = 0.22392178\n",
      "Iteration 3657, loss = 0.22382978\n",
      "Iteration 3658, loss = 0.22373553\n",
      "Iteration 3659, loss = 0.22364332\n",
      "Iteration 3660, loss = 0.22355114\n",
      "Iteration 3661, loss = 0.22345850\n",
      "Iteration 3662, loss = 0.22336587\n",
      "Iteration 3663, loss = 0.22327408\n",
      "Iteration 3664, loss = 0.22318256\n",
      "Iteration 3665, loss = 0.22308926\n",
      "Iteration 3666, loss = 0.22299894\n",
      "Iteration 3667, loss = 0.22290502\n",
      "Iteration 3668, loss = 0.22281347\n",
      "Iteration 3669, loss = 0.22272229\n",
      "Iteration 3670, loss = 0.22263068\n",
      "Iteration 3671, loss = 0.22253849\n",
      "Iteration 3672, loss = 0.22244671\n",
      "Iteration 3673, loss = 0.22235503\n",
      "Iteration 3674, loss = 0.22226415\n",
      "Iteration 3675, loss = 0.22217126\n",
      "Iteration 3676, loss = 0.22208055\n",
      "Iteration 3677, loss = 0.22199042\n",
      "Iteration 3678, loss = 0.22189839\n",
      "Iteration 3679, loss = 0.22180696\n",
      "Iteration 3680, loss = 0.22171609\n",
      "Iteration 3681, loss = 0.22162411\n",
      "Iteration 3682, loss = 0.22153470\n",
      "Iteration 3683, loss = 0.22144392\n",
      "Iteration 3684, loss = 0.22135316\n",
      "Iteration 3685, loss = 0.22126160\n",
      "Iteration 3686, loss = 0.22117037\n",
      "Iteration 3687, loss = 0.22108039\n",
      "Iteration 3688, loss = 0.22099007\n",
      "Iteration 3689, loss = 0.22089849\n",
      "Iteration 3690, loss = 0.22080912\n",
      "Iteration 3691, loss = 0.22071910\n",
      "Iteration 3692, loss = 0.22062883\n",
      "Iteration 3693, loss = 0.22053744\n",
      "Iteration 3694, loss = 0.22044732\n",
      "Iteration 3695, loss = 0.22035826\n",
      "Iteration 3696, loss = 0.22026813\n",
      "Iteration 3697, loss = 0.22017739\n",
      "Iteration 3698, loss = 0.22008801\n",
      "Iteration 3699, loss = 0.21999765\n",
      "Iteration 3700, loss = 0.21990808\n",
      "Iteration 3701, loss = 0.21981845\n",
      "Iteration 3702, loss = 0.21972805\n",
      "Iteration 3703, loss = 0.21963981\n",
      "Iteration 3704, loss = 0.21954955\n",
      "Iteration 3705, loss = 0.21946063\n",
      "Iteration 3706, loss = 0.21937047\n",
      "Iteration 3707, loss = 0.21928166\n",
      "Iteration 3708, loss = 0.21919228\n",
      "Iteration 3709, loss = 0.21910343\n",
      "Iteration 3710, loss = 0.21901216\n",
      "Iteration 3711, loss = 0.21892404\n",
      "Iteration 3712, loss = 0.21883507\n",
      "Iteration 3713, loss = 0.21874485\n",
      "Iteration 3714, loss = 0.21865650\n",
      "Iteration 3715, loss = 0.21856761\n",
      "Iteration 3716, loss = 0.21848010\n",
      "Iteration 3717, loss = 0.21838998\n",
      "Iteration 3718, loss = 0.21830233\n",
      "Iteration 3719, loss = 0.21821305\n",
      "Iteration 3720, loss = 0.21812597\n",
      "Iteration 3721, loss = 0.21803700\n",
      "Iteration 3722, loss = 0.21794880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3723, loss = 0.21785967\n",
      "Iteration 3724, loss = 0.21777159\n",
      "Iteration 3725, loss = 0.21768277\n",
      "Iteration 3726, loss = 0.21759628\n",
      "Iteration 3727, loss = 0.21750814\n",
      "Iteration 3728, loss = 0.21741921\n",
      "Iteration 3729, loss = 0.21733161\n",
      "Iteration 3730, loss = 0.21724344\n",
      "Iteration 3731, loss = 0.21715494\n",
      "Iteration 3732, loss = 0.21706666\n",
      "Iteration 3733, loss = 0.21697914\n",
      "Iteration 3734, loss = 0.21689055\n",
      "Iteration 3735, loss = 0.21680226\n",
      "Iteration 3736, loss = 0.21671604\n",
      "Iteration 3737, loss = 0.21662643\n",
      "Iteration 3738, loss = 0.21654140\n",
      "Iteration 3739, loss = 0.21645152\n",
      "Iteration 3740, loss = 0.21636472\n",
      "Iteration 3741, loss = 0.21627699\n",
      "Iteration 3742, loss = 0.21619018\n",
      "Iteration 3743, loss = 0.21610178\n",
      "Iteration 3744, loss = 0.21601620\n",
      "Iteration 3745, loss = 0.21592812\n",
      "Iteration 3746, loss = 0.21584178\n",
      "Iteration 3747, loss = 0.21575538\n",
      "Iteration 3748, loss = 0.21566825\n",
      "Iteration 3749, loss = 0.21558169\n",
      "Iteration 3750, loss = 0.21549458\n",
      "Iteration 3751, loss = 0.21540748\n",
      "Iteration 3752, loss = 0.21532208\n",
      "Iteration 3753, loss = 0.21523530\n",
      "Iteration 3754, loss = 0.21514884\n",
      "Iteration 3755, loss = 0.21506217\n",
      "Iteration 3756, loss = 0.21497648\n",
      "Iteration 3757, loss = 0.21488885\n",
      "Iteration 3758, loss = 0.21480443\n",
      "Iteration 3759, loss = 0.21471759\n",
      "Iteration 3760, loss = 0.21463226\n",
      "Iteration 3761, loss = 0.21454724\n",
      "Iteration 3762, loss = 0.21446005\n",
      "Iteration 3763, loss = 0.21437368\n",
      "Iteration 3764, loss = 0.21428849\n",
      "Iteration 3765, loss = 0.21420319\n",
      "Iteration 3766, loss = 0.21411672\n",
      "Iteration 3767, loss = 0.21403150\n",
      "Iteration 3768, loss = 0.21394528\n",
      "Iteration 3769, loss = 0.21385954\n",
      "Iteration 3770, loss = 0.21377491\n",
      "Iteration 3771, loss = 0.21368948\n",
      "Iteration 3772, loss = 0.21360438\n",
      "Iteration 3773, loss = 0.21351849\n",
      "Iteration 3774, loss = 0.21343248\n",
      "Iteration 3775, loss = 0.21334719\n",
      "Iteration 3776, loss = 0.21326236\n",
      "Iteration 3777, loss = 0.21317693\n",
      "Iteration 3778, loss = 0.21309197\n",
      "Iteration 3779, loss = 0.21300698\n",
      "Iteration 3780, loss = 0.21292284\n",
      "Iteration 3781, loss = 0.21283818\n",
      "Iteration 3782, loss = 0.21275257\n",
      "Iteration 3783, loss = 0.21266738\n",
      "Iteration 3784, loss = 0.21258336\n",
      "Iteration 3785, loss = 0.21249934\n",
      "Iteration 3786, loss = 0.21241494\n",
      "Iteration 3787, loss = 0.21233031\n",
      "Iteration 3788, loss = 0.21224533\n",
      "Iteration 3789, loss = 0.21216104\n",
      "Iteration 3790, loss = 0.21207696\n",
      "Iteration 3791, loss = 0.21199293\n",
      "Iteration 3792, loss = 0.21190871\n",
      "Iteration 3793, loss = 0.21182450\n",
      "Iteration 3794, loss = 0.21174081\n",
      "Iteration 3795, loss = 0.21165592\n",
      "Iteration 3796, loss = 0.21157380\n",
      "Iteration 3797, loss = 0.21148892\n",
      "Iteration 3798, loss = 0.21140516\n",
      "Iteration 3799, loss = 0.21132255\n",
      "Iteration 3800, loss = 0.21123838\n",
      "Iteration 3801, loss = 0.21115429\n",
      "Iteration 3802, loss = 0.21107202\n",
      "Iteration 3803, loss = 0.21098662\n",
      "Iteration 3804, loss = 0.21090360\n",
      "Iteration 3805, loss = 0.21082162\n",
      "Iteration 3806, loss = 0.21073757\n",
      "Iteration 3807, loss = 0.21065301\n",
      "Iteration 3808, loss = 0.21057060\n",
      "Iteration 3809, loss = 0.21048773\n",
      "Iteration 3810, loss = 0.21040457\n",
      "Iteration 3811, loss = 0.21032047\n",
      "Iteration 3812, loss = 0.21023764\n",
      "Iteration 3813, loss = 0.21015423\n",
      "Iteration 3814, loss = 0.21007223\n",
      "Iteration 3815, loss = 0.20998858\n",
      "Iteration 3816, loss = 0.20990553\n",
      "Iteration 3817, loss = 0.20982295\n",
      "Iteration 3818, loss = 0.20974022\n",
      "Iteration 3819, loss = 0.20965743\n",
      "Iteration 3820, loss = 0.20957416\n",
      "Iteration 3821, loss = 0.20949312\n",
      "Iteration 3822, loss = 0.20940994\n",
      "Iteration 3823, loss = 0.20932669\n",
      "Iteration 3824, loss = 0.20924390\n",
      "Iteration 3825, loss = 0.20916222\n",
      "Iteration 3826, loss = 0.20907974\n",
      "Iteration 3827, loss = 0.20899749\n",
      "Iteration 3828, loss = 0.20891684\n",
      "Iteration 3829, loss = 0.20883411\n",
      "Iteration 3830, loss = 0.20875285\n",
      "Iteration 3831, loss = 0.20866970\n",
      "Iteration 3832, loss = 0.20858861\n",
      "Iteration 3833, loss = 0.20850724\n",
      "Iteration 3834, loss = 0.20842554\n",
      "Iteration 3835, loss = 0.20834394\n",
      "Iteration 3836, loss = 0.20826378\n",
      "Iteration 3837, loss = 0.20818050\n",
      "Iteration 3838, loss = 0.20809988\n",
      "Iteration 3839, loss = 0.20801742\n",
      "Iteration 3840, loss = 0.20793666\n",
      "Iteration 3841, loss = 0.20785529\n",
      "Iteration 3842, loss = 0.20777442\n",
      "Iteration 3843, loss = 0.20769238\n",
      "Iteration 3844, loss = 0.20761140\n",
      "Iteration 3845, loss = 0.20752998\n",
      "Iteration 3846, loss = 0.20744911\n",
      "Iteration 3847, loss = 0.20736891\n",
      "Iteration 3848, loss = 0.20728735\n",
      "Iteration 3849, loss = 0.20720706\n",
      "Iteration 3850, loss = 0.20712519\n",
      "Iteration 3851, loss = 0.20704489\n",
      "Iteration 3852, loss = 0.20696364\n",
      "Iteration 3853, loss = 0.20688431\n",
      "Iteration 3854, loss = 0.20680408\n",
      "Iteration 3855, loss = 0.20672258\n",
      "Iteration 3856, loss = 0.20664237\n",
      "Iteration 3857, loss = 0.20656211\n",
      "Iteration 3858, loss = 0.20648205\n",
      "Iteration 3859, loss = 0.20640205\n",
      "Iteration 3860, loss = 0.20632178\n",
      "Iteration 3861, loss = 0.20624202\n",
      "Iteration 3862, loss = 0.20616218\n",
      "Iteration 3863, loss = 0.20608228\n",
      "Iteration 3864, loss = 0.20600221\n",
      "Iteration 3865, loss = 0.20592199\n",
      "Iteration 3866, loss = 0.20584255\n",
      "Iteration 3867, loss = 0.20576279\n",
      "Iteration 3868, loss = 0.20568380\n",
      "Iteration 3869, loss = 0.20560396\n",
      "Iteration 3870, loss = 0.20552527\n",
      "Iteration 3871, loss = 0.20544567\n",
      "Iteration 3872, loss = 0.20536555\n",
      "Iteration 3873, loss = 0.20528585\n",
      "Iteration 3874, loss = 0.20520787\n",
      "Iteration 3875, loss = 0.20512842\n",
      "Iteration 3876, loss = 0.20504882\n",
      "Iteration 3877, loss = 0.20496943\n",
      "Iteration 3878, loss = 0.20489057\n",
      "Iteration 3879, loss = 0.20481092\n",
      "Iteration 3880, loss = 0.20473318\n",
      "Iteration 3881, loss = 0.20465369\n",
      "Iteration 3882, loss = 0.20457519\n",
      "Iteration 3883, loss = 0.20449581\n",
      "Iteration 3884, loss = 0.20441799\n",
      "Iteration 3885, loss = 0.20433837\n",
      "Iteration 3886, loss = 0.20426020\n",
      "Iteration 3887, loss = 0.20418096\n",
      "Iteration 3888, loss = 0.20410156\n",
      "Iteration 3889, loss = 0.20402405\n",
      "Iteration 3890, loss = 0.20394575\n",
      "Iteration 3891, loss = 0.20386710\n",
      "Iteration 3892, loss = 0.20378899\n",
      "Iteration 3893, loss = 0.20370987\n",
      "Iteration 3894, loss = 0.20363194\n",
      "Iteration 3895, loss = 0.20355358\n",
      "Iteration 3896, loss = 0.20347482\n",
      "Iteration 3897, loss = 0.20339677\n",
      "Iteration 3898, loss = 0.20331968\n",
      "Iteration 3899, loss = 0.20323986\n",
      "Iteration 3900, loss = 0.20316376\n",
      "Iteration 3901, loss = 0.20308449\n",
      "Iteration 3902, loss = 0.20300679\n",
      "Iteration 3903, loss = 0.20292922\n",
      "Iteration 3904, loss = 0.20285097\n",
      "Iteration 3905, loss = 0.20277353\n",
      "Iteration 3906, loss = 0.20269544\n",
      "Iteration 3907, loss = 0.20261739\n",
      "Iteration 3908, loss = 0.20254010\n",
      "Iteration 3909, loss = 0.20246205\n",
      "Iteration 3910, loss = 0.20238469\n",
      "Iteration 3911, loss = 0.20230705\n",
      "Iteration 3912, loss = 0.20222896\n",
      "Iteration 3913, loss = 0.20215176\n",
      "Iteration 3914, loss = 0.20207396\n",
      "Iteration 3915, loss = 0.20199780\n",
      "Iteration 3916, loss = 0.20191963\n",
      "Iteration 3917, loss = 0.20184278\n",
      "Iteration 3918, loss = 0.20176530\n",
      "Iteration 3919, loss = 0.20168780\n",
      "Iteration 3920, loss = 0.20161066\n",
      "Iteration 3921, loss = 0.20153276\n",
      "Iteration 3922, loss = 0.20145697\n",
      "Iteration 3923, loss = 0.20137860\n",
      "Iteration 3924, loss = 0.20130175\n",
      "Iteration 3925, loss = 0.20122472\n",
      "Iteration 3926, loss = 0.20114791\n",
      "Iteration 3927, loss = 0.20107208\n",
      "Iteration 3928, loss = 0.20099423\n",
      "Iteration 3929, loss = 0.20091720\n",
      "Iteration 3930, loss = 0.20084002\n",
      "Iteration 3931, loss = 0.20076504\n",
      "Iteration 3932, loss = 0.20068757\n",
      "Iteration 3933, loss = 0.20061030\n",
      "Iteration 3934, loss = 0.20053469\n",
      "Iteration 3935, loss = 0.20045820\n",
      "Iteration 3936, loss = 0.20038318\n",
      "Iteration 3937, loss = 0.20030632\n",
      "Iteration 3938, loss = 0.20022984\n",
      "Iteration 3939, loss = 0.20015434\n",
      "Iteration 3940, loss = 0.20007765\n",
      "Iteration 3941, loss = 0.20000146\n",
      "Iteration 3942, loss = 0.19992560\n",
      "Iteration 3943, loss = 0.19985040\n",
      "Iteration 3944, loss = 0.19977427\n",
      "Iteration 3945, loss = 0.19969825\n",
      "Iteration 3946, loss = 0.19962381\n",
      "Iteration 3947, loss = 0.19954727\n",
      "Iteration 3948, loss = 0.19947238\n",
      "Iteration 3949, loss = 0.19939657\n",
      "Iteration 3950, loss = 0.19932124\n",
      "Iteration 3951, loss = 0.19924671\n",
      "Iteration 3952, loss = 0.19917125\n",
      "Iteration 3953, loss = 0.19909485\n",
      "Iteration 3954, loss = 0.19901950\n",
      "Iteration 3955, loss = 0.19894458\n",
      "Iteration 3956, loss = 0.19886995\n",
      "Iteration 3957, loss = 0.19879482\n",
      "Iteration 3958, loss = 0.19871987\n",
      "Iteration 3959, loss = 0.19864429\n",
      "Iteration 3960, loss = 0.19856924\n",
      "Iteration 3961, loss = 0.19849464\n",
      "Iteration 3962, loss = 0.19841912\n",
      "Iteration 3963, loss = 0.19834463\n",
      "Iteration 3964, loss = 0.19827060\n",
      "Iteration 3965, loss = 0.19819590\n",
      "Iteration 3966, loss = 0.19812046\n",
      "Iteration 3967, loss = 0.19804539\n",
      "Iteration 3968, loss = 0.19797234\n",
      "Iteration 3969, loss = 0.19789588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3970, loss = 0.19782154\n",
      "Iteration 3971, loss = 0.19774743\n",
      "Iteration 3972, loss = 0.19767315\n",
      "Iteration 3973, loss = 0.19759936\n",
      "Iteration 3974, loss = 0.19752389\n",
      "Iteration 3975, loss = 0.19744945\n",
      "Iteration 3976, loss = 0.19737581\n",
      "Iteration 3977, loss = 0.19730138\n",
      "Iteration 3978, loss = 0.19722807\n",
      "Iteration 3979, loss = 0.19715300\n",
      "Iteration 3980, loss = 0.19707974\n",
      "Iteration 3981, loss = 0.19700512\n",
      "Iteration 3982, loss = 0.19693293\n",
      "Iteration 3983, loss = 0.19685812\n",
      "Iteration 3984, loss = 0.19678388\n",
      "Iteration 3985, loss = 0.19671085\n",
      "Iteration 3986, loss = 0.19663674\n",
      "Iteration 3987, loss = 0.19656312\n",
      "Iteration 3988, loss = 0.19648980\n",
      "Iteration 3989, loss = 0.19641637\n",
      "Iteration 3990, loss = 0.19634259\n",
      "Iteration 3991, loss = 0.19626916\n",
      "Iteration 3992, loss = 0.19619550\n",
      "Iteration 3993, loss = 0.19612215\n",
      "Iteration 3994, loss = 0.19604930\n",
      "Iteration 3995, loss = 0.19597560\n",
      "Iteration 3996, loss = 0.19590178\n",
      "Iteration 3997, loss = 0.19582800\n",
      "Iteration 3998, loss = 0.19575507\n",
      "Iteration 3999, loss = 0.19568169\n",
      "Iteration 4000, loss = 0.19560837\n",
      "Iteration 4001, loss = 0.19553510\n",
      "Iteration 4002, loss = 0.19546191\n",
      "Iteration 4003, loss = 0.19538822\n",
      "Iteration 4004, loss = 0.19531483\n",
      "Iteration 4005, loss = 0.19524214\n",
      "Iteration 4006, loss = 0.19516871\n",
      "Iteration 4007, loss = 0.19509682\n",
      "Iteration 4008, loss = 0.19502376\n",
      "Iteration 4009, loss = 0.19495023\n",
      "Iteration 4010, loss = 0.19487818\n",
      "Iteration 4011, loss = 0.19480511\n",
      "Iteration 4012, loss = 0.19473220\n",
      "Iteration 4013, loss = 0.19465932\n",
      "Iteration 4014, loss = 0.19458733\n",
      "Iteration 4015, loss = 0.19451531\n",
      "Iteration 4016, loss = 0.19444247\n",
      "Iteration 4017, loss = 0.19437011\n",
      "Iteration 4018, loss = 0.19429755\n",
      "Iteration 4019, loss = 0.19422540\n",
      "Iteration 4020, loss = 0.19415286\n",
      "Iteration 4021, loss = 0.19408005\n",
      "Iteration 4022, loss = 0.19400811\n",
      "Iteration 4023, loss = 0.19393618\n",
      "Iteration 4024, loss = 0.19386423\n",
      "Iteration 4025, loss = 0.19379166\n",
      "Iteration 4026, loss = 0.19372060\n",
      "Iteration 4027, loss = 0.19364823\n",
      "Iteration 4028, loss = 0.19357687\n",
      "Iteration 4029, loss = 0.19350466\n",
      "Iteration 4030, loss = 0.19343312\n",
      "Iteration 4031, loss = 0.19336189\n",
      "Iteration 4032, loss = 0.19329030\n",
      "Iteration 4033, loss = 0.19321847\n",
      "Iteration 4034, loss = 0.19314740\n",
      "Iteration 4035, loss = 0.19307544\n",
      "Iteration 4036, loss = 0.19300463\n",
      "Iteration 4037, loss = 0.19293324\n",
      "Iteration 4038, loss = 0.19286247\n",
      "Iteration 4039, loss = 0.19279097\n",
      "Iteration 4040, loss = 0.19271887\n",
      "Iteration 4041, loss = 0.19264771\n",
      "Iteration 4042, loss = 0.19257738\n",
      "Iteration 4043, loss = 0.19250496\n",
      "Iteration 4044, loss = 0.19243424\n",
      "Iteration 4045, loss = 0.19236308\n",
      "Iteration 4046, loss = 0.19229208\n",
      "Iteration 4047, loss = 0.19222082\n",
      "Iteration 4048, loss = 0.19214996\n",
      "Iteration 4049, loss = 0.19207835\n",
      "Iteration 4050, loss = 0.19200823\n",
      "Iteration 4051, loss = 0.19193718\n",
      "Iteration 4052, loss = 0.19186680\n",
      "Iteration 4053, loss = 0.19179631\n",
      "Iteration 4054, loss = 0.19172523\n",
      "Iteration 4055, loss = 0.19165547\n",
      "Iteration 4056, loss = 0.19158478\n",
      "Iteration 4057, loss = 0.19151401\n",
      "Iteration 4058, loss = 0.19144449\n",
      "Iteration 4059, loss = 0.19137281\n",
      "Iteration 4060, loss = 0.19130358\n",
      "Iteration 4061, loss = 0.19123338\n",
      "Iteration 4062, loss = 0.19116268\n",
      "Iteration 4063, loss = 0.19109293\n",
      "Iteration 4064, loss = 0.19102328\n",
      "Iteration 4065, loss = 0.19095410\n",
      "Iteration 4066, loss = 0.19088325\n",
      "Iteration 4067, loss = 0.19081344\n",
      "Iteration 4068, loss = 0.19074395\n",
      "Iteration 4069, loss = 0.19067421\n",
      "Iteration 4070, loss = 0.19060386\n",
      "Iteration 4071, loss = 0.19053544\n",
      "Iteration 4072, loss = 0.19046445\n",
      "Iteration 4073, loss = 0.19039560\n",
      "Iteration 4074, loss = 0.19032588\n",
      "Iteration 4075, loss = 0.19025687\n",
      "Iteration 4076, loss = 0.19018696\n",
      "Iteration 4077, loss = 0.19011715\n",
      "Iteration 4078, loss = 0.19004950\n",
      "Iteration 4079, loss = 0.18997984\n",
      "Iteration 4080, loss = 0.18991045\n",
      "Iteration 4081, loss = 0.18984115\n",
      "Iteration 4082, loss = 0.18977220\n",
      "Iteration 4083, loss = 0.18970398\n",
      "Iteration 4084, loss = 0.18963377\n",
      "Iteration 4085, loss = 0.18956628\n",
      "Iteration 4086, loss = 0.18949622\n",
      "Iteration 4087, loss = 0.18942796\n",
      "Iteration 4088, loss = 0.18935858\n",
      "Iteration 4089, loss = 0.18929047\n",
      "Iteration 4090, loss = 0.18922121\n",
      "Iteration 4091, loss = 0.18915235\n",
      "Iteration 4092, loss = 0.18908373\n",
      "Iteration 4093, loss = 0.18901473\n",
      "Iteration 4094, loss = 0.18894566\n",
      "Iteration 4095, loss = 0.18887675\n",
      "Iteration 4096, loss = 0.18880855\n",
      "Iteration 4097, loss = 0.18873927\n",
      "Iteration 4098, loss = 0.18867034\n",
      "Iteration 4099, loss = 0.18860150\n",
      "Iteration 4100, loss = 0.18853394\n",
      "Iteration 4101, loss = 0.18846605\n",
      "Iteration 4102, loss = 0.18839626\n",
      "Iteration 4103, loss = 0.18832818\n",
      "Iteration 4104, loss = 0.18826007\n",
      "Iteration 4105, loss = 0.18819239\n",
      "Iteration 4106, loss = 0.18812454\n",
      "Iteration 4107, loss = 0.18805527\n",
      "Iteration 4108, loss = 0.18798767\n",
      "Iteration 4109, loss = 0.18791951\n",
      "Iteration 4110, loss = 0.18785216\n",
      "Iteration 4111, loss = 0.18778393\n",
      "Iteration 4112, loss = 0.18771646\n",
      "Iteration 4113, loss = 0.18764782\n",
      "Iteration 4114, loss = 0.18758014\n",
      "Iteration 4115, loss = 0.18751256\n",
      "Iteration 4116, loss = 0.18744411\n",
      "Iteration 4117, loss = 0.18737624\n",
      "Iteration 4118, loss = 0.18730928\n",
      "Iteration 4119, loss = 0.18724102\n",
      "Iteration 4120, loss = 0.18717309\n",
      "Iteration 4121, loss = 0.18710526\n",
      "Iteration 4122, loss = 0.18703751\n",
      "Iteration 4123, loss = 0.18696947\n",
      "Iteration 4124, loss = 0.18690246\n",
      "Iteration 4125, loss = 0.18683547\n",
      "Iteration 4126, loss = 0.18676729\n",
      "Iteration 4127, loss = 0.18669953\n",
      "Iteration 4128, loss = 0.18663221\n",
      "Iteration 4129, loss = 0.18656521\n",
      "Iteration 4130, loss = 0.18649716\n",
      "Iteration 4131, loss = 0.18643020\n",
      "Iteration 4132, loss = 0.18636227\n",
      "Iteration 4133, loss = 0.18629544\n",
      "Iteration 4134, loss = 0.18622884\n",
      "Iteration 4135, loss = 0.18616141\n",
      "Iteration 4136, loss = 0.18609478\n",
      "Iteration 4137, loss = 0.18602803\n",
      "Iteration 4138, loss = 0.18596054\n",
      "Iteration 4139, loss = 0.18589476\n",
      "Iteration 4140, loss = 0.18582704\n",
      "Iteration 4141, loss = 0.18576071\n",
      "Iteration 4142, loss = 0.18569326\n",
      "Iteration 4143, loss = 0.18562671\n",
      "Iteration 4144, loss = 0.18556002\n",
      "Iteration 4145, loss = 0.18549299\n",
      "Iteration 4146, loss = 0.18542665\n",
      "Iteration 4147, loss = 0.18536056\n",
      "Iteration 4148, loss = 0.18529415\n",
      "Iteration 4149, loss = 0.18522808\n",
      "Iteration 4150, loss = 0.18516195\n",
      "Iteration 4151, loss = 0.18509564\n",
      "Iteration 4152, loss = 0.18502868\n",
      "Iteration 4153, loss = 0.18496340\n",
      "Iteration 4154, loss = 0.18489702\n",
      "Iteration 4155, loss = 0.18483188\n",
      "Iteration 4156, loss = 0.18476535\n",
      "Iteration 4157, loss = 0.18469909\n",
      "Iteration 4158, loss = 0.18463421\n",
      "Iteration 4159, loss = 0.18456817\n",
      "Iteration 4160, loss = 0.18450274\n",
      "Iteration 4161, loss = 0.18443775\n",
      "Iteration 4162, loss = 0.18437097\n",
      "Iteration 4163, loss = 0.18430545\n",
      "Iteration 4164, loss = 0.18424064\n",
      "Iteration 4165, loss = 0.18417532\n",
      "Iteration 4166, loss = 0.18410967\n",
      "Iteration 4167, loss = 0.18404331\n",
      "Iteration 4168, loss = 0.18397866\n",
      "Iteration 4169, loss = 0.18391252\n",
      "Iteration 4170, loss = 0.18384763\n",
      "Iteration 4171, loss = 0.18378159\n",
      "Iteration 4172, loss = 0.18371761\n",
      "Iteration 4173, loss = 0.18365154\n",
      "Iteration 4174, loss = 0.18358660\n",
      "Iteration 4175, loss = 0.18352089\n",
      "Iteration 4176, loss = 0.18345608\n",
      "Iteration 4177, loss = 0.18339050\n",
      "Iteration 4178, loss = 0.18332522\n",
      "Iteration 4179, loss = 0.18326015\n",
      "Iteration 4180, loss = 0.18319609\n",
      "Iteration 4181, loss = 0.18313031\n",
      "Iteration 4182, loss = 0.18306582\n",
      "Iteration 4183, loss = 0.18300019\n",
      "Iteration 4184, loss = 0.18293625\n",
      "Iteration 4185, loss = 0.18287137\n",
      "Iteration 4186, loss = 0.18280622\n",
      "Iteration 4187, loss = 0.18274140\n",
      "Iteration 4188, loss = 0.18267668\n",
      "Iteration 4189, loss = 0.18261299\n",
      "Iteration 4190, loss = 0.18254786\n",
      "Iteration 4191, loss = 0.18248298\n",
      "Iteration 4192, loss = 0.18241909\n",
      "Iteration 4193, loss = 0.18235477\n",
      "Iteration 4194, loss = 0.18229028\n",
      "Iteration 4195, loss = 0.18222486\n",
      "Iteration 4196, loss = 0.18216178\n",
      "Iteration 4197, loss = 0.18209623\n",
      "Iteration 4198, loss = 0.18203267\n",
      "Iteration 4199, loss = 0.18196803\n",
      "Iteration 4200, loss = 0.18190327\n",
      "Iteration 4201, loss = 0.18183958\n",
      "Iteration 4202, loss = 0.18177494\n",
      "Iteration 4203, loss = 0.18171094\n",
      "Iteration 4204, loss = 0.18164656\n",
      "Iteration 4205, loss = 0.18158293\n",
      "Iteration 4206, loss = 0.18151783\n",
      "Iteration 4207, loss = 0.18145466\n",
      "Iteration 4208, loss = 0.18139072\n",
      "Iteration 4209, loss = 0.18132689\n",
      "Iteration 4210, loss = 0.18126318\n",
      "Iteration 4211, loss = 0.18119839\n",
      "Iteration 4212, loss = 0.18113586\n",
      "Iteration 4213, loss = 0.18107122\n",
      "Iteration 4214, loss = 0.18100760\n",
      "Iteration 4215, loss = 0.18094408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4216, loss = 0.18087978\n",
      "Iteration 4217, loss = 0.18081815\n",
      "Iteration 4218, loss = 0.18075255\n",
      "Iteration 4219, loss = 0.18068983\n",
      "Iteration 4220, loss = 0.18062644\n",
      "Iteration 4221, loss = 0.18056270\n",
      "Iteration 4222, loss = 0.18049983\n",
      "Iteration 4223, loss = 0.18043667\n",
      "Iteration 4224, loss = 0.18037209\n",
      "Iteration 4225, loss = 0.18030944\n",
      "Iteration 4226, loss = 0.18024669\n",
      "Iteration 4227, loss = 0.18018272\n",
      "Iteration 4228, loss = 0.18012037\n",
      "Iteration 4229, loss = 0.18005580\n",
      "Iteration 4230, loss = 0.17999328\n",
      "Iteration 4231, loss = 0.17993046\n",
      "Iteration 4232, loss = 0.17986599\n",
      "Iteration 4233, loss = 0.17980394\n",
      "Iteration 4234, loss = 0.17974116\n",
      "Iteration 4235, loss = 0.17967773\n",
      "Iteration 4236, loss = 0.17961502\n",
      "Iteration 4237, loss = 0.17955231\n",
      "Iteration 4238, loss = 0.17949003\n",
      "Iteration 4239, loss = 0.17942656\n",
      "Iteration 4240, loss = 0.17936420\n",
      "Iteration 4241, loss = 0.17930211\n",
      "Iteration 4242, loss = 0.17923791\n",
      "Iteration 4243, loss = 0.17917511\n",
      "Iteration 4244, loss = 0.17911249\n",
      "Iteration 4245, loss = 0.17905098\n",
      "Iteration 4246, loss = 0.17898757\n",
      "Iteration 4247, loss = 0.17892589\n",
      "Iteration 4248, loss = 0.17886274\n",
      "Iteration 4249, loss = 0.17880097\n",
      "Iteration 4250, loss = 0.17873836\n",
      "Iteration 4251, loss = 0.17867635\n",
      "Iteration 4252, loss = 0.17861407\n",
      "Iteration 4253, loss = 0.17855222\n",
      "Iteration 4254, loss = 0.17848991\n",
      "Iteration 4255, loss = 0.17842895\n",
      "Iteration 4256, loss = 0.17836686\n",
      "Iteration 4257, loss = 0.17830427\n",
      "Iteration 4258, loss = 0.17824339\n",
      "Iteration 4259, loss = 0.17818070\n",
      "Iteration 4260, loss = 0.17811882\n",
      "Iteration 4261, loss = 0.17805703\n",
      "Iteration 4262, loss = 0.17799496\n",
      "Iteration 4263, loss = 0.17793349\n",
      "Iteration 4264, loss = 0.17787207\n",
      "Iteration 4265, loss = 0.17780978\n",
      "Iteration 4266, loss = 0.17774854\n",
      "Iteration 4267, loss = 0.17768635\n",
      "Iteration 4268, loss = 0.17762477\n",
      "Iteration 4269, loss = 0.17756322\n",
      "Iteration 4270, loss = 0.17750086\n",
      "Iteration 4271, loss = 0.17743979\n",
      "Iteration 4272, loss = 0.17737964\n",
      "Iteration 4273, loss = 0.17731804\n",
      "Iteration 4274, loss = 0.17725623\n",
      "Iteration 4275, loss = 0.17719515\n",
      "Iteration 4276, loss = 0.17713349\n",
      "Iteration 4277, loss = 0.17707247\n",
      "Iteration 4278, loss = 0.17701154\n",
      "Iteration 4279, loss = 0.17695074\n",
      "Iteration 4280, loss = 0.17689054\n",
      "Iteration 4281, loss = 0.17682886\n",
      "Iteration 4282, loss = 0.17676805\n",
      "Iteration 4283, loss = 0.17670652\n",
      "Iteration 4284, loss = 0.17664654\n",
      "Iteration 4285, loss = 0.17658529\n",
      "Iteration 4286, loss = 0.17652505\n",
      "Iteration 4287, loss = 0.17646363\n",
      "Iteration 4288, loss = 0.17640359\n",
      "Iteration 4289, loss = 0.17634231\n",
      "Iteration 4290, loss = 0.17628278\n",
      "Iteration 4291, loss = 0.17622114\n",
      "Iteration 4292, loss = 0.17616182\n",
      "Iteration 4293, loss = 0.17610064\n",
      "Iteration 4294, loss = 0.17604026\n",
      "Iteration 4295, loss = 0.17597943\n",
      "Iteration 4296, loss = 0.17591868\n",
      "Iteration 4297, loss = 0.17585813\n",
      "Iteration 4298, loss = 0.17579783\n",
      "Iteration 4299, loss = 0.17573731\n",
      "Iteration 4300, loss = 0.17567640\n",
      "Iteration 4301, loss = 0.17561708\n",
      "Iteration 4302, loss = 0.17555594\n",
      "Iteration 4303, loss = 0.17549596\n",
      "Iteration 4304, loss = 0.17543557\n",
      "Iteration 4305, loss = 0.17537489\n",
      "Iteration 4306, loss = 0.17531457\n",
      "Iteration 4307, loss = 0.17525492\n",
      "Iteration 4308, loss = 0.17519394\n",
      "Iteration 4309, loss = 0.17513474\n",
      "Iteration 4310, loss = 0.17507494\n",
      "Iteration 4311, loss = 0.17501521\n",
      "Iteration 4312, loss = 0.17495424\n",
      "Iteration 4313, loss = 0.17489526\n",
      "Iteration 4314, loss = 0.17483478\n",
      "Iteration 4315, loss = 0.17477516\n",
      "Iteration 4316, loss = 0.17471577\n",
      "Iteration 4317, loss = 0.17465622\n",
      "Iteration 4318, loss = 0.17459539\n",
      "Iteration 4319, loss = 0.17453669\n",
      "Iteration 4320, loss = 0.17447661\n",
      "Iteration 4321, loss = 0.17441711\n",
      "Iteration 4322, loss = 0.17435777\n",
      "Iteration 4323, loss = 0.17429843\n",
      "Iteration 4324, loss = 0.17423886\n",
      "Iteration 4325, loss = 0.17417945\n",
      "Iteration 4326, loss = 0.17412025\n",
      "Iteration 4327, loss = 0.17406077\n",
      "Iteration 4328, loss = 0.17400197\n",
      "Iteration 4329, loss = 0.17394235\n",
      "Iteration 4330, loss = 0.17388285\n",
      "Iteration 4331, loss = 0.17382334\n",
      "Iteration 4332, loss = 0.17376500\n",
      "Iteration 4333, loss = 0.17370561\n",
      "Iteration 4334, loss = 0.17364647\n",
      "Iteration 4335, loss = 0.17358700\n",
      "Iteration 4336, loss = 0.17352854\n",
      "Iteration 4337, loss = 0.17346975\n",
      "Iteration 4338, loss = 0.17341048\n",
      "Iteration 4339, loss = 0.17335089\n",
      "Iteration 4340, loss = 0.17329304\n",
      "Iteration 4341, loss = 0.17323324\n",
      "Iteration 4342, loss = 0.17317402\n",
      "Iteration 4343, loss = 0.17311585\n",
      "Iteration 4344, loss = 0.17305762\n",
      "Iteration 4345, loss = 0.17299844\n",
      "Iteration 4346, loss = 0.17293994\n",
      "Iteration 4347, loss = 0.17288115\n",
      "Iteration 4348, loss = 0.17282202\n",
      "Iteration 4349, loss = 0.17276330\n",
      "Iteration 4350, loss = 0.17270535\n",
      "Iteration 4351, loss = 0.17264694\n",
      "Iteration 4352, loss = 0.17258813\n",
      "Iteration 4353, loss = 0.17253010\n",
      "Iteration 4354, loss = 0.17247097\n",
      "Iteration 4355, loss = 0.17241297\n",
      "Iteration 4356, loss = 0.17235495\n",
      "Iteration 4357, loss = 0.17229640\n",
      "Iteration 4358, loss = 0.17223730\n",
      "Iteration 4359, loss = 0.17217924\n",
      "Iteration 4360, loss = 0.17212078\n",
      "Iteration 4361, loss = 0.17206253\n",
      "Iteration 4362, loss = 0.17200461\n",
      "Iteration 4363, loss = 0.17194624\n",
      "Iteration 4364, loss = 0.17188827\n",
      "Iteration 4365, loss = 0.17183095\n",
      "Iteration 4366, loss = 0.17177234\n",
      "Iteration 4367, loss = 0.17171443\n",
      "Iteration 4368, loss = 0.17165702\n",
      "Iteration 4369, loss = 0.17159811\n",
      "Iteration 4370, loss = 0.17154062\n",
      "Iteration 4371, loss = 0.17148257\n",
      "Iteration 4372, loss = 0.17142472\n",
      "Iteration 4373, loss = 0.17136711\n",
      "Iteration 4374, loss = 0.17130855\n",
      "Iteration 4375, loss = 0.17125101\n",
      "Iteration 4376, loss = 0.17119344\n",
      "Iteration 4377, loss = 0.17113589\n",
      "Iteration 4378, loss = 0.17107852\n",
      "Iteration 4379, loss = 0.17102028\n",
      "Iteration 4380, loss = 0.17096235\n",
      "Iteration 4381, loss = 0.17090503\n",
      "Iteration 4382, loss = 0.17084745\n",
      "Iteration 4383, loss = 0.17079004\n",
      "Iteration 4384, loss = 0.17073304\n",
      "Iteration 4385, loss = 0.17067524\n",
      "Iteration 4386, loss = 0.17061719\n",
      "Iteration 4387, loss = 0.17056006\n",
      "Iteration 4388, loss = 0.17050275\n",
      "Iteration 4389, loss = 0.17044596\n",
      "Iteration 4390, loss = 0.17038900\n",
      "Iteration 4391, loss = 0.17033143\n",
      "Iteration 4392, loss = 0.17027380\n",
      "Iteration 4393, loss = 0.17021805\n",
      "Iteration 4394, loss = 0.17016106\n",
      "Iteration 4395, loss = 0.17010422\n",
      "Iteration 4396, loss = 0.17004675\n",
      "Iteration 4397, loss = 0.16999067\n",
      "Iteration 4398, loss = 0.16993322\n",
      "Iteration 4399, loss = 0.16987640\n",
      "Iteration 4400, loss = 0.16981970\n",
      "Iteration 4401, loss = 0.16976402\n",
      "Iteration 4402, loss = 0.16970658\n",
      "Iteration 4403, loss = 0.16964925\n",
      "Iteration 4404, loss = 0.16959286\n",
      "Iteration 4405, loss = 0.16953671\n",
      "Iteration 4406, loss = 0.16947952\n",
      "Iteration 4407, loss = 0.16942266\n",
      "Iteration 4408, loss = 0.16936662\n",
      "Iteration 4409, loss = 0.16930993\n",
      "Iteration 4410, loss = 0.16925369\n",
      "Iteration 4411, loss = 0.16919637\n",
      "Iteration 4412, loss = 0.16914027\n",
      "Iteration 4413, loss = 0.16908376\n",
      "Iteration 4414, loss = 0.16902689\n",
      "Iteration 4415, loss = 0.16897070\n",
      "Iteration 4416, loss = 0.16891434\n",
      "Iteration 4417, loss = 0.16885759\n",
      "Iteration 4418, loss = 0.16880116\n",
      "Iteration 4419, loss = 0.16874543\n",
      "Iteration 4420, loss = 0.16868879\n",
      "Iteration 4421, loss = 0.16863250\n",
      "Iteration 4422, loss = 0.16857662\n",
      "Iteration 4423, loss = 0.16852079\n",
      "Iteration 4424, loss = 0.16846403\n",
      "Iteration 4425, loss = 0.16840895\n",
      "Iteration 4426, loss = 0.16835196\n",
      "Iteration 4427, loss = 0.16829701\n",
      "Iteration 4428, loss = 0.16824050\n",
      "Iteration 4429, loss = 0.16818458\n",
      "Iteration 4430, loss = 0.16812888\n",
      "Iteration 4431, loss = 0.16807298\n",
      "Iteration 4432, loss = 0.16801743\n",
      "Iteration 4433, loss = 0.16796227\n",
      "Iteration 4434, loss = 0.16790661\n",
      "Iteration 4435, loss = 0.16785067\n",
      "Iteration 4436, loss = 0.16779516\n",
      "Iteration 4437, loss = 0.16773996\n",
      "Iteration 4438, loss = 0.16768388\n",
      "Iteration 4439, loss = 0.16762867\n",
      "Iteration 4440, loss = 0.16757350\n",
      "Iteration 4441, loss = 0.16751874\n",
      "Iteration 4442, loss = 0.16746195\n",
      "Iteration 4443, loss = 0.16740787\n",
      "Iteration 4444, loss = 0.16735198\n",
      "Iteration 4445, loss = 0.16729689\n",
      "Iteration 4446, loss = 0.16724198\n",
      "Iteration 4447, loss = 0.16718652\n",
      "Iteration 4448, loss = 0.16713093\n",
      "Iteration 4449, loss = 0.16707670\n",
      "Iteration 4450, loss = 0.16702145\n",
      "Iteration 4451, loss = 0.16696601\n",
      "Iteration 4452, loss = 0.16691246\n",
      "Iteration 4453, loss = 0.16685696\n",
      "Iteration 4454, loss = 0.16680160\n",
      "Iteration 4455, loss = 0.16674648\n",
      "Iteration 4456, loss = 0.16669172\n",
      "Iteration 4457, loss = 0.16663732\n",
      "Iteration 4458, loss = 0.16658214\n",
      "Iteration 4459, loss = 0.16652719\n",
      "Iteration 4460, loss = 0.16647324\n",
      "Iteration 4461, loss = 0.16641779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4462, loss = 0.16636264\n",
      "Iteration 4463, loss = 0.16630830\n",
      "Iteration 4464, loss = 0.16625339\n",
      "Iteration 4465, loss = 0.16619874\n",
      "Iteration 4466, loss = 0.16614393\n",
      "Iteration 4467, loss = 0.16608915\n",
      "Iteration 4468, loss = 0.16603527\n",
      "Iteration 4469, loss = 0.16598038\n",
      "Iteration 4470, loss = 0.16592643\n",
      "Iteration 4471, loss = 0.16587234\n",
      "Iteration 4472, loss = 0.16581767\n",
      "Iteration 4473, loss = 0.16576319\n",
      "Iteration 4474, loss = 0.16570862\n",
      "Iteration 4475, loss = 0.16565516\n",
      "Iteration 4476, loss = 0.16560157\n",
      "Iteration 4477, loss = 0.16554690\n",
      "Iteration 4478, loss = 0.16549310\n",
      "Iteration 4479, loss = 0.16543880\n",
      "Iteration 4480, loss = 0.16538446\n",
      "Iteration 4481, loss = 0.16533063\n",
      "Iteration 4482, loss = 0.16527685\n",
      "Iteration 4483, loss = 0.16522296\n",
      "Iteration 4484, loss = 0.16516938\n",
      "Iteration 4485, loss = 0.16511473\n",
      "Iteration 4486, loss = 0.16506197\n",
      "Iteration 4487, loss = 0.16500837\n",
      "Iteration 4488, loss = 0.16495416\n",
      "Iteration 4489, loss = 0.16490081\n",
      "Iteration 4490, loss = 0.16484788\n",
      "Iteration 4491, loss = 0.16479413\n",
      "Iteration 4492, loss = 0.16474078\n",
      "Iteration 4493, loss = 0.16468639\n",
      "Iteration 4494, loss = 0.16463373\n",
      "Iteration 4495, loss = 0.16457972\n",
      "Iteration 4496, loss = 0.16452712\n",
      "Iteration 4497, loss = 0.16447276\n",
      "Iteration 4498, loss = 0.16441987\n",
      "Iteration 4499, loss = 0.16436617\n",
      "Iteration 4500, loss = 0.16431246\n",
      "Iteration 4501, loss = 0.16425948\n",
      "Iteration 4502, loss = 0.16420577\n",
      "Iteration 4503, loss = 0.16415326\n",
      "Iteration 4504, loss = 0.16409925\n",
      "Iteration 4505, loss = 0.16404624\n",
      "Iteration 4506, loss = 0.16399288\n",
      "Iteration 4507, loss = 0.16394005\n",
      "Iteration 4508, loss = 0.16388691\n",
      "Iteration 4509, loss = 0.16383269\n",
      "Iteration 4510, loss = 0.16377989\n",
      "Iteration 4511, loss = 0.16372694\n",
      "Iteration 4512, loss = 0.16367416\n",
      "Iteration 4513, loss = 0.16362078\n",
      "Iteration 4514, loss = 0.16356752\n",
      "Iteration 4515, loss = 0.16351490\n",
      "Iteration 4516, loss = 0.16346143\n",
      "Iteration 4517, loss = 0.16340827\n",
      "Iteration 4518, loss = 0.16335593\n",
      "Iteration 4519, loss = 0.16330333\n",
      "Iteration 4520, loss = 0.16325046\n",
      "Iteration 4521, loss = 0.16319735\n",
      "Iteration 4522, loss = 0.16314445\n",
      "Iteration 4523, loss = 0.16309210\n",
      "Iteration 4524, loss = 0.16303900\n",
      "Iteration 4525, loss = 0.16298649\n",
      "Iteration 4526, loss = 0.16293371\n",
      "Iteration 4527, loss = 0.16288124\n",
      "Iteration 4528, loss = 0.16282853\n",
      "Iteration 4529, loss = 0.16277632\n",
      "Iteration 4530, loss = 0.16272335\n",
      "Iteration 4531, loss = 0.16267068\n",
      "Iteration 4532, loss = 0.16261865\n",
      "Iteration 4533, loss = 0.16256572\n",
      "Iteration 4534, loss = 0.16251343\n",
      "Iteration 4535, loss = 0.16246114\n",
      "Iteration 4536, loss = 0.16240875\n",
      "Iteration 4537, loss = 0.16235672\n",
      "Iteration 4538, loss = 0.16230449\n",
      "Iteration 4539, loss = 0.16225240\n",
      "Iteration 4540, loss = 0.16220036\n",
      "Iteration 4541, loss = 0.16214760\n",
      "Iteration 4542, loss = 0.16209472\n",
      "Iteration 4543, loss = 0.16204358\n",
      "Iteration 4544, loss = 0.16199066\n",
      "Iteration 4545, loss = 0.16193893\n",
      "Iteration 4546, loss = 0.16188648\n",
      "Iteration 4547, loss = 0.16183556\n",
      "Iteration 4548, loss = 0.16178346\n",
      "Iteration 4549, loss = 0.16173136\n",
      "Iteration 4550, loss = 0.16167916\n",
      "Iteration 4551, loss = 0.16162732\n",
      "Iteration 4552, loss = 0.16157595\n",
      "Iteration 4553, loss = 0.16152388\n",
      "Iteration 4554, loss = 0.16147244\n",
      "Iteration 4555, loss = 0.16141990\n",
      "Iteration 4556, loss = 0.16136807\n",
      "Iteration 4557, loss = 0.16131676\n",
      "Iteration 4558, loss = 0.16126509\n",
      "Iteration 4559, loss = 0.16121418\n",
      "Iteration 4560, loss = 0.16116194\n",
      "Iteration 4561, loss = 0.16111043\n",
      "Iteration 4562, loss = 0.16105904\n",
      "Iteration 4563, loss = 0.16100695\n",
      "Iteration 4564, loss = 0.16095536\n",
      "Iteration 4565, loss = 0.16090425\n",
      "Iteration 4566, loss = 0.16085243\n",
      "Iteration 4567, loss = 0.16080121\n",
      "Iteration 4568, loss = 0.16074962\n",
      "Iteration 4569, loss = 0.16069850\n",
      "Iteration 4570, loss = 0.16064731\n",
      "Iteration 4571, loss = 0.16059532\n",
      "Iteration 4572, loss = 0.16054493\n",
      "Iteration 4573, loss = 0.16049279\n",
      "Iteration 4574, loss = 0.16044221\n",
      "Iteration 4575, loss = 0.16039083\n",
      "Iteration 4576, loss = 0.16034012\n",
      "Iteration 4577, loss = 0.16028938\n",
      "Iteration 4578, loss = 0.16023745\n",
      "Iteration 4579, loss = 0.16018683\n",
      "Iteration 4580, loss = 0.16013588\n",
      "Iteration 4581, loss = 0.16008491\n",
      "Iteration 4582, loss = 0.16003410\n",
      "Iteration 4583, loss = 0.15998342\n",
      "Iteration 4584, loss = 0.15993355\n",
      "Iteration 4585, loss = 0.15988199\n",
      "Iteration 4586, loss = 0.15983187\n",
      "Iteration 4587, loss = 0.15978091\n",
      "Iteration 4588, loss = 0.15972999\n",
      "Iteration 4589, loss = 0.15967982\n",
      "Iteration 4590, loss = 0.15962908\n",
      "Iteration 4591, loss = 0.15957771\n",
      "Iteration 4592, loss = 0.15952771\n",
      "Iteration 4593, loss = 0.15947769\n",
      "Iteration 4594, loss = 0.15942677\n",
      "Iteration 4595, loss = 0.15937576\n",
      "Iteration 4596, loss = 0.15932561\n",
      "Iteration 4597, loss = 0.15927582\n",
      "Iteration 4598, loss = 0.15922441\n",
      "Iteration 4599, loss = 0.15917449\n",
      "Iteration 4600, loss = 0.15912432\n",
      "Iteration 4601, loss = 0.15907398\n",
      "Iteration 4602, loss = 0.15902376\n",
      "Iteration 4603, loss = 0.15897361\n",
      "Iteration 4604, loss = 0.15892322\n",
      "Iteration 4605, loss = 0.15887298\n",
      "Iteration 4606, loss = 0.15882311\n",
      "Iteration 4607, loss = 0.15877261\n",
      "Iteration 4608, loss = 0.15872253\n",
      "Iteration 4609, loss = 0.15867244\n",
      "Iteration 4610, loss = 0.15862182\n",
      "Iteration 4611, loss = 0.15857187\n",
      "Iteration 4612, loss = 0.15852220\n",
      "Iteration 4613, loss = 0.15847149\n",
      "Iteration 4614, loss = 0.15842166\n",
      "Iteration 4615, loss = 0.15837177\n",
      "Iteration 4616, loss = 0.15832200\n",
      "Iteration 4617, loss = 0.15827231\n",
      "Iteration 4618, loss = 0.15822241\n",
      "Iteration 4619, loss = 0.15817229\n",
      "Iteration 4620, loss = 0.15812335\n",
      "Iteration 4621, loss = 0.15807347\n",
      "Iteration 4622, loss = 0.15802371\n",
      "Iteration 4623, loss = 0.15797434\n",
      "Iteration 4624, loss = 0.15792422\n",
      "Iteration 4625, loss = 0.15787629\n",
      "Iteration 4626, loss = 0.15782552\n",
      "Iteration 4627, loss = 0.15777670\n",
      "Iteration 4628, loss = 0.15772650\n",
      "Iteration 4629, loss = 0.15767797\n",
      "Iteration 4630, loss = 0.15762814\n",
      "Iteration 4631, loss = 0.15757925\n",
      "Iteration 4632, loss = 0.15752928\n",
      "Iteration 4633, loss = 0.15747994\n",
      "Iteration 4634, loss = 0.15743145\n",
      "Iteration 4635, loss = 0.15738168\n",
      "Iteration 4636, loss = 0.15733274\n",
      "Iteration 4637, loss = 0.15728307\n",
      "Iteration 4638, loss = 0.15723386\n",
      "Iteration 4639, loss = 0.15718488\n",
      "Iteration 4640, loss = 0.15713580\n",
      "Iteration 4641, loss = 0.15708615\n",
      "Iteration 4642, loss = 0.15703760\n",
      "Iteration 4643, loss = 0.15698815\n",
      "Iteration 4644, loss = 0.15693881\n",
      "Iteration 4645, loss = 0.15688964\n",
      "Iteration 4646, loss = 0.15684041\n",
      "Iteration 4647, loss = 0.15679155\n",
      "Iteration 4648, loss = 0.15674289\n",
      "Iteration 4649, loss = 0.15669321\n",
      "Iteration 4650, loss = 0.15664407\n",
      "Iteration 4651, loss = 0.15659594\n",
      "Iteration 4652, loss = 0.15654740\n",
      "Iteration 4653, loss = 0.15649818\n",
      "Iteration 4654, loss = 0.15644936\n",
      "Iteration 4655, loss = 0.15640066\n",
      "Iteration 4656, loss = 0.15635263\n",
      "Iteration 4657, loss = 0.15630385\n",
      "Iteration 4658, loss = 0.15625548\n",
      "Iteration 4659, loss = 0.15620620\n",
      "Iteration 4660, loss = 0.15615738\n",
      "Iteration 4661, loss = 0.15610943\n",
      "Iteration 4662, loss = 0.15605987\n",
      "Iteration 4663, loss = 0.15601160\n",
      "Iteration 4664, loss = 0.15596304\n",
      "Iteration 4665, loss = 0.15591430\n",
      "Iteration 4666, loss = 0.15586587\n",
      "Iteration 4667, loss = 0.15581789\n",
      "Iteration 4668, loss = 0.15576921\n",
      "Iteration 4669, loss = 0.15572052\n",
      "Iteration 4670, loss = 0.15567193\n",
      "Iteration 4671, loss = 0.15562392\n",
      "Iteration 4672, loss = 0.15557552\n",
      "Iteration 4673, loss = 0.15552713\n",
      "Iteration 4674, loss = 0.15547876\n",
      "Iteration 4675, loss = 0.15543073\n",
      "Iteration 4676, loss = 0.15538292\n",
      "Iteration 4677, loss = 0.15533435\n",
      "Iteration 4678, loss = 0.15528622\n",
      "Iteration 4679, loss = 0.15523788\n",
      "Iteration 4680, loss = 0.15519009\n",
      "Iteration 4681, loss = 0.15514139\n",
      "Iteration 4682, loss = 0.15509389\n",
      "Iteration 4683, loss = 0.15504552\n",
      "Iteration 4684, loss = 0.15499794\n",
      "Iteration 4685, loss = 0.15494972\n",
      "Iteration 4686, loss = 0.15490244\n",
      "Iteration 4687, loss = 0.15485397\n",
      "Iteration 4688, loss = 0.15480675\n",
      "Iteration 4689, loss = 0.15475798\n",
      "Iteration 4690, loss = 0.15471090\n",
      "Iteration 4691, loss = 0.15466332\n",
      "Iteration 4692, loss = 0.15461555\n",
      "Iteration 4693, loss = 0.15456773\n",
      "Iteration 4694, loss = 0.15451982\n",
      "Iteration 4695, loss = 0.15447227\n",
      "Iteration 4696, loss = 0.15442427\n",
      "Iteration 4697, loss = 0.15437750\n",
      "Iteration 4698, loss = 0.15432982\n",
      "Iteration 4699, loss = 0.15428201\n",
      "Iteration 4700, loss = 0.15423430\n",
      "Iteration 4701, loss = 0.15418696\n",
      "Iteration 4702, loss = 0.15414003\n",
      "Iteration 4703, loss = 0.15409220\n",
      "Iteration 4704, loss = 0.15404493\n",
      "Iteration 4705, loss = 0.15399758\n",
      "Iteration 4706, loss = 0.15395056\n",
      "Iteration 4707, loss = 0.15390362\n",
      "Iteration 4708, loss = 0.15385576\n",
      "Iteration 4709, loss = 0.15380920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4710, loss = 0.15376181\n",
      "Iteration 4711, loss = 0.15371436\n",
      "Iteration 4712, loss = 0.15366669\n",
      "Iteration 4713, loss = 0.15361963\n",
      "Iteration 4714, loss = 0.15357225\n",
      "Iteration 4715, loss = 0.15352565\n",
      "Iteration 4716, loss = 0.15347793\n",
      "Iteration 4717, loss = 0.15343111\n",
      "Iteration 4718, loss = 0.15338354\n",
      "Iteration 4719, loss = 0.15333727\n",
      "Iteration 4720, loss = 0.15328996\n",
      "Iteration 4721, loss = 0.15324289\n",
      "Iteration 4722, loss = 0.15319673\n",
      "Iteration 4723, loss = 0.15314957\n",
      "Iteration 4724, loss = 0.15310246\n",
      "Iteration 4725, loss = 0.15305524\n",
      "Iteration 4726, loss = 0.15300860\n",
      "Iteration 4727, loss = 0.15296201\n",
      "Iteration 4728, loss = 0.15291444\n",
      "Iteration 4729, loss = 0.15286777\n",
      "Iteration 4730, loss = 0.15282114\n",
      "Iteration 4731, loss = 0.15277437\n",
      "Iteration 4732, loss = 0.15272758\n",
      "Iteration 4733, loss = 0.15268075\n",
      "Iteration 4734, loss = 0.15263384\n",
      "Iteration 4735, loss = 0.15258722\n",
      "Iteration 4736, loss = 0.15254120\n",
      "Iteration 4737, loss = 0.15249464\n",
      "Iteration 4738, loss = 0.15244761\n",
      "Iteration 4739, loss = 0.15240092\n",
      "Iteration 4740, loss = 0.15235472\n",
      "Iteration 4741, loss = 0.15230834\n",
      "Iteration 4742, loss = 0.15226178\n",
      "Iteration 4743, loss = 0.15221551\n",
      "Iteration 4744, loss = 0.15216879\n",
      "Iteration 4745, loss = 0.15212273\n",
      "Iteration 4746, loss = 0.15207598\n",
      "Iteration 4747, loss = 0.15202992\n",
      "Iteration 4748, loss = 0.15198343\n",
      "Iteration 4749, loss = 0.15193834\n",
      "Iteration 4750, loss = 0.15189079\n",
      "Iteration 4751, loss = 0.15184533\n",
      "Iteration 4752, loss = 0.15179904\n",
      "Iteration 4753, loss = 0.15175238\n",
      "Iteration 4754, loss = 0.15170677\n",
      "Iteration 4755, loss = 0.15166073\n",
      "Iteration 4756, loss = 0.15161521\n",
      "Iteration 4757, loss = 0.15156903\n",
      "Iteration 4758, loss = 0.15152334\n",
      "Iteration 4759, loss = 0.15147670\n",
      "Iteration 4760, loss = 0.15143104\n",
      "Iteration 4761, loss = 0.15138561\n",
      "Iteration 4762, loss = 0.15133963\n",
      "Iteration 4763, loss = 0.15129398\n",
      "Iteration 4764, loss = 0.15124844\n",
      "Iteration 4765, loss = 0.15120267\n",
      "Iteration 4766, loss = 0.15115649\n",
      "Iteration 4767, loss = 0.15111087\n",
      "Iteration 4768, loss = 0.15106539\n",
      "Iteration 4769, loss = 0.15102020\n",
      "Iteration 4770, loss = 0.15097415\n",
      "Iteration 4771, loss = 0.15092887\n",
      "Iteration 4772, loss = 0.15088287\n",
      "Iteration 4773, loss = 0.15083745\n",
      "Iteration 4774, loss = 0.15079220\n",
      "Iteration 4775, loss = 0.15074657\n",
      "Iteration 4776, loss = 0.15070119\n",
      "Iteration 4777, loss = 0.15065569\n",
      "Iteration 4778, loss = 0.15061053\n",
      "Iteration 4779, loss = 0.15056480\n",
      "Iteration 4780, loss = 0.15052023\n",
      "Iteration 4781, loss = 0.15047415\n",
      "Iteration 4782, loss = 0.15042956\n",
      "Iteration 4783, loss = 0.15038403\n",
      "Iteration 4784, loss = 0.15033863\n",
      "Iteration 4785, loss = 0.15029340\n",
      "Iteration 4786, loss = 0.15024900\n",
      "Iteration 4787, loss = 0.15020316\n",
      "Iteration 4788, loss = 0.15015850\n",
      "Iteration 4789, loss = 0.15011343\n",
      "Iteration 4790, loss = 0.15006798\n",
      "Iteration 4791, loss = 0.15002314\n",
      "Iteration 4792, loss = 0.14997875\n",
      "Iteration 4793, loss = 0.14993394\n",
      "Iteration 4794, loss = 0.14988908\n",
      "Iteration 4795, loss = 0.14984310\n",
      "Iteration 4796, loss = 0.14979890\n",
      "Iteration 4797, loss = 0.14975401\n",
      "Iteration 4798, loss = 0.14970884\n",
      "Iteration 4799, loss = 0.14966459\n",
      "Iteration 4800, loss = 0.14961949\n",
      "Iteration 4801, loss = 0.14957528\n",
      "Iteration 4802, loss = 0.14953003\n",
      "Iteration 4803, loss = 0.14948498\n",
      "Iteration 4804, loss = 0.14944049\n",
      "Iteration 4805, loss = 0.14939617\n",
      "Iteration 4806, loss = 0.14935148\n",
      "Iteration 4807, loss = 0.14930641\n",
      "Iteration 4808, loss = 0.14926184\n",
      "Iteration 4809, loss = 0.14921745\n",
      "Iteration 4810, loss = 0.14917229\n",
      "Iteration 4811, loss = 0.14912728\n",
      "Iteration 4812, loss = 0.14908294\n",
      "Iteration 4813, loss = 0.14903871\n",
      "Iteration 4814, loss = 0.14899393\n",
      "Iteration 4815, loss = 0.14894971\n",
      "Iteration 4816, loss = 0.14890549\n",
      "Iteration 4817, loss = 0.14886088\n",
      "Iteration 4818, loss = 0.14881681\n",
      "Iteration 4819, loss = 0.14877263\n",
      "Iteration 4820, loss = 0.14872778\n",
      "Iteration 4821, loss = 0.14868359\n",
      "Iteration 4822, loss = 0.14863910\n",
      "Iteration 4823, loss = 0.14859606\n",
      "Iteration 4824, loss = 0.14855105\n",
      "Iteration 4825, loss = 0.14850706\n",
      "Iteration 4826, loss = 0.14846283\n",
      "Iteration 4827, loss = 0.14841904\n",
      "Iteration 4828, loss = 0.14837505\n",
      "Iteration 4829, loss = 0.14833114\n",
      "Iteration 4830, loss = 0.14828721\n",
      "Iteration 4831, loss = 0.14824289\n",
      "Iteration 4832, loss = 0.14819863\n",
      "Iteration 4833, loss = 0.14815476\n",
      "Iteration 4834, loss = 0.14811087\n",
      "Iteration 4835, loss = 0.14806729\n",
      "Iteration 4836, loss = 0.14802299\n",
      "Iteration 4837, loss = 0.14797926\n",
      "Iteration 4838, loss = 0.14793533\n",
      "Iteration 4839, loss = 0.14789197\n",
      "Iteration 4840, loss = 0.14784793\n",
      "Iteration 4841, loss = 0.14780428\n",
      "Iteration 4842, loss = 0.14776005\n",
      "Iteration 4843, loss = 0.14771629\n",
      "Iteration 4844, loss = 0.14767284\n",
      "Iteration 4845, loss = 0.14762890\n",
      "Iteration 4846, loss = 0.14758560\n",
      "Iteration 4847, loss = 0.14754135\n",
      "Iteration 4848, loss = 0.14749795\n",
      "Iteration 4849, loss = 0.14745438\n",
      "Iteration 4850, loss = 0.14741033\n",
      "Iteration 4851, loss = 0.14736723\n",
      "Iteration 4852, loss = 0.14732271\n",
      "Iteration 4853, loss = 0.14727996\n",
      "Iteration 4854, loss = 0.14723623\n",
      "Iteration 4855, loss = 0.14719271\n",
      "Iteration 4856, loss = 0.14714903\n",
      "Iteration 4857, loss = 0.14710621\n",
      "Iteration 4858, loss = 0.14706223\n",
      "Iteration 4859, loss = 0.14701895\n",
      "Iteration 4860, loss = 0.14697607\n",
      "Iteration 4861, loss = 0.14693239\n",
      "Iteration 4862, loss = 0.14688905\n",
      "Iteration 4863, loss = 0.14684585\n",
      "Iteration 4864, loss = 0.14680246\n",
      "Iteration 4865, loss = 0.14675982\n",
      "Iteration 4866, loss = 0.14671605\n",
      "Iteration 4867, loss = 0.14667345\n",
      "Iteration 4868, loss = 0.14663037\n",
      "Iteration 4869, loss = 0.14658673\n",
      "Iteration 4870, loss = 0.14654355\n",
      "Iteration 4871, loss = 0.14650055\n",
      "Iteration 4872, loss = 0.14645735\n",
      "Iteration 4873, loss = 0.14641456\n",
      "Iteration 4874, loss = 0.14637121\n",
      "Iteration 4875, loss = 0.14632823\n",
      "Iteration 4876, loss = 0.14628531\n",
      "Iteration 4877, loss = 0.14624217\n",
      "Iteration 4878, loss = 0.14619926\n",
      "Iteration 4879, loss = 0.14615645\n",
      "Iteration 4880, loss = 0.14611377\n",
      "Iteration 4881, loss = 0.14607185\n",
      "Iteration 4882, loss = 0.14602741\n",
      "Iteration 4883, loss = 0.14598467\n",
      "Iteration 4884, loss = 0.14594280\n",
      "Iteration 4885, loss = 0.14590006\n",
      "Iteration 4886, loss = 0.14585672\n",
      "Iteration 4887, loss = 0.14581450\n",
      "Iteration 4888, loss = 0.14577113\n",
      "Iteration 4889, loss = 0.14572852\n",
      "Iteration 4890, loss = 0.14568572\n",
      "Iteration 4891, loss = 0.14564310\n",
      "Iteration 4892, loss = 0.14560054\n",
      "Iteration 4893, loss = 0.14555788\n",
      "Iteration 4894, loss = 0.14551519\n",
      "Iteration 4895, loss = 0.14547285\n",
      "Iteration 4896, loss = 0.14543072\n",
      "Iteration 4897, loss = 0.14538703\n",
      "Iteration 4898, loss = 0.14534502\n",
      "Iteration 4899, loss = 0.14530237\n",
      "Iteration 4900, loss = 0.14526043\n",
      "Iteration 4901, loss = 0.14521746\n",
      "Iteration 4902, loss = 0.14517468\n",
      "Iteration 4903, loss = 0.14513246\n",
      "Iteration 4904, loss = 0.14509055\n",
      "Iteration 4905, loss = 0.14504754\n",
      "Iteration 4906, loss = 0.14500586\n",
      "Iteration 4907, loss = 0.14496337\n",
      "Iteration 4908, loss = 0.14492085\n",
      "Iteration 4909, loss = 0.14487861\n",
      "Iteration 4910, loss = 0.14483679\n",
      "Iteration 4911, loss = 0.14479467\n",
      "Iteration 4912, loss = 0.14475206\n",
      "Iteration 4913, loss = 0.14471043\n",
      "Iteration 4914, loss = 0.14466775\n",
      "Iteration 4915, loss = 0.14462592\n",
      "Iteration 4916, loss = 0.14458408\n",
      "Iteration 4917, loss = 0.14454165\n",
      "Iteration 4918, loss = 0.14449954\n",
      "Iteration 4919, loss = 0.14445836\n",
      "Iteration 4920, loss = 0.14441624\n",
      "Iteration 4921, loss = 0.14437406\n",
      "Iteration 4922, loss = 0.14433246\n",
      "Iteration 4923, loss = 0.14429038\n",
      "Iteration 4924, loss = 0.14424874\n",
      "Iteration 4925, loss = 0.14420689\n",
      "Iteration 4926, loss = 0.14416515\n",
      "Iteration 4927, loss = 0.14412286\n",
      "Iteration 4928, loss = 0.14408186\n",
      "Iteration 4929, loss = 0.14403999\n",
      "Iteration 4930, loss = 0.14399792\n",
      "Iteration 4931, loss = 0.14395616\n",
      "Iteration 4932, loss = 0.14391435\n",
      "Iteration 4933, loss = 0.14387272\n",
      "Iteration 4934, loss = 0.14383087\n",
      "Iteration 4935, loss = 0.14378958\n",
      "Iteration 4936, loss = 0.14374773\n",
      "Iteration 4937, loss = 0.14370636\n",
      "Iteration 4938, loss = 0.14366492\n",
      "Iteration 4939, loss = 0.14362311\n",
      "Iteration 4940, loss = 0.14358150\n",
      "Iteration 4941, loss = 0.14353977\n",
      "Iteration 4942, loss = 0.14349840\n",
      "Iteration 4943, loss = 0.14345712\n",
      "Iteration 4944, loss = 0.14341590\n",
      "Iteration 4945, loss = 0.14337392\n",
      "Iteration 4946, loss = 0.14333259\n",
      "Iteration 4947, loss = 0.14329140\n",
      "Iteration 4948, loss = 0.14324978\n",
      "Iteration 4949, loss = 0.14320903\n",
      "Iteration 4950, loss = 0.14316717\n",
      "Iteration 4951, loss = 0.14312611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4952, loss = 0.14308516\n",
      "Iteration 4953, loss = 0.14304379\n",
      "Iteration 4954, loss = 0.14300231\n",
      "Iteration 4955, loss = 0.14296108\n",
      "Iteration 4956, loss = 0.14292040\n",
      "Iteration 4957, loss = 0.14287908\n",
      "Iteration 4958, loss = 0.14283821\n",
      "Iteration 4959, loss = 0.14279724\n",
      "Iteration 4960, loss = 0.14275520\n",
      "Iteration 4961, loss = 0.14271472\n",
      "Iteration 4962, loss = 0.14267378\n",
      "Iteration 4963, loss = 0.14263211\n",
      "Iteration 4964, loss = 0.14259186\n",
      "Iteration 4965, loss = 0.14255062\n",
      "Iteration 4966, loss = 0.14250975\n",
      "Iteration 4967, loss = 0.14246884\n",
      "Iteration 4968, loss = 0.14242804\n",
      "Iteration 4969, loss = 0.14238706\n",
      "Iteration 4970, loss = 0.14234664\n",
      "Iteration 4971, loss = 0.14230551\n",
      "Iteration 4972, loss = 0.14226431\n",
      "Iteration 4973, loss = 0.14222356\n",
      "Iteration 4974, loss = 0.14218344\n",
      "Iteration 4975, loss = 0.14214314\n",
      "Iteration 4976, loss = 0.14210197\n",
      "Iteration 4977, loss = 0.14206102\n",
      "Iteration 4978, loss = 0.14202060\n",
      "Iteration 4979, loss = 0.14197955\n",
      "Iteration 4980, loss = 0.14193897\n",
      "Iteration 4981, loss = 0.14189856\n",
      "Iteration 4982, loss = 0.14185815\n",
      "Iteration 4983, loss = 0.14181743\n",
      "Iteration 4984, loss = 0.14177721\n",
      "Iteration 4985, loss = 0.14173627\n",
      "Iteration 4986, loss = 0.14169601\n",
      "Iteration 4987, loss = 0.14165512\n",
      "Iteration 4988, loss = 0.14161596\n",
      "Iteration 4989, loss = 0.14157493\n",
      "Iteration 4990, loss = 0.14153426\n",
      "Iteration 4991, loss = 0.14149359\n",
      "Iteration 4992, loss = 0.14145404\n",
      "Iteration 4993, loss = 0.14141353\n",
      "Iteration 4994, loss = 0.14137330\n",
      "Iteration 4995, loss = 0.14133306\n",
      "Iteration 4996, loss = 0.14129232\n",
      "Iteration 4997, loss = 0.14125190\n",
      "Iteration 4998, loss = 0.14121244\n",
      "Iteration 4999, loss = 0.14117214\n",
      "Iteration 5000, loss = 0.14113155\n",
      "Iteration 5001, loss = 0.14109097\n",
      "Iteration 5002, loss = 0.14105126\n",
      "Iteration 5003, loss = 0.14101166\n",
      "Iteration 5004, loss = 0.14097112\n",
      "Iteration 5005, loss = 0.14093091\n",
      "Iteration 5006, loss = 0.14089118\n",
      "Iteration 5007, loss = 0.14085065\n",
      "Iteration 5008, loss = 0.14081004\n",
      "Iteration 5009, loss = 0.14077023\n",
      "Iteration 5010, loss = 0.14073058\n",
      "Iteration 5011, loss = 0.14069053\n",
      "Iteration 5012, loss = 0.14065047\n",
      "Iteration 5013, loss = 0.14061024\n",
      "Iteration 5014, loss = 0.14057048\n",
      "Iteration 5015, loss = 0.14053049\n",
      "Iteration 5016, loss = 0.14049040\n",
      "Iteration 5017, loss = 0.14045129\n",
      "Iteration 5018, loss = 0.14041127\n",
      "Iteration 5019, loss = 0.14037142\n",
      "Iteration 5020, loss = 0.14033188\n",
      "Iteration 5021, loss = 0.14029188\n",
      "Iteration 5022, loss = 0.14025214\n",
      "Iteration 5023, loss = 0.14021255\n",
      "Iteration 5024, loss = 0.14017262\n",
      "Iteration 5025, loss = 0.14013299\n",
      "Iteration 5026, loss = 0.14009305\n",
      "Iteration 5027, loss = 0.14005363\n",
      "Iteration 5028, loss = 0.14001419\n",
      "Iteration 5029, loss = 0.13997522\n",
      "Iteration 5030, loss = 0.13993506\n",
      "Iteration 5031, loss = 0.13989595\n",
      "Iteration 5032, loss = 0.13985598\n",
      "Iteration 5033, loss = 0.13981636\n",
      "Iteration 5034, loss = 0.13977760\n",
      "Iteration 5035, loss = 0.13973772\n",
      "Iteration 5036, loss = 0.13969896\n",
      "Iteration 5037, loss = 0.13965932\n",
      "Iteration 5038, loss = 0.13961956\n",
      "Iteration 5039, loss = 0.13958016\n",
      "Iteration 5040, loss = 0.13954107\n",
      "Iteration 5041, loss = 0.13950187\n",
      "Iteration 5042, loss = 0.13946277\n",
      "Iteration 5043, loss = 0.13942344\n",
      "Iteration 5044, loss = 0.13938415\n",
      "Iteration 5045, loss = 0.13934564\n",
      "Iteration 5046, loss = 0.13930619\n",
      "Iteration 5047, loss = 0.13926715\n",
      "Iteration 5048, loss = 0.13922798\n",
      "Iteration 5049, loss = 0.13918888\n",
      "Iteration 5050, loss = 0.13914999\n",
      "Iteration 5051, loss = 0.13911087\n",
      "Iteration 5052, loss = 0.13907145\n",
      "Iteration 5053, loss = 0.13903241\n",
      "Iteration 5054, loss = 0.13899382\n",
      "Iteration 5055, loss = 0.13895407\n",
      "Iteration 5056, loss = 0.13891540\n",
      "Iteration 5057, loss = 0.13887615\n",
      "Iteration 5058, loss = 0.13883733\n",
      "Iteration 5059, loss = 0.13879860\n",
      "Iteration 5060, loss = 0.13875925\n",
      "Iteration 5061, loss = 0.13872072\n",
      "Iteration 5062, loss = 0.13868197\n",
      "Iteration 5063, loss = 0.13864302\n",
      "Iteration 5064, loss = 0.13860478\n",
      "Iteration 5065, loss = 0.13856527\n",
      "Iteration 5066, loss = 0.13852701\n",
      "Iteration 5067, loss = 0.13848825\n",
      "Iteration 5068, loss = 0.13844946\n",
      "Iteration 5069, loss = 0.13841081\n",
      "Iteration 5070, loss = 0.13837212\n",
      "Iteration 5071, loss = 0.13833353\n",
      "Iteration 5072, loss = 0.13829485\n",
      "Iteration 5073, loss = 0.13825678\n",
      "Iteration 5074, loss = 0.13821744\n",
      "Iteration 5075, loss = 0.13817931\n",
      "Iteration 5076, loss = 0.13814029\n",
      "Iteration 5077, loss = 0.13810269\n",
      "Iteration 5078, loss = 0.13806345\n",
      "Iteration 5079, loss = 0.13802499\n",
      "Iteration 5080, loss = 0.13798627\n",
      "Iteration 5081, loss = 0.13794789\n",
      "Iteration 5082, loss = 0.13790915\n",
      "Iteration 5083, loss = 0.13787087\n",
      "Iteration 5084, loss = 0.13783257\n",
      "Iteration 5085, loss = 0.13779368\n",
      "Iteration 5086, loss = 0.13775554\n",
      "Iteration 5087, loss = 0.13771667\n",
      "Iteration 5088, loss = 0.13767830\n",
      "Iteration 5089, loss = 0.13764003\n",
      "Iteration 5090, loss = 0.13760143\n",
      "Iteration 5091, loss = 0.13756354\n",
      "Iteration 5092, loss = 0.13752508\n",
      "Iteration 5093, loss = 0.13748650\n",
      "Iteration 5094, loss = 0.13744880\n",
      "Iteration 5095, loss = 0.13740978\n",
      "Iteration 5096, loss = 0.13737190\n",
      "Iteration 5097, loss = 0.13733371\n",
      "Iteration 5098, loss = 0.13729556\n",
      "Iteration 5099, loss = 0.13725729\n",
      "Iteration 5100, loss = 0.13721943\n",
      "Iteration 5101, loss = 0.13718101\n",
      "Iteration 5102, loss = 0.13714267\n",
      "Iteration 5103, loss = 0.13710465\n",
      "Iteration 5104, loss = 0.13706690\n",
      "Iteration 5105, loss = 0.13702857\n",
      "Iteration 5106, loss = 0.13699100\n",
      "Iteration 5107, loss = 0.13695223\n",
      "Iteration 5108, loss = 0.13691484\n",
      "Iteration 5109, loss = 0.13687712\n",
      "Iteration 5110, loss = 0.13683936\n",
      "Iteration 5111, loss = 0.13680119\n",
      "Iteration 5112, loss = 0.13676344\n",
      "Iteration 5113, loss = 0.13672588\n",
      "Iteration 5114, loss = 0.13668784\n",
      "Iteration 5115, loss = 0.13665003\n",
      "Iteration 5116, loss = 0.13661226\n",
      "Iteration 5117, loss = 0.13657500\n",
      "Iteration 5118, loss = 0.13653722\n",
      "Iteration 5119, loss = 0.13649910\n",
      "Iteration 5120, loss = 0.13646172\n",
      "Iteration 5121, loss = 0.13642374\n",
      "Iteration 5122, loss = 0.13638563\n",
      "Iteration 5123, loss = 0.13634822\n",
      "Iteration 5124, loss = 0.13631038\n",
      "Iteration 5125, loss = 0.13627282\n",
      "Iteration 5126, loss = 0.13623565\n",
      "Iteration 5127, loss = 0.13619752\n",
      "Iteration 5128, loss = 0.13616018\n",
      "Iteration 5129, loss = 0.13612239\n",
      "Iteration 5130, loss = 0.13608434\n",
      "Iteration 5131, loss = 0.13604697\n",
      "Iteration 5132, loss = 0.13600964\n",
      "Iteration 5133, loss = 0.13597204\n",
      "Iteration 5134, loss = 0.13593468\n",
      "Iteration 5135, loss = 0.13589698\n",
      "Iteration 5136, loss = 0.13585969\n",
      "Iteration 5137, loss = 0.13582247\n",
      "Iteration 5138, loss = 0.13578491\n",
      "Iteration 5139, loss = 0.13574712\n",
      "Iteration 5140, loss = 0.13570966\n",
      "Iteration 5141, loss = 0.13567248\n",
      "Iteration 5142, loss = 0.13563527\n",
      "Iteration 5143, loss = 0.13559796\n",
      "Iteration 5144, loss = 0.13556039\n",
      "Iteration 5145, loss = 0.13552326\n",
      "Iteration 5146, loss = 0.13548624\n",
      "Iteration 5147, loss = 0.13544901\n",
      "Iteration 5148, loss = 0.13541201\n",
      "Iteration 5149, loss = 0.13537459\n",
      "Iteration 5150, loss = 0.13533695\n",
      "Iteration 5151, loss = 0.13529996\n",
      "Iteration 5152, loss = 0.13526285\n",
      "Iteration 5153, loss = 0.13522544\n",
      "Iteration 5154, loss = 0.13518855\n",
      "Iteration 5155, loss = 0.13515195\n",
      "Iteration 5156, loss = 0.13511481\n",
      "Iteration 5157, loss = 0.13507756\n",
      "Iteration 5158, loss = 0.13504075\n",
      "Iteration 5159, loss = 0.13500353\n",
      "Iteration 5160, loss = 0.13496709\n",
      "Iteration 5161, loss = 0.13492961\n",
      "Iteration 5162, loss = 0.13489248\n",
      "Iteration 5163, loss = 0.13485582\n",
      "Iteration 5164, loss = 0.13481907\n",
      "Iteration 5165, loss = 0.13478189\n",
      "Iteration 5166, loss = 0.13474507\n",
      "Iteration 5167, loss = 0.13470837\n",
      "Iteration 5168, loss = 0.13467200\n",
      "Iteration 5169, loss = 0.13463407\n",
      "Iteration 5170, loss = 0.13459796\n",
      "Iteration 5171, loss = 0.13456108\n",
      "Iteration 5172, loss = 0.13452448\n",
      "Iteration 5173, loss = 0.13448790\n",
      "Iteration 5174, loss = 0.13445079\n",
      "Iteration 5175, loss = 0.13441434\n",
      "Iteration 5176, loss = 0.13437757\n",
      "Iteration 5177, loss = 0.13434070\n",
      "Iteration 5178, loss = 0.13430426\n",
      "Iteration 5179, loss = 0.13426763\n",
      "Iteration 5180, loss = 0.13423084\n",
      "Iteration 5181, loss = 0.13419455\n",
      "Iteration 5182, loss = 0.13415787\n",
      "Iteration 5183, loss = 0.13412184\n",
      "Iteration 5184, loss = 0.13408477\n",
      "Iteration 5185, loss = 0.13404879\n",
      "Iteration 5186, loss = 0.13401242\n",
      "Iteration 5187, loss = 0.13397528\n",
      "Iteration 5188, loss = 0.13393968\n",
      "Iteration 5189, loss = 0.13390275\n",
      "Iteration 5190, loss = 0.13386691\n",
      "Iteration 5191, loss = 0.13383041\n",
      "Iteration 5192, loss = 0.13379405\n",
      "Iteration 5193, loss = 0.13375792\n",
      "Iteration 5194, loss = 0.13372165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5195, loss = 0.13368509\n",
      "Iteration 5196, loss = 0.13364916\n",
      "Iteration 5197, loss = 0.13361291\n",
      "Iteration 5198, loss = 0.13357645\n",
      "Iteration 5199, loss = 0.13354033\n",
      "Iteration 5200, loss = 0.13350421\n",
      "Iteration 5201, loss = 0.13346857\n",
      "Iteration 5202, loss = 0.13343223\n",
      "Iteration 5203, loss = 0.13339604\n",
      "Iteration 5204, loss = 0.13335964\n",
      "Iteration 5205, loss = 0.13332386\n",
      "Iteration 5206, loss = 0.13328777\n",
      "Iteration 5207, loss = 0.13325151\n",
      "Iteration 5208, loss = 0.13321567\n",
      "Iteration 5209, loss = 0.13317981\n",
      "Iteration 5210, loss = 0.13314344\n",
      "Iteration 5211, loss = 0.13310731\n",
      "Iteration 5212, loss = 0.13307165\n",
      "Iteration 5213, loss = 0.13303579\n",
      "Iteration 5214, loss = 0.13299968\n",
      "Iteration 5215, loss = 0.13296392\n",
      "Iteration 5216, loss = 0.13292768\n",
      "Iteration 5217, loss = 0.13289203\n",
      "Iteration 5218, loss = 0.13285584\n",
      "Iteration 5219, loss = 0.13282029\n",
      "Iteration 5220, loss = 0.13278441\n",
      "Iteration 5221, loss = 0.13274876\n",
      "Iteration 5222, loss = 0.13271259\n",
      "Iteration 5223, loss = 0.13267710\n",
      "Iteration 5224, loss = 0.13264143\n",
      "Iteration 5225, loss = 0.13260569\n",
      "Iteration 5226, loss = 0.13256971\n",
      "Iteration 5227, loss = 0.13253416\n",
      "Iteration 5228, loss = 0.13249875\n",
      "Iteration 5229, loss = 0.13246243\n",
      "Iteration 5230, loss = 0.13242671\n",
      "Iteration 5231, loss = 0.13239118\n",
      "Iteration 5232, loss = 0.13235607\n",
      "Iteration 5233, loss = 0.13232004\n",
      "Iteration 5234, loss = 0.13228449\n",
      "Iteration 5235, loss = 0.13224924\n",
      "Iteration 5236, loss = 0.13221354\n",
      "Iteration 5237, loss = 0.13217799\n",
      "Iteration 5238, loss = 0.13214234\n",
      "Iteration 5239, loss = 0.13210657\n",
      "Iteration 5240, loss = 0.13207130\n",
      "Iteration 5241, loss = 0.13203564\n",
      "Iteration 5242, loss = 0.13199973\n",
      "Iteration 5243, loss = 0.13196443\n",
      "Iteration 5244, loss = 0.13192923\n",
      "Iteration 5245, loss = 0.13189407\n",
      "Iteration 5246, loss = 0.13185871\n",
      "Iteration 5247, loss = 0.13182275\n",
      "Iteration 5248, loss = 0.13178764\n",
      "Iteration 5249, loss = 0.13175230\n",
      "Iteration 5250, loss = 0.13171718\n",
      "Iteration 5251, loss = 0.13168171\n",
      "Iteration 5252, loss = 0.13164657\n",
      "Iteration 5253, loss = 0.13161106\n",
      "Iteration 5254, loss = 0.13157588\n",
      "Iteration 5255, loss = 0.13154084\n",
      "Iteration 5256, loss = 0.13150550\n",
      "Iteration 5257, loss = 0.13147004\n",
      "Iteration 5258, loss = 0.13143566\n",
      "Iteration 5259, loss = 0.13139955\n",
      "Iteration 5260, loss = 0.13136475\n",
      "Iteration 5261, loss = 0.13132984\n",
      "Iteration 5262, loss = 0.13129436\n",
      "Iteration 5263, loss = 0.13125962\n",
      "Iteration 5264, loss = 0.13122469\n",
      "Iteration 5265, loss = 0.13118948\n",
      "Iteration 5266, loss = 0.13115445\n",
      "Iteration 5267, loss = 0.13111961\n",
      "Iteration 5268, loss = 0.13108475\n",
      "Iteration 5269, loss = 0.13104950\n",
      "Iteration 5270, loss = 0.13101466\n",
      "Iteration 5271, loss = 0.13097952\n",
      "Iteration 5272, loss = 0.13094465\n",
      "Iteration 5273, loss = 0.13090969\n",
      "Iteration 5274, loss = 0.13087448\n",
      "Iteration 5275, loss = 0.13084000\n",
      "Iteration 5276, loss = 0.13080480\n",
      "Iteration 5277, loss = 0.13077025\n",
      "Iteration 5278, loss = 0.13073490\n",
      "Iteration 5279, loss = 0.13070040\n",
      "Iteration 5280, loss = 0.13066554\n",
      "Iteration 5281, loss = 0.13063103\n",
      "Iteration 5282, loss = 0.13059613\n",
      "Iteration 5283, loss = 0.13056176\n",
      "Iteration 5284, loss = 0.13052688\n",
      "Iteration 5285, loss = 0.13049240\n",
      "Iteration 5286, loss = 0.13045768\n",
      "Iteration 5287, loss = 0.13042295\n",
      "Iteration 5288, loss = 0.13038795\n",
      "Iteration 5289, loss = 0.13035331\n",
      "Iteration 5290, loss = 0.13031855\n",
      "Iteration 5291, loss = 0.13028371\n",
      "Iteration 5292, loss = 0.13024914\n",
      "Iteration 5293, loss = 0.13021452\n",
      "Iteration 5294, loss = 0.13017957\n",
      "Iteration 5295, loss = 0.13014527\n",
      "Iteration 5296, loss = 0.13011043\n",
      "Iteration 5297, loss = 0.13007628\n",
      "Iteration 5298, loss = 0.13004128\n",
      "Iteration 5299, loss = 0.13000669\n",
      "Iteration 5300, loss = 0.12997218\n",
      "Iteration 5301, loss = 0.12993815\n",
      "Iteration 5302, loss = 0.12990321\n",
      "Iteration 5303, loss = 0.12986900\n",
      "Iteration 5304, loss = 0.12983488\n",
      "Iteration 5305, loss = 0.12979996\n",
      "Iteration 5306, loss = 0.12976569\n",
      "Iteration 5307, loss = 0.12973143\n",
      "Iteration 5308, loss = 0.12969682\n",
      "Iteration 5309, loss = 0.12966256\n",
      "Iteration 5310, loss = 0.12962869\n",
      "Iteration 5311, loss = 0.12959424\n",
      "Iteration 5312, loss = 0.12955943\n",
      "Iteration 5313, loss = 0.12952543\n",
      "Iteration 5314, loss = 0.12949084\n",
      "Iteration 5315, loss = 0.12945688\n",
      "Iteration 5316, loss = 0.12942256\n",
      "Iteration 5317, loss = 0.12938860\n",
      "Iteration 5318, loss = 0.12935445\n",
      "Iteration 5319, loss = 0.12931997\n",
      "Iteration 5320, loss = 0.12928568\n",
      "Iteration 5321, loss = 0.12925171\n",
      "Iteration 5322, loss = 0.12921770\n",
      "Iteration 5323, loss = 0.12918365\n",
      "Iteration 5324, loss = 0.12914934\n",
      "Iteration 5325, loss = 0.12911546\n",
      "Iteration 5326, loss = 0.12908158\n",
      "Iteration 5327, loss = 0.12904743\n",
      "Iteration 5328, loss = 0.12901340\n",
      "Iteration 5329, loss = 0.12897929\n",
      "Iteration 5330, loss = 0.12894590\n",
      "Iteration 5331, loss = 0.12891153\n",
      "Iteration 5332, loss = 0.12887813\n",
      "Iteration 5333, loss = 0.12884352\n",
      "Iteration 5334, loss = 0.12880982\n",
      "Iteration 5335, loss = 0.12877606\n",
      "Iteration 5336, loss = 0.12874248\n",
      "Iteration 5337, loss = 0.12870844\n",
      "Iteration 5338, loss = 0.12867441\n",
      "Iteration 5339, loss = 0.12864108\n",
      "Iteration 5340, loss = 0.12860741\n",
      "Iteration 5341, loss = 0.12857333\n",
      "Iteration 5342, loss = 0.12853979\n",
      "Iteration 5343, loss = 0.12850599\n",
      "Iteration 5344, loss = 0.12847287\n",
      "Iteration 5345, loss = 0.12843953\n",
      "Iteration 5346, loss = 0.12840540\n",
      "Iteration 5347, loss = 0.12837161\n",
      "Iteration 5348, loss = 0.12833847\n",
      "Iteration 5349, loss = 0.12830508\n",
      "Iteration 5350, loss = 0.12827154\n",
      "Iteration 5351, loss = 0.12823784\n",
      "Iteration 5352, loss = 0.12820406\n",
      "Iteration 5353, loss = 0.12817090\n",
      "Iteration 5354, loss = 0.12813723\n",
      "Iteration 5355, loss = 0.12810387\n",
      "Iteration 5356, loss = 0.12807046\n",
      "Iteration 5357, loss = 0.12803710\n",
      "Iteration 5358, loss = 0.12800338\n",
      "Iteration 5359, loss = 0.12797003\n",
      "Iteration 5360, loss = 0.12793639\n",
      "Iteration 5361, loss = 0.12790312\n",
      "Iteration 5362, loss = 0.12786991\n",
      "Iteration 5363, loss = 0.12783653\n",
      "Iteration 5364, loss = 0.12780290\n",
      "Iteration 5365, loss = 0.12777011\n",
      "Iteration 5366, loss = 0.12773627\n",
      "Iteration 5367, loss = 0.12770332\n",
      "Iteration 5368, loss = 0.12766999\n",
      "Iteration 5369, loss = 0.12763674\n",
      "Iteration 5370, loss = 0.12760355\n",
      "Iteration 5371, loss = 0.12757035\n",
      "Iteration 5372, loss = 0.12753734\n",
      "Iteration 5373, loss = 0.12750466\n",
      "Iteration 5374, loss = 0.12747092\n",
      "Iteration 5375, loss = 0.12743783\n",
      "Iteration 5376, loss = 0.12740472\n",
      "Iteration 5377, loss = 0.12737170\n",
      "Iteration 5378, loss = 0.12733837\n",
      "Iteration 5379, loss = 0.12730551\n",
      "Iteration 5380, loss = 0.12727237\n",
      "Iteration 5381, loss = 0.12723910\n",
      "Iteration 5382, loss = 0.12720618\n",
      "Iteration 5383, loss = 0.12717305\n",
      "Iteration 5384, loss = 0.12713980\n",
      "Iteration 5385, loss = 0.12710726\n",
      "Iteration 5386, loss = 0.12707406\n",
      "Iteration 5387, loss = 0.12704138\n",
      "Iteration 5388, loss = 0.12700832\n",
      "Iteration 5389, loss = 0.12697578\n",
      "Iteration 5390, loss = 0.12694213\n",
      "Iteration 5391, loss = 0.12690934\n",
      "Iteration 5392, loss = 0.12687651\n",
      "Iteration 5393, loss = 0.12684382\n",
      "Iteration 5394, loss = 0.12681054\n",
      "Iteration 5395, loss = 0.12677813\n",
      "Iteration 5396, loss = 0.12674505\n",
      "Iteration 5397, loss = 0.12671243\n",
      "Iteration 5398, loss = 0.12667931\n",
      "Iteration 5399, loss = 0.12664661\n",
      "Iteration 5400, loss = 0.12661412\n",
      "Iteration 5401, loss = 0.12658131\n",
      "Iteration 5402, loss = 0.12654816\n",
      "Iteration 5403, loss = 0.12651582\n",
      "Iteration 5404, loss = 0.12648298\n",
      "Iteration 5405, loss = 0.12645042\n",
      "Iteration 5406, loss = 0.12641773\n",
      "Iteration 5407, loss = 0.12638470\n",
      "Iteration 5408, loss = 0.12635240\n",
      "Iteration 5409, loss = 0.12631957\n",
      "Iteration 5410, loss = 0.12628712\n",
      "Iteration 5411, loss = 0.12625481\n",
      "Iteration 5412, loss = 0.12622180\n",
      "Iteration 5413, loss = 0.12618883\n",
      "Iteration 5414, loss = 0.12615688\n",
      "Iteration 5415, loss = 0.12612410\n",
      "Iteration 5416, loss = 0.12609156\n",
      "Iteration 5417, loss = 0.12605904\n",
      "Iteration 5418, loss = 0.12602692\n",
      "Iteration 5419, loss = 0.12599434\n",
      "Iteration 5420, loss = 0.12596199\n",
      "Iteration 5421, loss = 0.12592950\n",
      "Iteration 5422, loss = 0.12589739\n",
      "Iteration 5423, loss = 0.12586470\n",
      "Iteration 5424, loss = 0.12583230\n",
      "Iteration 5425, loss = 0.12579986\n",
      "Iteration 5426, loss = 0.12576739\n",
      "Iteration 5427, loss = 0.12573537\n",
      "Iteration 5428, loss = 0.12570281\n",
      "Iteration 5429, loss = 0.12567039\n",
      "Iteration 5430, loss = 0.12563807\n",
      "Iteration 5431, loss = 0.12560576\n",
      "Iteration 5432, loss = 0.12557340\n",
      "Iteration 5433, loss = 0.12554137\n",
      "Iteration 5434, loss = 0.12550875\n",
      "Iteration 5435, loss = 0.12547633\n",
      "Iteration 5436, loss = 0.12544426\n",
      "Iteration 5437, loss = 0.12541226\n",
      "Iteration 5438, loss = 0.12537964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5439, loss = 0.12534803\n",
      "Iteration 5440, loss = 0.12531539\n",
      "Iteration 5441, loss = 0.12528326\n",
      "Iteration 5442, loss = 0.12525136\n",
      "Iteration 5443, loss = 0.12521913\n",
      "Iteration 5444, loss = 0.12518696\n",
      "Iteration 5445, loss = 0.12515466\n",
      "Iteration 5446, loss = 0.12512229\n",
      "Iteration 5447, loss = 0.12509073\n",
      "Iteration 5448, loss = 0.12505804\n",
      "Iteration 5449, loss = 0.12502643\n",
      "Iteration 5450, loss = 0.12499386\n",
      "Iteration 5451, loss = 0.12496180\n",
      "Iteration 5452, loss = 0.12493010\n",
      "Iteration 5453, loss = 0.12489818\n",
      "Iteration 5454, loss = 0.12486606\n",
      "Iteration 5455, loss = 0.12483403\n",
      "Iteration 5456, loss = 0.12480215\n",
      "Iteration 5457, loss = 0.12477026\n",
      "Iteration 5458, loss = 0.12473880\n",
      "Iteration 5459, loss = 0.12470602\n",
      "Iteration 5460, loss = 0.12467484\n",
      "Iteration 5461, loss = 0.12464281\n",
      "Iteration 5462, loss = 0.12461119\n",
      "Iteration 5463, loss = 0.12457963\n",
      "Iteration 5464, loss = 0.12454775\n",
      "Iteration 5465, loss = 0.12451622\n",
      "Iteration 5466, loss = 0.12448445\n",
      "Iteration 5467, loss = 0.12445298\n",
      "Iteration 5468, loss = 0.12442135\n",
      "Iteration 5469, loss = 0.12438970\n",
      "Iteration 5470, loss = 0.12435802\n",
      "Iteration 5471, loss = 0.12432671\n",
      "Iteration 5472, loss = 0.12429513\n",
      "Iteration 5473, loss = 0.12426314\n",
      "Iteration 5474, loss = 0.12423139\n",
      "Iteration 5475, loss = 0.12419992\n",
      "Iteration 5476, loss = 0.12416817\n",
      "Iteration 5477, loss = 0.12413656\n",
      "Iteration 5478, loss = 0.12410451\n",
      "Iteration 5479, loss = 0.12407327\n",
      "Iteration 5480, loss = 0.12404174\n",
      "Iteration 5481, loss = 0.12401000\n",
      "Iteration 5482, loss = 0.12397837\n",
      "Iteration 5483, loss = 0.12394671\n",
      "Iteration 5484, loss = 0.12391545\n",
      "Iteration 5485, loss = 0.12388368\n",
      "Iteration 5486, loss = 0.12385222\n",
      "Iteration 5487, loss = 0.12382055\n",
      "Iteration 5488, loss = 0.12378916\n",
      "Iteration 5489, loss = 0.12375744\n",
      "Iteration 5490, loss = 0.12372649\n",
      "Iteration 5491, loss = 0.12369485\n",
      "Iteration 5492, loss = 0.12366332\n",
      "Iteration 5493, loss = 0.12363187\n",
      "Iteration 5494, loss = 0.12360078\n",
      "Iteration 5495, loss = 0.12356913\n",
      "Iteration 5496, loss = 0.12353791\n",
      "Iteration 5497, loss = 0.12350698\n",
      "Iteration 5498, loss = 0.12347500\n",
      "Iteration 5499, loss = 0.12344390\n",
      "Iteration 5500, loss = 0.12341282\n",
      "Iteration 5501, loss = 0.12338219\n",
      "Iteration 5502, loss = 0.12335022\n",
      "Iteration 5503, loss = 0.12331913\n",
      "Iteration 5504, loss = 0.12328761\n",
      "Iteration 5505, loss = 0.12325676\n",
      "Iteration 5506, loss = 0.12322529\n",
      "Iteration 5507, loss = 0.12319397\n",
      "Iteration 5508, loss = 0.12316326\n",
      "Iteration 5509, loss = 0.12313194\n",
      "Iteration 5510, loss = 0.12310115\n",
      "Iteration 5511, loss = 0.12306957\n",
      "Iteration 5512, loss = 0.12303848\n",
      "Iteration 5513, loss = 0.12300777\n",
      "Iteration 5514, loss = 0.12297636\n",
      "Iteration 5515, loss = 0.12294598\n",
      "Iteration 5516, loss = 0.12291462\n",
      "Iteration 5517, loss = 0.12288377\n",
      "Iteration 5518, loss = 0.12285290\n",
      "Iteration 5519, loss = 0.12282159\n",
      "Iteration 5520, loss = 0.12279081\n",
      "Iteration 5521, loss = 0.12275966\n",
      "Iteration 5522, loss = 0.12272882\n",
      "Iteration 5523, loss = 0.12269801\n",
      "Iteration 5524, loss = 0.12266688\n",
      "Iteration 5525, loss = 0.12263646\n",
      "Iteration 5526, loss = 0.12260551\n",
      "Iteration 5527, loss = 0.12257428\n",
      "Iteration 5528, loss = 0.12254342\n",
      "Iteration 5529, loss = 0.12251293\n",
      "Iteration 5530, loss = 0.12248190\n",
      "Iteration 5531, loss = 0.12245125\n",
      "Iteration 5532, loss = 0.12242024\n",
      "Iteration 5533, loss = 0.12238959\n",
      "Iteration 5534, loss = 0.12235901\n",
      "Iteration 5535, loss = 0.12232835\n",
      "Iteration 5536, loss = 0.12229778\n",
      "Iteration 5537, loss = 0.12226659\n",
      "Iteration 5538, loss = 0.12223628\n",
      "Iteration 5539, loss = 0.12220542\n",
      "Iteration 5540, loss = 0.12217492\n",
      "Iteration 5541, loss = 0.12214448\n",
      "Iteration 5542, loss = 0.12211338\n",
      "Iteration 5543, loss = 0.12208320\n",
      "Iteration 5544, loss = 0.12205267\n",
      "Iteration 5545, loss = 0.12202205\n",
      "Iteration 5546, loss = 0.12199143\n",
      "Iteration 5547, loss = 0.12196049\n",
      "Iteration 5548, loss = 0.12192988\n",
      "Iteration 5549, loss = 0.12189932\n",
      "Iteration 5550, loss = 0.12186900\n",
      "Iteration 5551, loss = 0.12183812\n",
      "Iteration 5552, loss = 0.12180789\n",
      "Iteration 5553, loss = 0.12177753\n",
      "Iteration 5554, loss = 0.12174707\n",
      "Iteration 5555, loss = 0.12171643\n",
      "Iteration 5556, loss = 0.12168617\n",
      "Iteration 5557, loss = 0.12165570\n",
      "Iteration 5558, loss = 0.12162519\n",
      "Iteration 5559, loss = 0.12159500\n",
      "Iteration 5560, loss = 0.12156495\n",
      "Iteration 5561, loss = 0.12153444\n",
      "Iteration 5562, loss = 0.12150375\n",
      "Iteration 5563, loss = 0.12147331\n",
      "Iteration 5564, loss = 0.12144321\n",
      "Iteration 5565, loss = 0.12141288\n",
      "Iteration 5566, loss = 0.12138222\n",
      "Iteration 5567, loss = 0.12135206\n",
      "Iteration 5568, loss = 0.12132164\n",
      "Iteration 5569, loss = 0.12129154\n",
      "Iteration 5570, loss = 0.12126133\n",
      "Iteration 5571, loss = 0.12123102\n",
      "Iteration 5572, loss = 0.12120084\n",
      "Iteration 5573, loss = 0.12117077\n",
      "Iteration 5574, loss = 0.12114058\n",
      "Iteration 5575, loss = 0.12111034\n",
      "Iteration 5576, loss = 0.12108046\n",
      "Iteration 5577, loss = 0.12104999\n",
      "Iteration 5578, loss = 0.12102034\n",
      "Iteration 5579, loss = 0.12099005\n",
      "Iteration 5580, loss = 0.12096025\n",
      "Iteration 5581, loss = 0.12092957\n",
      "Iteration 5582, loss = 0.12089998\n",
      "Iteration 5583, loss = 0.12086990\n",
      "Iteration 5584, loss = 0.12083970\n",
      "Iteration 5585, loss = 0.12080975\n",
      "Iteration 5586, loss = 0.12077981\n",
      "Iteration 5587, loss = 0.12074983\n",
      "Iteration 5588, loss = 0.12071950\n",
      "Iteration 5589, loss = 0.12068984\n",
      "Iteration 5590, loss = 0.12065974\n",
      "Iteration 5591, loss = 0.12063027\n",
      "Iteration 5592, loss = 0.12060012\n",
      "Iteration 5593, loss = 0.12057022\n",
      "Iteration 5594, loss = 0.12054041\n",
      "Iteration 5595, loss = 0.12051073\n",
      "Iteration 5596, loss = 0.12048089\n",
      "Iteration 5597, loss = 0.12045075\n",
      "Iteration 5598, loss = 0.12042100\n",
      "Iteration 5599, loss = 0.12039123\n",
      "Iteration 5600, loss = 0.12036158\n",
      "Iteration 5601, loss = 0.12033166\n",
      "Iteration 5602, loss = 0.12030201\n",
      "Iteration 5603, loss = 0.12027195\n",
      "Iteration 5604, loss = 0.12024205\n",
      "Iteration 5605, loss = 0.12021264\n",
      "Iteration 5606, loss = 0.12018281\n",
      "Iteration 5607, loss = 0.12015327\n",
      "Iteration 5608, loss = 0.12012368\n",
      "Iteration 5609, loss = 0.12009370\n",
      "Iteration 5610, loss = 0.12006433\n",
      "Iteration 5611, loss = 0.12003476\n",
      "Iteration 5612, loss = 0.12000486\n",
      "Iteration 5613, loss = 0.11997543\n",
      "Iteration 5614, loss = 0.11994605\n",
      "Iteration 5615, loss = 0.11991613\n",
      "Iteration 5616, loss = 0.11988662\n",
      "Iteration 5617, loss = 0.11985676\n",
      "Iteration 5618, loss = 0.11982734\n",
      "Iteration 5619, loss = 0.11979742\n",
      "Iteration 5620, loss = 0.11976778\n",
      "Iteration 5621, loss = 0.11973805\n",
      "Iteration 5622, loss = 0.11970893\n",
      "Iteration 5623, loss = 0.11967905\n",
      "Iteration 5624, loss = 0.11964957\n",
      "Iteration 5625, loss = 0.11962010\n",
      "Iteration 5626, loss = 0.11959030\n",
      "Iteration 5627, loss = 0.11956074\n",
      "Iteration 5628, loss = 0.11953121\n",
      "Iteration 5629, loss = 0.11950156\n",
      "Iteration 5630, loss = 0.11947242\n",
      "Iteration 5631, loss = 0.11944287\n",
      "Iteration 5632, loss = 0.11941341\n",
      "Iteration 5633, loss = 0.11938403\n",
      "Iteration 5634, loss = 0.11935470\n",
      "Iteration 5635, loss = 0.11932508\n",
      "Iteration 5636, loss = 0.11929566\n",
      "Iteration 5637, loss = 0.11926685\n",
      "Iteration 5638, loss = 0.11923733\n",
      "Iteration 5639, loss = 0.11920804\n",
      "Iteration 5640, loss = 0.11917866\n",
      "Iteration 5641, loss = 0.11914945\n",
      "Iteration 5642, loss = 0.11912040\n",
      "Iteration 5643, loss = 0.11909136\n",
      "Iteration 5644, loss = 0.11906248\n",
      "Iteration 5645, loss = 0.11903277\n",
      "Iteration 5646, loss = 0.11900380\n",
      "Iteration 5647, loss = 0.11897467\n",
      "Iteration 5648, loss = 0.11894574\n",
      "Iteration 5649, loss = 0.11891658\n",
      "Iteration 5650, loss = 0.11888716\n",
      "Iteration 5651, loss = 0.11885852\n",
      "Iteration 5652, loss = 0.11882902\n",
      "Iteration 5653, loss = 0.11880012\n",
      "Iteration 5654, loss = 0.11877113\n",
      "Iteration 5655, loss = 0.11874218\n",
      "Iteration 5656, loss = 0.11871352\n",
      "Iteration 5657, loss = 0.11868452\n",
      "Iteration 5658, loss = 0.11865531\n",
      "Iteration 5659, loss = 0.11862672\n",
      "Iteration 5660, loss = 0.11859756\n",
      "Iteration 5661, loss = 0.11856872\n",
      "Iteration 5662, loss = 0.11853977\n",
      "Iteration 5663, loss = 0.11851078\n",
      "Iteration 5664, loss = 0.11848170\n",
      "Iteration 5665, loss = 0.11845284\n",
      "Iteration 5666, loss = 0.11842439\n",
      "Iteration 5667, loss = 0.11839501\n",
      "Iteration 5668, loss = 0.11836634\n",
      "Iteration 5669, loss = 0.11833767\n",
      "Iteration 5670, loss = 0.11830842\n",
      "Iteration 5671, loss = 0.11828005\n",
      "Iteration 5672, loss = 0.11825104\n",
      "Iteration 5673, loss = 0.11822209\n",
      "Iteration 5674, loss = 0.11819357\n",
      "Iteration 5675, loss = 0.11816426\n",
      "Iteration 5676, loss = 0.11813601\n",
      "Iteration 5677, loss = 0.11810683\n",
      "Iteration 5678, loss = 0.11807814\n",
      "Iteration 5679, loss = 0.11804936\n",
      "Iteration 5680, loss = 0.11802075\n",
      "Iteration 5681, loss = 0.11799194\n",
      "Iteration 5682, loss = 0.11796371\n",
      "Iteration 5683, loss = 0.11793473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5684, loss = 0.11790591\n",
      "Iteration 5685, loss = 0.11787732\n",
      "Iteration 5686, loss = 0.11784861\n",
      "Iteration 5687, loss = 0.11782011\n",
      "Iteration 5688, loss = 0.11779143\n",
      "Iteration 5689, loss = 0.11776316\n",
      "Iteration 5690, loss = 0.11773420\n",
      "Iteration 5691, loss = 0.11770596\n",
      "Iteration 5692, loss = 0.11767764\n",
      "Iteration 5693, loss = 0.11764866\n",
      "Iteration 5694, loss = 0.11762014\n",
      "Iteration 5695, loss = 0.11759159\n",
      "Iteration 5696, loss = 0.11756384\n",
      "Iteration 5697, loss = 0.11753472\n",
      "Iteration 5698, loss = 0.11750635\n",
      "Iteration 5699, loss = 0.11747730\n",
      "Iteration 5700, loss = 0.11744922\n",
      "Iteration 5701, loss = 0.11742105\n",
      "Iteration 5702, loss = 0.11739219\n",
      "Iteration 5703, loss = 0.11736402\n",
      "Iteration 5704, loss = 0.11733595\n",
      "Iteration 5705, loss = 0.11730727\n",
      "Iteration 5706, loss = 0.11727908\n",
      "Iteration 5707, loss = 0.11725078\n",
      "Iteration 5708, loss = 0.11722221\n",
      "Iteration 5709, loss = 0.11719424\n",
      "Iteration 5710, loss = 0.11716614\n",
      "Iteration 5711, loss = 0.11713775\n",
      "Iteration 5712, loss = 0.11710977\n",
      "Iteration 5713, loss = 0.11708121\n",
      "Iteration 5714, loss = 0.11705302\n",
      "Iteration 5715, loss = 0.11702481\n",
      "Iteration 5716, loss = 0.11699664\n",
      "Iteration 5717, loss = 0.11696827\n",
      "Iteration 5718, loss = 0.11694005\n",
      "Iteration 5719, loss = 0.11691172\n",
      "Iteration 5720, loss = 0.11688381\n",
      "Iteration 5721, loss = 0.11685583\n",
      "Iteration 5722, loss = 0.11682736\n",
      "Iteration 5723, loss = 0.11679919\n",
      "Iteration 5724, loss = 0.11677082\n",
      "Iteration 5725, loss = 0.11674293\n",
      "Iteration 5726, loss = 0.11671478\n",
      "Iteration 5727, loss = 0.11668651\n",
      "Iteration 5728, loss = 0.11665829\n",
      "Iteration 5729, loss = 0.11663029\n",
      "Iteration 5730, loss = 0.11660224\n",
      "Iteration 5731, loss = 0.11657441\n",
      "Iteration 5732, loss = 0.11654599\n",
      "Iteration 5733, loss = 0.11651790\n",
      "Iteration 5734, loss = 0.11649030\n",
      "Iteration 5735, loss = 0.11646229\n",
      "Iteration 5736, loss = 0.11643438\n",
      "Iteration 5737, loss = 0.11640624\n",
      "Iteration 5738, loss = 0.11637848\n",
      "Iteration 5739, loss = 0.11635060\n",
      "Iteration 5740, loss = 0.11632277\n",
      "Iteration 5741, loss = 0.11629430\n",
      "Iteration 5742, loss = 0.11626639\n",
      "Iteration 5743, loss = 0.11623871\n",
      "Iteration 5744, loss = 0.11621081\n",
      "Iteration 5745, loss = 0.11618281\n",
      "Iteration 5746, loss = 0.11615542\n",
      "Iteration 5747, loss = 0.11612704\n",
      "Iteration 5748, loss = 0.11609927\n",
      "Iteration 5749, loss = 0.11607145\n",
      "Iteration 5750, loss = 0.11604360\n",
      "Iteration 5751, loss = 0.11601562\n",
      "Iteration 5752, loss = 0.11598796\n",
      "Iteration 5753, loss = 0.11596030\n",
      "Iteration 5754, loss = 0.11593249\n",
      "Iteration 5755, loss = 0.11590470\n",
      "Iteration 5756, loss = 0.11587710\n",
      "Iteration 5757, loss = 0.11584922\n",
      "Iteration 5758, loss = 0.11582137\n",
      "Iteration 5759, loss = 0.11579348\n",
      "Iteration 5760, loss = 0.11576594\n",
      "Iteration 5761, loss = 0.11573833\n",
      "Iteration 5762, loss = 0.11571050\n",
      "Iteration 5763, loss = 0.11568281\n",
      "Iteration 5764, loss = 0.11565546\n",
      "Iteration 5765, loss = 0.11562810\n",
      "Iteration 5766, loss = 0.11560029\n",
      "Iteration 5767, loss = 0.11557251\n",
      "Iteration 5768, loss = 0.11554519\n",
      "Iteration 5769, loss = 0.11551727\n",
      "Iteration 5770, loss = 0.11549026\n",
      "Iteration 5771, loss = 0.11546226\n",
      "Iteration 5772, loss = 0.11543501\n",
      "Iteration 5773, loss = 0.11540758\n",
      "Iteration 5774, loss = 0.11537977\n",
      "Iteration 5775, loss = 0.11535250\n",
      "Iteration 5776, loss = 0.11532535\n",
      "Iteration 5777, loss = 0.11529736\n",
      "Iteration 5778, loss = 0.11526965\n",
      "Iteration 5779, loss = 0.11524267\n",
      "Iteration 5780, loss = 0.11521498\n",
      "Iteration 5781, loss = 0.11518751\n",
      "Iteration 5782, loss = 0.11515994\n",
      "Iteration 5783, loss = 0.11513237\n",
      "Iteration 5784, loss = 0.11510459\n",
      "Iteration 5785, loss = 0.11507773\n",
      "Iteration 5786, loss = 0.11505029\n",
      "Iteration 5787, loss = 0.11502285\n",
      "Iteration 5788, loss = 0.11499559\n",
      "Iteration 5789, loss = 0.11496829\n",
      "Iteration 5790, loss = 0.11494088\n",
      "Iteration 5791, loss = 0.11491366\n",
      "Iteration 5792, loss = 0.11488669\n",
      "Iteration 5793, loss = 0.11485892\n",
      "Iteration 5794, loss = 0.11483178\n",
      "Iteration 5795, loss = 0.11480436\n",
      "Iteration 5796, loss = 0.11477701\n",
      "Iteration 5797, loss = 0.11475063\n",
      "Iteration 5798, loss = 0.11472300\n",
      "Iteration 5799, loss = 0.11469513\n",
      "Iteration 5800, loss = 0.11466804\n",
      "Iteration 5801, loss = 0.11464092\n",
      "Iteration 5802, loss = 0.11461359\n",
      "Iteration 5803, loss = 0.11458679\n",
      "Iteration 5804, loss = 0.11455916\n",
      "Iteration 5805, loss = 0.11453202\n",
      "Iteration 5806, loss = 0.11450499\n",
      "Iteration 5807, loss = 0.11447779\n",
      "Iteration 5808, loss = 0.11445105\n",
      "Iteration 5809, loss = 0.11442400\n",
      "Iteration 5810, loss = 0.11439650\n",
      "Iteration 5811, loss = 0.11436964\n",
      "Iteration 5812, loss = 0.11434264\n",
      "Iteration 5813, loss = 0.11431508\n",
      "Iteration 5814, loss = 0.11428838\n",
      "Iteration 5815, loss = 0.11426194\n",
      "Iteration 5816, loss = 0.11423432\n",
      "Iteration 5817, loss = 0.11420736\n",
      "Iteration 5818, loss = 0.11418042\n",
      "Iteration 5819, loss = 0.11415358\n",
      "Iteration 5820, loss = 0.11412678\n",
      "Iteration 5821, loss = 0.11409936\n",
      "Iteration 5822, loss = 0.11407269\n",
      "Iteration 5823, loss = 0.11404531\n",
      "Iteration 5824, loss = 0.11401875\n",
      "Iteration 5825, loss = 0.11399154\n",
      "Iteration 5826, loss = 0.11396449\n",
      "Iteration 5827, loss = 0.11393780\n",
      "Iteration 5828, loss = 0.11391089\n",
      "Iteration 5829, loss = 0.11388403\n",
      "Iteration 5830, loss = 0.11385683\n",
      "Iteration 5831, loss = 0.11383038\n",
      "Iteration 5832, loss = 0.11380288\n",
      "Iteration 5833, loss = 0.11377635\n",
      "Iteration 5834, loss = 0.11374921\n",
      "Iteration 5835, loss = 0.11372240\n",
      "Iteration 5836, loss = 0.11369559\n",
      "Iteration 5837, loss = 0.11366873\n",
      "Iteration 5838, loss = 0.11364197\n",
      "Iteration 5839, loss = 0.11361491\n",
      "Iteration 5840, loss = 0.11358808\n",
      "Iteration 5841, loss = 0.11356168\n",
      "Iteration 5842, loss = 0.11353461\n",
      "Iteration 5843, loss = 0.11350769\n",
      "Iteration 5844, loss = 0.11348118\n",
      "Iteration 5845, loss = 0.11345420\n",
      "Iteration 5846, loss = 0.11342731\n",
      "Iteration 5847, loss = 0.11340061\n",
      "Iteration 5848, loss = 0.11337385\n",
      "Iteration 5849, loss = 0.11334748\n",
      "Iteration 5850, loss = 0.11332086\n",
      "Iteration 5851, loss = 0.11329408\n",
      "Iteration 5852, loss = 0.11326737\n",
      "Iteration 5853, loss = 0.11324046\n",
      "Iteration 5854, loss = 0.11321385\n",
      "Iteration 5855, loss = 0.11318725\n",
      "Iteration 5856, loss = 0.11316059\n",
      "Iteration 5857, loss = 0.11313353\n",
      "Iteration 5858, loss = 0.11310748\n",
      "Iteration 5859, loss = 0.11308073\n",
      "Iteration 5860, loss = 0.11305423\n",
      "Iteration 5861, loss = 0.11302747\n",
      "Iteration 5862, loss = 0.11300072\n",
      "Iteration 5863, loss = 0.11297434\n",
      "Iteration 5864, loss = 0.11294770\n",
      "Iteration 5865, loss = 0.11292102\n",
      "Iteration 5866, loss = 0.11289408\n",
      "Iteration 5867, loss = 0.11286784\n",
      "Iteration 5868, loss = 0.11284136\n",
      "Iteration 5869, loss = 0.11281511\n",
      "Iteration 5870, loss = 0.11278851\n",
      "Iteration 5871, loss = 0.11276190\n",
      "Iteration 5872, loss = 0.11273550\n",
      "Iteration 5873, loss = 0.11270909\n",
      "Iteration 5874, loss = 0.11268268\n",
      "Iteration 5875, loss = 0.11265619\n",
      "Iteration 5876, loss = 0.11263009\n",
      "Iteration 5877, loss = 0.11260383\n",
      "Iteration 5878, loss = 0.11257776\n",
      "Iteration 5879, loss = 0.11255083\n",
      "Iteration 5880, loss = 0.11252488\n",
      "Iteration 5881, loss = 0.11249856\n",
      "Iteration 5882, loss = 0.11247246\n",
      "Iteration 5883, loss = 0.11244568\n",
      "Iteration 5884, loss = 0.11241988\n",
      "Iteration 5885, loss = 0.11239344\n",
      "Iteration 5886, loss = 0.11236794\n",
      "Iteration 5887, loss = 0.11234094\n",
      "Iteration 5888, loss = 0.11231530\n",
      "Iteration 5889, loss = 0.11228922\n",
      "Iteration 5890, loss = 0.11226272\n",
      "Iteration 5891, loss = 0.11223633\n",
      "Iteration 5892, loss = 0.11221040\n",
      "Iteration 5893, loss = 0.11218429\n",
      "Iteration 5894, loss = 0.11215833\n",
      "Iteration 5895, loss = 0.11213242\n",
      "Iteration 5896, loss = 0.11210572\n",
      "Iteration 5897, loss = 0.11207990\n",
      "Iteration 5898, loss = 0.11205368\n",
      "Iteration 5899, loss = 0.11202772\n",
      "Iteration 5900, loss = 0.11200141\n",
      "Iteration 5901, loss = 0.11197559\n",
      "Iteration 5902, loss = 0.11194920\n",
      "Iteration 5903, loss = 0.11192324\n",
      "Iteration 5904, loss = 0.11189740\n",
      "Iteration 5905, loss = 0.11187116\n",
      "Iteration 5906, loss = 0.11184503\n",
      "Iteration 5907, loss = 0.11181888\n",
      "Iteration 5908, loss = 0.11179280\n",
      "Iteration 5909, loss = 0.11176689\n",
      "Iteration 5910, loss = 0.11174089\n",
      "Iteration 5911, loss = 0.11171503\n",
      "Iteration 5912, loss = 0.11168870\n",
      "Iteration 5913, loss = 0.11166304\n",
      "Iteration 5914, loss = 0.11163666\n",
      "Iteration 5915, loss = 0.11161106\n",
      "Iteration 5916, loss = 0.11158515\n",
      "Iteration 5917, loss = 0.11155921\n",
      "Iteration 5918, loss = 0.11153329\n",
      "Iteration 5919, loss = 0.11150730\n",
      "Iteration 5920, loss = 0.11148148\n",
      "Iteration 5921, loss = 0.11145574\n",
      "Iteration 5922, loss = 0.11142969\n",
      "Iteration 5923, loss = 0.11140401\n",
      "Iteration 5924, loss = 0.11137812\n",
      "Iteration 5925, loss = 0.11135259\n",
      "Iteration 5926, loss = 0.11132644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5927, loss = 0.11130088\n",
      "Iteration 5928, loss = 0.11127514\n",
      "Iteration 5929, loss = 0.11124937\n",
      "Iteration 5930, loss = 0.11122362\n",
      "Iteration 5931, loss = 0.11119762\n",
      "Iteration 5932, loss = 0.11117190\n",
      "Iteration 5933, loss = 0.11114629\n",
      "Iteration 5934, loss = 0.11112092\n",
      "Iteration 5935, loss = 0.11109483\n",
      "Iteration 5936, loss = 0.11106949\n",
      "Iteration 5937, loss = 0.11104374\n",
      "Iteration 5938, loss = 0.11101827\n",
      "Iteration 5939, loss = 0.11099248\n",
      "Iteration 5940, loss = 0.11096686\n",
      "Iteration 5941, loss = 0.11094118\n",
      "Iteration 5942, loss = 0.11091558\n",
      "Iteration 5943, loss = 0.11088990\n",
      "Iteration 5944, loss = 0.11086431\n",
      "Iteration 5945, loss = 0.11083852\n",
      "Iteration 5946, loss = 0.11081304\n",
      "Iteration 5947, loss = 0.11078728\n",
      "Iteration 5948, loss = 0.11076187\n",
      "Iteration 5949, loss = 0.11073628\n",
      "Iteration 5950, loss = 0.11071061\n",
      "Iteration 5951, loss = 0.11068501\n",
      "Iteration 5952, loss = 0.11065961\n",
      "Iteration 5953, loss = 0.11063437\n",
      "Iteration 5954, loss = 0.11060875\n",
      "Iteration 5955, loss = 0.11058304\n",
      "Iteration 5956, loss = 0.11055759\n",
      "Iteration 5957, loss = 0.11053241\n",
      "Iteration 5958, loss = 0.11050667\n",
      "Iteration 5959, loss = 0.11048110\n",
      "Iteration 5960, loss = 0.11045604\n",
      "Iteration 5961, loss = 0.11043074\n",
      "Iteration 5962, loss = 0.11040542\n",
      "Iteration 5963, loss = 0.11038030\n",
      "Iteration 5964, loss = 0.11035455\n",
      "Iteration 5965, loss = 0.11032960\n",
      "Iteration 5966, loss = 0.11030405\n",
      "Iteration 5967, loss = 0.11027911\n",
      "Iteration 5968, loss = 0.11025371\n",
      "Iteration 5969, loss = 0.11022836\n",
      "Iteration 5970, loss = 0.11020295\n",
      "Iteration 5971, loss = 0.11017754\n",
      "Iteration 5972, loss = 0.11015219\n",
      "Iteration 5973, loss = 0.11012721\n",
      "Iteration 5974, loss = 0.11010179\n",
      "Iteration 5975, loss = 0.11007654\n",
      "Iteration 5976, loss = 0.11005126\n",
      "Iteration 5977, loss = 0.11002584\n",
      "Iteration 5978, loss = 0.11000077\n",
      "Iteration 5979, loss = 0.10997567\n",
      "Iteration 5980, loss = 0.10995022\n",
      "Iteration 5981, loss = 0.10992482\n",
      "Iteration 5982, loss = 0.10989990\n",
      "Iteration 5983, loss = 0.10987435\n",
      "Iteration 5984, loss = 0.10984961\n",
      "Iteration 5985, loss = 0.10982446\n",
      "Iteration 5986, loss = 0.10979929\n",
      "Iteration 5987, loss = 0.10977408\n",
      "Iteration 5988, loss = 0.10974889\n",
      "Iteration 5989, loss = 0.10972364\n",
      "Iteration 5990, loss = 0.10969904\n",
      "Iteration 5991, loss = 0.10967358\n",
      "Iteration 5992, loss = 0.10964883\n",
      "Iteration 5993, loss = 0.10962333\n",
      "Iteration 5994, loss = 0.10959879\n",
      "Iteration 5995, loss = 0.10957343\n",
      "Iteration 5996, loss = 0.10954844\n",
      "Iteration 5997, loss = 0.10952352\n",
      "Iteration 5998, loss = 0.10949832\n",
      "Iteration 5999, loss = 0.10947365\n",
      "Iteration 6000, loss = 0.10944832\n",
      "Iteration 6001, loss = 0.10942362\n",
      "Iteration 6002, loss = 0.10939861\n",
      "Iteration 6003, loss = 0.10937356\n",
      "Iteration 6004, loss = 0.10934887\n",
      "Iteration 6005, loss = 0.10932386\n",
      "Iteration 6006, loss = 0.10929876\n",
      "Iteration 6007, loss = 0.10927421\n",
      "Iteration 6008, loss = 0.10924922\n",
      "Iteration 6009, loss = 0.10922421\n",
      "Iteration 6010, loss = 0.10919914\n",
      "Iteration 6011, loss = 0.10917411\n",
      "Iteration 6012, loss = 0.10914911\n",
      "Iteration 6013, loss = 0.10912424\n",
      "Iteration 6014, loss = 0.10909950\n",
      "Iteration 6015, loss = 0.10907445\n",
      "Iteration 6016, loss = 0.10904958\n",
      "Iteration 6017, loss = 0.10902446\n",
      "Iteration 6018, loss = 0.10899989\n",
      "Iteration 6019, loss = 0.10897479\n",
      "Iteration 6020, loss = 0.10895022\n",
      "Iteration 6021, loss = 0.10892556\n",
      "Iteration 6022, loss = 0.10890053\n",
      "Iteration 6023, loss = 0.10887576\n",
      "Iteration 6024, loss = 0.10885093\n",
      "Iteration 6025, loss = 0.10882651\n",
      "Iteration 6026, loss = 0.10880154\n",
      "Iteration 6027, loss = 0.10877702\n",
      "Iteration 6028, loss = 0.10875248\n",
      "Iteration 6029, loss = 0.10872778\n",
      "Iteration 6030, loss = 0.10870289\n",
      "Iteration 6031, loss = 0.10867839\n",
      "Iteration 6032, loss = 0.10865355\n",
      "Iteration 6033, loss = 0.10862930\n",
      "Iteration 6034, loss = 0.10860457\n",
      "Iteration 6035, loss = 0.10858009\n",
      "Iteration 6036, loss = 0.10855554\n",
      "Iteration 6037, loss = 0.10853095\n",
      "Iteration 6038, loss = 0.10850646\n",
      "Iteration 6039, loss = 0.10848163\n",
      "Iteration 6040, loss = 0.10845742\n",
      "Iteration 6041, loss = 0.10843292\n",
      "Iteration 6042, loss = 0.10840834\n",
      "Iteration 6043, loss = 0.10838373\n",
      "Iteration 6044, loss = 0.10835931\n",
      "Iteration 6045, loss = 0.10833531\n",
      "Iteration 6046, loss = 0.10831027\n",
      "Iteration 6047, loss = 0.10828597\n",
      "Iteration 6048, loss = 0.10826182\n",
      "Iteration 6049, loss = 0.10823695\n",
      "Iteration 6050, loss = 0.10821261\n",
      "Iteration 6051, loss = 0.10818802\n",
      "Iteration 6052, loss = 0.10816345\n",
      "Iteration 6053, loss = 0.10813913\n",
      "Iteration 6054, loss = 0.10811451\n",
      "Iteration 6055, loss = 0.10809004\n",
      "Iteration 6056, loss = 0.10806580\n",
      "Iteration 6057, loss = 0.10804125\n",
      "Iteration 6058, loss = 0.10801686\n",
      "Iteration 6059, loss = 0.10799260\n",
      "Iteration 6060, loss = 0.10796809\n",
      "Iteration 6061, loss = 0.10794394\n",
      "Iteration 6062, loss = 0.10791930\n",
      "Iteration 6063, loss = 0.10789486\n",
      "Iteration 6064, loss = 0.10787038\n",
      "Iteration 6065, loss = 0.10784630\n",
      "Iteration 6066, loss = 0.10782178\n",
      "Iteration 6067, loss = 0.10779728\n",
      "Iteration 6068, loss = 0.10777340\n",
      "Iteration 6069, loss = 0.10774871\n",
      "Iteration 6070, loss = 0.10772453\n",
      "Iteration 6071, loss = 0.10770027\n",
      "Iteration 6072, loss = 0.10767567\n",
      "Iteration 6073, loss = 0.10765178\n",
      "Iteration 6074, loss = 0.10762756\n",
      "Iteration 6075, loss = 0.10760288\n",
      "Iteration 6076, loss = 0.10757899\n",
      "Iteration 6077, loss = 0.10755475\n",
      "Iteration 6078, loss = 0.10753064\n",
      "Iteration 6079, loss = 0.10750599\n",
      "Iteration 6080, loss = 0.10748224\n",
      "Iteration 6081, loss = 0.10745766\n",
      "Iteration 6082, loss = 0.10743376\n",
      "Iteration 6083, loss = 0.10740978\n",
      "Iteration 6084, loss = 0.10738551\n",
      "Iteration 6085, loss = 0.10736128\n",
      "Iteration 6086, loss = 0.10733706\n",
      "Iteration 6087, loss = 0.10731290\n",
      "Iteration 6088, loss = 0.10728902\n",
      "Iteration 6089, loss = 0.10726496\n",
      "Iteration 6090, loss = 0.10724101\n",
      "Iteration 6091, loss = 0.10721666\n",
      "Iteration 6092, loss = 0.10719294\n",
      "Iteration 6093, loss = 0.10716895\n",
      "Iteration 6094, loss = 0.10714497\n",
      "Iteration 6095, loss = 0.10712090\n",
      "Iteration 6096, loss = 0.10709695\n",
      "Iteration 6097, loss = 0.10707300\n",
      "Iteration 6098, loss = 0.10704916\n",
      "Iteration 6099, loss = 0.10702534\n",
      "Iteration 6100, loss = 0.10700140\n",
      "Iteration 6101, loss = 0.10697737\n",
      "Iteration 6102, loss = 0.10695341\n",
      "Iteration 6103, loss = 0.10692983\n",
      "Iteration 6104, loss = 0.10690597\n",
      "Iteration 6105, loss = 0.10688193\n",
      "Iteration 6106, loss = 0.10685778\n",
      "Iteration 6107, loss = 0.10683391\n",
      "Iteration 6108, loss = 0.10681063\n",
      "Iteration 6109, loss = 0.10678678\n",
      "Iteration 6110, loss = 0.10676256\n",
      "Iteration 6111, loss = 0.10673888\n",
      "Iteration 6112, loss = 0.10671521\n",
      "Iteration 6113, loss = 0.10669124\n",
      "Iteration 6114, loss = 0.10666782\n",
      "Iteration 6115, loss = 0.10664404\n",
      "Iteration 6116, loss = 0.10662034\n",
      "Iteration 6117, loss = 0.10659655\n",
      "Iteration 6118, loss = 0.10657275\n",
      "Iteration 6119, loss = 0.10654923\n",
      "Iteration 6120, loss = 0.10652518\n",
      "Iteration 6121, loss = 0.10650161\n",
      "Iteration 6122, loss = 0.10647801\n",
      "Iteration 6123, loss = 0.10645431\n",
      "Iteration 6124, loss = 0.10643073\n",
      "Iteration 6125, loss = 0.10640715\n",
      "Iteration 6126, loss = 0.10638333\n",
      "Iteration 6127, loss = 0.10636007\n",
      "Iteration 6128, loss = 0.10633621\n",
      "Iteration 6129, loss = 0.10631273\n",
      "Iteration 6130, loss = 0.10628909\n",
      "Iteration 6131, loss = 0.10626497\n",
      "Iteration 6132, loss = 0.10624173\n",
      "Iteration 6133, loss = 0.10621810\n",
      "Iteration 6134, loss = 0.10619436\n",
      "Iteration 6135, loss = 0.10617059\n",
      "Iteration 6136, loss = 0.10614724\n",
      "Iteration 6137, loss = 0.10612375\n",
      "Iteration 6138, loss = 0.10610021\n",
      "Iteration 6139, loss = 0.10607652\n",
      "Iteration 6140, loss = 0.10605271\n",
      "Iteration 6141, loss = 0.10602942\n",
      "Iteration 6142, loss = 0.10600579\n",
      "Iteration 6143, loss = 0.10598249\n",
      "Iteration 6144, loss = 0.10595893\n",
      "Iteration 6145, loss = 0.10593545\n",
      "Iteration 6146, loss = 0.10591209\n",
      "Iteration 6147, loss = 0.10588842\n",
      "Iteration 6148, loss = 0.10586485\n",
      "Iteration 6149, loss = 0.10584159\n",
      "Iteration 6150, loss = 0.10581787\n",
      "Iteration 6151, loss = 0.10579432\n",
      "Iteration 6152, loss = 0.10577104\n",
      "Iteration 6153, loss = 0.10574758\n",
      "Iteration 6154, loss = 0.10572408\n",
      "Iteration 6155, loss = 0.10570096\n",
      "Iteration 6156, loss = 0.10567759\n",
      "Iteration 6157, loss = 0.10565405\n",
      "Iteration 6158, loss = 0.10563063\n",
      "Iteration 6159, loss = 0.10560746\n",
      "Iteration 6160, loss = 0.10558414\n",
      "Iteration 6161, loss = 0.10556074\n",
      "Iteration 6162, loss = 0.10553771\n",
      "Iteration 6163, loss = 0.10551413\n",
      "Iteration 6164, loss = 0.10549110\n",
      "Iteration 6165, loss = 0.10546801\n",
      "Iteration 6166, loss = 0.10544433\n",
      "Iteration 6167, loss = 0.10542118\n",
      "Iteration 6168, loss = 0.10539839\n",
      "Iteration 6169, loss = 0.10537468\n",
      "Iteration 6170, loss = 0.10535153\n",
      "Iteration 6171, loss = 0.10532823\n",
      "Iteration 6172, loss = 0.10530516\n",
      "Iteration 6173, loss = 0.10528169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6174, loss = 0.10525894\n",
      "Iteration 6175, loss = 0.10523561\n",
      "Iteration 6176, loss = 0.10521244\n",
      "Iteration 6177, loss = 0.10518891\n",
      "Iteration 6178, loss = 0.10516616\n",
      "Iteration 6179, loss = 0.10514289\n",
      "Iteration 6180, loss = 0.10511976\n",
      "Iteration 6181, loss = 0.10509657\n",
      "Iteration 6182, loss = 0.10507340\n",
      "Iteration 6183, loss = 0.10505033\n",
      "Iteration 6184, loss = 0.10502706\n",
      "Iteration 6185, loss = 0.10500376\n",
      "Iteration 6186, loss = 0.10498071\n",
      "Iteration 6187, loss = 0.10495768\n",
      "Iteration 6188, loss = 0.10493423\n",
      "Iteration 6189, loss = 0.10491137\n",
      "Iteration 6190, loss = 0.10488841\n",
      "Iteration 6191, loss = 0.10486512\n",
      "Iteration 6192, loss = 0.10484222\n",
      "Iteration 6193, loss = 0.10481940\n",
      "Iteration 6194, loss = 0.10479584\n",
      "Iteration 6195, loss = 0.10477304\n",
      "Iteration 6196, loss = 0.10475016\n",
      "Iteration 6197, loss = 0.10472680\n",
      "Iteration 6198, loss = 0.10470386\n",
      "Iteration 6199, loss = 0.10468077\n",
      "Iteration 6200, loss = 0.10465766\n",
      "Iteration 6201, loss = 0.10463492\n",
      "Iteration 6202, loss = 0.10461189\n",
      "Iteration 6203, loss = 0.10458880\n",
      "Iteration 6204, loss = 0.10456593\n",
      "Iteration 6205, loss = 0.10454317\n",
      "Iteration 6206, loss = 0.10452023\n",
      "Iteration 6207, loss = 0.10449720\n",
      "Iteration 6208, loss = 0.10447410\n",
      "Iteration 6209, loss = 0.10445149\n",
      "Iteration 6210, loss = 0.10442855\n",
      "Iteration 6211, loss = 0.10440579\n",
      "Iteration 6212, loss = 0.10438319\n",
      "Iteration 6213, loss = 0.10435993\n",
      "Iteration 6214, loss = 0.10433729\n",
      "Iteration 6215, loss = 0.10431456\n",
      "Iteration 6216, loss = 0.10429189\n",
      "Iteration 6217, loss = 0.10426925\n",
      "Iteration 6218, loss = 0.10424657\n",
      "Iteration 6219, loss = 0.10422356\n",
      "Iteration 6220, loss = 0.10420110\n",
      "Iteration 6221, loss = 0.10417829\n",
      "Iteration 6222, loss = 0.10415563\n",
      "Iteration 6223, loss = 0.10413287\n",
      "Iteration 6224, loss = 0.10411052\n",
      "Iteration 6225, loss = 0.10408788\n",
      "Iteration 6226, loss = 0.10406502\n",
      "Iteration 6227, loss = 0.10404266\n",
      "Iteration 6228, loss = 0.10401986\n",
      "Iteration 6229, loss = 0.10399725\n",
      "Iteration 6230, loss = 0.10397480\n",
      "Iteration 6231, loss = 0.10395216\n",
      "Iteration 6232, loss = 0.10392918\n",
      "Iteration 6233, loss = 0.10390651\n",
      "Iteration 6234, loss = 0.10388401\n",
      "Iteration 6235, loss = 0.10386130\n",
      "Iteration 6236, loss = 0.10383902\n",
      "Iteration 6237, loss = 0.10381636\n",
      "Iteration 6238, loss = 0.10379382\n",
      "Iteration 6239, loss = 0.10377076\n",
      "Iteration 6240, loss = 0.10374831\n",
      "Iteration 6241, loss = 0.10372594\n",
      "Iteration 6242, loss = 0.10370352\n",
      "Iteration 6243, loss = 0.10368059\n",
      "Iteration 6244, loss = 0.10365801\n",
      "Iteration 6245, loss = 0.10363567\n",
      "Iteration 6246, loss = 0.10361281\n",
      "Iteration 6247, loss = 0.10359050\n",
      "Iteration 6248, loss = 0.10356782\n",
      "Iteration 6249, loss = 0.10354514\n",
      "Iteration 6250, loss = 0.10352267\n",
      "Iteration 6251, loss = 0.10349999\n",
      "Iteration 6252, loss = 0.10347749\n",
      "Iteration 6253, loss = 0.10345490\n",
      "Iteration 6254, loss = 0.10343238\n",
      "Iteration 6255, loss = 0.10340997\n",
      "Iteration 6256, loss = 0.10338744\n",
      "Iteration 6257, loss = 0.10336486\n",
      "Iteration 6258, loss = 0.10334251\n",
      "Iteration 6259, loss = 0.10332024\n",
      "Iteration 6260, loss = 0.10329749\n",
      "Iteration 6261, loss = 0.10327562\n",
      "Iteration 6262, loss = 0.10325276\n",
      "Iteration 6263, loss = 0.10323061\n",
      "Iteration 6264, loss = 0.10320793\n",
      "Iteration 6265, loss = 0.10318588\n",
      "Iteration 6266, loss = 0.10316332\n",
      "Iteration 6267, loss = 0.10314103\n",
      "Iteration 6268, loss = 0.10311864\n",
      "Iteration 6269, loss = 0.10309607\n",
      "Iteration 6270, loss = 0.10307375\n",
      "Iteration 6271, loss = 0.10305162\n",
      "Iteration 6272, loss = 0.10302928\n",
      "Iteration 6273, loss = 0.10300687\n",
      "Iteration 6274, loss = 0.10298488\n",
      "Iteration 6275, loss = 0.10296242\n",
      "Iteration 6276, loss = 0.10294022\n",
      "Iteration 6277, loss = 0.10291761\n",
      "Iteration 6278, loss = 0.10289556\n",
      "Iteration 6279, loss = 0.10287312\n",
      "Iteration 6280, loss = 0.10285076\n",
      "Iteration 6281, loss = 0.10282867\n",
      "Iteration 6282, loss = 0.10280647\n",
      "Iteration 6283, loss = 0.10278405\n",
      "Iteration 6284, loss = 0.10276204\n",
      "Iteration 6285, loss = 0.10273954\n",
      "Iteration 6286, loss = 0.10271773\n",
      "Iteration 6287, loss = 0.10269524\n",
      "Iteration 6288, loss = 0.10267333\n",
      "Iteration 6289, loss = 0.10265108\n",
      "Iteration 6290, loss = 0.10262908\n",
      "Iteration 6291, loss = 0.10260674\n",
      "Iteration 6292, loss = 0.10258486\n",
      "Iteration 6293, loss = 0.10256248\n",
      "Iteration 6294, loss = 0.10254071\n",
      "Iteration 6295, loss = 0.10251853\n",
      "Iteration 6296, loss = 0.10249617\n",
      "Iteration 6297, loss = 0.10247451\n",
      "Iteration 6298, loss = 0.10245231\n",
      "Iteration 6299, loss = 0.10243038\n",
      "Iteration 6300, loss = 0.10240827\n",
      "Iteration 6301, loss = 0.10238633\n",
      "Iteration 6302, loss = 0.10236399\n",
      "Iteration 6303, loss = 0.10234211\n",
      "Iteration 6304, loss = 0.10231994\n",
      "Iteration 6305, loss = 0.10229830\n",
      "Iteration 6306, loss = 0.10227599\n",
      "Iteration 6307, loss = 0.10225416\n",
      "Iteration 6308, loss = 0.10223198\n",
      "Iteration 6309, loss = 0.10221005\n",
      "Iteration 6310, loss = 0.10218815\n",
      "Iteration 6311, loss = 0.10216605\n",
      "Iteration 6312, loss = 0.10214436\n",
      "Iteration 6313, loss = 0.10212248\n",
      "Iteration 6314, loss = 0.10210035\n",
      "Iteration 6315, loss = 0.10207829\n",
      "Iteration 6316, loss = 0.10205641\n",
      "Iteration 6317, loss = 0.10203465\n",
      "Iteration 6318, loss = 0.10201255\n",
      "Iteration 6319, loss = 0.10199078\n",
      "Iteration 6320, loss = 0.10196896\n",
      "Iteration 6321, loss = 0.10194720\n",
      "Iteration 6322, loss = 0.10192514\n",
      "Iteration 6323, loss = 0.10190372\n",
      "Iteration 6324, loss = 0.10188171\n",
      "Iteration 6325, loss = 0.10185962\n",
      "Iteration 6326, loss = 0.10183835\n",
      "Iteration 6327, loss = 0.10181636\n",
      "Iteration 6328, loss = 0.10179460\n",
      "Iteration 6329, loss = 0.10177266\n",
      "Iteration 6330, loss = 0.10175118\n",
      "Iteration 6331, loss = 0.10172931\n",
      "Iteration 6332, loss = 0.10170741\n",
      "Iteration 6333, loss = 0.10168578\n",
      "Iteration 6334, loss = 0.10166404\n",
      "Iteration 6335, loss = 0.10164212\n",
      "Iteration 6336, loss = 0.10162026\n",
      "Iteration 6337, loss = 0.10159862\n",
      "Iteration 6338, loss = 0.10157678\n",
      "Iteration 6339, loss = 0.10155526\n",
      "Iteration 6340, loss = 0.10153353\n",
      "Iteration 6341, loss = 0.10151174\n",
      "Iteration 6342, loss = 0.10148987\n",
      "Iteration 6343, loss = 0.10146840\n",
      "Iteration 6344, loss = 0.10144678\n",
      "Iteration 6345, loss = 0.10142472\n",
      "Iteration 6346, loss = 0.10140357\n",
      "Iteration 6347, loss = 0.10138163\n",
      "Iteration 6348, loss = 0.10136016\n",
      "Iteration 6349, loss = 0.10133866\n",
      "Iteration 6350, loss = 0.10131701\n",
      "Iteration 6351, loss = 0.10129531\n",
      "Iteration 6352, loss = 0.10127381\n",
      "Iteration 6353, loss = 0.10125238\n",
      "Iteration 6354, loss = 0.10123091\n",
      "Iteration 6355, loss = 0.10120908\n",
      "Iteration 6356, loss = 0.10118801\n",
      "Iteration 6357, loss = 0.10116603\n",
      "Iteration 6358, loss = 0.10114469\n",
      "Iteration 6359, loss = 0.10112334\n",
      "Iteration 6360, loss = 0.10110155\n",
      "Iteration 6361, loss = 0.10108015\n",
      "Iteration 6362, loss = 0.10105860\n",
      "Iteration 6363, loss = 0.10103731\n",
      "Iteration 6364, loss = 0.10101612\n",
      "Iteration 6365, loss = 0.10099454\n",
      "Iteration 6366, loss = 0.10097306\n",
      "Iteration 6367, loss = 0.10095176\n",
      "Iteration 6368, loss = 0.10093019\n",
      "Iteration 6369, loss = 0.10090856\n",
      "Iteration 6370, loss = 0.10088759\n",
      "Iteration 6371, loss = 0.10086599\n",
      "Iteration 6372, loss = 0.10084457\n",
      "Iteration 6373, loss = 0.10082290\n",
      "Iteration 6374, loss = 0.10080149\n",
      "Iteration 6375, loss = 0.10078037\n",
      "Iteration 6376, loss = 0.10075890\n",
      "Iteration 6377, loss = 0.10073695\n",
      "Iteration 6378, loss = 0.10071580\n",
      "Iteration 6379, loss = 0.10069451\n",
      "Iteration 6380, loss = 0.10067341\n",
      "Iteration 6381, loss = 0.10065132\n",
      "Iteration 6382, loss = 0.10063069\n",
      "Iteration 6383, loss = 0.10060907\n",
      "Iteration 6384, loss = 0.10058775\n",
      "Iteration 6385, loss = 0.10056652\n",
      "Iteration 6386, loss = 0.10054504\n",
      "Iteration 6387, loss = 0.10052375\n",
      "Iteration 6388, loss = 0.10050243\n",
      "Iteration 6389, loss = 0.10048121\n",
      "Iteration 6390, loss = 0.10045981\n",
      "Iteration 6391, loss = 0.10043875\n",
      "Iteration 6392, loss = 0.10041738\n",
      "Iteration 6393, loss = 0.10039611\n",
      "Iteration 6394, loss = 0.10037489\n",
      "Iteration 6395, loss = 0.10035338\n",
      "Iteration 6396, loss = 0.10033234\n",
      "Iteration 6397, loss = 0.10031106\n",
      "Iteration 6398, loss = 0.10028985\n",
      "Iteration 6399, loss = 0.10026871\n",
      "Iteration 6400, loss = 0.10024758\n",
      "Iteration 6401, loss = 0.10022646\n",
      "Iteration 6402, loss = 0.10020516\n",
      "Iteration 6403, loss = 0.10018422\n",
      "Iteration 6404, loss = 0.10016313\n",
      "Iteration 6405, loss = 0.10014190\n",
      "Iteration 6406, loss = 0.10012070\n",
      "Iteration 6407, loss = 0.10009994\n",
      "Iteration 6408, loss = 0.10007865\n",
      "Iteration 6409, loss = 0.10005757\n",
      "Iteration 6410, loss = 0.10003664\n",
      "Iteration 6411, loss = 0.10001533\n",
      "Iteration 6412, loss = 0.09999433\n",
      "Iteration 6413, loss = 0.09997338\n",
      "Iteration 6414, loss = 0.09995208\n",
      "Iteration 6415, loss = 0.09993082\n",
      "Iteration 6416, loss = 0.09991012\n",
      "Iteration 6417, loss = 0.09988894\n",
      "Iteration 6418, loss = 0.09986753\n",
      "Iteration 6419, loss = 0.09984681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6420, loss = 0.09982589\n",
      "Iteration 6421, loss = 0.09980441\n",
      "Iteration 6422, loss = 0.09978381\n",
      "Iteration 6423, loss = 0.09976251\n",
      "Iteration 6424, loss = 0.09974165\n",
      "Iteration 6425, loss = 0.09972056\n",
      "Iteration 6426, loss = 0.09969991\n",
      "Iteration 6427, loss = 0.09967864\n",
      "Iteration 6428, loss = 0.09965782\n",
      "Iteration 6429, loss = 0.09963690\n",
      "Iteration 6430, loss = 0.09961567\n",
      "Iteration 6431, loss = 0.09959528\n",
      "Iteration 6432, loss = 0.09957410\n",
      "Iteration 6433, loss = 0.09955267\n",
      "Iteration 6434, loss = 0.09953171\n",
      "Iteration 6435, loss = 0.09951092\n",
      "Iteration 6436, loss = 0.09949001\n",
      "Iteration 6437, loss = 0.09946929\n",
      "Iteration 6438, loss = 0.09944781\n",
      "Iteration 6439, loss = 0.09942721\n",
      "Iteration 6440, loss = 0.09940634\n",
      "Iteration 6441, loss = 0.09938546\n",
      "Iteration 6442, loss = 0.09936453\n",
      "Iteration 6443, loss = 0.09934374\n",
      "Iteration 6444, loss = 0.09932311\n",
      "Iteration 6445, loss = 0.09930201\n",
      "Iteration 6446, loss = 0.09928115\n",
      "Iteration 6447, loss = 0.09926053\n",
      "Iteration 6448, loss = 0.09923980\n",
      "Iteration 6449, loss = 0.09921868\n",
      "Iteration 6450, loss = 0.09919792\n",
      "Iteration 6451, loss = 0.09917730\n",
      "Iteration 6452, loss = 0.09915680\n",
      "Iteration 6453, loss = 0.09913569\n",
      "Iteration 6454, loss = 0.09911509\n",
      "Iteration 6455, loss = 0.09909413\n",
      "Iteration 6456, loss = 0.09907364\n",
      "Iteration 6457, loss = 0.09905287\n",
      "Iteration 6458, loss = 0.09903205\n",
      "Iteration 6459, loss = 0.09901116\n",
      "Iteration 6460, loss = 0.09899072\n",
      "Iteration 6461, loss = 0.09897015\n",
      "Iteration 6462, loss = 0.09894925\n",
      "Iteration 6463, loss = 0.09892863\n",
      "Iteration 6464, loss = 0.09890806\n",
      "Iteration 6465, loss = 0.09888750\n",
      "Iteration 6466, loss = 0.09886652\n",
      "Iteration 6467, loss = 0.09884628\n",
      "Iteration 6468, loss = 0.09882556\n",
      "Iteration 6469, loss = 0.09880492\n",
      "Iteration 6470, loss = 0.09878445\n",
      "Iteration 6471, loss = 0.09876399\n",
      "Iteration 6472, loss = 0.09874341\n",
      "Iteration 6473, loss = 0.09872255\n",
      "Iteration 6474, loss = 0.09870222\n",
      "Iteration 6475, loss = 0.09868150\n",
      "Iteration 6476, loss = 0.09866108\n",
      "Iteration 6477, loss = 0.09864044\n",
      "Iteration 6478, loss = 0.09861987\n",
      "Iteration 6479, loss = 0.09859951\n",
      "Iteration 6480, loss = 0.09857866\n",
      "Iteration 6481, loss = 0.09855826\n",
      "Iteration 6482, loss = 0.09853778\n",
      "Iteration 6483, loss = 0.09851749\n",
      "Iteration 6484, loss = 0.09849690\n",
      "Iteration 6485, loss = 0.09847616\n",
      "Iteration 6486, loss = 0.09845558\n",
      "Iteration 6487, loss = 0.09843526\n",
      "Iteration 6488, loss = 0.09841497\n",
      "Iteration 6489, loss = 0.09839428\n",
      "Iteration 6490, loss = 0.09837392\n",
      "Iteration 6491, loss = 0.09835341\n",
      "Iteration 6492, loss = 0.09833314\n",
      "Iteration 6493, loss = 0.09831267\n",
      "Iteration 6494, loss = 0.09829220\n",
      "Iteration 6495, loss = 0.09827208\n",
      "Iteration 6496, loss = 0.09825165\n",
      "Iteration 6497, loss = 0.09823133\n",
      "Iteration 6498, loss = 0.09821093\n",
      "Iteration 6499, loss = 0.09819067\n",
      "Iteration 6500, loss = 0.09817016\n",
      "Iteration 6501, loss = 0.09814971\n",
      "Iteration 6502, loss = 0.09812951\n",
      "Iteration 6503, loss = 0.09810929\n",
      "Iteration 6504, loss = 0.09808879\n",
      "Iteration 6505, loss = 0.09806859\n",
      "Iteration 6506, loss = 0.09804842\n",
      "Iteration 6507, loss = 0.09802831\n",
      "Iteration 6508, loss = 0.09800777\n",
      "Iteration 6509, loss = 0.09798757\n",
      "Iteration 6510, loss = 0.09796726\n",
      "Iteration 6511, loss = 0.09794701\n",
      "Iteration 6512, loss = 0.09792719\n",
      "Iteration 6513, loss = 0.09790670\n",
      "Iteration 6514, loss = 0.09788651\n",
      "Iteration 6515, loss = 0.09786639\n",
      "Iteration 6516, loss = 0.09784626\n",
      "Iteration 6517, loss = 0.09782616\n",
      "Iteration 6518, loss = 0.09780582\n",
      "Iteration 6519, loss = 0.09778567\n",
      "Iteration 6520, loss = 0.09776546\n",
      "Iteration 6521, loss = 0.09774534\n",
      "Iteration 6522, loss = 0.09772527\n",
      "Iteration 6523, loss = 0.09770495\n",
      "Iteration 6524, loss = 0.09768499\n",
      "Iteration 6525, loss = 0.09766457\n",
      "Iteration 6526, loss = 0.09764454\n",
      "Iteration 6527, loss = 0.09762457\n",
      "Iteration 6528, loss = 0.09760422\n",
      "Iteration 6529, loss = 0.09758411\n",
      "Iteration 6530, loss = 0.09756397\n",
      "Iteration 6531, loss = 0.09754385\n",
      "Iteration 6532, loss = 0.09752415\n",
      "Iteration 6533, loss = 0.09750340\n",
      "Iteration 6534, loss = 0.09748355\n",
      "Iteration 6535, loss = 0.09746342\n",
      "Iteration 6536, loss = 0.09744334\n",
      "Iteration 6537, loss = 0.09742343\n",
      "Iteration 6538, loss = 0.09740330\n",
      "Iteration 6539, loss = 0.09738323\n",
      "Iteration 6540, loss = 0.09736334\n",
      "Iteration 6541, loss = 0.09734312\n",
      "Iteration 6542, loss = 0.09732319\n",
      "Iteration 6543, loss = 0.09730345\n",
      "Iteration 6544, loss = 0.09728331\n",
      "Iteration 6545, loss = 0.09726323\n",
      "Iteration 6546, loss = 0.09724340\n",
      "Iteration 6547, loss = 0.09722345\n",
      "Iteration 6548, loss = 0.09720355\n",
      "Iteration 6549, loss = 0.09718351\n",
      "Iteration 6550, loss = 0.09716396\n",
      "Iteration 6551, loss = 0.09714389\n",
      "Iteration 6552, loss = 0.09712376\n",
      "Iteration 6553, loss = 0.09710383\n",
      "Iteration 6554, loss = 0.09708401\n",
      "Iteration 6555, loss = 0.09706417\n",
      "Iteration 6556, loss = 0.09704447\n",
      "Iteration 6557, loss = 0.09702467\n",
      "Iteration 6558, loss = 0.09700455\n",
      "Iteration 6559, loss = 0.09698476\n",
      "Iteration 6560, loss = 0.09696473\n",
      "Iteration 6561, loss = 0.09694483\n",
      "Iteration 6562, loss = 0.09692534\n",
      "Iteration 6563, loss = 0.09690510\n",
      "Iteration 6564, loss = 0.09688540\n",
      "Iteration 6565, loss = 0.09686551\n",
      "Iteration 6566, loss = 0.09684567\n",
      "Iteration 6567, loss = 0.09682575\n",
      "Iteration 6568, loss = 0.09680605\n",
      "Iteration 6569, loss = 0.09678650\n",
      "Iteration 6570, loss = 0.09676648\n",
      "Iteration 6571, loss = 0.09674683\n",
      "Iteration 6572, loss = 0.09672707\n",
      "Iteration 6573, loss = 0.09670719\n",
      "Iteration 6574, loss = 0.09668755\n",
      "Iteration 6575, loss = 0.09666789\n",
      "Iteration 6576, loss = 0.09664785\n",
      "Iteration 6577, loss = 0.09662821\n",
      "Iteration 6578, loss = 0.09660854\n",
      "Iteration 6579, loss = 0.09658862\n",
      "Iteration 6580, loss = 0.09656905\n",
      "Iteration 6581, loss = 0.09654948\n",
      "Iteration 6582, loss = 0.09652958\n",
      "Iteration 6583, loss = 0.09650979\n",
      "Iteration 6584, loss = 0.09649040\n",
      "Iteration 6585, loss = 0.09647060\n",
      "Iteration 6586, loss = 0.09645080\n",
      "Iteration 6587, loss = 0.09643144\n",
      "Iteration 6588, loss = 0.09641142\n",
      "Iteration 6589, loss = 0.09639195\n",
      "Iteration 6590, loss = 0.09637233\n",
      "Iteration 6591, loss = 0.09635289\n",
      "Iteration 6592, loss = 0.09633317\n",
      "Iteration 6593, loss = 0.09631349\n",
      "Iteration 6594, loss = 0.09629395\n",
      "Iteration 6595, loss = 0.09627467\n",
      "Iteration 6596, loss = 0.09625488\n",
      "Iteration 6597, loss = 0.09623536\n",
      "Iteration 6598, loss = 0.09621575\n",
      "Iteration 6599, loss = 0.09619650\n",
      "Iteration 6600, loss = 0.09617675\n",
      "Iteration 6601, loss = 0.09615705\n",
      "Iteration 6602, loss = 0.09613765\n",
      "Iteration 6603, loss = 0.09611818\n",
      "Iteration 6604, loss = 0.09609858\n",
      "Iteration 6605, loss = 0.09607919\n",
      "Iteration 6606, loss = 0.09605962\n",
      "Iteration 6607, loss = 0.09604029\n",
      "Iteration 6608, loss = 0.09602084\n",
      "Iteration 6609, loss = 0.09600137\n",
      "Iteration 6610, loss = 0.09598182\n",
      "Iteration 6611, loss = 0.09596250\n",
      "Iteration 6612, loss = 0.09594297\n",
      "Iteration 6613, loss = 0.09592354\n",
      "Iteration 6614, loss = 0.09590409\n",
      "Iteration 6615, loss = 0.09588432\n",
      "Iteration 6616, loss = 0.09586505\n",
      "Iteration 6617, loss = 0.09584551\n",
      "Iteration 6618, loss = 0.09582640\n",
      "Iteration 6619, loss = 0.09580671\n",
      "Iteration 6620, loss = 0.09578722\n",
      "Iteration 6621, loss = 0.09576790\n",
      "Iteration 6622, loss = 0.09574840\n",
      "Iteration 6623, loss = 0.09572887\n",
      "Iteration 6624, loss = 0.09570944\n",
      "Iteration 6625, loss = 0.09569028\n",
      "Iteration 6626, loss = 0.09567096\n",
      "Iteration 6627, loss = 0.09565166\n",
      "Iteration 6628, loss = 0.09563207\n",
      "Iteration 6629, loss = 0.09561290\n",
      "Iteration 6630, loss = 0.09559346\n",
      "Iteration 6631, loss = 0.09557399\n",
      "Iteration 6632, loss = 0.09555477\n",
      "Iteration 6633, loss = 0.09553544\n",
      "Iteration 6634, loss = 0.09551622\n",
      "Iteration 6635, loss = 0.09549703\n",
      "Iteration 6636, loss = 0.09547792\n",
      "Iteration 6637, loss = 0.09545835\n",
      "Iteration 6638, loss = 0.09543918\n",
      "Iteration 6639, loss = 0.09541989\n",
      "Iteration 6640, loss = 0.09540066\n",
      "Iteration 6641, loss = 0.09538118\n",
      "Iteration 6642, loss = 0.09536220\n",
      "Iteration 6643, loss = 0.09534255\n",
      "Iteration 6644, loss = 0.09532375\n",
      "Iteration 6645, loss = 0.09530421\n",
      "Iteration 6646, loss = 0.09528520\n",
      "Iteration 6647, loss = 0.09526569\n",
      "Iteration 6648, loss = 0.09524643\n",
      "Iteration 6649, loss = 0.09522742\n",
      "Iteration 6650, loss = 0.09520808\n",
      "Iteration 6651, loss = 0.09518898\n",
      "Iteration 6652, loss = 0.09516980\n",
      "Iteration 6653, loss = 0.09515049\n",
      "Iteration 6654, loss = 0.09513145\n",
      "Iteration 6655, loss = 0.09511239\n",
      "Iteration 6656, loss = 0.09509313\n",
      "Iteration 6657, loss = 0.09507388\n",
      "Iteration 6658, loss = 0.09505494\n",
      "Iteration 6659, loss = 0.09503579\n",
      "Iteration 6660, loss = 0.09501678\n",
      "Iteration 6661, loss = 0.09499774\n",
      "Iteration 6662, loss = 0.09497857\n",
      "Iteration 6663, loss = 0.09495967\n",
      "Iteration 6664, loss = 0.09494063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6665, loss = 0.09492153\n",
      "Iteration 6666, loss = 0.09490261\n",
      "Iteration 6667, loss = 0.09488361\n",
      "Iteration 6668, loss = 0.09486447\n",
      "Iteration 6669, loss = 0.09484562\n",
      "Iteration 6670, loss = 0.09482656\n",
      "Iteration 6671, loss = 0.09480746\n",
      "Iteration 6672, loss = 0.09478845\n",
      "Iteration 6673, loss = 0.09476974\n",
      "Iteration 6674, loss = 0.09475042\n",
      "Iteration 6675, loss = 0.09473179\n",
      "Iteration 6676, loss = 0.09471260\n",
      "Iteration 6677, loss = 0.09469368\n",
      "Iteration 6678, loss = 0.09467464\n",
      "Iteration 6679, loss = 0.09465545\n",
      "Iteration 6680, loss = 0.09463664\n",
      "Iteration 6681, loss = 0.09461789\n",
      "Iteration 6682, loss = 0.09459863\n",
      "Iteration 6683, loss = 0.09457965\n",
      "Iteration 6684, loss = 0.09456097\n",
      "Iteration 6685, loss = 0.09454186\n",
      "Iteration 6686, loss = 0.09452272\n",
      "Iteration 6687, loss = 0.09450380\n",
      "Iteration 6688, loss = 0.09448508\n",
      "Iteration 6689, loss = 0.09446624\n",
      "Iteration 6690, loss = 0.09444719\n",
      "Iteration 6691, loss = 0.09442833\n",
      "Iteration 6692, loss = 0.09440947\n",
      "Iteration 6693, loss = 0.09439058\n",
      "Iteration 6694, loss = 0.09437159\n",
      "Iteration 6695, loss = 0.09435272\n",
      "Iteration 6696, loss = 0.09433401\n",
      "Iteration 6697, loss = 0.09431540\n",
      "Iteration 6698, loss = 0.09429612\n",
      "Iteration 6699, loss = 0.09427756\n",
      "Iteration 6700, loss = 0.09425860\n",
      "Iteration 6701, loss = 0.09423984\n",
      "Iteration 6702, loss = 0.09422106\n",
      "Iteration 6703, loss = 0.09420238\n",
      "Iteration 6704, loss = 0.09418376\n",
      "Iteration 6705, loss = 0.09416475\n",
      "Iteration 6706, loss = 0.09414593\n",
      "Iteration 6707, loss = 0.09412755\n",
      "Iteration 6708, loss = 0.09410851\n",
      "Iteration 6709, loss = 0.09408988\n",
      "Iteration 6710, loss = 0.09407123\n",
      "Iteration 6711, loss = 0.09405245\n",
      "Iteration 6712, loss = 0.09403361\n",
      "Iteration 6713, loss = 0.09401483\n",
      "Iteration 6714, loss = 0.09399626\n",
      "Iteration 6715, loss = 0.09397738\n",
      "Iteration 6716, loss = 0.09395873\n",
      "Iteration 6717, loss = 0.09394005\n",
      "Iteration 6718, loss = 0.09392119\n",
      "Iteration 6719, loss = 0.09390275\n",
      "Iteration 6720, loss = 0.09388381\n",
      "Iteration 6721, loss = 0.09386534\n",
      "Iteration 6722, loss = 0.09384661\n",
      "Iteration 6723, loss = 0.09382795\n",
      "Iteration 6724, loss = 0.09380923\n",
      "Iteration 6725, loss = 0.09379046\n",
      "Iteration 6726, loss = 0.09377199\n",
      "Iteration 6727, loss = 0.09375331\n",
      "Iteration 6728, loss = 0.09373473\n",
      "Iteration 6729, loss = 0.09371619\n",
      "Iteration 6730, loss = 0.09369747\n",
      "Iteration 6731, loss = 0.09367898\n",
      "Iteration 6732, loss = 0.09366024\n",
      "Iteration 6733, loss = 0.09364150\n",
      "Iteration 6734, loss = 0.09362310\n",
      "Iteration 6735, loss = 0.09360452\n",
      "Iteration 6736, loss = 0.09358596\n",
      "Iteration 6737, loss = 0.09356743\n",
      "Iteration 6738, loss = 0.09354883\n",
      "Iteration 6739, loss = 0.09353014\n",
      "Iteration 6740, loss = 0.09351157\n",
      "Iteration 6741, loss = 0.09349342\n",
      "Iteration 6742, loss = 0.09347481\n",
      "Iteration 6743, loss = 0.09345605\n",
      "Iteration 6744, loss = 0.09343782\n",
      "Iteration 6745, loss = 0.09341911\n",
      "Iteration 6746, loss = 0.09340074\n",
      "Iteration 6747, loss = 0.09338221\n",
      "Iteration 6748, loss = 0.09336382\n",
      "Iteration 6749, loss = 0.09334544\n",
      "Iteration 6750, loss = 0.09332695\n",
      "Iteration 6751, loss = 0.09330837\n",
      "Iteration 6752, loss = 0.09329003\n",
      "Iteration 6753, loss = 0.09327143\n",
      "Iteration 6754, loss = 0.09325303\n",
      "Iteration 6755, loss = 0.09323459\n",
      "Iteration 6756, loss = 0.09321608\n",
      "Iteration 6757, loss = 0.09319773\n",
      "Iteration 6758, loss = 0.09317935\n",
      "Iteration 6759, loss = 0.09316098\n",
      "Iteration 6760, loss = 0.09314252\n",
      "Iteration 6761, loss = 0.09312455\n",
      "Iteration 6762, loss = 0.09310553\n",
      "Iteration 6763, loss = 0.09308732\n",
      "Iteration 6764, loss = 0.09306864\n",
      "Iteration 6765, loss = 0.09305055\n",
      "Iteration 6766, loss = 0.09303214\n",
      "Iteration 6767, loss = 0.09301378\n",
      "Iteration 6768, loss = 0.09299558\n",
      "Iteration 6769, loss = 0.09297720\n",
      "Iteration 6770, loss = 0.09295854\n",
      "Iteration 6771, loss = 0.09294047\n",
      "Iteration 6772, loss = 0.09292221\n",
      "Iteration 6773, loss = 0.09290360\n",
      "Iteration 6774, loss = 0.09288542\n",
      "Iteration 6775, loss = 0.09286727\n",
      "Iteration 6776, loss = 0.09284891\n",
      "Iteration 6777, loss = 0.09283046\n",
      "Iteration 6778, loss = 0.09281230\n",
      "Iteration 6779, loss = 0.09279412\n",
      "Iteration 6780, loss = 0.09277572\n",
      "Iteration 6781, loss = 0.09275749\n",
      "Iteration 6782, loss = 0.09273928\n",
      "Iteration 6783, loss = 0.09272112\n",
      "Iteration 6784, loss = 0.09270291\n",
      "Iteration 6785, loss = 0.09268449\n",
      "Iteration 6786, loss = 0.09266661\n",
      "Iteration 6787, loss = 0.09264813\n",
      "Iteration 6788, loss = 0.09263000\n",
      "Iteration 6789, loss = 0.09261222\n",
      "Iteration 6790, loss = 0.09259381\n",
      "Iteration 6791, loss = 0.09257559\n",
      "Iteration 6792, loss = 0.09255730\n",
      "Iteration 6793, loss = 0.09253910\n",
      "Iteration 6794, loss = 0.09252110\n",
      "Iteration 6795, loss = 0.09250278\n",
      "Iteration 6796, loss = 0.09248468\n",
      "Iteration 6797, loss = 0.09246654\n",
      "Iteration 6798, loss = 0.09244866\n",
      "Iteration 6799, loss = 0.09243023\n",
      "Iteration 6800, loss = 0.09241201\n",
      "Iteration 6801, loss = 0.09239415\n",
      "Iteration 6802, loss = 0.09237582\n",
      "Iteration 6803, loss = 0.09235784\n",
      "Iteration 6804, loss = 0.09233979\n",
      "Iteration 6805, loss = 0.09232151\n",
      "Iteration 6806, loss = 0.09230326\n",
      "Iteration 6807, loss = 0.09228537\n",
      "Iteration 6808, loss = 0.09226732\n",
      "Iteration 6809, loss = 0.09224897\n",
      "Iteration 6810, loss = 0.09223103\n",
      "Iteration 6811, loss = 0.09221285\n",
      "Iteration 6812, loss = 0.09219472\n",
      "Iteration 6813, loss = 0.09217654\n",
      "Iteration 6814, loss = 0.09215884\n",
      "Iteration 6815, loss = 0.09214081\n",
      "Iteration 6816, loss = 0.09212293\n",
      "Iteration 6817, loss = 0.09210438\n",
      "Iteration 6818, loss = 0.09208664\n",
      "Iteration 6819, loss = 0.09206856\n",
      "Iteration 6820, loss = 0.09205063\n",
      "Iteration 6821, loss = 0.09203291\n",
      "Iteration 6822, loss = 0.09201483\n",
      "Iteration 6823, loss = 0.09199699\n",
      "Iteration 6824, loss = 0.09197915\n",
      "Iteration 6825, loss = 0.09196113\n",
      "Iteration 6826, loss = 0.09194356\n",
      "Iteration 6827, loss = 0.09192562\n",
      "Iteration 6828, loss = 0.09190772\n",
      "Iteration 6829, loss = 0.09188971\n",
      "Iteration 6830, loss = 0.09187173\n",
      "Iteration 6831, loss = 0.09185403\n",
      "Iteration 6832, loss = 0.09183603\n",
      "Iteration 6833, loss = 0.09181835\n",
      "Iteration 6834, loss = 0.09180058\n",
      "Iteration 6835, loss = 0.09178256\n",
      "Iteration 6836, loss = 0.09176463\n",
      "Iteration 6837, loss = 0.09174700\n",
      "Iteration 6838, loss = 0.09172903\n",
      "Iteration 6839, loss = 0.09171132\n",
      "Iteration 6840, loss = 0.09169332\n",
      "Iteration 6841, loss = 0.09167518\n",
      "Iteration 6842, loss = 0.09165759\n",
      "Iteration 6843, loss = 0.09163961\n",
      "Iteration 6844, loss = 0.09162188\n",
      "Iteration 6845, loss = 0.09160419\n",
      "Iteration 6846, loss = 0.09158626\n",
      "Iteration 6847, loss = 0.09156840\n",
      "Iteration 6848, loss = 0.09155076\n",
      "Iteration 6849, loss = 0.09153266\n",
      "Iteration 6850, loss = 0.09151488\n",
      "Iteration 6851, loss = 0.09149719\n",
      "Iteration 6852, loss = 0.09147939\n",
      "Iteration 6853, loss = 0.09146144\n",
      "Iteration 6854, loss = 0.09144376\n",
      "Iteration 6855, loss = 0.09142604\n",
      "Iteration 6856, loss = 0.09140819\n",
      "Iteration 6857, loss = 0.09139060\n",
      "Iteration 6858, loss = 0.09137257\n",
      "Iteration 6859, loss = 0.09135526\n",
      "Iteration 6860, loss = 0.09133742\n",
      "Iteration 6861, loss = 0.09131978\n",
      "Iteration 6862, loss = 0.09130202\n",
      "Iteration 6863, loss = 0.09128431\n",
      "Iteration 6864, loss = 0.09126681\n",
      "Iteration 6865, loss = 0.09124904\n",
      "Iteration 6866, loss = 0.09123140\n",
      "Iteration 6867, loss = 0.09121418\n",
      "Iteration 6868, loss = 0.09119663\n",
      "Iteration 6869, loss = 0.09117903\n",
      "Iteration 6870, loss = 0.09116118\n",
      "Iteration 6871, loss = 0.09114356\n",
      "Iteration 6872, loss = 0.09112588\n",
      "Iteration 6873, loss = 0.09110846\n",
      "Iteration 6874, loss = 0.09109071\n",
      "Iteration 6875, loss = 0.09107339\n",
      "Iteration 6876, loss = 0.09105554\n",
      "Iteration 6877, loss = 0.09103793\n",
      "Iteration 6878, loss = 0.09102045\n",
      "Iteration 6879, loss = 0.09100284\n",
      "Iteration 6880, loss = 0.09098515\n",
      "Iteration 6881, loss = 0.09096759\n",
      "Iteration 6882, loss = 0.09095010\n",
      "Iteration 6883, loss = 0.09093258\n",
      "Iteration 6884, loss = 0.09091484\n",
      "Iteration 6885, loss = 0.09089715\n",
      "Iteration 6886, loss = 0.09087977\n",
      "Iteration 6887, loss = 0.09086227\n",
      "Iteration 6888, loss = 0.09084480\n",
      "Iteration 6889, loss = 0.09082715\n",
      "Iteration 6890, loss = 0.09080967\n",
      "Iteration 6891, loss = 0.09079201\n",
      "Iteration 6892, loss = 0.09077463\n",
      "Iteration 6893, loss = 0.09075706\n",
      "Iteration 6894, loss = 0.09073963\n",
      "Iteration 6895, loss = 0.09072226\n",
      "Iteration 6896, loss = 0.09070480\n",
      "Iteration 6897, loss = 0.09068730\n",
      "Iteration 6898, loss = 0.09067002\n",
      "Iteration 6899, loss = 0.09065227\n",
      "Iteration 6900, loss = 0.09063495\n",
      "Iteration 6901, loss = 0.09061762\n",
      "Iteration 6902, loss = 0.09060005\n",
      "Iteration 6903, loss = 0.09058276\n",
      "Iteration 6904, loss = 0.09056507\n",
      "Iteration 6905, loss = 0.09054793\n",
      "Iteration 6906, loss = 0.09053040\n",
      "Iteration 6907, loss = 0.09051318\n",
      "Iteration 6908, loss = 0.09049542\n",
      "Iteration 6909, loss = 0.09047836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6910, loss = 0.09046076\n",
      "Iteration 6911, loss = 0.09044338\n",
      "Iteration 6912, loss = 0.09042613\n",
      "Iteration 6913, loss = 0.09040854\n",
      "Iteration 6914, loss = 0.09039159\n",
      "Iteration 6915, loss = 0.09037417\n",
      "Iteration 6916, loss = 0.09035670\n",
      "Iteration 6917, loss = 0.09033932\n",
      "Iteration 6918, loss = 0.09032203\n",
      "Iteration 6919, loss = 0.09030467\n",
      "Iteration 6920, loss = 0.09028741\n",
      "Iteration 6921, loss = 0.09027027\n",
      "Iteration 6922, loss = 0.09025291\n",
      "Iteration 6923, loss = 0.09023566\n",
      "Iteration 6924, loss = 0.09021816\n",
      "Iteration 6925, loss = 0.09020111\n",
      "Iteration 6926, loss = 0.09018378\n",
      "Iteration 6927, loss = 0.09016663\n",
      "Iteration 6928, loss = 0.09014938\n",
      "Iteration 6929, loss = 0.09013176\n",
      "Iteration 6930, loss = 0.09011472\n",
      "Iteration 6931, loss = 0.09009774\n",
      "Iteration 6932, loss = 0.09008032\n",
      "Iteration 6933, loss = 0.09006299\n",
      "Iteration 6934, loss = 0.09004594\n",
      "Iteration 6935, loss = 0.09002893\n",
      "Iteration 6936, loss = 0.09001163\n",
      "Iteration 6937, loss = 0.08999445\n",
      "Iteration 6938, loss = 0.08997716\n",
      "Iteration 6939, loss = 0.08996016\n",
      "Iteration 6940, loss = 0.08994306\n",
      "Iteration 6941, loss = 0.08992573\n",
      "Iteration 6942, loss = 0.08990852\n",
      "Iteration 6943, loss = 0.08989132\n",
      "Iteration 6944, loss = 0.08987425\n",
      "Iteration 6945, loss = 0.08985706\n",
      "Iteration 6946, loss = 0.08984014\n",
      "Iteration 6947, loss = 0.08982271\n",
      "Iteration 6948, loss = 0.08980551\n",
      "Iteration 6949, loss = 0.08978854\n",
      "Iteration 6950, loss = 0.08977146\n",
      "Iteration 6951, loss = 0.08975436\n",
      "Iteration 6952, loss = 0.08973709\n",
      "Iteration 6953, loss = 0.08972024\n",
      "Iteration 6954, loss = 0.08970317\n",
      "Iteration 6955, loss = 0.08968599\n",
      "Iteration 6956, loss = 0.08966894\n",
      "Iteration 6957, loss = 0.08965190\n",
      "Iteration 6958, loss = 0.08963491\n",
      "Iteration 6959, loss = 0.08961791\n",
      "Iteration 6960, loss = 0.08960097\n",
      "Iteration 6961, loss = 0.08958387\n",
      "Iteration 6962, loss = 0.08956695\n",
      "Iteration 6963, loss = 0.08954981\n",
      "Iteration 6964, loss = 0.08953301\n",
      "Iteration 6965, loss = 0.08951588\n",
      "Iteration 6966, loss = 0.08949906\n",
      "Iteration 6967, loss = 0.08948208\n",
      "Iteration 6968, loss = 0.08946494\n",
      "Iteration 6969, loss = 0.08944800\n",
      "Iteration 6970, loss = 0.08943120\n",
      "Iteration 6971, loss = 0.08941391\n",
      "Iteration 6972, loss = 0.08939698\n",
      "Iteration 6973, loss = 0.08938018\n",
      "Iteration 6974, loss = 0.08936323\n",
      "Iteration 6975, loss = 0.08934623\n",
      "Iteration 6976, loss = 0.08932921\n",
      "Iteration 6977, loss = 0.08931216\n",
      "Iteration 6978, loss = 0.08929508\n",
      "Iteration 6979, loss = 0.08927823\n",
      "Iteration 6980, loss = 0.08926141\n",
      "Iteration 6981, loss = 0.08924443\n",
      "Iteration 6982, loss = 0.08922748\n",
      "Iteration 6983, loss = 0.08921062\n",
      "Iteration 6984, loss = 0.08919361\n",
      "Iteration 6985, loss = 0.08917654\n",
      "Iteration 6986, loss = 0.08915963\n",
      "Iteration 6987, loss = 0.08914296\n",
      "Iteration 6988, loss = 0.08912624\n",
      "Iteration 6989, loss = 0.08910897\n",
      "Iteration 6990, loss = 0.08909261\n",
      "Iteration 6991, loss = 0.08907543\n",
      "Iteration 6992, loss = 0.08905868\n",
      "Iteration 6993, loss = 0.08904176\n",
      "Iteration 6994, loss = 0.08902497\n",
      "Iteration 6995, loss = 0.08900810\n",
      "Iteration 6996, loss = 0.08899134\n",
      "Iteration 6997, loss = 0.08897442\n",
      "Iteration 6998, loss = 0.08895798\n",
      "Iteration 6999, loss = 0.08894096\n",
      "Iteration 7000, loss = 0.08892415\n",
      "Iteration 7001, loss = 0.08890753\n",
      "Iteration 7002, loss = 0.08889085\n",
      "Iteration 7003, loss = 0.08887409\n",
      "Iteration 7004, loss = 0.08885714\n",
      "Iteration 7005, loss = 0.08884062\n",
      "Iteration 7006, loss = 0.08882362\n",
      "Iteration 7007, loss = 0.08880720\n",
      "Iteration 7008, loss = 0.08879019\n",
      "Iteration 7009, loss = 0.08877371\n",
      "Iteration 7010, loss = 0.08875686\n",
      "Iteration 7011, loss = 0.08873991\n",
      "Iteration 7012, loss = 0.08872346\n",
      "Iteration 7013, loss = 0.08870652\n",
      "Iteration 7014, loss = 0.08868994\n",
      "Iteration 7015, loss = 0.08867310\n",
      "Iteration 7016, loss = 0.08865637\n",
      "Iteration 7017, loss = 0.08863967\n",
      "Iteration 7018, loss = 0.08862312\n",
      "Iteration 7019, loss = 0.08860630\n",
      "Iteration 7020, loss = 0.08858984\n",
      "Iteration 7021, loss = 0.08857293\n",
      "Iteration 7022, loss = 0.08855633\n",
      "Iteration 7023, loss = 0.08853976\n",
      "Iteration 7024, loss = 0.08852308\n",
      "Iteration 7025, loss = 0.08850657\n",
      "Iteration 7026, loss = 0.08848973\n",
      "Iteration 7027, loss = 0.08847335\n",
      "Iteration 7028, loss = 0.08845667\n",
      "Iteration 7029, loss = 0.08844003\n",
      "Iteration 7030, loss = 0.08842340\n",
      "Iteration 7031, loss = 0.08840673\n",
      "Iteration 7032, loss = 0.08839012\n",
      "Iteration 7033, loss = 0.08837346\n",
      "Iteration 7034, loss = 0.08835702\n",
      "Iteration 7035, loss = 0.08834041\n",
      "Iteration 7036, loss = 0.08832396\n",
      "Iteration 7037, loss = 0.08830715\n",
      "Iteration 7038, loss = 0.08829071\n",
      "Iteration 7039, loss = 0.08827397\n",
      "Iteration 7040, loss = 0.08825771\n",
      "Iteration 7041, loss = 0.08824084\n",
      "Iteration 7042, loss = 0.08822448\n",
      "Iteration 7043, loss = 0.08820799\n",
      "Iteration 7044, loss = 0.08819165\n",
      "Iteration 7045, loss = 0.08817493\n",
      "Iteration 7046, loss = 0.08815845\n",
      "Iteration 7047, loss = 0.08814186\n",
      "Iteration 7048, loss = 0.08812553\n",
      "Iteration 7049, loss = 0.08810892\n",
      "Iteration 7050, loss = 0.08809241\n",
      "Iteration 7051, loss = 0.08807621\n",
      "Iteration 7052, loss = 0.08805964\n",
      "Iteration 7053, loss = 0.08804305\n",
      "Iteration 7054, loss = 0.08802652\n",
      "Iteration 7055, loss = 0.08801020\n",
      "Iteration 7056, loss = 0.08799363\n",
      "Iteration 7057, loss = 0.08797726\n",
      "Iteration 7058, loss = 0.08796067\n",
      "Iteration 7059, loss = 0.08794448\n",
      "Iteration 7060, loss = 0.08792800\n",
      "Iteration 7061, loss = 0.08791167\n",
      "Iteration 7062, loss = 0.08789532\n",
      "Iteration 7063, loss = 0.08787882\n",
      "Iteration 7064, loss = 0.08786242\n",
      "Iteration 7065, loss = 0.08784584\n",
      "Iteration 7066, loss = 0.08782959\n",
      "Iteration 7067, loss = 0.08781336\n",
      "Iteration 7068, loss = 0.08779684\n",
      "Iteration 7069, loss = 0.08778033\n",
      "Iteration 7070, loss = 0.08776417\n",
      "Iteration 7071, loss = 0.08774769\n",
      "Iteration 7072, loss = 0.08773148\n",
      "Iteration 7073, loss = 0.08771489\n",
      "Iteration 7074, loss = 0.08769867\n",
      "Iteration 7075, loss = 0.08768228\n",
      "Iteration 7076, loss = 0.08766612\n",
      "Iteration 7077, loss = 0.08764944\n",
      "Iteration 7078, loss = 0.08763323\n",
      "Iteration 7079, loss = 0.08761696\n",
      "Iteration 7080, loss = 0.08760054\n",
      "Iteration 7081, loss = 0.08758435\n",
      "Iteration 7082, loss = 0.08756784\n",
      "Iteration 7083, loss = 0.08755165\n",
      "Iteration 7084, loss = 0.08753538\n",
      "Iteration 7085, loss = 0.08751917\n",
      "Iteration 7086, loss = 0.08750282\n",
      "Iteration 7087, loss = 0.08748651\n",
      "Iteration 7088, loss = 0.08747018\n",
      "Iteration 7089, loss = 0.08745397\n",
      "Iteration 7090, loss = 0.08743769\n",
      "Iteration 7091, loss = 0.08742150\n",
      "Iteration 7092, loss = 0.08740521\n",
      "Iteration 7093, loss = 0.08738901\n",
      "Iteration 7094, loss = 0.08737287\n",
      "Iteration 7095, loss = 0.08735637\n",
      "Iteration 7096, loss = 0.08734040\n",
      "Iteration 7097, loss = 0.08732396\n",
      "Iteration 7098, loss = 0.08730791\n",
      "Iteration 7099, loss = 0.08729141\n",
      "Iteration 7100, loss = 0.08727532\n",
      "Iteration 7101, loss = 0.08725913\n",
      "Iteration 7102, loss = 0.08724330\n",
      "Iteration 7103, loss = 0.08722680\n",
      "Iteration 7104, loss = 0.08721064\n",
      "Iteration 7105, loss = 0.08719443\n",
      "Iteration 7106, loss = 0.08717840\n",
      "Iteration 7107, loss = 0.08716233\n",
      "Iteration 7108, loss = 0.08714613\n",
      "Iteration 7109, loss = 0.08712994\n",
      "Iteration 7110, loss = 0.08711362\n",
      "Iteration 7111, loss = 0.08709776\n",
      "Iteration 7112, loss = 0.08708152\n",
      "Iteration 7113, loss = 0.08706588\n",
      "Iteration 7114, loss = 0.08704964\n",
      "Iteration 7115, loss = 0.08703335\n",
      "Iteration 7116, loss = 0.08701716\n",
      "Iteration 7117, loss = 0.08700131\n",
      "Iteration 7118, loss = 0.08698527\n",
      "Iteration 7119, loss = 0.08696911\n",
      "Iteration 7120, loss = 0.08695318\n",
      "Iteration 7121, loss = 0.08693736\n",
      "Iteration 7122, loss = 0.08692105\n",
      "Iteration 7123, loss = 0.08690497\n",
      "Iteration 7124, loss = 0.08688892\n",
      "Iteration 7125, loss = 0.08687273\n",
      "Iteration 7126, loss = 0.08685655\n",
      "Iteration 7127, loss = 0.08684060\n",
      "Iteration 7128, loss = 0.08682463\n",
      "Iteration 7129, loss = 0.08680870\n",
      "Iteration 7130, loss = 0.08679235\n",
      "Iteration 7131, loss = 0.08677628\n",
      "Iteration 7132, loss = 0.08676045\n",
      "Iteration 7133, loss = 0.08674435\n",
      "Iteration 7134, loss = 0.08672824\n",
      "Iteration 7135, loss = 0.08671229\n",
      "Iteration 7136, loss = 0.08669636\n",
      "Iteration 7137, loss = 0.08668050\n",
      "Iteration 7138, loss = 0.08666442\n",
      "Iteration 7139, loss = 0.08664831\n",
      "Iteration 7140, loss = 0.08663235\n",
      "Iteration 7141, loss = 0.08661657\n",
      "Iteration 7142, loss = 0.08660058\n",
      "Iteration 7143, loss = 0.08658449\n",
      "Iteration 7144, loss = 0.08656876\n",
      "Iteration 7145, loss = 0.08655263\n",
      "Iteration 7146, loss = 0.08653670\n",
      "Iteration 7147, loss = 0.08652061\n",
      "Iteration 7148, loss = 0.08650494\n",
      "Iteration 7149, loss = 0.08648878\n",
      "Iteration 7150, loss = 0.08647285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7151, loss = 0.08645733\n",
      "Iteration 7152, loss = 0.08644127\n",
      "Iteration 7153, loss = 0.08642529\n",
      "Iteration 7154, loss = 0.08640935\n",
      "Iteration 7155, loss = 0.08639368\n",
      "Iteration 7156, loss = 0.08637791\n",
      "Iteration 7157, loss = 0.08636214\n",
      "Iteration 7158, loss = 0.08634612\n",
      "Iteration 7159, loss = 0.08633024\n",
      "Iteration 7160, loss = 0.08631451\n",
      "Iteration 7161, loss = 0.08629865\n",
      "Iteration 7162, loss = 0.08628293\n",
      "Iteration 7163, loss = 0.08626704\n",
      "Iteration 7164, loss = 0.08625129\n",
      "Iteration 7165, loss = 0.08623539\n",
      "Iteration 7166, loss = 0.08621957\n",
      "Iteration 7167, loss = 0.08620367\n",
      "Iteration 7168, loss = 0.08618774\n",
      "Iteration 7169, loss = 0.08617220\n",
      "Iteration 7170, loss = 0.08615636\n",
      "Iteration 7171, loss = 0.08614056\n",
      "Iteration 7172, loss = 0.08612483\n",
      "Iteration 7173, loss = 0.08610914\n",
      "Iteration 7174, loss = 0.08609325\n",
      "Iteration 7175, loss = 0.08607737\n",
      "Iteration 7176, loss = 0.08606178\n",
      "Iteration 7177, loss = 0.08604605\n",
      "Iteration 7178, loss = 0.08603045\n",
      "Iteration 7179, loss = 0.08601442\n",
      "Iteration 7180, loss = 0.08599870\n",
      "Iteration 7181, loss = 0.08598314\n",
      "Iteration 7182, loss = 0.08596728\n",
      "Iteration 7183, loss = 0.08595148\n",
      "Iteration 7184, loss = 0.08593577\n",
      "Iteration 7185, loss = 0.08592017\n",
      "Iteration 7186, loss = 0.08590431\n",
      "Iteration 7187, loss = 0.08588871\n",
      "Iteration 7188, loss = 0.08587309\n",
      "Iteration 7189, loss = 0.08585748\n",
      "Iteration 7190, loss = 0.08584170\n",
      "Iteration 7191, loss = 0.08582600\n",
      "Iteration 7192, loss = 0.08581031\n",
      "Iteration 7193, loss = 0.08579488\n",
      "Iteration 7194, loss = 0.08577915\n",
      "Iteration 7195, loss = 0.08576347\n",
      "Iteration 7196, loss = 0.08574784\n",
      "Iteration 7197, loss = 0.08573224\n",
      "Iteration 7198, loss = 0.08571655\n",
      "Iteration 7199, loss = 0.08570093\n",
      "Iteration 7200, loss = 0.08568529\n",
      "Iteration 7201, loss = 0.08566971\n",
      "Iteration 7202, loss = 0.08565418\n",
      "Iteration 7203, loss = 0.08563862\n",
      "Iteration 7204, loss = 0.08562293\n",
      "Iteration 7205, loss = 0.08560746\n",
      "Iteration 7206, loss = 0.08559190\n",
      "Iteration 7207, loss = 0.08557626\n",
      "Iteration 7208, loss = 0.08556071\n",
      "Iteration 7209, loss = 0.08554524\n",
      "Iteration 7210, loss = 0.08552939\n",
      "Iteration 7211, loss = 0.08551387\n",
      "Iteration 7212, loss = 0.08549846\n",
      "Iteration 7213, loss = 0.08548292\n",
      "Iteration 7214, loss = 0.08546754\n",
      "Iteration 7215, loss = 0.08545176\n",
      "Iteration 7216, loss = 0.08543631\n",
      "Iteration 7217, loss = 0.08542096\n",
      "Iteration 7218, loss = 0.08540530\n",
      "Iteration 7219, loss = 0.08538974\n",
      "Iteration 7220, loss = 0.08537453\n",
      "Iteration 7221, loss = 0.08535882\n",
      "Iteration 7222, loss = 0.08534307\n",
      "Iteration 7223, loss = 0.08532773\n",
      "Iteration 7224, loss = 0.08531230\n",
      "Iteration 7225, loss = 0.08529682\n",
      "Iteration 7226, loss = 0.08528123\n",
      "Iteration 7227, loss = 0.08526573\n",
      "Iteration 7228, loss = 0.08525030\n",
      "Iteration 7229, loss = 0.08523474\n",
      "Iteration 7230, loss = 0.08521966\n",
      "Iteration 7231, loss = 0.08520392\n",
      "Iteration 7232, loss = 0.08518848\n",
      "Iteration 7233, loss = 0.08517304\n",
      "Iteration 7234, loss = 0.08515741\n",
      "Iteration 7235, loss = 0.08514233\n",
      "Iteration 7236, loss = 0.08512688\n",
      "Iteration 7237, loss = 0.08511139\n",
      "Iteration 7238, loss = 0.08509600\n",
      "Iteration 7239, loss = 0.08508069\n",
      "Iteration 7240, loss = 0.08506526\n",
      "Iteration 7241, loss = 0.08504978\n",
      "Iteration 7242, loss = 0.08503455\n",
      "Iteration 7243, loss = 0.08501930\n",
      "Iteration 7244, loss = 0.08500378\n",
      "Iteration 7245, loss = 0.08498859\n",
      "Iteration 7246, loss = 0.08497331\n",
      "Iteration 7247, loss = 0.08495769\n",
      "Iteration 7248, loss = 0.08494234\n",
      "Iteration 7249, loss = 0.08492686\n",
      "Iteration 7250, loss = 0.08491166\n",
      "Iteration 7251, loss = 0.08489622\n",
      "Iteration 7252, loss = 0.08488090\n",
      "Iteration 7253, loss = 0.08486565\n",
      "Iteration 7254, loss = 0.08485007\n",
      "Iteration 7255, loss = 0.08483483\n",
      "Iteration 7256, loss = 0.08481936\n",
      "Iteration 7257, loss = 0.08480400\n",
      "Iteration 7258, loss = 0.08478883\n",
      "Iteration 7259, loss = 0.08477358\n",
      "Iteration 7260, loss = 0.08475815\n",
      "Iteration 7261, loss = 0.08474299\n",
      "Iteration 7262, loss = 0.08472748\n",
      "Iteration 7263, loss = 0.08471246\n",
      "Iteration 7264, loss = 0.08469698\n",
      "Iteration 7265, loss = 0.08468168\n",
      "Iteration 7266, loss = 0.08466667\n",
      "Iteration 7267, loss = 0.08465131\n",
      "Iteration 7268, loss = 0.08463587\n",
      "Iteration 7269, loss = 0.08462068\n",
      "Iteration 7270, loss = 0.08460537\n",
      "Iteration 7271, loss = 0.08459018\n",
      "Iteration 7272, loss = 0.08457479\n",
      "Iteration 7273, loss = 0.08455964\n",
      "Iteration 7274, loss = 0.08454431\n",
      "Iteration 7275, loss = 0.08452914\n",
      "Iteration 7276, loss = 0.08451392\n",
      "Iteration 7277, loss = 0.08449861\n",
      "Iteration 7278, loss = 0.08448328\n",
      "Iteration 7279, loss = 0.08446825\n",
      "Iteration 7280, loss = 0.08445301\n",
      "Iteration 7281, loss = 0.08443758\n",
      "Iteration 7282, loss = 0.08442247\n",
      "Iteration 7283, loss = 0.08440744\n",
      "Iteration 7284, loss = 0.08439222\n",
      "Iteration 7285, loss = 0.08437703\n",
      "Iteration 7286, loss = 0.08436183\n",
      "Iteration 7287, loss = 0.08434657\n",
      "Iteration 7288, loss = 0.08433157\n",
      "Iteration 7289, loss = 0.08431618\n",
      "Iteration 7290, loss = 0.08430127\n",
      "Iteration 7291, loss = 0.08428591\n",
      "Iteration 7292, loss = 0.08427080\n",
      "Iteration 7293, loss = 0.08425577\n",
      "Iteration 7294, loss = 0.08424048\n",
      "Iteration 7295, loss = 0.08422547\n",
      "Iteration 7296, loss = 0.08421026\n",
      "Iteration 7297, loss = 0.08419535\n",
      "Iteration 7298, loss = 0.08418005\n",
      "Iteration 7299, loss = 0.08416499\n",
      "Iteration 7300, loss = 0.08414997\n",
      "Iteration 7301, loss = 0.08413491\n",
      "Iteration 7302, loss = 0.08411982\n",
      "Iteration 7303, loss = 0.08410476\n",
      "Iteration 7304, loss = 0.08408977\n",
      "Iteration 7305, loss = 0.08407466\n",
      "Iteration 7306, loss = 0.08405956\n",
      "Iteration 7307, loss = 0.08404436\n",
      "Iteration 7308, loss = 0.08402968\n",
      "Iteration 7309, loss = 0.08401441\n",
      "Iteration 7310, loss = 0.08399936\n",
      "Iteration 7311, loss = 0.08398434\n",
      "Iteration 7312, loss = 0.08396923\n",
      "Iteration 7313, loss = 0.08395418\n",
      "Iteration 7314, loss = 0.08393920\n",
      "Iteration 7315, loss = 0.08392419\n",
      "Iteration 7316, loss = 0.08390912\n",
      "Iteration 7317, loss = 0.08389433\n",
      "Iteration 7318, loss = 0.08387922\n",
      "Iteration 7319, loss = 0.08386418\n",
      "Iteration 7320, loss = 0.08384920\n",
      "Iteration 7321, loss = 0.08383408\n",
      "Iteration 7322, loss = 0.08381926\n",
      "Iteration 7323, loss = 0.08380435\n",
      "Iteration 7324, loss = 0.08378935\n",
      "Iteration 7325, loss = 0.08377436\n",
      "Iteration 7326, loss = 0.08375950\n",
      "Iteration 7327, loss = 0.08374457\n",
      "Iteration 7328, loss = 0.08372975\n",
      "Iteration 7329, loss = 0.08371485\n",
      "Iteration 7330, loss = 0.08369975\n",
      "Iteration 7331, loss = 0.08368488\n",
      "Iteration 7332, loss = 0.08367000\n",
      "Iteration 7333, loss = 0.08365520\n",
      "Iteration 7334, loss = 0.08364039\n",
      "Iteration 7335, loss = 0.08362540\n",
      "Iteration 7336, loss = 0.08361054\n",
      "Iteration 7337, loss = 0.08359568\n",
      "Iteration 7338, loss = 0.08358070\n",
      "Iteration 7339, loss = 0.08356581\n",
      "Iteration 7340, loss = 0.08355100\n",
      "Iteration 7341, loss = 0.08353619\n",
      "Iteration 7342, loss = 0.08352123\n",
      "Iteration 7343, loss = 0.08350642\n",
      "Iteration 7344, loss = 0.08349160\n",
      "Iteration 7345, loss = 0.08347693\n",
      "Iteration 7346, loss = 0.08346188\n",
      "Iteration 7347, loss = 0.08344711\n",
      "Iteration 7348, loss = 0.08343240\n",
      "Iteration 7349, loss = 0.08341748\n",
      "Iteration 7350, loss = 0.08340275\n",
      "Iteration 7351, loss = 0.08338794\n",
      "Iteration 7352, loss = 0.08337322\n",
      "Iteration 7353, loss = 0.08335849\n",
      "Iteration 7354, loss = 0.08334344\n",
      "Iteration 7355, loss = 0.08332886\n",
      "Iteration 7356, loss = 0.08331399\n",
      "Iteration 7357, loss = 0.08329914\n",
      "Iteration 7358, loss = 0.08328452\n",
      "Iteration 7359, loss = 0.08326994\n",
      "Iteration 7360, loss = 0.08325494\n",
      "Iteration 7361, loss = 0.08324017\n",
      "Iteration 7362, loss = 0.08322566\n",
      "Iteration 7363, loss = 0.08321061\n",
      "Iteration 7364, loss = 0.08319607\n",
      "Iteration 7365, loss = 0.08318118\n",
      "Iteration 7366, loss = 0.08316655\n",
      "Iteration 7367, loss = 0.08315175\n",
      "Iteration 7368, loss = 0.08313696\n",
      "Iteration 7369, loss = 0.08312225\n",
      "Iteration 7370, loss = 0.08310761\n",
      "Iteration 7371, loss = 0.08309273\n",
      "Iteration 7372, loss = 0.08307809\n",
      "Iteration 7373, loss = 0.08306335\n",
      "Iteration 7374, loss = 0.08304877\n",
      "Iteration 7375, loss = 0.08303402\n",
      "Iteration 7376, loss = 0.08301955\n",
      "Iteration 7377, loss = 0.08300459\n",
      "Iteration 7378, loss = 0.08299011\n",
      "Iteration 7379, loss = 0.08297522\n",
      "Iteration 7380, loss = 0.08296065\n",
      "Iteration 7381, loss = 0.08294619\n",
      "Iteration 7382, loss = 0.08293138\n",
      "Iteration 7383, loss = 0.08291677\n",
      "Iteration 7384, loss = 0.08290238\n",
      "Iteration 7385, loss = 0.08288757\n",
      "Iteration 7386, loss = 0.08287281\n",
      "Iteration 7387, loss = 0.08285832\n",
      "Iteration 7388, loss = 0.08284363\n",
      "Iteration 7389, loss = 0.08282905\n",
      "Iteration 7390, loss = 0.08281436\n",
      "Iteration 7391, loss = 0.08279990\n",
      "Iteration 7392, loss = 0.08278538\n",
      "Iteration 7393, loss = 0.08277066\n",
      "Iteration 7394, loss = 0.08275612\n",
      "Iteration 7395, loss = 0.08274139\n",
      "Iteration 7396, loss = 0.08272711\n",
      "Iteration 7397, loss = 0.08271254\n",
      "Iteration 7398, loss = 0.08269768\n",
      "Iteration 7399, loss = 0.08268307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7400, loss = 0.08266873\n",
      "Iteration 7401, loss = 0.08265395\n",
      "Iteration 7402, loss = 0.08263939\n",
      "Iteration 7403, loss = 0.08262500\n",
      "Iteration 7404, loss = 0.08261019\n",
      "Iteration 7405, loss = 0.08259569\n",
      "Iteration 7406, loss = 0.08258132\n",
      "Iteration 7407, loss = 0.08256637\n",
      "Iteration 7408, loss = 0.08255197\n",
      "Iteration 7409, loss = 0.08253759\n",
      "Iteration 7410, loss = 0.08252299\n",
      "Iteration 7411, loss = 0.08250857\n",
      "Iteration 7412, loss = 0.08249394\n",
      "Iteration 7413, loss = 0.08247945\n",
      "Iteration 7414, loss = 0.08246501\n",
      "Iteration 7415, loss = 0.08245041\n",
      "Iteration 7416, loss = 0.08243625\n",
      "Iteration 7417, loss = 0.08242152\n",
      "Iteration 7418, loss = 0.08240713\n",
      "Iteration 7419, loss = 0.08239260\n",
      "Iteration 7420, loss = 0.08237818\n",
      "Iteration 7421, loss = 0.08236376\n",
      "Iteration 7422, loss = 0.08234924\n",
      "Iteration 7423, loss = 0.08233491\n",
      "Iteration 7424, loss = 0.08232048\n",
      "Iteration 7425, loss = 0.08230605\n",
      "Iteration 7426, loss = 0.08229168\n",
      "Iteration 7427, loss = 0.08227720\n",
      "Iteration 7428, loss = 0.08226290\n",
      "Iteration 7429, loss = 0.08224854\n",
      "Iteration 7430, loss = 0.08223421\n",
      "Iteration 7431, loss = 0.08221980\n",
      "Iteration 7432, loss = 0.08220532\n",
      "Iteration 7433, loss = 0.08219096\n",
      "Iteration 7434, loss = 0.08217644\n",
      "Iteration 7435, loss = 0.08216206\n",
      "Iteration 7436, loss = 0.08214780\n",
      "Iteration 7437, loss = 0.08213344\n",
      "Iteration 7438, loss = 0.08211891\n",
      "Iteration 7439, loss = 0.08210475\n",
      "Iteration 7440, loss = 0.08209038\n",
      "Iteration 7441, loss = 0.08207588\n",
      "Iteration 7442, loss = 0.08206150\n",
      "Iteration 7443, loss = 0.08204710\n",
      "Iteration 7444, loss = 0.08203297\n",
      "Iteration 7445, loss = 0.08201847\n",
      "Iteration 7446, loss = 0.08200401\n",
      "Iteration 7447, loss = 0.08198971\n",
      "Iteration 7448, loss = 0.08197574\n",
      "Iteration 7449, loss = 0.08196131\n",
      "Iteration 7450, loss = 0.08194672\n",
      "Iteration 7451, loss = 0.08193248\n",
      "Iteration 7452, loss = 0.08191833\n",
      "Iteration 7453, loss = 0.08190391\n",
      "Iteration 7454, loss = 0.08188977\n",
      "Iteration 7455, loss = 0.08187521\n",
      "Iteration 7456, loss = 0.08186127\n",
      "Iteration 7457, loss = 0.08184674\n",
      "Iteration 7458, loss = 0.08183240\n",
      "Iteration 7459, loss = 0.08181809\n",
      "Iteration 7460, loss = 0.08180394\n",
      "Iteration 7461, loss = 0.08178972\n",
      "Iteration 7462, loss = 0.08177543\n",
      "Iteration 7463, loss = 0.08176125\n",
      "Iteration 7464, loss = 0.08174682\n",
      "Iteration 7465, loss = 0.08173268\n",
      "Iteration 7466, loss = 0.08171854\n",
      "Iteration 7467, loss = 0.08170416\n",
      "Iteration 7468, loss = 0.08169003\n",
      "Iteration 7469, loss = 0.08167578\n",
      "Iteration 7470, loss = 0.08166150\n",
      "Iteration 7471, loss = 0.08164728\n",
      "Iteration 7472, loss = 0.08163303\n",
      "Iteration 7473, loss = 0.08161878\n",
      "Iteration 7474, loss = 0.08160471\n",
      "Iteration 7475, loss = 0.08159039\n",
      "Iteration 7476, loss = 0.08157623\n",
      "Iteration 7477, loss = 0.08156198\n",
      "Iteration 7478, loss = 0.08154761\n",
      "Iteration 7479, loss = 0.08153354\n",
      "Iteration 7480, loss = 0.08151941\n",
      "Iteration 7481, loss = 0.08150524\n",
      "Iteration 7482, loss = 0.08149088\n",
      "Iteration 7483, loss = 0.08147692\n",
      "Iteration 7484, loss = 0.08146266\n",
      "Iteration 7485, loss = 0.08144838\n",
      "Iteration 7486, loss = 0.08143434\n",
      "Iteration 7487, loss = 0.08142016\n",
      "Iteration 7488, loss = 0.08140603\n",
      "Iteration 7489, loss = 0.08139190\n",
      "Iteration 7490, loss = 0.08137788\n",
      "Iteration 7491, loss = 0.08136364\n",
      "Iteration 7492, loss = 0.08134935\n",
      "Iteration 7493, loss = 0.08133541\n",
      "Iteration 7494, loss = 0.08132116\n",
      "Iteration 7495, loss = 0.08130704\n",
      "Iteration 7496, loss = 0.08129291\n",
      "Iteration 7497, loss = 0.08127903\n",
      "Iteration 7498, loss = 0.08126494\n",
      "Iteration 7499, loss = 0.08125060\n",
      "Iteration 7500, loss = 0.08123647\n",
      "Iteration 7501, loss = 0.08122256\n",
      "Iteration 7502, loss = 0.08120832\n",
      "Iteration 7503, loss = 0.08119429\n",
      "Iteration 7504, loss = 0.08118016\n",
      "Iteration 7505, loss = 0.08116612\n",
      "Iteration 7506, loss = 0.08115202\n",
      "Iteration 7507, loss = 0.08113799\n",
      "Iteration 7508, loss = 0.08112403\n",
      "Iteration 7509, loss = 0.08111007\n",
      "Iteration 7510, loss = 0.08109570\n",
      "Iteration 7511, loss = 0.08108189\n",
      "Iteration 7512, loss = 0.08106779\n",
      "Iteration 7513, loss = 0.08105364\n",
      "Iteration 7514, loss = 0.08103968\n",
      "Iteration 7515, loss = 0.08102566\n",
      "Iteration 7516, loss = 0.08101160\n",
      "Iteration 7517, loss = 0.08099758\n",
      "Iteration 7518, loss = 0.08098370\n",
      "Iteration 7519, loss = 0.08096949\n",
      "Iteration 7520, loss = 0.08095557\n",
      "Iteration 7521, loss = 0.08094151\n",
      "Iteration 7522, loss = 0.08092755\n",
      "Iteration 7523, loss = 0.08091347\n",
      "Iteration 7524, loss = 0.08089953\n",
      "Iteration 7525, loss = 0.08088573\n",
      "Iteration 7526, loss = 0.08087152\n",
      "Iteration 7527, loss = 0.08085770\n",
      "Iteration 7528, loss = 0.08084366\n",
      "Iteration 7529, loss = 0.08082973\n",
      "Iteration 7530, loss = 0.08081596\n",
      "Iteration 7531, loss = 0.08080195\n",
      "Iteration 7532, loss = 0.08078806\n",
      "Iteration 7533, loss = 0.08077419\n",
      "Iteration 7534, loss = 0.08075997\n",
      "Iteration 7535, loss = 0.08074617\n",
      "Iteration 7536, loss = 0.08073228\n",
      "Iteration 7537, loss = 0.08071829\n",
      "Iteration 7538, loss = 0.08070441\n",
      "Iteration 7539, loss = 0.08069057\n",
      "Iteration 7540, loss = 0.08067647\n",
      "Iteration 7541, loss = 0.08066260\n",
      "Iteration 7542, loss = 0.08064880\n",
      "Iteration 7543, loss = 0.08063473\n",
      "Iteration 7544, loss = 0.08062090\n",
      "Iteration 7545, loss = 0.08060694\n",
      "Iteration 7546, loss = 0.08059308\n",
      "Iteration 7547, loss = 0.08057907\n",
      "Iteration 7548, loss = 0.08056544\n",
      "Iteration 7549, loss = 0.08055142\n",
      "Iteration 7550, loss = 0.08053765\n",
      "Iteration 7551, loss = 0.08052365\n",
      "Iteration 7552, loss = 0.08050982\n",
      "Iteration 7553, loss = 0.08049604\n",
      "Iteration 7554, loss = 0.08048214\n",
      "Iteration 7555, loss = 0.08046832\n",
      "Iteration 7556, loss = 0.08045460\n",
      "Iteration 7557, loss = 0.08044072\n",
      "Iteration 7558, loss = 0.08042670\n",
      "Iteration 7559, loss = 0.08041309\n",
      "Iteration 7560, loss = 0.08039936\n",
      "Iteration 7561, loss = 0.08038539\n",
      "Iteration 7562, loss = 0.08037158\n",
      "Iteration 7563, loss = 0.08035781\n",
      "Iteration 7564, loss = 0.08034399\n",
      "Iteration 7565, loss = 0.08033023\n",
      "Iteration 7566, loss = 0.08031627\n",
      "Iteration 7567, loss = 0.08030263\n",
      "Iteration 7568, loss = 0.08028900\n",
      "Iteration 7569, loss = 0.08027512\n",
      "Iteration 7570, loss = 0.08026140\n",
      "Iteration 7571, loss = 0.08024763\n",
      "Iteration 7572, loss = 0.08023390\n",
      "Iteration 7573, loss = 0.08022012\n",
      "Iteration 7574, loss = 0.08020639\n",
      "Iteration 7575, loss = 0.08019263\n",
      "Iteration 7576, loss = 0.08017881\n",
      "Iteration 7577, loss = 0.08016523\n",
      "Iteration 7578, loss = 0.08015146\n",
      "Iteration 7579, loss = 0.08013785\n",
      "Iteration 7580, loss = 0.08012398\n",
      "Iteration 7581, loss = 0.08011034\n",
      "Iteration 7582, loss = 0.08009665\n",
      "Iteration 7583, loss = 0.08008312\n",
      "Iteration 7584, loss = 0.08006927\n",
      "Iteration 7585, loss = 0.08005561\n",
      "Iteration 7586, loss = 0.08004211\n",
      "Iteration 7587, loss = 0.08002842\n",
      "Iteration 7588, loss = 0.08001478\n",
      "Iteration 7589, loss = 0.08000098\n",
      "Iteration 7590, loss = 0.07998733\n",
      "Iteration 7591, loss = 0.07997374\n",
      "Iteration 7592, loss = 0.07996025\n",
      "Iteration 7593, loss = 0.07994641\n",
      "Iteration 7594, loss = 0.07993281\n",
      "Iteration 7595, loss = 0.07991930\n",
      "Iteration 7596, loss = 0.07990566\n",
      "Iteration 7597, loss = 0.07989195\n",
      "Iteration 7598, loss = 0.07987835\n",
      "Iteration 7599, loss = 0.07986483\n",
      "Iteration 7600, loss = 0.07985124\n",
      "Iteration 7601, loss = 0.07983768\n",
      "Iteration 7602, loss = 0.07982393\n",
      "Iteration 7603, loss = 0.07981036\n",
      "Iteration 7604, loss = 0.07979663\n",
      "Iteration 7605, loss = 0.07978315\n",
      "Iteration 7606, loss = 0.07976963\n",
      "Iteration 7607, loss = 0.07975593\n",
      "Iteration 7608, loss = 0.07974225\n",
      "Iteration 7609, loss = 0.07972887\n",
      "Iteration 7610, loss = 0.07971531\n",
      "Iteration 7611, loss = 0.07970144\n",
      "Iteration 7612, loss = 0.07968808\n",
      "Iteration 7613, loss = 0.07967439\n",
      "Iteration 7614, loss = 0.07966086\n",
      "Iteration 7615, loss = 0.07964735\n",
      "Iteration 7616, loss = 0.07963382\n",
      "Iteration 7617, loss = 0.07962021\n",
      "Iteration 7618, loss = 0.07960668\n",
      "Iteration 7619, loss = 0.07959317\n",
      "Iteration 7620, loss = 0.07957958\n",
      "Iteration 7621, loss = 0.07956605\n",
      "Iteration 7622, loss = 0.07955263\n",
      "Iteration 7623, loss = 0.07953900\n",
      "Iteration 7624, loss = 0.07952565\n",
      "Iteration 7625, loss = 0.07951201\n",
      "Iteration 7626, loss = 0.07949859\n",
      "Iteration 7627, loss = 0.07948502\n",
      "Iteration 7628, loss = 0.07947161\n",
      "Iteration 7629, loss = 0.07945805\n",
      "Iteration 7630, loss = 0.07944467\n",
      "Iteration 7631, loss = 0.07943127\n",
      "Iteration 7632, loss = 0.07941770\n",
      "Iteration 7633, loss = 0.07940422\n",
      "Iteration 7634, loss = 0.07939063\n",
      "Iteration 7635, loss = 0.07937716\n",
      "Iteration 7636, loss = 0.07936394\n",
      "Iteration 7637, loss = 0.07935020\n",
      "Iteration 7638, loss = 0.07933697\n",
      "Iteration 7639, loss = 0.07932328\n",
      "Iteration 7640, loss = 0.07930989\n",
      "Iteration 7641, loss = 0.07929644\n",
      "Iteration 7642, loss = 0.07928301\n",
      "Iteration 7643, loss = 0.07926947\n",
      "Iteration 7644, loss = 0.07925613\n",
      "Iteration 7645, loss = 0.07924248\n",
      "Iteration 7646, loss = 0.07922923\n",
      "Iteration 7647, loss = 0.07921587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7648, loss = 0.07920237\n",
      "Iteration 7649, loss = 0.07918899\n",
      "Iteration 7650, loss = 0.07917538\n",
      "Iteration 7651, loss = 0.07916205\n",
      "Iteration 7652, loss = 0.07914869\n",
      "Iteration 7653, loss = 0.07913528\n",
      "Iteration 7654, loss = 0.07912190\n",
      "Iteration 7655, loss = 0.07910863\n",
      "Iteration 7656, loss = 0.07909519\n",
      "Iteration 7657, loss = 0.07908179\n",
      "Iteration 7658, loss = 0.07906846\n",
      "Iteration 7659, loss = 0.07905508\n",
      "Iteration 7660, loss = 0.07904167\n",
      "Iteration 7661, loss = 0.07902827\n",
      "Iteration 7662, loss = 0.07901509\n",
      "Iteration 7663, loss = 0.07900168\n",
      "Iteration 7664, loss = 0.07898838\n",
      "Iteration 7665, loss = 0.07897494\n",
      "Iteration 7666, loss = 0.07896176\n",
      "Iteration 7667, loss = 0.07894833\n",
      "Iteration 7668, loss = 0.07893500\n",
      "Iteration 7669, loss = 0.07892179\n",
      "Iteration 7670, loss = 0.07890841\n",
      "Iteration 7671, loss = 0.07889503\n",
      "Iteration 7672, loss = 0.07888179\n",
      "Iteration 7673, loss = 0.07886843\n",
      "Iteration 7674, loss = 0.07885513\n",
      "Iteration 7675, loss = 0.07884177\n",
      "Iteration 7676, loss = 0.07882840\n",
      "Iteration 7677, loss = 0.07881530\n",
      "Iteration 7678, loss = 0.07880193\n",
      "Iteration 7679, loss = 0.07878851\n",
      "Iteration 7680, loss = 0.07877526\n",
      "Iteration 7681, loss = 0.07876190\n",
      "Iteration 7682, loss = 0.07874858\n",
      "Iteration 7683, loss = 0.07873539\n",
      "Iteration 7684, loss = 0.07872205\n",
      "Iteration 7685, loss = 0.07870876\n",
      "Iteration 7686, loss = 0.07869546\n",
      "Iteration 7687, loss = 0.07868216\n",
      "Iteration 7688, loss = 0.07866902\n",
      "Iteration 7689, loss = 0.07865569\n",
      "Iteration 7690, loss = 0.07864242\n",
      "Iteration 7691, loss = 0.07862901\n",
      "Iteration 7692, loss = 0.07861588\n",
      "Iteration 7693, loss = 0.07860258\n",
      "Iteration 7694, loss = 0.07858944\n",
      "Iteration 7695, loss = 0.07857616\n",
      "Iteration 7696, loss = 0.07856317\n",
      "Iteration 7697, loss = 0.07854983\n",
      "Iteration 7698, loss = 0.07853655\n",
      "Iteration 7699, loss = 0.07852348\n",
      "Iteration 7700, loss = 0.07851022\n",
      "Iteration 7701, loss = 0.07849699\n",
      "Iteration 7702, loss = 0.07848420\n",
      "Iteration 7703, loss = 0.07847067\n",
      "Iteration 7704, loss = 0.07845750\n",
      "Iteration 7705, loss = 0.07844447\n",
      "Iteration 7706, loss = 0.07843134\n",
      "Iteration 7707, loss = 0.07841815\n",
      "Iteration 7708, loss = 0.07840483\n",
      "Iteration 7709, loss = 0.07839173\n",
      "Iteration 7710, loss = 0.07837867\n",
      "Iteration 7711, loss = 0.07836547\n",
      "Iteration 7712, loss = 0.07835235\n",
      "Iteration 7713, loss = 0.07833917\n",
      "Iteration 7714, loss = 0.07832607\n",
      "Iteration 7715, loss = 0.07831281\n",
      "Iteration 7716, loss = 0.07829980\n",
      "Iteration 7717, loss = 0.07828667\n",
      "Iteration 7718, loss = 0.07827354\n",
      "Iteration 7719, loss = 0.07826043\n",
      "Iteration 7720, loss = 0.07824731\n",
      "Iteration 7721, loss = 0.07823409\n",
      "Iteration 7722, loss = 0.07822104\n",
      "Iteration 7723, loss = 0.07820805\n",
      "Iteration 7724, loss = 0.07819499\n",
      "Iteration 7725, loss = 0.07818175\n",
      "Iteration 7726, loss = 0.07816875\n",
      "Iteration 7727, loss = 0.07815574\n",
      "Iteration 7728, loss = 0.07814272\n",
      "Iteration 7729, loss = 0.07812959\n",
      "Iteration 7730, loss = 0.07811659\n",
      "Iteration 7731, loss = 0.07810370\n",
      "Iteration 7732, loss = 0.07809068\n",
      "Iteration 7733, loss = 0.07807757\n",
      "Iteration 7734, loss = 0.07806455\n",
      "Iteration 7735, loss = 0.07805149\n",
      "Iteration 7736, loss = 0.07803855\n",
      "Iteration 7737, loss = 0.07802562\n",
      "Iteration 7738, loss = 0.07801261\n",
      "Iteration 7739, loss = 0.07799953\n",
      "Iteration 7740, loss = 0.07798646\n",
      "Iteration 7741, loss = 0.07797356\n",
      "Iteration 7742, loss = 0.07796051\n",
      "Iteration 7743, loss = 0.07794753\n",
      "Iteration 7744, loss = 0.07793462\n",
      "Iteration 7745, loss = 0.07792161\n",
      "Iteration 7746, loss = 0.07790872\n",
      "Iteration 7747, loss = 0.07789588\n",
      "Iteration 7748, loss = 0.07788292\n",
      "Iteration 7749, loss = 0.07786987\n",
      "Iteration 7750, loss = 0.07785700\n",
      "Iteration 7751, loss = 0.07784424\n",
      "Iteration 7752, loss = 0.07783117\n",
      "Iteration 7753, loss = 0.07781834\n",
      "Iteration 7754, loss = 0.07780528\n",
      "Iteration 7755, loss = 0.07779232\n",
      "Iteration 7756, loss = 0.07777951\n",
      "Iteration 7757, loss = 0.07776645\n",
      "Iteration 7758, loss = 0.07775372\n",
      "Iteration 7759, loss = 0.07774053\n",
      "Iteration 7760, loss = 0.07772780\n",
      "Iteration 7761, loss = 0.07771469\n",
      "Iteration 7762, loss = 0.07770175\n",
      "Iteration 7763, loss = 0.07768876\n",
      "Iteration 7764, loss = 0.07767595\n",
      "Iteration 7765, loss = 0.07766302\n",
      "Iteration 7766, loss = 0.07765010\n",
      "Iteration 7767, loss = 0.07763710\n",
      "Iteration 7768, loss = 0.07762420\n",
      "Iteration 7769, loss = 0.07761126\n",
      "Iteration 7770, loss = 0.07759830\n",
      "Iteration 7771, loss = 0.07758542\n",
      "Iteration 7772, loss = 0.07757273\n",
      "Iteration 7773, loss = 0.07755980\n",
      "Iteration 7774, loss = 0.07754695\n",
      "Iteration 7775, loss = 0.07753392\n",
      "Iteration 7776, loss = 0.07752115\n",
      "Iteration 7777, loss = 0.07750828\n",
      "Iteration 7778, loss = 0.07749550\n",
      "Iteration 7779, loss = 0.07748250\n",
      "Iteration 7780, loss = 0.07746977\n",
      "Iteration 7781, loss = 0.07745694\n",
      "Iteration 7782, loss = 0.07744403\n",
      "Iteration 7783, loss = 0.07743126\n",
      "Iteration 7784, loss = 0.07741843\n",
      "Iteration 7785, loss = 0.07740567\n",
      "Iteration 7786, loss = 0.07739285\n",
      "Iteration 7787, loss = 0.07738013\n",
      "Iteration 7788, loss = 0.07736734\n",
      "Iteration 7789, loss = 0.07735444\n",
      "Iteration 7790, loss = 0.07734161\n",
      "Iteration 7791, loss = 0.07732890\n",
      "Iteration 7792, loss = 0.07731616\n",
      "Iteration 7793, loss = 0.07730316\n",
      "Iteration 7794, loss = 0.07729045\n",
      "Iteration 7795, loss = 0.07727762\n",
      "Iteration 7796, loss = 0.07726475\n",
      "Iteration 7797, loss = 0.07725223\n",
      "Iteration 7798, loss = 0.07723930\n",
      "Iteration 7799, loss = 0.07722656\n",
      "Iteration 7800, loss = 0.07721382\n",
      "Iteration 7801, loss = 0.07720114\n",
      "Iteration 7802, loss = 0.07718811\n",
      "Iteration 7803, loss = 0.07717537\n",
      "Iteration 7804, loss = 0.07716262\n",
      "Iteration 7805, loss = 0.07714990\n",
      "Iteration 7806, loss = 0.07713703\n",
      "Iteration 7807, loss = 0.07712438\n",
      "Iteration 7808, loss = 0.07711155\n",
      "Iteration 7809, loss = 0.07709901\n",
      "Iteration 7810, loss = 0.07708617\n",
      "Iteration 7811, loss = 0.07707354\n",
      "Iteration 7812, loss = 0.07706067\n",
      "Iteration 7813, loss = 0.07704801\n",
      "Iteration 7814, loss = 0.07703546\n",
      "Iteration 7815, loss = 0.07702261\n",
      "Iteration 7816, loss = 0.07701008\n",
      "Iteration 7817, loss = 0.07699740\n",
      "Iteration 7818, loss = 0.07698462\n",
      "Iteration 7819, loss = 0.07697196\n",
      "Iteration 7820, loss = 0.07695928\n",
      "Iteration 7821, loss = 0.07694668\n",
      "Iteration 7822, loss = 0.07693392\n",
      "Iteration 7823, loss = 0.07692121\n",
      "Iteration 7824, loss = 0.07690858\n",
      "Iteration 7825, loss = 0.07689595\n",
      "Iteration 7826, loss = 0.07688341\n",
      "Iteration 7827, loss = 0.07687069\n",
      "Iteration 7828, loss = 0.07685813\n",
      "Iteration 7829, loss = 0.07684537\n",
      "Iteration 7830, loss = 0.07683291\n",
      "Iteration 7831, loss = 0.07682033\n",
      "Iteration 7832, loss = 0.07680774\n",
      "Iteration 7833, loss = 0.07679504\n",
      "Iteration 7834, loss = 0.07678233\n",
      "Iteration 7835, loss = 0.07676981\n",
      "Iteration 7836, loss = 0.07675729\n",
      "Iteration 7837, loss = 0.07674440\n",
      "Iteration 7838, loss = 0.07673222\n",
      "Iteration 7839, loss = 0.07671933\n",
      "Iteration 7840, loss = 0.07670673\n",
      "Iteration 7841, loss = 0.07669422\n",
      "Iteration 7842, loss = 0.07668152\n",
      "Iteration 7843, loss = 0.07666891\n",
      "Iteration 7844, loss = 0.07665616\n",
      "Iteration 7845, loss = 0.07664374\n",
      "Iteration 7846, loss = 0.07663142\n",
      "Iteration 7847, loss = 0.07661844\n",
      "Iteration 7848, loss = 0.07660599\n",
      "Iteration 7849, loss = 0.07659337\n",
      "Iteration 7850, loss = 0.07658107\n",
      "Iteration 7851, loss = 0.07656830\n",
      "Iteration 7852, loss = 0.07655590\n",
      "Iteration 7853, loss = 0.07654330\n",
      "Iteration 7854, loss = 0.07653067\n",
      "Iteration 7855, loss = 0.07651813\n",
      "Iteration 7856, loss = 0.07650586\n",
      "Iteration 7857, loss = 0.07649318\n",
      "Iteration 7858, loss = 0.07648055\n",
      "Iteration 7859, loss = 0.07646816\n",
      "Iteration 7860, loss = 0.07645573\n",
      "Iteration 7861, loss = 0.07644323\n",
      "Iteration 7862, loss = 0.07643055\n",
      "Iteration 7863, loss = 0.07641811\n",
      "Iteration 7864, loss = 0.07640566\n",
      "Iteration 7865, loss = 0.07639314\n",
      "Iteration 7866, loss = 0.07638069\n",
      "Iteration 7867, loss = 0.07636822\n",
      "Iteration 7868, loss = 0.07635561\n",
      "Iteration 7869, loss = 0.07634324\n",
      "Iteration 7870, loss = 0.07633064\n",
      "Iteration 7871, loss = 0.07631812\n",
      "Iteration 7872, loss = 0.07630567\n",
      "Iteration 7873, loss = 0.07629315\n",
      "Iteration 7874, loss = 0.07628084\n",
      "Iteration 7875, loss = 0.07626826\n",
      "Iteration 7876, loss = 0.07625579\n",
      "Iteration 7877, loss = 0.07624357\n",
      "Iteration 7878, loss = 0.07623087\n",
      "Iteration 7879, loss = 0.07621845\n",
      "Iteration 7880, loss = 0.07620595\n",
      "Iteration 7881, loss = 0.07619368\n",
      "Iteration 7882, loss = 0.07618145\n",
      "Iteration 7883, loss = 0.07616868\n",
      "Iteration 7884, loss = 0.07615629\n",
      "Iteration 7885, loss = 0.07614408\n",
      "Iteration 7886, loss = 0.07613165\n",
      "Iteration 7887, loss = 0.07611914\n",
      "Iteration 7888, loss = 0.07610694\n",
      "Iteration 7889, loss = 0.07609439\n",
      "Iteration 7890, loss = 0.07608222\n",
      "Iteration 7891, loss = 0.07606961\n",
      "Iteration 7892, loss = 0.07605718\n",
      "Iteration 7893, loss = 0.07604494\n",
      "Iteration 7894, loss = 0.07603257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7895, loss = 0.07602001\n",
      "Iteration 7896, loss = 0.07600762\n",
      "Iteration 7897, loss = 0.07599538\n",
      "Iteration 7898, loss = 0.07598296\n",
      "Iteration 7899, loss = 0.07597052\n",
      "Iteration 7900, loss = 0.07595817\n",
      "Iteration 7901, loss = 0.07594569\n",
      "Iteration 7902, loss = 0.07593334\n",
      "Iteration 7903, loss = 0.07592111\n",
      "Iteration 7904, loss = 0.07590883\n",
      "Iteration 7905, loss = 0.07589638\n",
      "Iteration 7906, loss = 0.07588397\n",
      "Iteration 7907, loss = 0.07587182\n",
      "Iteration 7908, loss = 0.07585937\n",
      "Iteration 7909, loss = 0.07584712\n",
      "Iteration 7910, loss = 0.07583463\n",
      "Iteration 7911, loss = 0.07582245\n",
      "Iteration 7912, loss = 0.07581009\n",
      "Iteration 7913, loss = 0.07579794\n",
      "Iteration 7914, loss = 0.07578557\n",
      "Iteration 7915, loss = 0.07577325\n",
      "Iteration 7916, loss = 0.07576101\n",
      "Iteration 7917, loss = 0.07574864\n",
      "Iteration 7918, loss = 0.07573629\n",
      "Iteration 7919, loss = 0.07572404\n",
      "Iteration 7920, loss = 0.07571188\n",
      "Iteration 7921, loss = 0.07569943\n",
      "Iteration 7922, loss = 0.07568721\n",
      "Iteration 7923, loss = 0.07567498\n",
      "Iteration 7924, loss = 0.07566267\n",
      "Iteration 7925, loss = 0.07565034\n",
      "Iteration 7926, loss = 0.07563808\n",
      "Iteration 7927, loss = 0.07562593\n",
      "Iteration 7928, loss = 0.07561369\n",
      "Iteration 7929, loss = 0.07560136\n",
      "Iteration 7930, loss = 0.07558923\n",
      "Iteration 7931, loss = 0.07557702\n",
      "Iteration 7932, loss = 0.07556467\n",
      "Iteration 7933, loss = 0.07555243\n",
      "Iteration 7934, loss = 0.07554014\n",
      "Iteration 7935, loss = 0.07552794\n",
      "Iteration 7936, loss = 0.07551579\n",
      "Iteration 7937, loss = 0.07550370\n",
      "Iteration 7938, loss = 0.07549108\n",
      "Iteration 7939, loss = 0.07547908\n",
      "Iteration 7940, loss = 0.07546681\n",
      "Iteration 7941, loss = 0.07545457\n",
      "Iteration 7942, loss = 0.07544239\n",
      "Iteration 7943, loss = 0.07543017\n",
      "Iteration 7944, loss = 0.07541796\n",
      "Iteration 7945, loss = 0.07540585\n",
      "Iteration 7946, loss = 0.07539356\n",
      "Iteration 7947, loss = 0.07538154\n",
      "Iteration 7948, loss = 0.07536926\n",
      "Iteration 7949, loss = 0.07535721\n",
      "Iteration 7950, loss = 0.07534511\n",
      "Iteration 7951, loss = 0.07533279\n",
      "Iteration 7952, loss = 0.07532066\n",
      "Iteration 7953, loss = 0.07530854\n",
      "Iteration 7954, loss = 0.07529628\n",
      "Iteration 7955, loss = 0.07528425\n",
      "Iteration 7956, loss = 0.07527200\n",
      "Iteration 7957, loss = 0.07526013\n",
      "Iteration 7958, loss = 0.07524776\n",
      "Iteration 7959, loss = 0.07523559\n",
      "Iteration 7960, loss = 0.07522349\n",
      "Iteration 7961, loss = 0.07521127\n",
      "Iteration 7962, loss = 0.07519933\n",
      "Iteration 7963, loss = 0.07518720\n",
      "Iteration 7964, loss = 0.07517495\n",
      "Iteration 7965, loss = 0.07516297\n",
      "Iteration 7966, loss = 0.07515073\n",
      "Iteration 7967, loss = 0.07513884\n",
      "Iteration 7968, loss = 0.07512668\n",
      "Iteration 7969, loss = 0.07511457\n",
      "Iteration 7970, loss = 0.07510265\n",
      "Iteration 7971, loss = 0.07509054\n",
      "Iteration 7972, loss = 0.07507831\n",
      "Iteration 7973, loss = 0.07506632\n",
      "Iteration 7974, loss = 0.07505435\n",
      "Iteration 7975, loss = 0.07504229\n",
      "Iteration 7976, loss = 0.07503011\n",
      "Iteration 7977, loss = 0.07501826\n",
      "Iteration 7978, loss = 0.07500606\n",
      "Iteration 7979, loss = 0.07499401\n",
      "Iteration 7980, loss = 0.07498196\n",
      "Iteration 7981, loss = 0.07496997\n",
      "Iteration 7982, loss = 0.07495787\n",
      "Iteration 7983, loss = 0.07494588\n",
      "Iteration 7984, loss = 0.07493374\n",
      "Iteration 7985, loss = 0.07492167\n",
      "Iteration 7986, loss = 0.07490962\n",
      "Iteration 7987, loss = 0.07489762\n",
      "Iteration 7988, loss = 0.07488565\n",
      "Iteration 7989, loss = 0.07487358\n",
      "Iteration 7990, loss = 0.07486139\n",
      "Iteration 7991, loss = 0.07484950\n",
      "Iteration 7992, loss = 0.07483743\n",
      "Iteration 7993, loss = 0.07482541\n",
      "Iteration 7994, loss = 0.07481326\n",
      "Iteration 7995, loss = 0.07480141\n",
      "Iteration 7996, loss = 0.07478940\n",
      "Iteration 7997, loss = 0.07477739\n",
      "Iteration 7998, loss = 0.07476540\n",
      "Iteration 7999, loss = 0.07475356\n",
      "Iteration 8000, loss = 0.07474141\n",
      "Iteration 8001, loss = 0.07472947\n",
      "Iteration 8002, loss = 0.07471742\n",
      "Iteration 8003, loss = 0.07470553\n",
      "Iteration 8004, loss = 0.07469363\n",
      "Iteration 8005, loss = 0.07468162\n",
      "Iteration 8006, loss = 0.07466966\n",
      "Iteration 8007, loss = 0.07465771\n",
      "Iteration 8008, loss = 0.07464590\n",
      "Iteration 8009, loss = 0.07463397\n",
      "Iteration 8010, loss = 0.07462191\n",
      "Iteration 8011, loss = 0.07461004\n",
      "Iteration 8012, loss = 0.07459812\n",
      "Iteration 8013, loss = 0.07458613\n",
      "Iteration 8014, loss = 0.07457419\n",
      "Iteration 8015, loss = 0.07456233\n",
      "Iteration 8016, loss = 0.07455035\n",
      "Iteration 8017, loss = 0.07453846\n",
      "Iteration 8018, loss = 0.07452667\n",
      "Iteration 8019, loss = 0.07451475\n",
      "Iteration 8020, loss = 0.07450291\n",
      "Iteration 8021, loss = 0.07449109\n",
      "Iteration 8022, loss = 0.07447906\n",
      "Iteration 8023, loss = 0.07446730\n",
      "Iteration 8024, loss = 0.07445533\n",
      "Iteration 8025, loss = 0.07444352\n",
      "Iteration 8026, loss = 0.07443177\n",
      "Iteration 8027, loss = 0.07441994\n",
      "Iteration 8028, loss = 0.07440801\n",
      "Iteration 8029, loss = 0.07439617\n",
      "Iteration 8030, loss = 0.07438432\n",
      "Iteration 8031, loss = 0.07437238\n",
      "Iteration 8032, loss = 0.07436053\n",
      "Iteration 8033, loss = 0.07434850\n",
      "Iteration 8034, loss = 0.07433685\n",
      "Iteration 8035, loss = 0.07432498\n",
      "Iteration 8036, loss = 0.07431296\n",
      "Iteration 8037, loss = 0.07430110\n",
      "Iteration 8038, loss = 0.07428918\n",
      "Iteration 8039, loss = 0.07427755\n",
      "Iteration 8040, loss = 0.07426554\n",
      "Iteration 8041, loss = 0.07425363\n",
      "Iteration 8042, loss = 0.07424186\n",
      "Iteration 8043, loss = 0.07423006\n",
      "Iteration 8044, loss = 0.07421831\n",
      "Iteration 8045, loss = 0.07420628\n",
      "Iteration 8046, loss = 0.07419443\n",
      "Iteration 8047, loss = 0.07418253\n",
      "Iteration 8048, loss = 0.07417099\n",
      "Iteration 8049, loss = 0.07415910\n",
      "Iteration 8050, loss = 0.07414718\n",
      "Iteration 8051, loss = 0.07413522\n",
      "Iteration 8052, loss = 0.07412358\n",
      "Iteration 8053, loss = 0.07411157\n",
      "Iteration 8054, loss = 0.07409973\n",
      "Iteration 8055, loss = 0.07408816\n",
      "Iteration 8056, loss = 0.07407633\n",
      "Iteration 8057, loss = 0.07406448\n",
      "Iteration 8058, loss = 0.07405277\n",
      "Iteration 8059, loss = 0.07404084\n",
      "Iteration 8060, loss = 0.07402933\n",
      "Iteration 8061, loss = 0.07401740\n",
      "Iteration 8062, loss = 0.07400567\n",
      "Iteration 8063, loss = 0.07399395\n",
      "Iteration 8064, loss = 0.07398220\n",
      "Iteration 8065, loss = 0.07397039\n",
      "Iteration 8066, loss = 0.07395874\n",
      "Iteration 8067, loss = 0.07394695\n",
      "Iteration 8068, loss = 0.07393528\n",
      "Iteration 8069, loss = 0.07392329\n",
      "Iteration 8070, loss = 0.07391155\n",
      "Iteration 8071, loss = 0.07389993\n",
      "Iteration 8072, loss = 0.07388803\n",
      "Iteration 8073, loss = 0.07387640\n",
      "Iteration 8074, loss = 0.07386462\n",
      "Iteration 8075, loss = 0.07385322\n",
      "Iteration 8076, loss = 0.07384112\n",
      "Iteration 8077, loss = 0.07382942\n",
      "Iteration 8078, loss = 0.07381774\n",
      "Iteration 8079, loss = 0.07380593\n",
      "Iteration 8080, loss = 0.07379437\n",
      "Iteration 8081, loss = 0.07378257\n",
      "Iteration 8082, loss = 0.07377094\n",
      "Iteration 8083, loss = 0.07375911\n",
      "Iteration 8084, loss = 0.07374767\n",
      "Iteration 8085, loss = 0.07373581\n",
      "Iteration 8086, loss = 0.07372421\n",
      "Iteration 8087, loss = 0.07371255\n",
      "Iteration 8088, loss = 0.07370094\n",
      "Iteration 8089, loss = 0.07368910\n",
      "Iteration 8090, loss = 0.07367754\n",
      "Iteration 8091, loss = 0.07366583\n",
      "Iteration 8092, loss = 0.07365419\n",
      "Iteration 8093, loss = 0.07364264\n",
      "Iteration 8094, loss = 0.07363080\n",
      "Iteration 8095, loss = 0.07361910\n",
      "Iteration 8096, loss = 0.07360765\n",
      "Iteration 8097, loss = 0.07359598\n",
      "Iteration 8098, loss = 0.07358414\n",
      "Iteration 8099, loss = 0.07357259\n",
      "Iteration 8100, loss = 0.07356109\n",
      "Iteration 8101, loss = 0.07354936\n",
      "Iteration 8102, loss = 0.07353789\n",
      "Iteration 8103, loss = 0.07352614\n",
      "Iteration 8104, loss = 0.07351459\n",
      "Iteration 8105, loss = 0.07350283\n",
      "Iteration 8106, loss = 0.07349132\n",
      "Iteration 8107, loss = 0.07347967\n",
      "Iteration 8108, loss = 0.07346813\n",
      "Iteration 8109, loss = 0.07345645\n",
      "Iteration 8110, loss = 0.07344472\n",
      "Iteration 8111, loss = 0.07343314\n",
      "Iteration 8112, loss = 0.07342159\n",
      "Iteration 8113, loss = 0.07341007\n",
      "Iteration 8114, loss = 0.07339845\n",
      "Iteration 8115, loss = 0.07338689\n",
      "Iteration 8116, loss = 0.07337528\n",
      "Iteration 8117, loss = 0.07336380\n",
      "Iteration 8118, loss = 0.07335206\n",
      "Iteration 8119, loss = 0.07334056\n",
      "Iteration 8120, loss = 0.07332893\n",
      "Iteration 8121, loss = 0.07331758\n",
      "Iteration 8122, loss = 0.07330588\n",
      "Iteration 8123, loss = 0.07329434\n",
      "Iteration 8124, loss = 0.07328285\n",
      "Iteration 8125, loss = 0.07327142\n",
      "Iteration 8126, loss = 0.07325981\n",
      "Iteration 8127, loss = 0.07324849\n",
      "Iteration 8128, loss = 0.07323667\n",
      "Iteration 8129, loss = 0.07322528\n",
      "Iteration 8130, loss = 0.07321391\n",
      "Iteration 8131, loss = 0.07320209\n",
      "Iteration 8132, loss = 0.07319080\n",
      "Iteration 8133, loss = 0.07317915\n",
      "Iteration 8134, loss = 0.07316759\n",
      "Iteration 8135, loss = 0.07315610\n",
      "Iteration 8136, loss = 0.07314478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8137, loss = 0.07313314\n",
      "Iteration 8138, loss = 0.07312172\n",
      "Iteration 8139, loss = 0.07311011\n",
      "Iteration 8140, loss = 0.07309861\n",
      "Iteration 8141, loss = 0.07308713\n",
      "Iteration 8142, loss = 0.07307580\n",
      "Iteration 8143, loss = 0.07306426\n",
      "Iteration 8144, loss = 0.07305292\n",
      "Iteration 8145, loss = 0.07304122\n",
      "Iteration 8146, loss = 0.07302980\n",
      "Iteration 8147, loss = 0.07301845\n",
      "Iteration 8148, loss = 0.07300709\n",
      "Iteration 8149, loss = 0.07299548\n",
      "Iteration 8150, loss = 0.07298407\n",
      "Iteration 8151, loss = 0.07297243\n",
      "Iteration 8152, loss = 0.07296123\n",
      "Iteration 8153, loss = 0.07294973\n",
      "Iteration 8154, loss = 0.07293817\n",
      "Iteration 8155, loss = 0.07292681\n",
      "Iteration 8156, loss = 0.07291538\n",
      "Iteration 8157, loss = 0.07290388\n",
      "Iteration 8158, loss = 0.07289250\n",
      "Iteration 8159, loss = 0.07288099\n",
      "Iteration 8160, loss = 0.07286965\n",
      "Iteration 8161, loss = 0.07285809\n",
      "Iteration 8162, loss = 0.07284679\n",
      "Iteration 8163, loss = 0.07283560\n",
      "Iteration 8164, loss = 0.07282394\n",
      "Iteration 8165, loss = 0.07281258\n",
      "Iteration 8166, loss = 0.07280119\n",
      "Iteration 8167, loss = 0.07278995\n",
      "Iteration 8168, loss = 0.07277836\n",
      "Iteration 8169, loss = 0.07276711\n",
      "Iteration 8170, loss = 0.07275549\n",
      "Iteration 8171, loss = 0.07274427\n",
      "Iteration 8172, loss = 0.07273294\n",
      "Iteration 8173, loss = 0.07272156\n",
      "Iteration 8174, loss = 0.07271008\n",
      "Iteration 8175, loss = 0.07269868\n",
      "Iteration 8176, loss = 0.07268737\n",
      "Iteration 8177, loss = 0.07267609\n",
      "Iteration 8178, loss = 0.07266466\n",
      "Iteration 8179, loss = 0.07265330\n",
      "Iteration 8180, loss = 0.07264209\n",
      "Iteration 8181, loss = 0.07263069\n",
      "Iteration 8182, loss = 0.07261917\n",
      "Iteration 8183, loss = 0.07260782\n",
      "Iteration 8184, loss = 0.07259660\n",
      "Iteration 8185, loss = 0.07258518\n",
      "Iteration 8186, loss = 0.07257374\n",
      "Iteration 8187, loss = 0.07256244\n",
      "Iteration 8188, loss = 0.07255112\n",
      "Iteration 8189, loss = 0.07253975\n",
      "Iteration 8190, loss = 0.07252834\n",
      "Iteration 8191, loss = 0.07251717\n",
      "Iteration 8192, loss = 0.07250580\n",
      "Iteration 8193, loss = 0.07249447\n",
      "Iteration 8194, loss = 0.07248313\n",
      "Iteration 8195, loss = 0.07247186\n",
      "Iteration 8196, loss = 0.07246045\n",
      "Iteration 8197, loss = 0.07244922\n",
      "Iteration 8198, loss = 0.07243782\n",
      "Iteration 8199, loss = 0.07242669\n",
      "Iteration 8200, loss = 0.07241542\n",
      "Iteration 8201, loss = 0.07240407\n",
      "Iteration 8202, loss = 0.07239285\n",
      "Iteration 8203, loss = 0.07238164\n",
      "Iteration 8204, loss = 0.07237025\n",
      "Iteration 8205, loss = 0.07235904\n",
      "Iteration 8206, loss = 0.07234778\n",
      "Iteration 8207, loss = 0.07233644\n",
      "Iteration 8208, loss = 0.07232513\n",
      "Iteration 8209, loss = 0.07231377\n",
      "Iteration 8210, loss = 0.07230264\n",
      "Iteration 8211, loss = 0.07229145\n",
      "Iteration 8212, loss = 0.07228007\n",
      "Iteration 8213, loss = 0.07226904\n",
      "Iteration 8214, loss = 0.07225762\n",
      "Iteration 8215, loss = 0.07224657\n",
      "Iteration 8216, loss = 0.07223530\n",
      "Iteration 8217, loss = 0.07222408\n",
      "Iteration 8218, loss = 0.07221280\n",
      "Iteration 8219, loss = 0.07220164\n",
      "Iteration 8220, loss = 0.07219062\n",
      "Iteration 8221, loss = 0.07217915\n",
      "Iteration 8222, loss = 0.07216796\n",
      "Iteration 8223, loss = 0.07215664\n",
      "Iteration 8224, loss = 0.07214550\n",
      "Iteration 8225, loss = 0.07213442\n",
      "Iteration 8226, loss = 0.07212317\n",
      "Iteration 8227, loss = 0.07211195\n",
      "Iteration 8228, loss = 0.07210064\n",
      "Iteration 8229, loss = 0.07208943\n",
      "Iteration 8230, loss = 0.07207837\n",
      "Iteration 8231, loss = 0.07206700\n",
      "Iteration 8232, loss = 0.07205592\n",
      "Iteration 8233, loss = 0.07204479\n",
      "Iteration 8234, loss = 0.07203349\n",
      "Iteration 8235, loss = 0.07202240\n",
      "Iteration 8236, loss = 0.07201118\n",
      "Iteration 8237, loss = 0.07200007\n",
      "Iteration 8238, loss = 0.07198892\n",
      "Iteration 8239, loss = 0.07197772\n",
      "Iteration 8240, loss = 0.07196664\n",
      "Iteration 8241, loss = 0.07195548\n",
      "Iteration 8242, loss = 0.07194438\n",
      "Iteration 8243, loss = 0.07193321\n",
      "Iteration 8244, loss = 0.07192211\n",
      "Iteration 8245, loss = 0.07191108\n",
      "Iteration 8246, loss = 0.07189978\n",
      "Iteration 8247, loss = 0.07188868\n",
      "Iteration 8248, loss = 0.07187747\n",
      "Iteration 8249, loss = 0.07186642\n",
      "Iteration 8250, loss = 0.07185532\n",
      "Iteration 8251, loss = 0.07184414\n",
      "Iteration 8252, loss = 0.07183301\n",
      "Iteration 8253, loss = 0.07182205\n",
      "Iteration 8254, loss = 0.07181089\n",
      "Iteration 8255, loss = 0.07179956\n",
      "Iteration 8256, loss = 0.07178859\n",
      "Iteration 8257, loss = 0.07177741\n",
      "Iteration 8258, loss = 0.07176624\n",
      "Iteration 8259, loss = 0.07175521\n",
      "Iteration 8260, loss = 0.07174407\n",
      "Iteration 8261, loss = 0.07173293\n",
      "Iteration 8262, loss = 0.07172193\n",
      "Iteration 8263, loss = 0.07171079\n",
      "Iteration 8264, loss = 0.07169967\n",
      "Iteration 8265, loss = 0.07168866\n",
      "Iteration 8266, loss = 0.07167766\n",
      "Iteration 8267, loss = 0.07166657\n",
      "Iteration 8268, loss = 0.07165561\n",
      "Iteration 8269, loss = 0.07164454\n",
      "Iteration 8270, loss = 0.07163359\n",
      "Iteration 8271, loss = 0.07162256\n",
      "Iteration 8272, loss = 0.07161163\n",
      "Iteration 8273, loss = 0.07160045\n",
      "Iteration 8274, loss = 0.07158946\n",
      "Iteration 8275, loss = 0.07157836\n",
      "Iteration 8276, loss = 0.07156749\n",
      "Iteration 8277, loss = 0.07155629\n",
      "Iteration 8278, loss = 0.07154539\n",
      "Iteration 8279, loss = 0.07153430\n",
      "Iteration 8280, loss = 0.07152327\n",
      "Iteration 8281, loss = 0.07151226\n",
      "Iteration 8282, loss = 0.07150120\n",
      "Iteration 8283, loss = 0.07149027\n",
      "Iteration 8284, loss = 0.07147896\n",
      "Iteration 8285, loss = 0.07146799\n",
      "Iteration 8286, loss = 0.07145694\n",
      "Iteration 8287, loss = 0.07144588\n",
      "Iteration 8288, loss = 0.07143481\n",
      "Iteration 8289, loss = 0.07142396\n",
      "Iteration 8290, loss = 0.07141289\n",
      "Iteration 8291, loss = 0.07140183\n",
      "Iteration 8292, loss = 0.07139067\n",
      "Iteration 8293, loss = 0.07137967\n",
      "Iteration 8294, loss = 0.07136886\n",
      "Iteration 8295, loss = 0.07135792\n",
      "Iteration 8296, loss = 0.07134673\n",
      "Iteration 8297, loss = 0.07133582\n",
      "Iteration 8298, loss = 0.07132484\n",
      "Iteration 8299, loss = 0.07131386\n",
      "Iteration 8300, loss = 0.07130294\n",
      "Iteration 8301, loss = 0.07129208\n",
      "Iteration 8302, loss = 0.07128096\n",
      "Iteration 8303, loss = 0.07127009\n",
      "Iteration 8304, loss = 0.07125912\n",
      "Iteration 8305, loss = 0.07124813\n",
      "Iteration 8306, loss = 0.07123726\n",
      "Iteration 8307, loss = 0.07122626\n",
      "Iteration 8308, loss = 0.07121538\n",
      "Iteration 8309, loss = 0.07120438\n",
      "Iteration 8310, loss = 0.07119359\n",
      "Iteration 8311, loss = 0.07118256\n",
      "Iteration 8312, loss = 0.07117167\n",
      "Iteration 8313, loss = 0.07116062\n",
      "Iteration 8314, loss = 0.07114979\n",
      "Iteration 8315, loss = 0.07113871\n",
      "Iteration 8316, loss = 0.07112802\n",
      "Iteration 8317, loss = 0.07111694\n",
      "Iteration 8318, loss = 0.07110605\n",
      "Iteration 8319, loss = 0.07109512\n",
      "Iteration 8320, loss = 0.07108409\n",
      "Iteration 8321, loss = 0.07107319\n",
      "Iteration 8322, loss = 0.07106227\n",
      "Iteration 8323, loss = 0.07105163\n",
      "Iteration 8324, loss = 0.07104054\n",
      "Iteration 8325, loss = 0.07102980\n",
      "Iteration 8326, loss = 0.07101878\n",
      "Iteration 8327, loss = 0.07100800\n",
      "Iteration 8328, loss = 0.07099718\n",
      "Iteration 8329, loss = 0.07098634\n",
      "Iteration 8330, loss = 0.07097549\n",
      "Iteration 8331, loss = 0.07096464\n",
      "Iteration 8332, loss = 0.07095383\n",
      "Iteration 8333, loss = 0.07094295\n",
      "Iteration 8334, loss = 0.07093216\n",
      "Iteration 8335, loss = 0.07092137\n",
      "Iteration 8336, loss = 0.07091060\n",
      "Iteration 8337, loss = 0.07089977\n",
      "Iteration 8338, loss = 0.07088899\n",
      "Iteration 8339, loss = 0.07087815\n",
      "Iteration 8340, loss = 0.07086717\n",
      "Iteration 8341, loss = 0.07085634\n",
      "Iteration 8342, loss = 0.07084560\n",
      "Iteration 8343, loss = 0.07083482\n",
      "Iteration 8344, loss = 0.07082403\n",
      "Iteration 8345, loss = 0.07081321\n",
      "Iteration 8346, loss = 0.07080233\n",
      "Iteration 8347, loss = 0.07079161\n",
      "Iteration 8348, loss = 0.07078072\n",
      "Iteration 8349, loss = 0.07076999\n",
      "Iteration 8350, loss = 0.07075916\n",
      "Iteration 8351, loss = 0.07074850\n",
      "Iteration 8352, loss = 0.07073757\n",
      "Iteration 8353, loss = 0.07072680\n",
      "Iteration 8354, loss = 0.07071608\n",
      "Iteration 8355, loss = 0.07070530\n",
      "Iteration 8356, loss = 0.07069470\n",
      "Iteration 8357, loss = 0.07068372\n",
      "Iteration 8358, loss = 0.07067307\n",
      "Iteration 8359, loss = 0.07066230\n",
      "Iteration 8360, loss = 0.07065152\n",
      "Iteration 8361, loss = 0.07064064\n",
      "Iteration 8362, loss = 0.07062996\n",
      "Iteration 8363, loss = 0.07061921\n",
      "Iteration 8364, loss = 0.07060846\n",
      "Iteration 8365, loss = 0.07059766\n",
      "Iteration 8366, loss = 0.07058705\n",
      "Iteration 8367, loss = 0.07057630\n",
      "Iteration 8368, loss = 0.07056566\n",
      "Iteration 8369, loss = 0.07055486\n",
      "Iteration 8370, loss = 0.07054407\n",
      "Iteration 8371, loss = 0.07053327\n",
      "Iteration 8372, loss = 0.07052247\n",
      "Iteration 8373, loss = 0.07051188\n",
      "Iteration 8374, loss = 0.07050098\n",
      "Iteration 8375, loss = 0.07049040\n",
      "Iteration 8376, loss = 0.07047954\n",
      "Iteration 8377, loss = 0.07046881\n",
      "Iteration 8378, loss = 0.07045831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8379, loss = 0.07044724\n",
      "Iteration 8380, loss = 0.07043667\n",
      "Iteration 8381, loss = 0.07042600\n",
      "Iteration 8382, loss = 0.07041532\n",
      "Iteration 8383, loss = 0.07040457\n",
      "Iteration 8384, loss = 0.07039384\n",
      "Iteration 8385, loss = 0.07038304\n",
      "Iteration 8386, loss = 0.07037247\n",
      "Iteration 8387, loss = 0.07036181\n",
      "Iteration 8388, loss = 0.07035106\n",
      "Iteration 8389, loss = 0.07034040\n",
      "Iteration 8390, loss = 0.07032967\n",
      "Iteration 8391, loss = 0.07031905\n",
      "Iteration 8392, loss = 0.07030827\n",
      "Iteration 8393, loss = 0.07029762\n",
      "Iteration 8394, loss = 0.07028703\n",
      "Iteration 8395, loss = 0.07027644\n",
      "Iteration 8396, loss = 0.07026570\n",
      "Iteration 8397, loss = 0.07025495\n",
      "Iteration 8398, loss = 0.07024421\n",
      "Iteration 8399, loss = 0.07023377\n",
      "Iteration 8400, loss = 0.07022318\n",
      "Iteration 8401, loss = 0.07021240\n",
      "Iteration 8402, loss = 0.07020183\n",
      "Iteration 8403, loss = 0.07019122\n",
      "Iteration 8404, loss = 0.07018063\n",
      "Iteration 8405, loss = 0.07016975\n",
      "Iteration 8406, loss = 0.07015920\n",
      "Iteration 8407, loss = 0.07014871\n",
      "Iteration 8408, loss = 0.07013816\n",
      "Iteration 8409, loss = 0.07012739\n",
      "Iteration 8410, loss = 0.07011673\n",
      "Iteration 8411, loss = 0.07010618\n",
      "Iteration 8412, loss = 0.07009566\n",
      "Iteration 8413, loss = 0.07008498\n",
      "Iteration 8414, loss = 0.07007444\n",
      "Iteration 8415, loss = 0.07006400\n",
      "Iteration 8416, loss = 0.07005332\n",
      "Iteration 8417, loss = 0.07004266\n",
      "Iteration 8418, loss = 0.07003226\n",
      "Iteration 8419, loss = 0.07002164\n",
      "Iteration 8420, loss = 0.07001100\n",
      "Iteration 8421, loss = 0.07000048\n",
      "Iteration 8422, loss = 0.06998991\n",
      "Iteration 8423, loss = 0.06997939\n",
      "Iteration 8424, loss = 0.06996882\n",
      "Iteration 8425, loss = 0.06995833\n",
      "Iteration 8426, loss = 0.06994772\n",
      "Iteration 8427, loss = 0.06993717\n",
      "Iteration 8428, loss = 0.06992661\n",
      "Iteration 8429, loss = 0.06991610\n",
      "Iteration 8430, loss = 0.06990557\n",
      "Iteration 8431, loss = 0.06989508\n",
      "Iteration 8432, loss = 0.06988462\n",
      "Iteration 8433, loss = 0.06987396\n",
      "Iteration 8434, loss = 0.06986347\n",
      "Iteration 8435, loss = 0.06985292\n",
      "Iteration 8436, loss = 0.06984237\n",
      "Iteration 8437, loss = 0.06983183\n",
      "Iteration 8438, loss = 0.06982125\n",
      "Iteration 8439, loss = 0.06981070\n",
      "Iteration 8440, loss = 0.06980025\n",
      "Iteration 8441, loss = 0.06978957\n",
      "Iteration 8442, loss = 0.06977916\n",
      "Iteration 8443, loss = 0.06976861\n",
      "Iteration 8444, loss = 0.06975798\n",
      "Iteration 8445, loss = 0.06974750\n",
      "Iteration 8446, loss = 0.06973702\n",
      "Iteration 8447, loss = 0.06972662\n",
      "Iteration 8448, loss = 0.06971611\n",
      "Iteration 8449, loss = 0.06970560\n",
      "Iteration 8450, loss = 0.06969499\n",
      "Iteration 8451, loss = 0.06968449\n",
      "Iteration 8452, loss = 0.06967405\n",
      "Iteration 8453, loss = 0.06966364\n",
      "Iteration 8454, loss = 0.06965298\n",
      "Iteration 8455, loss = 0.06964261\n",
      "Iteration 8456, loss = 0.06963211\n",
      "Iteration 8457, loss = 0.06962156\n",
      "Iteration 8458, loss = 0.06961113\n",
      "Iteration 8459, loss = 0.06960063\n",
      "Iteration 8460, loss = 0.06959017\n",
      "Iteration 8461, loss = 0.06957970\n",
      "Iteration 8462, loss = 0.06956922\n",
      "Iteration 8463, loss = 0.06955871\n",
      "Iteration 8464, loss = 0.06954819\n",
      "Iteration 8465, loss = 0.06953775\n",
      "Iteration 8466, loss = 0.06952737\n",
      "Iteration 8467, loss = 0.06951696\n",
      "Iteration 8468, loss = 0.06950633\n",
      "Iteration 8469, loss = 0.06949581\n",
      "Iteration 8470, loss = 0.06948546\n",
      "Iteration 8471, loss = 0.06947500\n",
      "Iteration 8472, loss = 0.06946443\n",
      "Iteration 8473, loss = 0.06945408\n",
      "Iteration 8474, loss = 0.06944359\n",
      "Iteration 8475, loss = 0.06943323\n",
      "Iteration 8476, loss = 0.06942282\n",
      "Iteration 8477, loss = 0.06941232\n",
      "Iteration 8478, loss = 0.06940207\n",
      "Iteration 8479, loss = 0.06939152\n",
      "Iteration 8480, loss = 0.06938112\n",
      "Iteration 8481, loss = 0.06937073\n",
      "Iteration 8482, loss = 0.06936028\n",
      "Iteration 8483, loss = 0.06934997\n",
      "Iteration 8484, loss = 0.06933941\n",
      "Iteration 8485, loss = 0.06932903\n",
      "Iteration 8486, loss = 0.06931864\n",
      "Iteration 8487, loss = 0.06930817\n",
      "Iteration 8488, loss = 0.06929772\n",
      "Iteration 8489, loss = 0.06928752\n",
      "Iteration 8490, loss = 0.06927719\n",
      "Iteration 8491, loss = 0.06926666\n",
      "Iteration 8492, loss = 0.06925618\n",
      "Iteration 8493, loss = 0.06924592\n",
      "Iteration 8494, loss = 0.06923555\n",
      "Iteration 8495, loss = 0.06922521\n",
      "Iteration 8496, loss = 0.06921489\n",
      "Iteration 8497, loss = 0.06920453\n",
      "Iteration 8498, loss = 0.06919418\n",
      "Iteration 8499, loss = 0.06918378\n",
      "Iteration 8500, loss = 0.06917349\n",
      "Iteration 8501, loss = 0.06916321\n",
      "Iteration 8502, loss = 0.06915288\n",
      "Iteration 8503, loss = 0.06914245\n",
      "Iteration 8504, loss = 0.06913229\n",
      "Iteration 8505, loss = 0.06912191\n",
      "Iteration 8506, loss = 0.06911154\n",
      "Iteration 8507, loss = 0.06910143\n",
      "Iteration 8508, loss = 0.06909094\n",
      "Iteration 8509, loss = 0.06908068\n",
      "Iteration 8510, loss = 0.06907011\n",
      "Iteration 8511, loss = 0.06906007\n",
      "Iteration 8512, loss = 0.06904969\n",
      "Iteration 8513, loss = 0.06903939\n",
      "Iteration 8514, loss = 0.06902895\n",
      "Iteration 8515, loss = 0.06901873\n",
      "Iteration 8516, loss = 0.06900844\n",
      "Iteration 8517, loss = 0.06899819\n",
      "Iteration 8518, loss = 0.06898780\n",
      "Iteration 8519, loss = 0.06897739\n",
      "Iteration 8520, loss = 0.06896709\n",
      "Iteration 8521, loss = 0.06895682\n",
      "Iteration 8522, loss = 0.06894672\n",
      "Iteration 8523, loss = 0.06893621\n",
      "Iteration 8524, loss = 0.06892584\n",
      "Iteration 8525, loss = 0.06891575\n",
      "Iteration 8526, loss = 0.06890531\n",
      "Iteration 8527, loss = 0.06889493\n",
      "Iteration 8528, loss = 0.06888476\n",
      "Iteration 8529, loss = 0.06887452\n",
      "Iteration 8530, loss = 0.06886415\n",
      "Iteration 8531, loss = 0.06885393\n",
      "Iteration 8532, loss = 0.06884360\n",
      "Iteration 8533, loss = 0.06883337\n",
      "Iteration 8534, loss = 0.06882319\n",
      "Iteration 8535, loss = 0.06881310\n",
      "Iteration 8536, loss = 0.06880261\n",
      "Iteration 8537, loss = 0.06879245\n",
      "Iteration 8538, loss = 0.06878224\n",
      "Iteration 8539, loss = 0.06877196\n",
      "Iteration 8540, loss = 0.06876180\n",
      "Iteration 8541, loss = 0.06875160\n",
      "Iteration 8542, loss = 0.06874132\n",
      "Iteration 8543, loss = 0.06873119\n",
      "Iteration 8544, loss = 0.06872096\n",
      "Iteration 8545, loss = 0.06871082\n",
      "Iteration 8546, loss = 0.06870058\n",
      "Iteration 8547, loss = 0.06869032\n",
      "Iteration 8548, loss = 0.06868001\n",
      "Iteration 8549, loss = 0.06866995\n",
      "Iteration 8550, loss = 0.06865975\n",
      "Iteration 8551, loss = 0.06864950\n",
      "Iteration 8552, loss = 0.06863950\n",
      "Iteration 8553, loss = 0.06862934\n",
      "Iteration 8554, loss = 0.06861906\n",
      "Iteration 8555, loss = 0.06860887\n",
      "Iteration 8556, loss = 0.06859873\n",
      "Iteration 8557, loss = 0.06858857\n",
      "Iteration 8558, loss = 0.06857845\n",
      "Iteration 8559, loss = 0.06856826\n",
      "Iteration 8560, loss = 0.06855805\n",
      "Iteration 8561, loss = 0.06854779\n",
      "Iteration 8562, loss = 0.06853769\n",
      "Iteration 8563, loss = 0.06852752\n",
      "Iteration 8564, loss = 0.06851745\n",
      "Iteration 8565, loss = 0.06850714\n",
      "Iteration 8566, loss = 0.06849701\n",
      "Iteration 8567, loss = 0.06848688\n",
      "Iteration 8568, loss = 0.06847681\n",
      "Iteration 8569, loss = 0.06846653\n",
      "Iteration 8570, loss = 0.06845642\n",
      "Iteration 8571, loss = 0.06844624\n",
      "Iteration 8572, loss = 0.06843608\n",
      "Iteration 8573, loss = 0.06842603\n",
      "Iteration 8574, loss = 0.06841588\n",
      "Iteration 8575, loss = 0.06840560\n",
      "Iteration 8576, loss = 0.06839559\n",
      "Iteration 8577, loss = 0.06838552\n",
      "Iteration 8578, loss = 0.06837535\n",
      "Iteration 8579, loss = 0.06836529\n",
      "Iteration 8580, loss = 0.06835510\n",
      "Iteration 8581, loss = 0.06834498\n",
      "Iteration 8582, loss = 0.06833489\n",
      "Iteration 8583, loss = 0.06832484\n",
      "Iteration 8584, loss = 0.06831469\n",
      "Iteration 8585, loss = 0.06830453\n",
      "Iteration 8586, loss = 0.06829445\n",
      "Iteration 8587, loss = 0.06828436\n",
      "Iteration 8588, loss = 0.06827440\n",
      "Iteration 8589, loss = 0.06826427\n",
      "Iteration 8590, loss = 0.06825420\n",
      "Iteration 8591, loss = 0.06824416\n",
      "Iteration 8592, loss = 0.06823389\n",
      "Iteration 8593, loss = 0.06822392\n",
      "Iteration 8594, loss = 0.06821388\n",
      "Iteration 8595, loss = 0.06820369\n",
      "Iteration 8596, loss = 0.06819366\n",
      "Iteration 8597, loss = 0.06818350\n",
      "Iteration 8598, loss = 0.06817343\n",
      "Iteration 8599, loss = 0.06816347\n",
      "Iteration 8600, loss = 0.06815343\n",
      "Iteration 8601, loss = 0.06814330\n",
      "Iteration 8602, loss = 0.06813333\n",
      "Iteration 8603, loss = 0.06812317\n",
      "Iteration 8604, loss = 0.06811308\n",
      "Iteration 8605, loss = 0.06810302\n",
      "Iteration 8606, loss = 0.06809297\n",
      "Iteration 8607, loss = 0.06808294\n",
      "Iteration 8608, loss = 0.06807294\n",
      "Iteration 8609, loss = 0.06806278\n",
      "Iteration 8610, loss = 0.06805294\n",
      "Iteration 8611, loss = 0.06804277\n",
      "Iteration 8612, loss = 0.06803301\n",
      "Iteration 8613, loss = 0.06802279\n",
      "Iteration 8614, loss = 0.06801283\n",
      "Iteration 8615, loss = 0.06800273\n",
      "Iteration 8616, loss = 0.06799274\n",
      "Iteration 8617, loss = 0.06798275\n",
      "Iteration 8618, loss = 0.06797271\n",
      "Iteration 8619, loss = 0.06796277\n",
      "Iteration 8620, loss = 0.06795275\n",
      "Iteration 8621, loss = 0.06794273\n",
      "Iteration 8622, loss = 0.06793274\n",
      "Iteration 8623, loss = 0.06792276\n",
      "Iteration 8624, loss = 0.06791275\n",
      "Iteration 8625, loss = 0.06790296\n",
      "Iteration 8626, loss = 0.06789283\n",
      "Iteration 8627, loss = 0.06788286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8628, loss = 0.06787284\n",
      "Iteration 8629, loss = 0.06786292\n",
      "Iteration 8630, loss = 0.06785291\n",
      "Iteration 8631, loss = 0.06784295\n",
      "Iteration 8632, loss = 0.06783302\n",
      "Iteration 8633, loss = 0.06782301\n",
      "Iteration 8634, loss = 0.06781304\n",
      "Iteration 8635, loss = 0.06780312\n",
      "Iteration 8636, loss = 0.06779322\n",
      "Iteration 8637, loss = 0.06778316\n",
      "Iteration 8638, loss = 0.06777317\n",
      "Iteration 8639, loss = 0.06776332\n",
      "Iteration 8640, loss = 0.06775347\n",
      "Iteration 8641, loss = 0.06774338\n",
      "Iteration 8642, loss = 0.06773334\n",
      "Iteration 8643, loss = 0.06772350\n",
      "Iteration 8644, loss = 0.06771352\n",
      "Iteration 8645, loss = 0.06770358\n",
      "Iteration 8646, loss = 0.06769366\n",
      "Iteration 8647, loss = 0.06768366\n",
      "Iteration 8648, loss = 0.06767379\n",
      "Iteration 8649, loss = 0.06766393\n",
      "Iteration 8650, loss = 0.06765400\n",
      "Iteration 8651, loss = 0.06764405\n",
      "Iteration 8652, loss = 0.06763412\n",
      "Iteration 8653, loss = 0.06762422\n",
      "Iteration 8654, loss = 0.06761429\n",
      "Iteration 8655, loss = 0.06760437\n",
      "Iteration 8656, loss = 0.06759442\n",
      "Iteration 8657, loss = 0.06758447\n",
      "Iteration 8658, loss = 0.06757473\n",
      "Iteration 8659, loss = 0.06756466\n",
      "Iteration 8660, loss = 0.06755485\n",
      "Iteration 8661, loss = 0.06754506\n",
      "Iteration 8662, loss = 0.06753489\n",
      "Iteration 8663, loss = 0.06752501\n",
      "Iteration 8664, loss = 0.06751519\n",
      "Iteration 8665, loss = 0.06750521\n",
      "Iteration 8666, loss = 0.06749534\n",
      "Iteration 8667, loss = 0.06748545\n",
      "Iteration 8668, loss = 0.06747567\n",
      "Iteration 8669, loss = 0.06746558\n",
      "Iteration 8670, loss = 0.06745588\n",
      "Iteration 8671, loss = 0.06744585\n",
      "Iteration 8672, loss = 0.06743606\n",
      "Iteration 8673, loss = 0.06742626\n",
      "Iteration 8674, loss = 0.06741623\n",
      "Iteration 8675, loss = 0.06740638\n",
      "Iteration 8676, loss = 0.06739664\n",
      "Iteration 8677, loss = 0.06738675\n",
      "Iteration 8678, loss = 0.06737678\n",
      "Iteration 8679, loss = 0.06736704\n",
      "Iteration 8680, loss = 0.06735710\n",
      "Iteration 8681, loss = 0.06734718\n",
      "Iteration 8682, loss = 0.06733744\n",
      "Iteration 8683, loss = 0.06732759\n",
      "Iteration 8684, loss = 0.06731772\n",
      "Iteration 8685, loss = 0.06730796\n",
      "Iteration 8686, loss = 0.06729797\n",
      "Iteration 8687, loss = 0.06728819\n",
      "Iteration 8688, loss = 0.06727839\n",
      "Iteration 8689, loss = 0.06726848\n",
      "Iteration 8690, loss = 0.06725868\n",
      "Iteration 8691, loss = 0.06724883\n",
      "Iteration 8692, loss = 0.06723908\n",
      "Iteration 8693, loss = 0.06722930\n",
      "Iteration 8694, loss = 0.06721947\n",
      "Iteration 8695, loss = 0.06720962\n",
      "Iteration 8696, loss = 0.06719985\n",
      "Iteration 8697, loss = 0.06719004\n",
      "Iteration 8698, loss = 0.06718031\n",
      "Iteration 8699, loss = 0.06717059\n",
      "Iteration 8700, loss = 0.06716084\n",
      "Iteration 8701, loss = 0.06715094\n",
      "Iteration 8702, loss = 0.06714123\n",
      "Iteration 8703, loss = 0.06713149\n",
      "Iteration 8704, loss = 0.06712163\n",
      "Iteration 8705, loss = 0.06711189\n",
      "Iteration 8706, loss = 0.06710211\n",
      "Iteration 8707, loss = 0.06709223\n",
      "Iteration 8708, loss = 0.06708240\n",
      "Iteration 8709, loss = 0.06707273\n",
      "Iteration 8710, loss = 0.06706294\n",
      "Iteration 8711, loss = 0.06705312\n",
      "Iteration 8712, loss = 0.06704338\n",
      "Iteration 8713, loss = 0.06703343\n",
      "Iteration 8714, loss = 0.06702391\n",
      "Iteration 8715, loss = 0.06701404\n",
      "Iteration 8716, loss = 0.06700429\n",
      "Iteration 8717, loss = 0.06699454\n",
      "Iteration 8718, loss = 0.06698480\n",
      "Iteration 8719, loss = 0.06697503\n",
      "Iteration 8720, loss = 0.06696529\n",
      "Iteration 8721, loss = 0.06695556\n",
      "Iteration 8722, loss = 0.06694584\n",
      "Iteration 8723, loss = 0.06693609\n",
      "Iteration 8724, loss = 0.06692626\n",
      "Iteration 8725, loss = 0.06691681\n",
      "Iteration 8726, loss = 0.06690692\n",
      "Iteration 8727, loss = 0.06689716\n",
      "Iteration 8728, loss = 0.06688738\n",
      "Iteration 8729, loss = 0.06687759\n",
      "Iteration 8730, loss = 0.06686803\n",
      "Iteration 8731, loss = 0.06685828\n",
      "Iteration 8732, loss = 0.06684856\n",
      "Iteration 8733, loss = 0.06683880\n",
      "Iteration 8734, loss = 0.06682925\n",
      "Iteration 8735, loss = 0.06681948\n",
      "Iteration 8736, loss = 0.06680971\n",
      "Iteration 8737, loss = 0.06680004\n",
      "Iteration 8738, loss = 0.06679045\n",
      "Iteration 8739, loss = 0.06678066\n",
      "Iteration 8740, loss = 0.06677102\n",
      "Iteration 8741, loss = 0.06676133\n",
      "Iteration 8742, loss = 0.06675153\n",
      "Iteration 8743, loss = 0.06674193\n",
      "Iteration 8744, loss = 0.06673221\n",
      "Iteration 8745, loss = 0.06672254\n",
      "Iteration 8746, loss = 0.06671282\n",
      "Iteration 8747, loss = 0.06670307\n",
      "Iteration 8748, loss = 0.06669360\n",
      "Iteration 8749, loss = 0.06668383\n",
      "Iteration 8750, loss = 0.06667410\n",
      "Iteration 8751, loss = 0.06666451\n",
      "Iteration 8752, loss = 0.06665492\n",
      "Iteration 8753, loss = 0.06664517\n",
      "Iteration 8754, loss = 0.06663561\n",
      "Iteration 8755, loss = 0.06662598\n",
      "Iteration 8756, loss = 0.06661634\n",
      "Iteration 8757, loss = 0.06660675\n",
      "Iteration 8758, loss = 0.06659708\n",
      "Iteration 8759, loss = 0.06658732\n",
      "Iteration 8760, loss = 0.06657791\n",
      "Iteration 8761, loss = 0.06656822\n",
      "Iteration 8762, loss = 0.06655866\n",
      "Iteration 8763, loss = 0.06654907\n",
      "Iteration 8764, loss = 0.06653939\n",
      "Iteration 8765, loss = 0.06652988\n",
      "Iteration 8766, loss = 0.06652023\n",
      "Iteration 8767, loss = 0.06651065\n",
      "Iteration 8768, loss = 0.06650106\n",
      "Iteration 8769, loss = 0.06649149\n",
      "Iteration 8770, loss = 0.06648192\n",
      "Iteration 8771, loss = 0.06647226\n",
      "Iteration 8772, loss = 0.06646276\n",
      "Iteration 8773, loss = 0.06645311\n",
      "Iteration 8774, loss = 0.06644336\n",
      "Iteration 8775, loss = 0.06643398\n",
      "Iteration 8776, loss = 0.06642430\n",
      "Iteration 8777, loss = 0.06641471\n",
      "Iteration 8778, loss = 0.06640524\n",
      "Iteration 8779, loss = 0.06639561\n",
      "Iteration 8780, loss = 0.06638607\n",
      "Iteration 8781, loss = 0.06637651\n",
      "Iteration 8782, loss = 0.06636685\n",
      "Iteration 8783, loss = 0.06635726\n",
      "Iteration 8784, loss = 0.06634764\n",
      "Iteration 8785, loss = 0.06633815\n",
      "Iteration 8786, loss = 0.06632853\n",
      "Iteration 8787, loss = 0.06631900\n",
      "Iteration 8788, loss = 0.06630942\n",
      "Iteration 8789, loss = 0.06630002\n",
      "Iteration 8790, loss = 0.06629025\n",
      "Iteration 8791, loss = 0.06628079\n",
      "Iteration 8792, loss = 0.06627124\n",
      "Iteration 8793, loss = 0.06626171\n",
      "Iteration 8794, loss = 0.06625215\n",
      "Iteration 8795, loss = 0.06624268\n",
      "Iteration 8796, loss = 0.06623308\n",
      "Iteration 8797, loss = 0.06622358\n",
      "Iteration 8798, loss = 0.06621396\n",
      "Iteration 8799, loss = 0.06620446\n",
      "Iteration 8800, loss = 0.06619509\n",
      "Iteration 8801, loss = 0.06618541\n",
      "Iteration 8802, loss = 0.06617595\n",
      "Iteration 8803, loss = 0.06616632\n",
      "Iteration 8804, loss = 0.06615678\n",
      "Iteration 8805, loss = 0.06614734\n",
      "Iteration 8806, loss = 0.06613779\n",
      "Iteration 8807, loss = 0.06612823\n",
      "Iteration 8808, loss = 0.06611868\n",
      "Iteration 8809, loss = 0.06610930\n",
      "Iteration 8810, loss = 0.06609983\n",
      "Iteration 8811, loss = 0.06609020\n",
      "Iteration 8812, loss = 0.06608073\n",
      "Iteration 8813, loss = 0.06607131\n",
      "Iteration 8814, loss = 0.06606179\n",
      "Iteration 8815, loss = 0.06605247\n",
      "Iteration 8816, loss = 0.06604283\n",
      "Iteration 8817, loss = 0.06603333\n",
      "Iteration 8818, loss = 0.06602394\n",
      "Iteration 8819, loss = 0.06601435\n",
      "Iteration 8820, loss = 0.06600490\n",
      "Iteration 8821, loss = 0.06599542\n",
      "Iteration 8822, loss = 0.06598588\n",
      "Iteration 8823, loss = 0.06597646\n",
      "Iteration 8824, loss = 0.06596702\n",
      "Iteration 8825, loss = 0.06595759\n",
      "Iteration 8826, loss = 0.06594798\n",
      "Iteration 8827, loss = 0.06593866\n",
      "Iteration 8828, loss = 0.06592913\n",
      "Iteration 8829, loss = 0.06591961\n",
      "Iteration 8830, loss = 0.06591014\n",
      "Iteration 8831, loss = 0.06590075\n",
      "Iteration 8832, loss = 0.06589129\n",
      "Iteration 8833, loss = 0.06588170\n",
      "Iteration 8834, loss = 0.06587244\n",
      "Iteration 8835, loss = 0.06586297\n",
      "Iteration 8836, loss = 0.06585354\n",
      "Iteration 8837, loss = 0.06584401\n",
      "Iteration 8838, loss = 0.06583454\n",
      "Iteration 8839, loss = 0.06582519\n",
      "Iteration 8840, loss = 0.06581583\n",
      "Iteration 8841, loss = 0.06580643\n",
      "Iteration 8842, loss = 0.06579701\n",
      "Iteration 8843, loss = 0.06578746\n",
      "Iteration 8844, loss = 0.06577808\n",
      "Iteration 8845, loss = 0.06576876\n",
      "Iteration 8846, loss = 0.06575942\n",
      "Iteration 8847, loss = 0.06574990\n",
      "Iteration 8848, loss = 0.06574046\n",
      "Iteration 8849, loss = 0.06573115\n",
      "Iteration 8850, loss = 0.06572178\n",
      "Iteration 8851, loss = 0.06571240\n",
      "Iteration 8852, loss = 0.06570296\n",
      "Iteration 8853, loss = 0.06569368\n",
      "Iteration 8854, loss = 0.06568424\n",
      "Iteration 8855, loss = 0.06567496\n",
      "Iteration 8856, loss = 0.06566557\n",
      "Iteration 8857, loss = 0.06565628\n",
      "Iteration 8858, loss = 0.06564691\n",
      "Iteration 8859, loss = 0.06563748\n",
      "Iteration 8860, loss = 0.06562821\n",
      "Iteration 8861, loss = 0.06561879\n",
      "Iteration 8862, loss = 0.06560953\n",
      "Iteration 8863, loss = 0.06560016\n",
      "Iteration 8864, loss = 0.06559084\n",
      "Iteration 8865, loss = 0.06558151\n",
      "Iteration 8866, loss = 0.06557209\n",
      "Iteration 8867, loss = 0.06556294\n",
      "Iteration 8868, loss = 0.06555353\n",
      "Iteration 8869, loss = 0.06554418\n",
      "Iteration 8870, loss = 0.06553474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8871, loss = 0.06552551\n",
      "Iteration 8872, loss = 0.06551618\n",
      "Iteration 8873, loss = 0.06550676\n",
      "Iteration 8874, loss = 0.06549742\n",
      "Iteration 8875, loss = 0.06548818\n",
      "Iteration 8876, loss = 0.06547866\n",
      "Iteration 8877, loss = 0.06546934\n",
      "Iteration 8878, loss = 0.06546002\n",
      "Iteration 8879, loss = 0.06545087\n",
      "Iteration 8880, loss = 0.06544132\n",
      "Iteration 8881, loss = 0.06543197\n",
      "Iteration 8882, loss = 0.06542275\n",
      "Iteration 8883, loss = 0.06541336\n",
      "Iteration 8884, loss = 0.06540398\n",
      "Iteration 8885, loss = 0.06539474\n",
      "Iteration 8886, loss = 0.06538547\n",
      "Iteration 8887, loss = 0.06537603\n",
      "Iteration 8888, loss = 0.06536685\n",
      "Iteration 8889, loss = 0.06535744\n",
      "Iteration 8890, loss = 0.06534824\n",
      "Iteration 8891, loss = 0.06533891\n",
      "Iteration 8892, loss = 0.06532957\n",
      "Iteration 8893, loss = 0.06532020\n",
      "Iteration 8894, loss = 0.06531083\n",
      "Iteration 8895, loss = 0.06530160\n",
      "Iteration 8896, loss = 0.06529239\n",
      "Iteration 8897, loss = 0.06528291\n",
      "Iteration 8898, loss = 0.06527376\n",
      "Iteration 8899, loss = 0.06526448\n",
      "Iteration 8900, loss = 0.06525494\n",
      "Iteration 8901, loss = 0.06524571\n",
      "Iteration 8902, loss = 0.06523661\n",
      "Iteration 8903, loss = 0.06522714\n",
      "Iteration 8904, loss = 0.06521774\n",
      "Iteration 8905, loss = 0.06520855\n",
      "Iteration 8906, loss = 0.06519936\n",
      "Iteration 8907, loss = 0.06519010\n",
      "Iteration 8908, loss = 0.06518083\n",
      "Iteration 8909, loss = 0.06517149\n",
      "Iteration 8910, loss = 0.06516223\n",
      "Iteration 8911, loss = 0.06515281\n",
      "Iteration 8912, loss = 0.06514384\n",
      "Iteration 8913, loss = 0.06513446\n",
      "Iteration 8914, loss = 0.06512532\n",
      "Iteration 8915, loss = 0.06511591\n",
      "Iteration 8916, loss = 0.06510679\n",
      "Iteration 8917, loss = 0.06509761\n",
      "Iteration 8918, loss = 0.06508823\n",
      "Iteration 8919, loss = 0.06507919\n",
      "Iteration 8920, loss = 0.06506985\n",
      "Iteration 8921, loss = 0.06506062\n",
      "Iteration 8922, loss = 0.06505143\n",
      "Iteration 8923, loss = 0.06504229\n",
      "Iteration 8924, loss = 0.06503320\n",
      "Iteration 8925, loss = 0.06502392\n",
      "Iteration 8926, loss = 0.06501466\n",
      "Iteration 8927, loss = 0.06500561\n",
      "Iteration 8928, loss = 0.06499632\n",
      "Iteration 8929, loss = 0.06498717\n",
      "Iteration 8930, loss = 0.06497799\n",
      "Iteration 8931, loss = 0.06496874\n",
      "Iteration 8932, loss = 0.06495963\n",
      "Iteration 8933, loss = 0.06495065\n",
      "Iteration 8934, loss = 0.06494131\n",
      "Iteration 8935, loss = 0.06493224\n",
      "Iteration 8936, loss = 0.06492309\n",
      "Iteration 8937, loss = 0.06491391\n",
      "Iteration 8938, loss = 0.06490481\n",
      "Iteration 8939, loss = 0.06489560\n",
      "Iteration 8940, loss = 0.06488656\n",
      "Iteration 8941, loss = 0.06487752\n",
      "Iteration 8942, loss = 0.06486821\n",
      "Iteration 8943, loss = 0.06485905\n",
      "Iteration 8944, loss = 0.06484991\n",
      "Iteration 8945, loss = 0.06484072\n",
      "Iteration 8946, loss = 0.06483159\n",
      "Iteration 8947, loss = 0.06482244\n",
      "Iteration 8948, loss = 0.06481332\n",
      "Iteration 8949, loss = 0.06480413\n",
      "Iteration 8950, loss = 0.06479493\n",
      "Iteration 8951, loss = 0.06478579\n",
      "Iteration 8952, loss = 0.06477669\n",
      "Iteration 8953, loss = 0.06476748\n",
      "Iteration 8954, loss = 0.06475845\n",
      "Iteration 8955, loss = 0.06474932\n",
      "Iteration 8956, loss = 0.06474009\n",
      "Iteration 8957, loss = 0.06473096\n",
      "Iteration 8958, loss = 0.06472180\n",
      "Iteration 8959, loss = 0.06471266\n",
      "Iteration 8960, loss = 0.06470375\n",
      "Iteration 8961, loss = 0.06469449\n",
      "Iteration 8962, loss = 0.06468541\n",
      "Iteration 8963, loss = 0.06467637\n",
      "Iteration 8964, loss = 0.06466725\n",
      "Iteration 8965, loss = 0.06465816\n",
      "Iteration 8966, loss = 0.06464898\n",
      "Iteration 8967, loss = 0.06463988\n",
      "Iteration 8968, loss = 0.06463097\n",
      "Iteration 8969, loss = 0.06462173\n",
      "Iteration 8970, loss = 0.06461268\n",
      "Iteration 8971, loss = 0.06460359\n",
      "Iteration 8972, loss = 0.06459439\n",
      "Iteration 8973, loss = 0.06458532\n",
      "Iteration 8974, loss = 0.06457622\n",
      "Iteration 8975, loss = 0.06456719\n",
      "Iteration 8976, loss = 0.06455810\n",
      "Iteration 8977, loss = 0.06454913\n",
      "Iteration 8978, loss = 0.06453991\n",
      "Iteration 8979, loss = 0.06453079\n",
      "Iteration 8980, loss = 0.06452182\n",
      "Iteration 8981, loss = 0.06451282\n",
      "Iteration 8982, loss = 0.06450365\n",
      "Iteration 8983, loss = 0.06449472\n",
      "Iteration 8984, loss = 0.06448555\n",
      "Iteration 8985, loss = 0.06447649\n",
      "Iteration 8986, loss = 0.06446755\n",
      "Iteration 8987, loss = 0.06445842\n",
      "Iteration 8988, loss = 0.06444941\n",
      "Iteration 8989, loss = 0.06444037\n",
      "Iteration 8990, loss = 0.06443132\n",
      "Iteration 8991, loss = 0.06442236\n",
      "Iteration 8992, loss = 0.06441335\n",
      "Iteration 8993, loss = 0.06440441\n",
      "Iteration 8994, loss = 0.06439531\n",
      "Iteration 8995, loss = 0.06438621\n",
      "Iteration 8996, loss = 0.06437715\n",
      "Iteration 8997, loss = 0.06436827\n",
      "Iteration 8998, loss = 0.06435910\n",
      "Iteration 8999, loss = 0.06435018\n",
      "Iteration 9000, loss = 0.06434107\n",
      "Iteration 9001, loss = 0.06433214\n",
      "Iteration 9002, loss = 0.06432315\n",
      "Iteration 9003, loss = 0.06431400\n",
      "Iteration 9004, loss = 0.06430502\n",
      "Iteration 9005, loss = 0.06429617\n",
      "Iteration 9006, loss = 0.06428702\n",
      "Iteration 9007, loss = 0.06427803\n",
      "Iteration 9008, loss = 0.06426917\n",
      "Iteration 9009, loss = 0.06426001\n",
      "Iteration 9010, loss = 0.06425102\n",
      "Iteration 9011, loss = 0.06424197\n",
      "Iteration 9012, loss = 0.06423312\n",
      "Iteration 9013, loss = 0.06422399\n",
      "Iteration 9014, loss = 0.06421509\n",
      "Iteration 9015, loss = 0.06420624\n",
      "Iteration 9016, loss = 0.06419710\n",
      "Iteration 9017, loss = 0.06418807\n",
      "Iteration 9018, loss = 0.06417921\n",
      "Iteration 9019, loss = 0.06417014\n",
      "Iteration 9020, loss = 0.06416123\n",
      "Iteration 9021, loss = 0.06415224\n",
      "Iteration 9022, loss = 0.06414329\n",
      "Iteration 9023, loss = 0.06413432\n",
      "Iteration 9024, loss = 0.06412529\n",
      "Iteration 9025, loss = 0.06411639\n",
      "Iteration 9026, loss = 0.06410755\n",
      "Iteration 9027, loss = 0.06409842\n",
      "Iteration 9028, loss = 0.06408955\n",
      "Iteration 9029, loss = 0.06408067\n",
      "Iteration 9030, loss = 0.06407165\n",
      "Iteration 9031, loss = 0.06406273\n",
      "Iteration 9032, loss = 0.06405371\n",
      "Iteration 9033, loss = 0.06404487\n",
      "Iteration 9034, loss = 0.06403590\n",
      "Iteration 9035, loss = 0.06402691\n",
      "Iteration 9036, loss = 0.06401803\n",
      "Iteration 9037, loss = 0.06400903\n",
      "Iteration 9038, loss = 0.06400025\n",
      "Iteration 9039, loss = 0.06399126\n",
      "Iteration 9040, loss = 0.06398234\n",
      "Iteration 9041, loss = 0.06397336\n",
      "Iteration 9042, loss = 0.06396443\n",
      "Iteration 9043, loss = 0.06395547\n",
      "Iteration 9044, loss = 0.06394666\n",
      "Iteration 9045, loss = 0.06393772\n",
      "Iteration 9046, loss = 0.06392874\n",
      "Iteration 9047, loss = 0.06391976\n",
      "Iteration 9048, loss = 0.06391097\n",
      "Iteration 9049, loss = 0.06390222\n",
      "Iteration 9050, loss = 0.06389314\n",
      "Iteration 9051, loss = 0.06388418\n",
      "Iteration 9052, loss = 0.06387536\n",
      "Iteration 9053, loss = 0.06386639\n",
      "Iteration 9054, loss = 0.06385755\n",
      "Iteration 9055, loss = 0.06384861\n",
      "Iteration 9056, loss = 0.06383972\n",
      "Iteration 9057, loss = 0.06383079\n",
      "Iteration 9058, loss = 0.06382195\n",
      "Iteration 9059, loss = 0.06381316\n",
      "Iteration 9060, loss = 0.06380432\n",
      "Iteration 9061, loss = 0.06379532\n",
      "Iteration 9062, loss = 0.06378656\n",
      "Iteration 9063, loss = 0.06377751\n",
      "Iteration 9064, loss = 0.06376866\n",
      "Iteration 9065, loss = 0.06375986\n",
      "Iteration 9066, loss = 0.06375108\n",
      "Iteration 9067, loss = 0.06374211\n",
      "Iteration 9068, loss = 0.06373326\n",
      "Iteration 9069, loss = 0.06372437\n",
      "Iteration 9070, loss = 0.06371550\n",
      "Iteration 9071, loss = 0.06370671\n",
      "Iteration 9072, loss = 0.06369791\n",
      "Iteration 9073, loss = 0.06368911\n",
      "Iteration 9074, loss = 0.06368017\n",
      "Iteration 9075, loss = 0.06367136\n",
      "Iteration 9076, loss = 0.06366258\n",
      "Iteration 9077, loss = 0.06365382\n",
      "Iteration 9078, loss = 0.06364504\n",
      "Iteration 9079, loss = 0.06363620\n",
      "Iteration 9080, loss = 0.06362734\n",
      "Iteration 9081, loss = 0.06361863\n",
      "Iteration 9082, loss = 0.06360979\n",
      "Iteration 9083, loss = 0.06360090\n",
      "Iteration 9084, loss = 0.06359215\n",
      "Iteration 9085, loss = 0.06358334\n",
      "Iteration 9086, loss = 0.06357460\n",
      "Iteration 9087, loss = 0.06356580\n",
      "Iteration 9088, loss = 0.06355712\n",
      "Iteration 9089, loss = 0.06354824\n",
      "Iteration 9090, loss = 0.06353942\n",
      "Iteration 9091, loss = 0.06353059\n",
      "Iteration 9092, loss = 0.06352182\n",
      "Iteration 9093, loss = 0.06351304\n",
      "Iteration 9094, loss = 0.06350421\n",
      "Iteration 9095, loss = 0.06349549\n",
      "Iteration 9096, loss = 0.06348672\n",
      "Iteration 9097, loss = 0.06347792\n",
      "Iteration 9098, loss = 0.06346918\n",
      "Iteration 9099, loss = 0.06346034\n",
      "Iteration 9100, loss = 0.06345160\n",
      "Iteration 9101, loss = 0.06344295\n",
      "Iteration 9102, loss = 0.06343405\n",
      "Iteration 9103, loss = 0.06342537\n",
      "Iteration 9104, loss = 0.06341662\n",
      "Iteration 9105, loss = 0.06340783\n",
      "Iteration 9106, loss = 0.06339897\n",
      "Iteration 9107, loss = 0.06339049\n",
      "Iteration 9108, loss = 0.06338161\n",
      "Iteration 9109, loss = 0.06337302\n",
      "Iteration 9110, loss = 0.06336415\n",
      "Iteration 9111, loss = 0.06335543\n",
      "Iteration 9112, loss = 0.06334671\n",
      "Iteration 9113, loss = 0.06333801\n",
      "Iteration 9114, loss = 0.06332938\n",
      "Iteration 9115, loss = 0.06332064\n",
      "Iteration 9116, loss = 0.06331186\n",
      "Iteration 9117, loss = 0.06330313\n",
      "Iteration 9118, loss = 0.06329461\n",
      "Iteration 9119, loss = 0.06328581\n",
      "Iteration 9120, loss = 0.06327699\n",
      "Iteration 9121, loss = 0.06326834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9122, loss = 0.06325968\n",
      "Iteration 9123, loss = 0.06325101\n",
      "Iteration 9124, loss = 0.06324239\n",
      "Iteration 9125, loss = 0.06323361\n",
      "Iteration 9126, loss = 0.06322481\n",
      "Iteration 9127, loss = 0.06321616\n",
      "Iteration 9128, loss = 0.06320745\n",
      "Iteration 9129, loss = 0.06319882\n",
      "Iteration 9130, loss = 0.06319004\n",
      "Iteration 9131, loss = 0.06318154\n",
      "Iteration 9132, loss = 0.06317273\n",
      "Iteration 9133, loss = 0.06316402\n",
      "Iteration 9134, loss = 0.06315524\n",
      "Iteration 9135, loss = 0.06314660\n",
      "Iteration 9136, loss = 0.06313791\n",
      "Iteration 9137, loss = 0.06312931\n",
      "Iteration 9138, loss = 0.06312055\n",
      "Iteration 9139, loss = 0.06311200\n",
      "Iteration 9140, loss = 0.06310326\n",
      "Iteration 9141, loss = 0.06309464\n",
      "Iteration 9142, loss = 0.06308597\n",
      "Iteration 9143, loss = 0.06307731\n",
      "Iteration 9144, loss = 0.06306865\n",
      "Iteration 9145, loss = 0.06305989\n",
      "Iteration 9146, loss = 0.06305137\n",
      "Iteration 9147, loss = 0.06304263\n",
      "Iteration 9148, loss = 0.06303403\n",
      "Iteration 9149, loss = 0.06302538\n",
      "Iteration 9150, loss = 0.06301666\n",
      "Iteration 9151, loss = 0.06300812\n",
      "Iteration 9152, loss = 0.06299946\n",
      "Iteration 9153, loss = 0.06299089\n",
      "Iteration 9154, loss = 0.06298220\n",
      "Iteration 9155, loss = 0.06297374\n",
      "Iteration 9156, loss = 0.06296491\n",
      "Iteration 9157, loss = 0.06295637\n",
      "Iteration 9158, loss = 0.06294765\n",
      "Iteration 9159, loss = 0.06293899\n",
      "Iteration 9160, loss = 0.06293052\n",
      "Iteration 9161, loss = 0.06292191\n",
      "Iteration 9162, loss = 0.06291319\n",
      "Iteration 9163, loss = 0.06290458\n",
      "Iteration 9164, loss = 0.06289595\n",
      "Iteration 9165, loss = 0.06288734\n",
      "Iteration 9166, loss = 0.06287878\n",
      "Iteration 9167, loss = 0.06287015\n",
      "Iteration 9168, loss = 0.06286142\n",
      "Iteration 9169, loss = 0.06285300\n",
      "Iteration 9170, loss = 0.06284420\n",
      "Iteration 9171, loss = 0.06283573\n",
      "Iteration 9172, loss = 0.06282713\n",
      "Iteration 9173, loss = 0.06281843\n",
      "Iteration 9174, loss = 0.06280997\n",
      "Iteration 9175, loss = 0.06280118\n",
      "Iteration 9176, loss = 0.06279279\n",
      "Iteration 9177, loss = 0.06278415\n",
      "Iteration 9178, loss = 0.06277553\n",
      "Iteration 9179, loss = 0.06276694\n",
      "Iteration 9180, loss = 0.06275827\n",
      "Iteration 9181, loss = 0.06274967\n",
      "Iteration 9182, loss = 0.06274105\n",
      "Iteration 9183, loss = 0.06273258\n",
      "Iteration 9184, loss = 0.06272385\n",
      "Iteration 9185, loss = 0.06271530\n",
      "Iteration 9186, loss = 0.06270679\n",
      "Iteration 9187, loss = 0.06269815\n",
      "Iteration 9188, loss = 0.06268955\n",
      "Iteration 9189, loss = 0.06268108\n",
      "Iteration 9190, loss = 0.06267243\n",
      "Iteration 9191, loss = 0.06266385\n",
      "Iteration 9192, loss = 0.06265532\n",
      "Iteration 9193, loss = 0.06264673\n",
      "Iteration 9194, loss = 0.06263825\n",
      "Iteration 9195, loss = 0.06262971\n",
      "Iteration 9196, loss = 0.06262115\n",
      "Iteration 9197, loss = 0.06261265\n",
      "Iteration 9198, loss = 0.06260421\n",
      "Iteration 9199, loss = 0.06259559\n",
      "Iteration 9200, loss = 0.06258709\n",
      "Iteration 9201, loss = 0.06257873\n",
      "Iteration 9202, loss = 0.06257002\n",
      "Iteration 9203, loss = 0.06256151\n",
      "Iteration 9204, loss = 0.06255301\n",
      "Iteration 9205, loss = 0.06254449\n",
      "Iteration 9206, loss = 0.06253591\n",
      "Iteration 9207, loss = 0.06252742\n",
      "Iteration 9208, loss = 0.06251887\n",
      "Iteration 9209, loss = 0.06251033\n",
      "Iteration 9210, loss = 0.06250193\n",
      "Iteration 9211, loss = 0.06249339\n",
      "Iteration 9212, loss = 0.06248484\n",
      "Iteration 9213, loss = 0.06247634\n",
      "Iteration 9214, loss = 0.06246773\n",
      "Iteration 9215, loss = 0.06245928\n",
      "Iteration 9216, loss = 0.06245090\n",
      "Iteration 9217, loss = 0.06244237\n",
      "Iteration 9218, loss = 0.06243378\n",
      "Iteration 9219, loss = 0.06242525\n",
      "Iteration 9220, loss = 0.06241689\n",
      "Iteration 9221, loss = 0.06240823\n",
      "Iteration 9222, loss = 0.06239982\n",
      "Iteration 9223, loss = 0.06239144\n",
      "Iteration 9224, loss = 0.06238282\n",
      "Iteration 9225, loss = 0.06237434\n",
      "Iteration 9226, loss = 0.06236579\n",
      "Iteration 9227, loss = 0.06235739\n",
      "Iteration 9228, loss = 0.06234894\n",
      "Iteration 9229, loss = 0.06234043\n",
      "Iteration 9230, loss = 0.06233197\n",
      "Iteration 9231, loss = 0.06232350\n",
      "Iteration 9232, loss = 0.06231504\n",
      "Iteration 9233, loss = 0.06230666\n",
      "Iteration 9234, loss = 0.06229813\n",
      "Iteration 9235, loss = 0.06228971\n",
      "Iteration 9236, loss = 0.06228132\n",
      "Iteration 9237, loss = 0.06227284\n",
      "Iteration 9238, loss = 0.06226441\n",
      "Iteration 9239, loss = 0.06225606\n",
      "Iteration 9240, loss = 0.06224751\n",
      "Iteration 9241, loss = 0.06223920\n",
      "Iteration 9242, loss = 0.06223067\n",
      "Iteration 9243, loss = 0.06222229\n",
      "Iteration 9244, loss = 0.06221388\n",
      "Iteration 9245, loss = 0.06220540\n",
      "Iteration 9246, loss = 0.06219700\n",
      "Iteration 9247, loss = 0.06218879\n",
      "Iteration 9248, loss = 0.06218020\n",
      "Iteration 9249, loss = 0.06217178\n",
      "Iteration 9250, loss = 0.06216341\n",
      "Iteration 9251, loss = 0.06215496\n",
      "Iteration 9252, loss = 0.06214654\n",
      "Iteration 9253, loss = 0.06213813\n",
      "Iteration 9254, loss = 0.06212971\n",
      "Iteration 9255, loss = 0.06212134\n",
      "Iteration 9256, loss = 0.06211295\n",
      "Iteration 9257, loss = 0.06210445\n",
      "Iteration 9258, loss = 0.06209610\n",
      "Iteration 9259, loss = 0.06208765\n",
      "Iteration 9260, loss = 0.06207915\n",
      "Iteration 9261, loss = 0.06207070\n",
      "Iteration 9262, loss = 0.06206226\n",
      "Iteration 9263, loss = 0.06205389\n",
      "Iteration 9264, loss = 0.06204549\n",
      "Iteration 9265, loss = 0.06203704\n",
      "Iteration 9266, loss = 0.06202858\n",
      "Iteration 9267, loss = 0.06202035\n",
      "Iteration 9268, loss = 0.06201180\n",
      "Iteration 9269, loss = 0.06200346\n",
      "Iteration 9270, loss = 0.06199513\n",
      "Iteration 9271, loss = 0.06198673\n",
      "Iteration 9272, loss = 0.06197840\n",
      "Iteration 9273, loss = 0.06197001\n",
      "Iteration 9274, loss = 0.06196162\n",
      "Iteration 9275, loss = 0.06195322\n",
      "Iteration 9276, loss = 0.06194489\n",
      "Iteration 9277, loss = 0.06193650\n",
      "Iteration 9278, loss = 0.06192816\n",
      "Iteration 9279, loss = 0.06191983\n",
      "Iteration 9280, loss = 0.06191146\n",
      "Iteration 9281, loss = 0.06190316\n",
      "Iteration 9282, loss = 0.06189480\n",
      "Iteration 9283, loss = 0.06188635\n",
      "Iteration 9284, loss = 0.06187812\n",
      "Iteration 9285, loss = 0.06186964\n",
      "Iteration 9286, loss = 0.06186138\n",
      "Iteration 9287, loss = 0.06185304\n",
      "Iteration 9288, loss = 0.06184465\n",
      "Iteration 9289, loss = 0.06183647\n",
      "Iteration 9290, loss = 0.06182794\n",
      "Iteration 9291, loss = 0.06181964\n",
      "Iteration 9292, loss = 0.06181140\n",
      "Iteration 9293, loss = 0.06180302\n",
      "Iteration 9294, loss = 0.06179470\n",
      "Iteration 9295, loss = 0.06178640\n",
      "Iteration 9296, loss = 0.06177815\n",
      "Iteration 9297, loss = 0.06176971\n",
      "Iteration 9298, loss = 0.06176139\n",
      "Iteration 9299, loss = 0.06175303\n",
      "Iteration 9300, loss = 0.06174472\n",
      "Iteration 9301, loss = 0.06173646\n",
      "Iteration 9302, loss = 0.06172814\n",
      "Iteration 9303, loss = 0.06171979\n",
      "Iteration 9304, loss = 0.06171150\n",
      "Iteration 9305, loss = 0.06170322\n",
      "Iteration 9306, loss = 0.06169486\n",
      "Iteration 9307, loss = 0.06168661\n",
      "Iteration 9308, loss = 0.06167820\n",
      "Iteration 9309, loss = 0.06167002\n",
      "Iteration 9310, loss = 0.06166174\n",
      "Iteration 9311, loss = 0.06165342\n",
      "Iteration 9312, loss = 0.06164527\n",
      "Iteration 9313, loss = 0.06163697\n",
      "Iteration 9314, loss = 0.06162860\n",
      "Iteration 9315, loss = 0.06162035\n",
      "Iteration 9316, loss = 0.06161202\n",
      "Iteration 9317, loss = 0.06160396\n",
      "Iteration 9318, loss = 0.06159563\n",
      "Iteration 9319, loss = 0.06158744\n",
      "Iteration 9320, loss = 0.06157911\n",
      "Iteration 9321, loss = 0.06157086\n",
      "Iteration 9322, loss = 0.06156274\n",
      "Iteration 9323, loss = 0.06155438\n",
      "Iteration 9324, loss = 0.06154608\n",
      "Iteration 9325, loss = 0.06153789\n",
      "Iteration 9326, loss = 0.06152964\n",
      "Iteration 9327, loss = 0.06152136\n",
      "Iteration 9328, loss = 0.06151310\n",
      "Iteration 9329, loss = 0.06150499\n",
      "Iteration 9330, loss = 0.06149658\n",
      "Iteration 9331, loss = 0.06148841\n",
      "Iteration 9332, loss = 0.06148018\n",
      "Iteration 9333, loss = 0.06147201\n",
      "Iteration 9334, loss = 0.06146371\n",
      "Iteration 9335, loss = 0.06145546\n",
      "Iteration 9336, loss = 0.06144728\n",
      "Iteration 9337, loss = 0.06143900\n",
      "Iteration 9338, loss = 0.06143069\n",
      "Iteration 9339, loss = 0.06142247\n",
      "Iteration 9340, loss = 0.06141421\n",
      "Iteration 9341, loss = 0.06140599\n",
      "Iteration 9342, loss = 0.06139772\n",
      "Iteration 9343, loss = 0.06138939\n",
      "Iteration 9344, loss = 0.06138117\n",
      "Iteration 9345, loss = 0.06137290\n",
      "Iteration 9346, loss = 0.06136462\n",
      "Iteration 9347, loss = 0.06135651\n",
      "Iteration 9348, loss = 0.06134833\n",
      "Iteration 9349, loss = 0.06134007\n",
      "Iteration 9350, loss = 0.06133177\n",
      "Iteration 9351, loss = 0.06132364\n",
      "Iteration 9352, loss = 0.06131557\n",
      "Iteration 9353, loss = 0.06130713\n",
      "Iteration 9354, loss = 0.06129903\n",
      "Iteration 9355, loss = 0.06129081\n",
      "Iteration 9356, loss = 0.06128258\n",
      "Iteration 9357, loss = 0.06127446\n",
      "Iteration 9358, loss = 0.06126618\n",
      "Iteration 9359, loss = 0.06125811\n",
      "Iteration 9360, loss = 0.06124981\n",
      "Iteration 9361, loss = 0.06124173\n",
      "Iteration 9362, loss = 0.06123347\n",
      "Iteration 9363, loss = 0.06122529\n",
      "Iteration 9364, loss = 0.06121710\n",
      "Iteration 9365, loss = 0.06120886\n",
      "Iteration 9366, loss = 0.06120066\n",
      "Iteration 9367, loss = 0.06119249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9368, loss = 0.06118435\n",
      "Iteration 9369, loss = 0.06117615\n",
      "Iteration 9370, loss = 0.06116802\n",
      "Iteration 9371, loss = 0.06115988\n",
      "Iteration 9372, loss = 0.06115164\n",
      "Iteration 9373, loss = 0.06114345\n",
      "Iteration 9374, loss = 0.06113527\n",
      "Iteration 9375, loss = 0.06112711\n",
      "Iteration 9376, loss = 0.06111896\n",
      "Iteration 9377, loss = 0.06111081\n",
      "Iteration 9378, loss = 0.06110267\n",
      "Iteration 9379, loss = 0.06109443\n",
      "Iteration 9380, loss = 0.06108634\n",
      "Iteration 9381, loss = 0.06107824\n",
      "Iteration 9382, loss = 0.06107001\n",
      "Iteration 9383, loss = 0.06106207\n",
      "Iteration 9384, loss = 0.06105386\n",
      "Iteration 9385, loss = 0.06104569\n",
      "Iteration 9386, loss = 0.06103750\n",
      "Iteration 9387, loss = 0.06102951\n",
      "Iteration 9388, loss = 0.06102126\n",
      "Iteration 9389, loss = 0.06101317\n",
      "Iteration 9390, loss = 0.06100502\n",
      "Iteration 9391, loss = 0.06099697\n",
      "Iteration 9392, loss = 0.06098881\n",
      "Iteration 9393, loss = 0.06098072\n",
      "Iteration 9394, loss = 0.06097262\n",
      "Iteration 9395, loss = 0.06096451\n",
      "Iteration 9396, loss = 0.06095640\n",
      "Iteration 9397, loss = 0.06094824\n",
      "Iteration 9398, loss = 0.06094027\n",
      "Iteration 9399, loss = 0.06093215\n",
      "Iteration 9400, loss = 0.06092410\n",
      "Iteration 9401, loss = 0.06091599\n",
      "Iteration 9402, loss = 0.06090779\n",
      "Iteration 9403, loss = 0.06089984\n",
      "Iteration 9404, loss = 0.06089171\n",
      "Iteration 9405, loss = 0.06088354\n",
      "Iteration 9406, loss = 0.06087555\n",
      "Iteration 9407, loss = 0.06086752\n",
      "Iteration 9408, loss = 0.06085942\n",
      "Iteration 9409, loss = 0.06085141\n",
      "Iteration 9410, loss = 0.06084328\n",
      "Iteration 9411, loss = 0.06083522\n",
      "Iteration 9412, loss = 0.06082728\n",
      "Iteration 9413, loss = 0.06081900\n",
      "Iteration 9414, loss = 0.06081105\n",
      "Iteration 9415, loss = 0.06080295\n",
      "Iteration 9416, loss = 0.06079488\n",
      "Iteration 9417, loss = 0.06078699\n",
      "Iteration 9418, loss = 0.06077891\n",
      "Iteration 9419, loss = 0.06077076\n",
      "Iteration 9420, loss = 0.06076283\n",
      "Iteration 9421, loss = 0.06075466\n",
      "Iteration 9422, loss = 0.06074674\n",
      "Iteration 9423, loss = 0.06073875\n",
      "Iteration 9424, loss = 0.06073066\n",
      "Iteration 9425, loss = 0.06072257\n",
      "Iteration 9426, loss = 0.06071456\n",
      "Iteration 9427, loss = 0.06070646\n",
      "Iteration 9428, loss = 0.06069850\n",
      "Iteration 9429, loss = 0.06069037\n",
      "Iteration 9430, loss = 0.06068240\n",
      "Iteration 9431, loss = 0.06067439\n",
      "Iteration 9432, loss = 0.06066625\n",
      "Iteration 9433, loss = 0.06065826\n",
      "Iteration 9434, loss = 0.06065025\n",
      "Iteration 9435, loss = 0.06064217\n",
      "Iteration 9436, loss = 0.06063422\n",
      "Iteration 9437, loss = 0.06062617\n",
      "Iteration 9438, loss = 0.06061815\n",
      "Iteration 9439, loss = 0.06061005\n",
      "Iteration 9440, loss = 0.06060204\n",
      "Iteration 9441, loss = 0.06059407\n",
      "Iteration 9442, loss = 0.06058612\n",
      "Iteration 9443, loss = 0.06057809\n",
      "Iteration 9444, loss = 0.06057010\n",
      "Iteration 9445, loss = 0.06056199\n",
      "Iteration 9446, loss = 0.06055412\n",
      "Iteration 9447, loss = 0.06054594\n",
      "Iteration 9448, loss = 0.06053795\n",
      "Iteration 9449, loss = 0.06053003\n",
      "Iteration 9450, loss = 0.06052205\n",
      "Iteration 9451, loss = 0.06051400\n",
      "Iteration 9452, loss = 0.06050598\n",
      "Iteration 9453, loss = 0.06049797\n",
      "Iteration 9454, loss = 0.06048996\n",
      "Iteration 9455, loss = 0.06048203\n",
      "Iteration 9456, loss = 0.06047404\n",
      "Iteration 9457, loss = 0.06046613\n",
      "Iteration 9458, loss = 0.06045803\n",
      "Iteration 9459, loss = 0.06045010\n",
      "Iteration 9460, loss = 0.06044206\n",
      "Iteration 9461, loss = 0.06043399\n",
      "Iteration 9462, loss = 0.06042609\n",
      "Iteration 9463, loss = 0.06041810\n",
      "Iteration 9464, loss = 0.06041014\n",
      "Iteration 9465, loss = 0.06040211\n",
      "Iteration 9466, loss = 0.06039418\n",
      "Iteration 9467, loss = 0.06038619\n",
      "Iteration 9468, loss = 0.06037818\n",
      "Iteration 9469, loss = 0.06037024\n",
      "Iteration 9470, loss = 0.06036232\n",
      "Iteration 9471, loss = 0.06035428\n",
      "Iteration 9472, loss = 0.06034629\n",
      "Iteration 9473, loss = 0.06033845\n",
      "Iteration 9474, loss = 0.06033039\n",
      "Iteration 9475, loss = 0.06032258\n",
      "Iteration 9476, loss = 0.06031462\n",
      "Iteration 9477, loss = 0.06030653\n",
      "Iteration 9478, loss = 0.06029881\n",
      "Iteration 9479, loss = 0.06029080\n",
      "Iteration 9480, loss = 0.06028294\n",
      "Iteration 9481, loss = 0.06027495\n",
      "Iteration 9482, loss = 0.06026703\n",
      "Iteration 9483, loss = 0.06025921\n",
      "Iteration 9484, loss = 0.06025116\n",
      "Iteration 9485, loss = 0.06024344\n",
      "Iteration 9486, loss = 0.06023555\n",
      "Iteration 9487, loss = 0.06022751\n",
      "Iteration 9488, loss = 0.06021971\n",
      "Iteration 9489, loss = 0.06021175\n",
      "Iteration 9490, loss = 0.06020380\n",
      "Iteration 9491, loss = 0.06019583\n",
      "Iteration 9492, loss = 0.06018788\n",
      "Iteration 9493, loss = 0.06018006\n",
      "Iteration 9494, loss = 0.06017203\n",
      "Iteration 9495, loss = 0.06016418\n",
      "Iteration 9496, loss = 0.06015618\n",
      "Iteration 9497, loss = 0.06014838\n",
      "Iteration 9498, loss = 0.06014049\n",
      "Iteration 9499, loss = 0.06013262\n",
      "Iteration 9500, loss = 0.06012470\n",
      "Iteration 9501, loss = 0.06011679\n",
      "Iteration 9502, loss = 0.06010887\n",
      "Iteration 9503, loss = 0.06010101\n",
      "Iteration 9504, loss = 0.06009312\n",
      "Iteration 9505, loss = 0.06008537\n",
      "Iteration 9506, loss = 0.06007734\n",
      "Iteration 9507, loss = 0.06006952\n",
      "Iteration 9508, loss = 0.06006164\n",
      "Iteration 9509, loss = 0.06005373\n",
      "Iteration 9510, loss = 0.06004583\n",
      "Iteration 9511, loss = 0.06003812\n",
      "Iteration 9512, loss = 0.06003016\n",
      "Iteration 9513, loss = 0.06002239\n",
      "Iteration 9514, loss = 0.06001441\n",
      "Iteration 9515, loss = 0.06000657\n",
      "Iteration 9516, loss = 0.05999882\n",
      "Iteration 9517, loss = 0.05999091\n",
      "Iteration 9518, loss = 0.05998302\n",
      "Iteration 9519, loss = 0.05997517\n",
      "Iteration 9520, loss = 0.05996727\n",
      "Iteration 9521, loss = 0.05995951\n",
      "Iteration 9522, loss = 0.05995168\n",
      "Iteration 9523, loss = 0.05994381\n",
      "Iteration 9524, loss = 0.05993593\n",
      "Iteration 9525, loss = 0.05992808\n",
      "Iteration 9526, loss = 0.05992024\n",
      "Iteration 9527, loss = 0.05991247\n",
      "Iteration 9528, loss = 0.05990457\n",
      "Iteration 9529, loss = 0.05989684\n",
      "Iteration 9530, loss = 0.05988905\n",
      "Iteration 9531, loss = 0.05988113\n",
      "Iteration 9532, loss = 0.05987335\n",
      "Iteration 9533, loss = 0.05986556\n",
      "Iteration 9534, loss = 0.05985777\n",
      "Iteration 9535, loss = 0.05984988\n",
      "Iteration 9536, loss = 0.05984200\n",
      "Iteration 9537, loss = 0.05983429\n",
      "Iteration 9538, loss = 0.05982654\n",
      "Iteration 9539, loss = 0.05981872\n",
      "Iteration 9540, loss = 0.05981080\n",
      "Iteration 9541, loss = 0.05980301\n",
      "Iteration 9542, loss = 0.05979520\n",
      "Iteration 9543, loss = 0.05978748\n",
      "Iteration 9544, loss = 0.05977959\n",
      "Iteration 9545, loss = 0.05977182\n",
      "Iteration 9546, loss = 0.05976400\n",
      "Iteration 9547, loss = 0.05975622\n",
      "Iteration 9548, loss = 0.05974846\n",
      "Iteration 9549, loss = 0.05974061\n",
      "Iteration 9550, loss = 0.05973287\n",
      "Iteration 9551, loss = 0.05972508\n",
      "Iteration 9552, loss = 0.05971727\n",
      "Iteration 9553, loss = 0.05970946\n",
      "Iteration 9554, loss = 0.05970175\n",
      "Iteration 9555, loss = 0.05969398\n",
      "Iteration 9556, loss = 0.05968609\n",
      "Iteration 9557, loss = 0.05967842\n",
      "Iteration 9558, loss = 0.05967054\n",
      "Iteration 9559, loss = 0.05966284\n",
      "Iteration 9560, loss = 0.05965500\n",
      "Iteration 9561, loss = 0.05964725\n",
      "Iteration 9562, loss = 0.05963956\n",
      "Iteration 9563, loss = 0.05963175\n",
      "Iteration 9564, loss = 0.05962390\n",
      "Iteration 9565, loss = 0.05961623\n",
      "Iteration 9566, loss = 0.05960839\n",
      "Iteration 9567, loss = 0.05960067\n",
      "Iteration 9568, loss = 0.05959298\n",
      "Iteration 9569, loss = 0.05958513\n",
      "Iteration 9570, loss = 0.05957745\n",
      "Iteration 9571, loss = 0.05956961\n",
      "Iteration 9572, loss = 0.05956191\n",
      "Iteration 9573, loss = 0.05955421\n",
      "Iteration 9574, loss = 0.05954636\n",
      "Iteration 9575, loss = 0.05953866\n",
      "Iteration 9576, loss = 0.05953081\n",
      "Iteration 9577, loss = 0.05952321\n",
      "Iteration 9578, loss = 0.05951553\n",
      "Iteration 9579, loss = 0.05950770\n",
      "Iteration 9580, loss = 0.05949996\n",
      "Iteration 9581, loss = 0.05949227\n",
      "Iteration 9582, loss = 0.05948456\n",
      "Iteration 9583, loss = 0.05947682\n",
      "Iteration 9584, loss = 0.05946914\n",
      "Iteration 9585, loss = 0.05946135\n",
      "Iteration 9586, loss = 0.05945360\n",
      "Iteration 9587, loss = 0.05944593\n",
      "Iteration 9588, loss = 0.05943826\n",
      "Iteration 9589, loss = 0.05943049\n",
      "Iteration 9590, loss = 0.05942283\n",
      "Iteration 9591, loss = 0.05941517\n",
      "Iteration 9592, loss = 0.05940744\n",
      "Iteration 9593, loss = 0.05939982\n",
      "Iteration 9594, loss = 0.05939207\n",
      "Iteration 9595, loss = 0.05938429\n",
      "Iteration 9596, loss = 0.05937663\n",
      "Iteration 9597, loss = 0.05936894\n",
      "Iteration 9598, loss = 0.05936139\n",
      "Iteration 9599, loss = 0.05935368\n",
      "Iteration 9600, loss = 0.05934601\n",
      "Iteration 9601, loss = 0.05933821\n",
      "Iteration 9602, loss = 0.05933056\n",
      "Iteration 9603, loss = 0.05932286\n",
      "Iteration 9604, loss = 0.05931539\n",
      "Iteration 9605, loss = 0.05930755\n",
      "Iteration 9606, loss = 0.05929999\n",
      "Iteration 9607, loss = 0.05929226\n",
      "Iteration 9608, loss = 0.05928458\n",
      "Iteration 9609, loss = 0.05927686\n",
      "Iteration 9610, loss = 0.05926922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9611, loss = 0.05926155\n",
      "Iteration 9612, loss = 0.05925396\n",
      "Iteration 9613, loss = 0.05924625\n",
      "Iteration 9614, loss = 0.05923857\n",
      "Iteration 9615, loss = 0.05923090\n",
      "Iteration 9616, loss = 0.05922332\n",
      "Iteration 9617, loss = 0.05921566\n",
      "Iteration 9618, loss = 0.05920815\n",
      "Iteration 9619, loss = 0.05920043\n",
      "Iteration 9620, loss = 0.05919273\n",
      "Iteration 9621, loss = 0.05918512\n",
      "Iteration 9622, loss = 0.05917740\n",
      "Iteration 9623, loss = 0.05916974\n",
      "Iteration 9624, loss = 0.05916211\n",
      "Iteration 9625, loss = 0.05915447\n",
      "Iteration 9626, loss = 0.05914681\n",
      "Iteration 9627, loss = 0.05913922\n",
      "Iteration 9628, loss = 0.05913153\n",
      "Iteration 9629, loss = 0.05912405\n",
      "Iteration 9630, loss = 0.05911627\n",
      "Iteration 9631, loss = 0.05910871\n",
      "Iteration 9632, loss = 0.05910115\n",
      "Iteration 9633, loss = 0.05909336\n",
      "Iteration 9634, loss = 0.05908583\n",
      "Iteration 9635, loss = 0.05907824\n",
      "Iteration 9636, loss = 0.05907056\n",
      "Iteration 9637, loss = 0.05906287\n",
      "Iteration 9638, loss = 0.05905531\n",
      "Iteration 9639, loss = 0.05904771\n",
      "Iteration 9640, loss = 0.05904019\n",
      "Iteration 9641, loss = 0.05903252\n",
      "Iteration 9642, loss = 0.05902495\n",
      "Iteration 9643, loss = 0.05901737\n",
      "Iteration 9644, loss = 0.05900975\n",
      "Iteration 9645, loss = 0.05900214\n",
      "Iteration 9646, loss = 0.05899462\n",
      "Iteration 9647, loss = 0.05898693\n",
      "Iteration 9648, loss = 0.05897940\n",
      "Iteration 9649, loss = 0.05897179\n",
      "Iteration 9650, loss = 0.05896416\n",
      "Iteration 9651, loss = 0.05895663\n",
      "Iteration 9652, loss = 0.05894900\n",
      "Iteration 9653, loss = 0.05894142\n",
      "Iteration 9654, loss = 0.05893375\n",
      "Iteration 9655, loss = 0.05892610\n",
      "Iteration 9656, loss = 0.05891865\n",
      "Iteration 9657, loss = 0.05891101\n",
      "Iteration 9658, loss = 0.05890346\n",
      "Iteration 9659, loss = 0.05889597\n",
      "Iteration 9660, loss = 0.05888838\n",
      "Iteration 9661, loss = 0.05888070\n",
      "Iteration 9662, loss = 0.05887314\n",
      "Iteration 9663, loss = 0.05886570\n",
      "Iteration 9664, loss = 0.05885807\n",
      "Iteration 9665, loss = 0.05885041\n",
      "Iteration 9666, loss = 0.05884296\n",
      "Iteration 9667, loss = 0.05883533\n",
      "Iteration 9668, loss = 0.05882786\n",
      "Iteration 9669, loss = 0.05882013\n",
      "Iteration 9670, loss = 0.05881275\n",
      "Iteration 9671, loss = 0.05880512\n",
      "Iteration 9672, loss = 0.05879752\n",
      "Iteration 9673, loss = 0.05878993\n",
      "Iteration 9674, loss = 0.05878246\n",
      "Iteration 9675, loss = 0.05877492\n",
      "Iteration 9676, loss = 0.05876727\n",
      "Iteration 9677, loss = 0.05875980\n",
      "Iteration 9678, loss = 0.05875224\n",
      "Iteration 9679, loss = 0.05874472\n",
      "Iteration 9680, loss = 0.05873719\n",
      "Iteration 9681, loss = 0.05872966\n",
      "Iteration 9682, loss = 0.05872214\n",
      "Iteration 9683, loss = 0.05871460\n",
      "Iteration 9684, loss = 0.05870707\n",
      "Iteration 9685, loss = 0.05869947\n",
      "Iteration 9686, loss = 0.05869201\n",
      "Iteration 9687, loss = 0.05868444\n",
      "Iteration 9688, loss = 0.05867687\n",
      "Iteration 9689, loss = 0.05866942\n",
      "Iteration 9690, loss = 0.05866186\n",
      "Iteration 9691, loss = 0.05865424\n",
      "Iteration 9692, loss = 0.05864680\n",
      "Iteration 9693, loss = 0.05863923\n",
      "Iteration 9694, loss = 0.05863173\n",
      "Iteration 9695, loss = 0.05862415\n",
      "Iteration 9696, loss = 0.05861664\n",
      "Iteration 9697, loss = 0.05860913\n",
      "Iteration 9698, loss = 0.05860162\n",
      "Iteration 9699, loss = 0.05859415\n",
      "Iteration 9700, loss = 0.05858659\n",
      "Iteration 9701, loss = 0.05857920\n",
      "Iteration 9702, loss = 0.05857166\n",
      "Iteration 9703, loss = 0.05856412\n",
      "Iteration 9704, loss = 0.05855662\n",
      "Iteration 9705, loss = 0.05854911\n",
      "Iteration 9706, loss = 0.05854164\n",
      "Iteration 9707, loss = 0.05853412\n",
      "Iteration 9708, loss = 0.05852670\n",
      "Iteration 9709, loss = 0.05851918\n",
      "Iteration 9710, loss = 0.05851166\n",
      "Iteration 9711, loss = 0.05850419\n",
      "Iteration 9712, loss = 0.05849672\n",
      "Iteration 9713, loss = 0.05848931\n",
      "Iteration 9714, loss = 0.05848179\n",
      "Iteration 9715, loss = 0.05847425\n",
      "Iteration 9716, loss = 0.05846691\n",
      "Iteration 9717, loss = 0.05845931\n",
      "Iteration 9718, loss = 0.05845194\n",
      "Iteration 9719, loss = 0.05844441\n",
      "Iteration 9720, loss = 0.05843695\n",
      "Iteration 9721, loss = 0.05842955\n",
      "Iteration 9722, loss = 0.05842200\n",
      "Iteration 9723, loss = 0.05841463\n",
      "Iteration 9724, loss = 0.05840702\n",
      "Iteration 9725, loss = 0.05839964\n",
      "Iteration 9726, loss = 0.05839222\n",
      "Iteration 9727, loss = 0.05838483\n",
      "Iteration 9728, loss = 0.05837735\n",
      "Iteration 9729, loss = 0.05836985\n",
      "Iteration 9730, loss = 0.05836242\n",
      "Iteration 9731, loss = 0.05835495\n",
      "Iteration 9732, loss = 0.05834758\n",
      "Iteration 9733, loss = 0.05834021\n",
      "Iteration 9734, loss = 0.05833270\n",
      "Iteration 9735, loss = 0.05832545\n",
      "Iteration 9736, loss = 0.05831800\n",
      "Iteration 9737, loss = 0.05831049\n",
      "Iteration 9738, loss = 0.05830310\n",
      "Iteration 9739, loss = 0.05829577\n",
      "Iteration 9740, loss = 0.05828840\n",
      "Iteration 9741, loss = 0.05828093\n",
      "Iteration 9742, loss = 0.05827352\n",
      "Iteration 9743, loss = 0.05826618\n",
      "Iteration 9744, loss = 0.05825882\n",
      "Iteration 9745, loss = 0.05825145\n",
      "Iteration 9746, loss = 0.05824401\n",
      "Iteration 9747, loss = 0.05823668\n",
      "Iteration 9748, loss = 0.05822928\n",
      "Iteration 9749, loss = 0.05822178\n",
      "Iteration 9750, loss = 0.05821443\n",
      "Iteration 9751, loss = 0.05820704\n",
      "Iteration 9752, loss = 0.05819968\n",
      "Iteration 9753, loss = 0.05819225\n",
      "Iteration 9754, loss = 0.05818487\n",
      "Iteration 9755, loss = 0.05817755\n",
      "Iteration 9756, loss = 0.05817002\n",
      "Iteration 9757, loss = 0.05816275\n",
      "Iteration 9758, loss = 0.05815537\n",
      "Iteration 9759, loss = 0.05814791\n",
      "Iteration 9760, loss = 0.05814050\n",
      "Iteration 9761, loss = 0.05813311\n",
      "Iteration 9762, loss = 0.05812576\n",
      "Iteration 9763, loss = 0.05811847\n",
      "Iteration 9764, loss = 0.05811112\n",
      "Iteration 9765, loss = 0.05810367\n",
      "Iteration 9766, loss = 0.05809627\n",
      "Iteration 9767, loss = 0.05808895\n",
      "Iteration 9768, loss = 0.05808155\n",
      "Iteration 9769, loss = 0.05807416\n",
      "Iteration 9770, loss = 0.05806683\n",
      "Iteration 9771, loss = 0.05805938\n",
      "Iteration 9772, loss = 0.05805197\n",
      "Iteration 9773, loss = 0.05804466\n",
      "Iteration 9774, loss = 0.05803722\n",
      "Iteration 9775, loss = 0.05802994\n",
      "Iteration 9776, loss = 0.05802269\n",
      "Iteration 9777, loss = 0.05801518\n",
      "Iteration 9778, loss = 0.05800788\n",
      "Iteration 9779, loss = 0.05800047\n",
      "Iteration 9780, loss = 0.05799316\n",
      "Iteration 9781, loss = 0.05798583\n",
      "Iteration 9782, loss = 0.05797851\n",
      "Iteration 9783, loss = 0.05797124\n",
      "Iteration 9784, loss = 0.05796378\n",
      "Iteration 9785, loss = 0.05795642\n",
      "Iteration 9786, loss = 0.05794911\n",
      "Iteration 9787, loss = 0.05794186\n",
      "Iteration 9788, loss = 0.05793443\n",
      "Iteration 9789, loss = 0.05792709\n",
      "Iteration 9790, loss = 0.05791971\n",
      "Iteration 9791, loss = 0.05791247\n",
      "Iteration 9792, loss = 0.05790505\n",
      "Iteration 9793, loss = 0.05789760\n",
      "Iteration 9794, loss = 0.05789036\n",
      "Iteration 9795, loss = 0.05788293\n",
      "Iteration 9796, loss = 0.05787567\n",
      "Iteration 9797, loss = 0.05786831\n",
      "Iteration 9798, loss = 0.05786095\n",
      "Iteration 9799, loss = 0.05785365\n",
      "Iteration 9800, loss = 0.05784628\n",
      "Iteration 9801, loss = 0.05783901\n",
      "Iteration 9802, loss = 0.05783162\n",
      "Iteration 9803, loss = 0.05782435\n",
      "Iteration 9804, loss = 0.05781699\n",
      "Iteration 9805, loss = 0.05780966\n",
      "Iteration 9806, loss = 0.05780230\n",
      "Iteration 9807, loss = 0.05779505\n",
      "Iteration 9808, loss = 0.05778777\n",
      "Iteration 9809, loss = 0.05778047\n",
      "Iteration 9810, loss = 0.05777314\n",
      "Iteration 9811, loss = 0.05776589\n",
      "Iteration 9812, loss = 0.05775855\n",
      "Iteration 9813, loss = 0.05775124\n",
      "Iteration 9814, loss = 0.05774409\n",
      "Iteration 9815, loss = 0.05773669\n",
      "Iteration 9816, loss = 0.05772950\n",
      "Iteration 9817, loss = 0.05772202\n",
      "Iteration 9818, loss = 0.05771477\n",
      "Iteration 9819, loss = 0.05770750\n",
      "Iteration 9820, loss = 0.05770027\n",
      "Iteration 9821, loss = 0.05769293\n",
      "Iteration 9822, loss = 0.05768565\n",
      "Iteration 9823, loss = 0.05767833\n",
      "Iteration 9824, loss = 0.05767117\n",
      "Iteration 9825, loss = 0.05766382\n",
      "Iteration 9826, loss = 0.05765659\n",
      "Iteration 9827, loss = 0.05764932\n",
      "Iteration 9828, loss = 0.05764198\n",
      "Iteration 9829, loss = 0.05763468\n",
      "Iteration 9830, loss = 0.05762748\n",
      "Iteration 9831, loss = 0.05762013\n",
      "Iteration 9832, loss = 0.05761296\n",
      "Iteration 9833, loss = 0.05760563\n",
      "Iteration 9834, loss = 0.05759841\n",
      "Iteration 9835, loss = 0.05759112\n",
      "Iteration 9836, loss = 0.05758392\n",
      "Iteration 9837, loss = 0.05757676\n",
      "Iteration 9838, loss = 0.05756939\n",
      "Iteration 9839, loss = 0.05756217\n",
      "Iteration 9840, loss = 0.05755490\n",
      "Iteration 9841, loss = 0.05754766\n",
      "Iteration 9842, loss = 0.05754047\n",
      "Iteration 9843, loss = 0.05753343\n",
      "Iteration 9844, loss = 0.05752604\n",
      "Iteration 9845, loss = 0.05751877\n",
      "Iteration 9846, loss = 0.05751160\n",
      "Iteration 9847, loss = 0.05750439\n",
      "Iteration 9848, loss = 0.05749719\n",
      "Iteration 9849, loss = 0.05748997\n",
      "Iteration 9850, loss = 0.05748269\n",
      "Iteration 9851, loss = 0.05747560\n",
      "Iteration 9852, loss = 0.05746829\n",
      "Iteration 9853, loss = 0.05746108\n",
      "Iteration 9854, loss = 0.05745386\n",
      "Iteration 9855, loss = 0.05744656\n",
      "Iteration 9856, loss = 0.05743941\n",
      "Iteration 9857, loss = 0.05743213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9858, loss = 0.05742503\n",
      "Iteration 9859, loss = 0.05741774\n",
      "Iteration 9860, loss = 0.05741061\n",
      "Iteration 9861, loss = 0.05740338\n",
      "Iteration 9862, loss = 0.05739616\n",
      "Iteration 9863, loss = 0.05738897\n",
      "Iteration 9864, loss = 0.05738167\n",
      "Iteration 9865, loss = 0.05737452\n",
      "Iteration 9866, loss = 0.05736727\n",
      "Iteration 9867, loss = 0.05736005\n",
      "Iteration 9868, loss = 0.05735302\n",
      "Iteration 9869, loss = 0.05734565\n",
      "Iteration 9870, loss = 0.05733848\n",
      "Iteration 9871, loss = 0.05733126\n",
      "Iteration 9872, loss = 0.05732411\n",
      "Iteration 9873, loss = 0.05731696\n",
      "Iteration 9874, loss = 0.05730973\n",
      "Iteration 9875, loss = 0.05730256\n",
      "Iteration 9876, loss = 0.05729539\n",
      "Iteration 9877, loss = 0.05728814\n",
      "Iteration 9878, loss = 0.05728102\n",
      "Iteration 9879, loss = 0.05727387\n",
      "Iteration 9880, loss = 0.05726664\n",
      "Iteration 9881, loss = 0.05725959\n",
      "Iteration 9882, loss = 0.05725226\n",
      "Iteration 9883, loss = 0.05724523\n",
      "Iteration 9884, loss = 0.05723802\n",
      "Iteration 9885, loss = 0.05723088\n",
      "Iteration 9886, loss = 0.05722375\n",
      "Iteration 9887, loss = 0.05721647\n",
      "Iteration 9888, loss = 0.05720936\n",
      "Iteration 9889, loss = 0.05720219\n",
      "Iteration 9890, loss = 0.05719501\n",
      "Iteration 9891, loss = 0.05718784\n",
      "Iteration 9892, loss = 0.05718058\n",
      "Iteration 9893, loss = 0.05717349\n",
      "Iteration 9894, loss = 0.05716628\n",
      "Iteration 9895, loss = 0.05715906\n",
      "Iteration 9896, loss = 0.05715191\n",
      "Iteration 9897, loss = 0.05714474\n",
      "Iteration 9898, loss = 0.05713760\n",
      "Iteration 9899, loss = 0.05713040\n",
      "Iteration 9900, loss = 0.05712328\n",
      "Iteration 9901, loss = 0.05711612\n",
      "Iteration 9902, loss = 0.05710896\n",
      "Iteration 9903, loss = 0.05710177\n",
      "Iteration 9904, loss = 0.05709480\n",
      "Iteration 9905, loss = 0.05708754\n",
      "Iteration 9906, loss = 0.05708041\n",
      "Iteration 9907, loss = 0.05707336\n",
      "Iteration 9908, loss = 0.05706614\n",
      "Iteration 9909, loss = 0.05705905\n",
      "Iteration 9910, loss = 0.05705184\n",
      "Iteration 9911, loss = 0.05704473\n",
      "Iteration 9912, loss = 0.05703763\n",
      "Iteration 9913, loss = 0.05703055\n",
      "Iteration 9914, loss = 0.05702335\n",
      "Iteration 9915, loss = 0.05701635\n",
      "Iteration 9916, loss = 0.05700914\n",
      "Iteration 9917, loss = 0.05700201\n",
      "Iteration 9918, loss = 0.05699491\n",
      "Iteration 9919, loss = 0.05698786\n",
      "Iteration 9920, loss = 0.05698071\n",
      "Iteration 9921, loss = 0.05697360\n",
      "Iteration 9922, loss = 0.05696661\n",
      "Iteration 9923, loss = 0.05695946\n",
      "Iteration 9924, loss = 0.05695235\n",
      "Iteration 9925, loss = 0.05694527\n",
      "Iteration 9926, loss = 0.05693817\n",
      "Iteration 9927, loss = 0.05693111\n",
      "Iteration 9928, loss = 0.05692413\n",
      "Iteration 9929, loss = 0.05691703\n",
      "Iteration 9930, loss = 0.05690988\n",
      "Iteration 9931, loss = 0.05690287\n",
      "Iteration 9932, loss = 0.05689576\n",
      "Iteration 9933, loss = 0.05688871\n",
      "Iteration 9934, loss = 0.05688162\n",
      "Iteration 9935, loss = 0.05687451\n",
      "Iteration 9936, loss = 0.05686750\n",
      "Iteration 9937, loss = 0.05686036\n",
      "Iteration 9938, loss = 0.05685337\n",
      "Iteration 9939, loss = 0.05684634\n",
      "Iteration 9940, loss = 0.05683925\n",
      "Iteration 9941, loss = 0.05683210\n",
      "Iteration 9942, loss = 0.05682497\n",
      "Iteration 9943, loss = 0.05681799\n",
      "Iteration 9944, loss = 0.05681088\n",
      "Iteration 9945, loss = 0.05680380\n",
      "Iteration 9946, loss = 0.05679680\n",
      "Iteration 9947, loss = 0.05678968\n",
      "Iteration 9948, loss = 0.05678258\n",
      "Iteration 9949, loss = 0.05677566\n",
      "Iteration 9950, loss = 0.05676843\n",
      "Iteration 9951, loss = 0.05676146\n",
      "Iteration 9952, loss = 0.05675442\n",
      "Iteration 9953, loss = 0.05674731\n",
      "Iteration 9954, loss = 0.05674024\n",
      "Iteration 9955, loss = 0.05673319\n",
      "Iteration 9956, loss = 0.05672611\n",
      "Iteration 9957, loss = 0.05671910\n",
      "Iteration 9958, loss = 0.05671206\n",
      "Iteration 9959, loss = 0.05670495\n",
      "Iteration 9960, loss = 0.05669796\n",
      "Iteration 9961, loss = 0.05669100\n",
      "Iteration 9962, loss = 0.05668390\n",
      "Iteration 9963, loss = 0.05667677\n",
      "Iteration 9964, loss = 0.05666989\n",
      "Iteration 9965, loss = 0.05666275\n",
      "Iteration 9966, loss = 0.05665577\n",
      "Iteration 9967, loss = 0.05664884\n",
      "Iteration 9968, loss = 0.05664177\n",
      "Iteration 9969, loss = 0.05663485\n",
      "Iteration 9970, loss = 0.05662769\n",
      "Iteration 9971, loss = 0.05662064\n",
      "Iteration 9972, loss = 0.05661360\n",
      "Iteration 9973, loss = 0.05660655\n",
      "Iteration 9974, loss = 0.05659952\n",
      "Iteration 9975, loss = 0.05659258\n",
      "Iteration 9976, loss = 0.05658549\n",
      "Iteration 9977, loss = 0.05657855\n",
      "Iteration 9978, loss = 0.05657143\n",
      "Iteration 9979, loss = 0.05656440\n",
      "Iteration 9980, loss = 0.05655745\n",
      "Iteration 9981, loss = 0.05655042\n",
      "Iteration 9982, loss = 0.05654334\n",
      "Iteration 9983, loss = 0.05653647\n",
      "Iteration 9984, loss = 0.05652940\n",
      "Iteration 9985, loss = 0.05652240\n",
      "Iteration 9986, loss = 0.05651542\n",
      "Iteration 9987, loss = 0.05650846\n",
      "Iteration 9988, loss = 0.05650142\n",
      "Iteration 9989, loss = 0.05649452\n",
      "Iteration 9990, loss = 0.05648757\n",
      "Iteration 9991, loss = 0.05648052\n",
      "Iteration 9992, loss = 0.05647349\n",
      "Iteration 9993, loss = 0.05646652\n",
      "Iteration 9994, loss = 0.05645956\n",
      "Iteration 9995, loss = 0.05645259\n",
      "Iteration 9996, loss = 0.05644555\n",
      "Iteration 9997, loss = 0.05643865\n",
      "Iteration 9998, loss = 0.05643165\n",
      "Iteration 9999, loss = 0.05642458\n",
      "Iteration 10000, loss = 0.05641771\n",
      "Iteration 10001, loss = 0.05641068\n",
      "Iteration 10002, loss = 0.05640365\n",
      "Iteration 10003, loss = 0.05639683\n",
      "Iteration 10004, loss = 0.05638975\n",
      "Iteration 10005, loss = 0.05638277\n",
      "Iteration 10006, loss = 0.05637581\n",
      "Iteration 10007, loss = 0.05636892\n",
      "Iteration 10008, loss = 0.05636191\n",
      "Iteration 10009, loss = 0.05635499\n",
      "Iteration 10010, loss = 0.05634807\n",
      "Iteration 10011, loss = 0.05634104\n",
      "Iteration 10012, loss = 0.05633415\n",
      "Iteration 10013, loss = 0.05632723\n",
      "Iteration 10014, loss = 0.05632029\n",
      "Iteration 10015, loss = 0.05631334\n",
      "Iteration 10016, loss = 0.05630649\n",
      "Iteration 10017, loss = 0.05629947\n",
      "Iteration 10018, loss = 0.05629257\n",
      "Iteration 10019, loss = 0.05628558\n",
      "Iteration 10020, loss = 0.05627868\n",
      "Iteration 10021, loss = 0.05627168\n",
      "Iteration 10022, loss = 0.05626472\n",
      "Iteration 10023, loss = 0.05625779\n",
      "Iteration 10024, loss = 0.05625084\n",
      "Iteration 10025, loss = 0.05624389\n",
      "Iteration 10026, loss = 0.05623694\n",
      "Iteration 10027, loss = 0.05623004\n",
      "Iteration 10028, loss = 0.05622310\n",
      "Iteration 10029, loss = 0.05621614\n",
      "Iteration 10030, loss = 0.05620920\n",
      "Iteration 10031, loss = 0.05620238\n",
      "Iteration 10032, loss = 0.05619534\n",
      "Iteration 10033, loss = 0.05618847\n",
      "Iteration 10034, loss = 0.05618161\n",
      "Iteration 10035, loss = 0.05617461\n",
      "Iteration 10036, loss = 0.05616782\n",
      "Iteration 10037, loss = 0.05616079\n",
      "Iteration 10038, loss = 0.05615388\n",
      "Iteration 10039, loss = 0.05614698\n",
      "Iteration 10040, loss = 0.05614009\n",
      "Iteration 10041, loss = 0.05613322\n",
      "Iteration 10042, loss = 0.05612626\n",
      "Iteration 10043, loss = 0.05611949\n",
      "Iteration 10044, loss = 0.05611245\n",
      "Iteration 10045, loss = 0.05610556\n",
      "Iteration 10046, loss = 0.05609863\n",
      "Iteration 10047, loss = 0.05609180\n",
      "Iteration 10048, loss = 0.05608489\n",
      "Iteration 10049, loss = 0.05607800\n",
      "Iteration 10050, loss = 0.05607113\n",
      "Iteration 10051, loss = 0.05606424\n",
      "Iteration 10052, loss = 0.05605735\n",
      "Iteration 10053, loss = 0.05605040\n",
      "Iteration 10054, loss = 0.05604352\n",
      "Iteration 10055, loss = 0.05603662\n",
      "Iteration 10056, loss = 0.05602973\n",
      "Iteration 10057, loss = 0.05602299\n",
      "Iteration 10058, loss = 0.05601599\n",
      "Iteration 10059, loss = 0.05600911\n",
      "Iteration 10060, loss = 0.05600225\n",
      "Iteration 10061, loss = 0.05599533\n",
      "Iteration 10062, loss = 0.05598842\n",
      "Iteration 10063, loss = 0.05598160\n",
      "Iteration 10064, loss = 0.05597480\n",
      "Iteration 10065, loss = 0.05596790\n",
      "Iteration 10066, loss = 0.05596096\n",
      "Iteration 10067, loss = 0.05595420\n",
      "Iteration 10068, loss = 0.05594729\n",
      "Iteration 10069, loss = 0.05594040\n",
      "Iteration 10070, loss = 0.05593356\n",
      "Iteration 10071, loss = 0.05592670\n",
      "Iteration 10072, loss = 0.05591982\n",
      "Iteration 10073, loss = 0.05591305\n",
      "Iteration 10074, loss = 0.05590611\n",
      "Iteration 10075, loss = 0.05589935\n",
      "Iteration 10076, loss = 0.05589235\n",
      "Iteration 10077, loss = 0.05588552\n",
      "Iteration 10078, loss = 0.05587875\n",
      "Iteration 10079, loss = 0.05587184\n",
      "Iteration 10080, loss = 0.05586504\n",
      "Iteration 10081, loss = 0.05585823\n",
      "Iteration 10082, loss = 0.05585124\n",
      "Iteration 10083, loss = 0.05584454\n",
      "Iteration 10084, loss = 0.05583750\n",
      "Iteration 10085, loss = 0.05583069\n",
      "Iteration 10086, loss = 0.05582389\n",
      "Iteration 10087, loss = 0.05581706\n",
      "Iteration 10088, loss = 0.05581023\n",
      "Iteration 10089, loss = 0.05580356\n",
      "Iteration 10090, loss = 0.05579663\n",
      "Iteration 10091, loss = 0.05578982\n",
      "Iteration 10092, loss = 0.05578309\n",
      "Iteration 10093, loss = 0.05577618\n",
      "Iteration 10094, loss = 0.05576943\n",
      "Iteration 10095, loss = 0.05576257\n",
      "Iteration 10096, loss = 0.05575590\n",
      "Iteration 10097, loss = 0.05574906\n",
      "Iteration 10098, loss = 0.05574219\n",
      "Iteration 10099, loss = 0.05573548\n",
      "Iteration 10100, loss = 0.05572861\n",
      "Iteration 10101, loss = 0.05572179\n",
      "Iteration 10102, loss = 0.05571509\n",
      "Iteration 10103, loss = 0.05570830\n",
      "Iteration 10104, loss = 0.05570149\n",
      "Iteration 10105, loss = 0.05569464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10106, loss = 0.05568792\n",
      "Iteration 10107, loss = 0.05568105\n",
      "Iteration 10108, loss = 0.05567432\n",
      "Iteration 10109, loss = 0.05566743\n",
      "Iteration 10110, loss = 0.05566071\n",
      "Iteration 10111, loss = 0.05565391\n",
      "Iteration 10112, loss = 0.05564715\n",
      "Iteration 10113, loss = 0.05564034\n",
      "Iteration 10114, loss = 0.05563348\n",
      "Iteration 10115, loss = 0.05562676\n",
      "Iteration 10116, loss = 0.05561996\n",
      "Iteration 10117, loss = 0.05561318\n",
      "Iteration 10118, loss = 0.05560637\n",
      "Iteration 10119, loss = 0.05559970\n",
      "Iteration 10120, loss = 0.05559294\n",
      "Iteration 10121, loss = 0.05558618\n",
      "Iteration 10122, loss = 0.05557941\n",
      "Iteration 10123, loss = 0.05557272\n",
      "Iteration 10124, loss = 0.05556593\n",
      "Iteration 10125, loss = 0.05555916\n",
      "Iteration 10126, loss = 0.05555242\n",
      "Iteration 10127, loss = 0.05554571\n",
      "Iteration 10128, loss = 0.05553901\n",
      "Iteration 10129, loss = 0.05553213\n",
      "Iteration 10130, loss = 0.05552545\n",
      "Iteration 10131, loss = 0.05551859\n",
      "Iteration 10132, loss = 0.05551189\n",
      "Iteration 10133, loss = 0.05550512\n",
      "Iteration 10134, loss = 0.05549839\n",
      "Iteration 10135, loss = 0.05549163\n",
      "Iteration 10136, loss = 0.05548490\n",
      "Iteration 10137, loss = 0.05547816\n",
      "Iteration 10138, loss = 0.05547142\n",
      "Iteration 10139, loss = 0.05546459\n",
      "Iteration 10140, loss = 0.05545797\n",
      "Iteration 10141, loss = 0.05545123\n",
      "Iteration 10142, loss = 0.05544440\n",
      "Iteration 10143, loss = 0.05543773\n",
      "Iteration 10144, loss = 0.05543100\n",
      "Iteration 10145, loss = 0.05542433\n",
      "Iteration 10146, loss = 0.05541753\n",
      "Iteration 10147, loss = 0.05541081\n",
      "Iteration 10148, loss = 0.05540403\n",
      "Iteration 10149, loss = 0.05539737\n",
      "Iteration 10150, loss = 0.05539063\n",
      "Iteration 10151, loss = 0.05538386\n",
      "Iteration 10152, loss = 0.05537722\n",
      "Iteration 10153, loss = 0.05537039\n",
      "Iteration 10154, loss = 0.05536368\n",
      "Iteration 10155, loss = 0.05535699\n",
      "Iteration 10156, loss = 0.05535020\n",
      "Iteration 10157, loss = 0.05534369\n",
      "Iteration 10158, loss = 0.05533674\n",
      "Iteration 10159, loss = 0.05533014\n",
      "Iteration 10160, loss = 0.05532338\n",
      "Iteration 10161, loss = 0.05531668\n",
      "Iteration 10162, loss = 0.05530998\n",
      "Iteration 10163, loss = 0.05530318\n",
      "Iteration 10164, loss = 0.05529660\n",
      "Iteration 10165, loss = 0.05528991\n",
      "Iteration 10166, loss = 0.05528314\n",
      "Iteration 10167, loss = 0.05527638\n",
      "Iteration 10168, loss = 0.05526975\n",
      "Iteration 10169, loss = 0.05526309\n",
      "Iteration 10170, loss = 0.05525632\n",
      "Iteration 10171, loss = 0.05524962\n",
      "Iteration 10172, loss = 0.05524296\n",
      "Iteration 10173, loss = 0.05523635\n",
      "Iteration 10174, loss = 0.05522964\n",
      "Iteration 10175, loss = 0.05522286\n",
      "Iteration 10176, loss = 0.05521625\n",
      "Iteration 10177, loss = 0.05520958\n",
      "Iteration 10178, loss = 0.05520289\n",
      "Iteration 10179, loss = 0.05519620\n",
      "Iteration 10180, loss = 0.05518953\n",
      "Iteration 10181, loss = 0.05518284\n",
      "Iteration 10182, loss = 0.05517619\n",
      "Iteration 10183, loss = 0.05516962\n",
      "Iteration 10184, loss = 0.05516290\n",
      "Iteration 10185, loss = 0.05515628\n",
      "Iteration 10186, loss = 0.05514961\n",
      "Iteration 10187, loss = 0.05514286\n",
      "Iteration 10188, loss = 0.05513623\n",
      "Iteration 10189, loss = 0.05512962\n",
      "Iteration 10190, loss = 0.05512293\n",
      "Iteration 10191, loss = 0.05511626\n",
      "Iteration 10192, loss = 0.05510957\n",
      "Iteration 10193, loss = 0.05510293\n",
      "Iteration 10194, loss = 0.05509630\n",
      "Iteration 10195, loss = 0.05508963\n",
      "Iteration 10196, loss = 0.05508302\n",
      "Iteration 10197, loss = 0.05507631\n",
      "Iteration 10198, loss = 0.05506966\n",
      "Iteration 10199, loss = 0.05506301\n",
      "Iteration 10200, loss = 0.05505633\n",
      "Iteration 10201, loss = 0.05504968\n",
      "Iteration 10202, loss = 0.05504306\n",
      "Iteration 10203, loss = 0.05503633\n",
      "Iteration 10204, loss = 0.05502967\n",
      "Iteration 10205, loss = 0.05502304\n",
      "Iteration 10206, loss = 0.05501638\n",
      "Iteration 10207, loss = 0.05500979\n",
      "Iteration 10208, loss = 0.05500304\n",
      "Iteration 10209, loss = 0.05499645\n",
      "Iteration 10210, loss = 0.05498977\n",
      "Iteration 10211, loss = 0.05498322\n",
      "Iteration 10212, loss = 0.05497656\n",
      "Iteration 10213, loss = 0.05496985\n",
      "Iteration 10214, loss = 0.05496330\n",
      "Iteration 10215, loss = 0.05495673\n",
      "Iteration 10216, loss = 0.05495004\n",
      "Iteration 10217, loss = 0.05494339\n",
      "Iteration 10218, loss = 0.05493679\n",
      "Iteration 10219, loss = 0.05493016\n",
      "Iteration 10220, loss = 0.05492364\n",
      "Iteration 10221, loss = 0.05491691\n",
      "Iteration 10222, loss = 0.05491033\n",
      "Iteration 10223, loss = 0.05490376\n",
      "Iteration 10224, loss = 0.05489716\n",
      "Iteration 10225, loss = 0.05489049\n",
      "Iteration 10226, loss = 0.05488387\n",
      "Iteration 10227, loss = 0.05487723\n",
      "Iteration 10228, loss = 0.05487071\n",
      "Iteration 10229, loss = 0.05486409\n",
      "Iteration 10230, loss = 0.05485747\n",
      "Iteration 10231, loss = 0.05485076\n",
      "Iteration 10232, loss = 0.05484417\n",
      "Iteration 10233, loss = 0.05483762\n",
      "Iteration 10234, loss = 0.05483099\n",
      "Iteration 10235, loss = 0.05482439\n",
      "Iteration 10236, loss = 0.05481781\n",
      "Iteration 10237, loss = 0.05481120\n",
      "Iteration 10238, loss = 0.05480466\n",
      "Iteration 10239, loss = 0.05479795\n",
      "Iteration 10240, loss = 0.05479149\n",
      "Iteration 10241, loss = 0.05478490\n",
      "Iteration 10242, loss = 0.05477832\n",
      "Iteration 10243, loss = 0.05477166\n",
      "Iteration 10244, loss = 0.05476508\n",
      "Iteration 10245, loss = 0.05475845\n",
      "Iteration 10246, loss = 0.05475204\n",
      "Iteration 10247, loss = 0.05474546\n",
      "Iteration 10248, loss = 0.05473889\n",
      "Iteration 10249, loss = 0.05473225\n",
      "Iteration 10250, loss = 0.05472563\n",
      "Iteration 10251, loss = 0.05471916\n",
      "Iteration 10252, loss = 0.05471254\n",
      "Iteration 10253, loss = 0.05470603\n",
      "Iteration 10254, loss = 0.05469946\n",
      "Iteration 10255, loss = 0.05469291\n",
      "Iteration 10256, loss = 0.05468632\n",
      "Iteration 10257, loss = 0.05467978\n",
      "Iteration 10258, loss = 0.05467318\n",
      "Iteration 10259, loss = 0.05466668\n",
      "Iteration 10260, loss = 0.05466010\n",
      "Iteration 10261, loss = 0.05465357\n",
      "Iteration 10262, loss = 0.05464697\n",
      "Iteration 10263, loss = 0.05464036\n",
      "Iteration 10264, loss = 0.05463387\n",
      "Iteration 10265, loss = 0.05462733\n",
      "Iteration 10266, loss = 0.05462066\n",
      "Iteration 10267, loss = 0.05461418\n",
      "Iteration 10268, loss = 0.05460772\n",
      "Iteration 10269, loss = 0.05460105\n",
      "Iteration 10270, loss = 0.05459449\n",
      "Iteration 10271, loss = 0.05458802\n",
      "Iteration 10272, loss = 0.05458146\n",
      "Iteration 10273, loss = 0.05457491\n",
      "Iteration 10274, loss = 0.05456835\n",
      "Iteration 10275, loss = 0.05456175\n",
      "Iteration 10276, loss = 0.05455527\n",
      "Iteration 10277, loss = 0.05454882\n",
      "Iteration 10278, loss = 0.05454219\n",
      "Iteration 10279, loss = 0.05453571\n",
      "Iteration 10280, loss = 0.05452919\n",
      "Iteration 10281, loss = 0.05452274\n",
      "Iteration 10282, loss = 0.05451612\n",
      "Iteration 10283, loss = 0.05450964\n",
      "Iteration 10284, loss = 0.05450321\n",
      "Iteration 10285, loss = 0.05449665\n",
      "Iteration 10286, loss = 0.05449010\n",
      "Iteration 10287, loss = 0.05448365\n",
      "Iteration 10288, loss = 0.05447717\n",
      "Iteration 10289, loss = 0.05447065\n",
      "Iteration 10290, loss = 0.05446409\n",
      "Iteration 10291, loss = 0.05445760\n",
      "Iteration 10292, loss = 0.05445120\n",
      "Iteration 10293, loss = 0.05444457\n",
      "Iteration 10294, loss = 0.05443809\n",
      "Iteration 10295, loss = 0.05443157\n",
      "Iteration 10296, loss = 0.05442514\n",
      "Iteration 10297, loss = 0.05441854\n",
      "Iteration 10298, loss = 0.05441201\n",
      "Iteration 10299, loss = 0.05440555\n",
      "Iteration 10300, loss = 0.05439899\n",
      "Iteration 10301, loss = 0.05439258\n",
      "Iteration 10302, loss = 0.05438602\n",
      "Iteration 10303, loss = 0.05437962\n",
      "Iteration 10304, loss = 0.05437307\n",
      "Iteration 10305, loss = 0.05436662\n",
      "Iteration 10306, loss = 0.05436013\n",
      "Iteration 10307, loss = 0.05435367\n",
      "Iteration 10308, loss = 0.05434711\n",
      "Iteration 10309, loss = 0.05434064\n",
      "Iteration 10310, loss = 0.05433416\n",
      "Iteration 10311, loss = 0.05432777\n",
      "Iteration 10312, loss = 0.05432127\n",
      "Iteration 10313, loss = 0.05431484\n",
      "Iteration 10314, loss = 0.05430833\n",
      "Iteration 10315, loss = 0.05430195\n",
      "Iteration 10316, loss = 0.05429537\n",
      "Iteration 10317, loss = 0.05428894\n",
      "Iteration 10318, loss = 0.05428238\n",
      "Iteration 10319, loss = 0.05427599\n",
      "Iteration 10320, loss = 0.05426950\n",
      "Iteration 10321, loss = 0.05426302\n",
      "Iteration 10322, loss = 0.05425654\n",
      "Iteration 10323, loss = 0.05425019\n",
      "Iteration 10324, loss = 0.05424366\n",
      "Iteration 10325, loss = 0.05423722\n",
      "Iteration 10326, loss = 0.05423069\n",
      "Iteration 10327, loss = 0.05422430\n",
      "Iteration 10328, loss = 0.05421790\n",
      "Iteration 10329, loss = 0.05421140\n",
      "Iteration 10330, loss = 0.05420492\n",
      "Iteration 10331, loss = 0.05419848\n",
      "Iteration 10332, loss = 0.05419206\n",
      "Iteration 10333, loss = 0.05418565\n",
      "Iteration 10334, loss = 0.05417912\n",
      "Iteration 10335, loss = 0.05417270\n",
      "Iteration 10336, loss = 0.05416631\n",
      "Iteration 10337, loss = 0.05415979\n",
      "Iteration 10338, loss = 0.05415331\n",
      "Iteration 10339, loss = 0.05414698\n",
      "Iteration 10340, loss = 0.05414046\n",
      "Iteration 10341, loss = 0.05413402\n",
      "Iteration 10342, loss = 0.05412764\n",
      "Iteration 10343, loss = 0.05412114\n",
      "Iteration 10344, loss = 0.05411472\n",
      "Iteration 10345, loss = 0.05410836\n",
      "Iteration 10346, loss = 0.05410188\n",
      "Iteration 10347, loss = 0.05409551\n",
      "Iteration 10348, loss = 0.05408907\n",
      "Iteration 10349, loss = 0.05408265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10350, loss = 0.05407625\n",
      "Iteration 10351, loss = 0.05406986\n",
      "Iteration 10352, loss = 0.05406344\n",
      "Iteration 10353, loss = 0.05405699\n",
      "Iteration 10354, loss = 0.05405060\n",
      "Iteration 10355, loss = 0.05404423\n",
      "Iteration 10356, loss = 0.05403770\n",
      "Iteration 10357, loss = 0.05403135\n",
      "Iteration 10358, loss = 0.05402493\n",
      "Iteration 10359, loss = 0.05401848\n",
      "Iteration 10360, loss = 0.05401210\n",
      "Iteration 10361, loss = 0.05400573\n",
      "Iteration 10362, loss = 0.05399925\n",
      "Iteration 10363, loss = 0.05399287\n",
      "Iteration 10364, loss = 0.05398648\n",
      "Iteration 10365, loss = 0.05398001\n",
      "Iteration 10366, loss = 0.05397364\n",
      "Iteration 10367, loss = 0.05396732\n",
      "Iteration 10368, loss = 0.05396092\n",
      "Iteration 10369, loss = 0.05395441\n",
      "Iteration 10370, loss = 0.05394808\n",
      "Iteration 10371, loss = 0.05394176\n",
      "Iteration 10372, loss = 0.05393538\n",
      "Iteration 10373, loss = 0.05392889\n",
      "Iteration 10374, loss = 0.05392253\n",
      "Iteration 10375, loss = 0.05391607\n",
      "Iteration 10376, loss = 0.05390986\n",
      "Iteration 10377, loss = 0.05390332\n",
      "Iteration 10378, loss = 0.05389700\n",
      "Iteration 10379, loss = 0.05389057\n",
      "Iteration 10380, loss = 0.05388422\n",
      "Iteration 10381, loss = 0.05387787\n",
      "Iteration 10382, loss = 0.05387155\n",
      "Iteration 10383, loss = 0.05386517\n",
      "Iteration 10384, loss = 0.05385877\n",
      "Iteration 10385, loss = 0.05385238\n",
      "Iteration 10386, loss = 0.05384601\n",
      "Iteration 10387, loss = 0.05383968\n",
      "Iteration 10388, loss = 0.05383339\n",
      "Iteration 10389, loss = 0.05382691\n",
      "Iteration 10390, loss = 0.05382063\n",
      "Iteration 10391, loss = 0.05381437\n",
      "Iteration 10392, loss = 0.05380795\n",
      "Iteration 10393, loss = 0.05380158\n",
      "Iteration 10394, loss = 0.05379522\n",
      "Iteration 10395, loss = 0.05378881\n",
      "Iteration 10396, loss = 0.05378254\n",
      "Iteration 10397, loss = 0.05377611\n",
      "Iteration 10398, loss = 0.05376982\n",
      "Iteration 10399, loss = 0.05376346\n",
      "Iteration 10400, loss = 0.05375720\n",
      "Iteration 10401, loss = 0.05375075\n",
      "Iteration 10402, loss = 0.05374444\n",
      "Iteration 10403, loss = 0.05373803\n",
      "Iteration 10404, loss = 0.05373180\n",
      "Iteration 10405, loss = 0.05372551\n",
      "Iteration 10406, loss = 0.05371905\n",
      "Iteration 10407, loss = 0.05371273\n",
      "Iteration 10408, loss = 0.05370643\n",
      "Iteration 10409, loss = 0.05370002\n",
      "Iteration 10410, loss = 0.05369368\n",
      "Iteration 10411, loss = 0.05368745\n",
      "Iteration 10412, loss = 0.05368102\n",
      "Iteration 10413, loss = 0.05367474\n",
      "Iteration 10414, loss = 0.05366835\n",
      "Iteration 10415, loss = 0.05366196\n",
      "Iteration 10416, loss = 0.05365573\n",
      "Iteration 10417, loss = 0.05364946\n",
      "Iteration 10418, loss = 0.05364307\n",
      "Iteration 10419, loss = 0.05363677\n",
      "Iteration 10420, loss = 0.05363047\n",
      "Iteration 10421, loss = 0.05362410\n",
      "Iteration 10422, loss = 0.05361783\n",
      "Iteration 10423, loss = 0.05361152\n",
      "Iteration 10424, loss = 0.05360517\n",
      "Iteration 10425, loss = 0.05359874\n",
      "Iteration 10426, loss = 0.05359259\n",
      "Iteration 10427, loss = 0.05358626\n",
      "Iteration 10428, loss = 0.05357988\n",
      "Iteration 10429, loss = 0.05357360\n",
      "Iteration 10430, loss = 0.05356733\n",
      "Iteration 10431, loss = 0.05356098\n",
      "Iteration 10432, loss = 0.05355474\n",
      "Iteration 10433, loss = 0.05354839\n",
      "Iteration 10434, loss = 0.05354214\n",
      "Iteration 10435, loss = 0.05353581\n",
      "Iteration 10436, loss = 0.05352951\n",
      "Iteration 10437, loss = 0.05352323\n",
      "Iteration 10438, loss = 0.05351701\n",
      "Iteration 10439, loss = 0.05351071\n",
      "Iteration 10440, loss = 0.05350441\n",
      "Iteration 10441, loss = 0.05349814\n",
      "Iteration 10442, loss = 0.05349181\n",
      "Iteration 10443, loss = 0.05348559\n",
      "Iteration 10444, loss = 0.05347935\n",
      "Iteration 10445, loss = 0.05347303\n",
      "Iteration 10446, loss = 0.05346683\n",
      "Iteration 10447, loss = 0.05346050\n",
      "Iteration 10448, loss = 0.05345431\n",
      "Iteration 10449, loss = 0.05344805\n",
      "Iteration 10450, loss = 0.05344173\n",
      "Iteration 10451, loss = 0.05343546\n",
      "Iteration 10452, loss = 0.05342917\n",
      "Iteration 10453, loss = 0.05342301\n",
      "Iteration 10454, loss = 0.05341673\n",
      "Iteration 10455, loss = 0.05341042\n",
      "Iteration 10456, loss = 0.05340422\n",
      "Iteration 10457, loss = 0.05339798\n",
      "Iteration 10458, loss = 0.05339170\n",
      "Iteration 10459, loss = 0.05338549\n",
      "Iteration 10460, loss = 0.05337928\n",
      "Iteration 10461, loss = 0.05337306\n",
      "Iteration 10462, loss = 0.05336675\n",
      "Iteration 10463, loss = 0.05336055\n",
      "Iteration 10464, loss = 0.05335425\n",
      "Iteration 10465, loss = 0.05334806\n",
      "Iteration 10466, loss = 0.05334181\n",
      "Iteration 10467, loss = 0.05333558\n",
      "Iteration 10468, loss = 0.05332930\n",
      "Iteration 10469, loss = 0.05332306\n",
      "Iteration 10470, loss = 0.05331687\n",
      "Iteration 10471, loss = 0.05331060\n",
      "Iteration 10472, loss = 0.05330444\n",
      "Iteration 10473, loss = 0.05329815\n",
      "Iteration 10474, loss = 0.05329201\n",
      "Iteration 10475, loss = 0.05328570\n",
      "Iteration 10476, loss = 0.05327950\n",
      "Iteration 10477, loss = 0.05327327\n",
      "Iteration 10478, loss = 0.05326695\n",
      "Iteration 10479, loss = 0.05326078\n",
      "Iteration 10480, loss = 0.05325455\n",
      "Iteration 10481, loss = 0.05324841\n",
      "Iteration 10482, loss = 0.05324208\n",
      "Iteration 10483, loss = 0.05323587\n",
      "Iteration 10484, loss = 0.05322967\n",
      "Iteration 10485, loss = 0.05322350\n",
      "Iteration 10486, loss = 0.05321733\n",
      "Iteration 10487, loss = 0.05321108\n",
      "Iteration 10488, loss = 0.05320489\n",
      "Iteration 10489, loss = 0.05319865\n",
      "Iteration 10490, loss = 0.05319248\n",
      "Iteration 10491, loss = 0.05318630\n",
      "Iteration 10492, loss = 0.05318009\n",
      "Iteration 10493, loss = 0.05317388\n",
      "Iteration 10494, loss = 0.05316764\n",
      "Iteration 10495, loss = 0.05316148\n",
      "Iteration 10496, loss = 0.05315528\n",
      "Iteration 10497, loss = 0.05314910\n",
      "Iteration 10498, loss = 0.05314280\n",
      "Iteration 10499, loss = 0.05313668\n",
      "Iteration 10500, loss = 0.05313049\n",
      "Iteration 10501, loss = 0.05312417\n",
      "Iteration 10502, loss = 0.05311803\n",
      "Iteration 10503, loss = 0.05311193\n",
      "Iteration 10504, loss = 0.05310561\n",
      "Iteration 10505, loss = 0.05309942\n",
      "Iteration 10506, loss = 0.05309321\n",
      "Iteration 10507, loss = 0.05308710\n",
      "Iteration 10508, loss = 0.05308083\n",
      "Iteration 10509, loss = 0.05307475\n",
      "Iteration 10510, loss = 0.05306850\n",
      "Iteration 10511, loss = 0.05306237\n",
      "Iteration 10512, loss = 0.05305621\n",
      "Iteration 10513, loss = 0.05304997\n",
      "Iteration 10514, loss = 0.05304398\n",
      "Iteration 10515, loss = 0.05303766\n",
      "Iteration 10516, loss = 0.05303149\n",
      "Iteration 10517, loss = 0.05302528\n",
      "Iteration 10518, loss = 0.05301929\n",
      "Iteration 10519, loss = 0.05301302\n",
      "Iteration 10520, loss = 0.05300692\n",
      "Iteration 10521, loss = 0.05300077\n",
      "Iteration 10522, loss = 0.05299460\n",
      "Iteration 10523, loss = 0.05298846\n",
      "Iteration 10524, loss = 0.05298239\n",
      "Iteration 10525, loss = 0.05297618\n",
      "Iteration 10526, loss = 0.05297003\n",
      "Iteration 10527, loss = 0.05296387\n",
      "Iteration 10528, loss = 0.05295772\n",
      "Iteration 10529, loss = 0.05295155\n",
      "Iteration 10530, loss = 0.05294552\n",
      "Iteration 10531, loss = 0.05293925\n",
      "Iteration 10532, loss = 0.05293311\n",
      "Iteration 10533, loss = 0.05292694\n",
      "Iteration 10534, loss = 0.05292075\n",
      "Iteration 10535, loss = 0.05291459\n",
      "Iteration 10536, loss = 0.05290846\n",
      "Iteration 10537, loss = 0.05290224\n",
      "Iteration 10538, loss = 0.05289616\n",
      "Iteration 10539, loss = 0.05289002\n",
      "Iteration 10540, loss = 0.05288391\n",
      "Iteration 10541, loss = 0.05287772\n",
      "Iteration 10542, loss = 0.05287159\n",
      "Iteration 10543, loss = 0.05286546\n",
      "Iteration 10544, loss = 0.05285926\n",
      "Iteration 10545, loss = 0.05285317\n",
      "Iteration 10546, loss = 0.05284706\n",
      "Iteration 10547, loss = 0.05284088\n",
      "Iteration 10548, loss = 0.05283485\n",
      "Iteration 10549, loss = 0.05282878\n",
      "Iteration 10550, loss = 0.05282257\n",
      "Iteration 10551, loss = 0.05281644\n",
      "Iteration 10552, loss = 0.05281030\n",
      "Iteration 10553, loss = 0.05280417\n",
      "Iteration 10554, loss = 0.05279809\n",
      "Iteration 10555, loss = 0.05279192\n",
      "Iteration 10556, loss = 0.05278589\n",
      "Iteration 10557, loss = 0.05277979\n",
      "Iteration 10558, loss = 0.05277359\n",
      "Iteration 10559, loss = 0.05276754\n",
      "Iteration 10560, loss = 0.05276142\n",
      "Iteration 10561, loss = 0.05275525\n",
      "Iteration 10562, loss = 0.05274920\n",
      "Iteration 10563, loss = 0.05274307\n",
      "Iteration 10564, loss = 0.05273699\n",
      "Iteration 10565, loss = 0.05273088\n",
      "Iteration 10566, loss = 0.05272476\n",
      "Iteration 10567, loss = 0.05271878\n",
      "Iteration 10568, loss = 0.05271261\n",
      "Iteration 10569, loss = 0.05270650\n",
      "Iteration 10570, loss = 0.05270047\n",
      "Iteration 10571, loss = 0.05269437\n",
      "Iteration 10572, loss = 0.05268818\n",
      "Iteration 10573, loss = 0.05268217\n",
      "Iteration 10574, loss = 0.05267612\n",
      "Iteration 10575, loss = 0.05267004\n",
      "Iteration 10576, loss = 0.05266399\n",
      "Iteration 10577, loss = 0.05265788\n",
      "Iteration 10578, loss = 0.05265182\n",
      "Iteration 10579, loss = 0.05264568\n",
      "Iteration 10580, loss = 0.05263961\n",
      "Iteration 10581, loss = 0.05263364\n",
      "Iteration 10582, loss = 0.05262755\n",
      "Iteration 10583, loss = 0.05262146\n",
      "Iteration 10584, loss = 0.05261531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10585, loss = 0.05260931\n",
      "Iteration 10586, loss = 0.05260323\n",
      "Iteration 10587, loss = 0.05259715\n",
      "Iteration 10588, loss = 0.05259114\n",
      "Iteration 10589, loss = 0.05258516\n",
      "Iteration 10590, loss = 0.05257903\n",
      "Iteration 10591, loss = 0.05257294\n",
      "Iteration 10592, loss = 0.05256691\n",
      "Iteration 10593, loss = 0.05256081\n",
      "Iteration 10594, loss = 0.05255471\n",
      "Iteration 10595, loss = 0.05254868\n",
      "Iteration 10596, loss = 0.05254269\n",
      "Iteration 10597, loss = 0.05253660\n",
      "Iteration 10598, loss = 0.05253054\n",
      "Iteration 10599, loss = 0.05252451\n",
      "Iteration 10600, loss = 0.05251844\n",
      "Iteration 10601, loss = 0.05251234\n",
      "Iteration 10602, loss = 0.05250632\n",
      "Iteration 10603, loss = 0.05250030\n",
      "Iteration 10604, loss = 0.05249419\n",
      "Iteration 10605, loss = 0.05248817\n",
      "Iteration 10606, loss = 0.05248218\n",
      "Iteration 10607, loss = 0.05247613\n",
      "Iteration 10608, loss = 0.05247007\n",
      "Iteration 10609, loss = 0.05246402\n",
      "Iteration 10610, loss = 0.05245804\n",
      "Iteration 10611, loss = 0.05245194\n",
      "Iteration 10612, loss = 0.05244595\n",
      "Iteration 10613, loss = 0.05243991\n",
      "Iteration 10614, loss = 0.05243390\n",
      "Iteration 10615, loss = 0.05242784\n",
      "Iteration 10616, loss = 0.05242184\n",
      "Iteration 10617, loss = 0.05241585\n",
      "Iteration 10618, loss = 0.05240978\n",
      "Iteration 10619, loss = 0.05240379\n",
      "Iteration 10620, loss = 0.05239777\n",
      "Iteration 10621, loss = 0.05239180\n",
      "Iteration 10622, loss = 0.05238582\n",
      "Iteration 10623, loss = 0.05237975\n",
      "Iteration 10624, loss = 0.05237375\n",
      "Iteration 10625, loss = 0.05236777\n",
      "Iteration 10626, loss = 0.05236177\n",
      "Iteration 10627, loss = 0.05235572\n",
      "Iteration 10628, loss = 0.05234976\n",
      "Iteration 10629, loss = 0.05234382\n",
      "Iteration 10630, loss = 0.05233776\n",
      "Iteration 10631, loss = 0.05233175\n",
      "Iteration 10632, loss = 0.05232571\n",
      "Iteration 10633, loss = 0.05231970\n",
      "Iteration 10634, loss = 0.05231376\n",
      "Iteration 10635, loss = 0.05230773\n",
      "Iteration 10636, loss = 0.05230176\n",
      "Iteration 10637, loss = 0.05229576\n",
      "Iteration 10638, loss = 0.05228973\n",
      "Iteration 10639, loss = 0.05228372\n",
      "Iteration 10640, loss = 0.05227773\n",
      "Iteration 10641, loss = 0.05227169\n",
      "Iteration 10642, loss = 0.05226576\n",
      "Iteration 10643, loss = 0.05225978\n",
      "Iteration 10644, loss = 0.05225384\n",
      "Iteration 10645, loss = 0.05224774\n",
      "Iteration 10646, loss = 0.05224173\n",
      "Iteration 10647, loss = 0.05223578\n",
      "Iteration 10648, loss = 0.05222980\n",
      "Iteration 10649, loss = 0.05222381\n",
      "Iteration 10650, loss = 0.05221781\n",
      "Iteration 10651, loss = 0.05221186\n",
      "Iteration 10652, loss = 0.05220583\n",
      "Iteration 10653, loss = 0.05219983\n",
      "Iteration 10654, loss = 0.05219389\n",
      "Iteration 10655, loss = 0.05218789\n",
      "Iteration 10656, loss = 0.05218193\n",
      "Iteration 10657, loss = 0.05217593\n",
      "Iteration 10658, loss = 0.05216998\n",
      "Iteration 10659, loss = 0.05216403\n",
      "Iteration 10660, loss = 0.05215801\n",
      "Iteration 10661, loss = 0.05215209\n",
      "Iteration 10662, loss = 0.05214601\n",
      "Iteration 10663, loss = 0.05214016\n",
      "Iteration 10664, loss = 0.05213412\n",
      "Iteration 10665, loss = 0.05212815\n",
      "Iteration 10666, loss = 0.05212217\n",
      "Iteration 10667, loss = 0.05211617\n",
      "Iteration 10668, loss = 0.05211030\n",
      "Iteration 10669, loss = 0.05210432\n",
      "Iteration 10670, loss = 0.05209835\n",
      "Iteration 10671, loss = 0.05209240\n",
      "Iteration 10672, loss = 0.05208639\n",
      "Iteration 10673, loss = 0.05208046\n",
      "Iteration 10674, loss = 0.05207459\n",
      "Iteration 10675, loss = 0.05206857\n",
      "Iteration 10676, loss = 0.05206262\n",
      "Iteration 10677, loss = 0.05205659\n",
      "Iteration 10678, loss = 0.05205079\n",
      "Iteration 10679, loss = 0.05204475\n",
      "Iteration 10680, loss = 0.05203881\n",
      "Iteration 10681, loss = 0.05203289\n",
      "Iteration 10682, loss = 0.05202694\n",
      "Iteration 10683, loss = 0.05202101\n",
      "Iteration 10684, loss = 0.05201505\n",
      "Iteration 10685, loss = 0.05200919\n",
      "Iteration 10686, loss = 0.05200321\n",
      "Iteration 10687, loss = 0.05199726\n",
      "Iteration 10688, loss = 0.05199135\n",
      "Iteration 10689, loss = 0.05198544\n",
      "Iteration 10690, loss = 0.05197950\n",
      "Iteration 10691, loss = 0.05197352\n",
      "Iteration 10692, loss = 0.05196756\n",
      "Iteration 10693, loss = 0.05196168\n",
      "Iteration 10694, loss = 0.05195581\n",
      "Iteration 10695, loss = 0.05194982\n",
      "Iteration 10696, loss = 0.05194391\n",
      "Iteration 10697, loss = 0.05193798\n",
      "Iteration 10698, loss = 0.05193212\n",
      "Iteration 10699, loss = 0.05192613\n",
      "Iteration 10700, loss = 0.05192015\n",
      "Iteration 10701, loss = 0.05191432\n",
      "Iteration 10702, loss = 0.05190839\n",
      "Iteration 10703, loss = 0.05190251\n",
      "Iteration 10704, loss = 0.05189661\n",
      "Iteration 10705, loss = 0.05189062\n",
      "Iteration 10706, loss = 0.05188478\n",
      "Iteration 10707, loss = 0.05187893\n",
      "Iteration 10708, loss = 0.05187298\n",
      "Iteration 10709, loss = 0.05186709\n",
      "Iteration 10710, loss = 0.05186121\n",
      "Iteration 10711, loss = 0.05185535\n",
      "Iteration 10712, loss = 0.05184934\n",
      "Iteration 10713, loss = 0.05184356\n",
      "Iteration 10714, loss = 0.05183765\n",
      "Iteration 10715, loss = 0.05183172\n",
      "Iteration 10716, loss = 0.05182587\n",
      "Iteration 10717, loss = 0.05182004\n",
      "Iteration 10718, loss = 0.05181412\n",
      "Iteration 10719, loss = 0.05180824\n",
      "Iteration 10720, loss = 0.05180244\n",
      "Iteration 10721, loss = 0.05179648\n",
      "Iteration 10722, loss = 0.05179058\n",
      "Iteration 10723, loss = 0.05178477\n",
      "Iteration 10724, loss = 0.05177893\n",
      "Iteration 10725, loss = 0.05177300\n",
      "Iteration 10726, loss = 0.05176714\n",
      "Iteration 10727, loss = 0.05176120\n",
      "Iteration 10728, loss = 0.05175541\n",
      "Iteration 10729, loss = 0.05174954\n",
      "Iteration 10730, loss = 0.05174358\n",
      "Iteration 10731, loss = 0.05173779\n",
      "Iteration 10732, loss = 0.05173190\n",
      "Iteration 10733, loss = 0.05172597\n",
      "Iteration 10734, loss = 0.05172021\n",
      "Iteration 10735, loss = 0.05171424\n",
      "Iteration 10736, loss = 0.05170850\n",
      "Iteration 10737, loss = 0.05170260\n",
      "Iteration 10738, loss = 0.05169681\n",
      "Iteration 10739, loss = 0.05169087\n",
      "Iteration 10740, loss = 0.05168500\n",
      "Iteration 10741, loss = 0.05167912\n",
      "Iteration 10742, loss = 0.05167334\n",
      "Iteration 10743, loss = 0.05166748\n",
      "Iteration 10744, loss = 0.05166168\n",
      "Iteration 10745, loss = 0.05165579\n",
      "Iteration 10746, loss = 0.05164997\n",
      "Iteration 10747, loss = 0.05164413\n",
      "Iteration 10748, loss = 0.05163827\n",
      "Iteration 10749, loss = 0.05163247\n",
      "Iteration 10750, loss = 0.05162659\n",
      "Iteration 10751, loss = 0.05162074\n",
      "Iteration 10752, loss = 0.05161490\n",
      "Iteration 10753, loss = 0.05160909\n",
      "Iteration 10754, loss = 0.05160322\n",
      "Iteration 10755, loss = 0.05159743\n",
      "Iteration 10756, loss = 0.05159155\n",
      "Iteration 10757, loss = 0.05158571\n",
      "Iteration 10758, loss = 0.05157990\n",
      "Iteration 10759, loss = 0.05157400\n",
      "Iteration 10760, loss = 0.05156816\n",
      "Iteration 10761, loss = 0.05156231\n",
      "Iteration 10762, loss = 0.05155655\n",
      "Iteration 10763, loss = 0.05155064\n",
      "Iteration 10764, loss = 0.05154479\n",
      "Iteration 10765, loss = 0.05153907\n",
      "Iteration 10766, loss = 0.05153317\n",
      "Iteration 10767, loss = 0.05152738\n",
      "Iteration 10768, loss = 0.05152148\n",
      "Iteration 10769, loss = 0.05151568\n",
      "Iteration 10770, loss = 0.05150983\n",
      "Iteration 10771, loss = 0.05150399\n",
      "Iteration 10772, loss = 0.05149815\n",
      "Iteration 10773, loss = 0.05149234\n",
      "Iteration 10774, loss = 0.05148664\n",
      "Iteration 10775, loss = 0.05148077\n",
      "Iteration 10776, loss = 0.05147494\n",
      "Iteration 10777, loss = 0.05146923\n",
      "Iteration 10778, loss = 0.05146349\n",
      "Iteration 10779, loss = 0.05145758\n",
      "Iteration 10780, loss = 0.05145178\n",
      "Iteration 10781, loss = 0.05144598\n",
      "Iteration 10782, loss = 0.05144023\n",
      "Iteration 10783, loss = 0.05143438\n",
      "Iteration 10784, loss = 0.05142865\n",
      "Iteration 10785, loss = 0.05142282\n",
      "Iteration 10786, loss = 0.05141706\n",
      "Iteration 10787, loss = 0.05141122\n",
      "Iteration 10788, loss = 0.05140543\n",
      "Iteration 10789, loss = 0.05139973\n",
      "Iteration 10790, loss = 0.05139393\n",
      "Iteration 10791, loss = 0.05138810\n",
      "Iteration 10792, loss = 0.05138232\n",
      "Iteration 10793, loss = 0.05137652\n",
      "Iteration 10794, loss = 0.05137076\n",
      "Iteration 10795, loss = 0.05136495\n",
      "Iteration 10796, loss = 0.05135915\n",
      "Iteration 10797, loss = 0.05135344\n",
      "Iteration 10798, loss = 0.05134758\n",
      "Iteration 10799, loss = 0.05134185\n",
      "Iteration 10800, loss = 0.05133602\n",
      "Iteration 10801, loss = 0.05133025\n",
      "Iteration 10802, loss = 0.05132446\n",
      "Iteration 10803, loss = 0.05131862\n",
      "Iteration 10804, loss = 0.05131288\n",
      "Iteration 10805, loss = 0.05130720\n",
      "Iteration 10806, loss = 0.05130131\n",
      "Iteration 10807, loss = 0.05129556\n",
      "Iteration 10808, loss = 0.05128968\n",
      "Iteration 10809, loss = 0.05128389\n",
      "Iteration 10810, loss = 0.05127810\n",
      "Iteration 10811, loss = 0.05127242\n",
      "Iteration 10812, loss = 0.05126650\n",
      "Iteration 10813, loss = 0.05126079\n",
      "Iteration 10814, loss = 0.05125499\n",
      "Iteration 10815, loss = 0.05124925\n",
      "Iteration 10816, loss = 0.05124340\n",
      "Iteration 10817, loss = 0.05123771\n",
      "Iteration 10818, loss = 0.05123196\n",
      "Iteration 10819, loss = 0.05122620\n",
      "Iteration 10820, loss = 0.05122034\n",
      "Iteration 10821, loss = 0.05121459\n",
      "Iteration 10822, loss = 0.05120893\n",
      "Iteration 10823, loss = 0.05120311\n",
      "Iteration 10824, loss = 0.05119741\n",
      "Iteration 10825, loss = 0.05119163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10826, loss = 0.05118588\n",
      "Iteration 10827, loss = 0.05118012\n",
      "Iteration 10828, loss = 0.05117430\n",
      "Iteration 10829, loss = 0.05116865\n",
      "Iteration 10830, loss = 0.05116301\n",
      "Iteration 10831, loss = 0.05115717\n",
      "Iteration 10832, loss = 0.05115148\n",
      "Iteration 10833, loss = 0.05114577\n",
      "Iteration 10834, loss = 0.05114006\n",
      "Iteration 10835, loss = 0.05113435\n",
      "Iteration 10836, loss = 0.05112859\n",
      "Iteration 10837, loss = 0.05112290\n",
      "Iteration 10838, loss = 0.05111718\n",
      "Iteration 10839, loss = 0.05111153\n",
      "Iteration 10840, loss = 0.05110578\n",
      "Iteration 10841, loss = 0.05110001\n",
      "Iteration 10842, loss = 0.05109441\n",
      "Iteration 10843, loss = 0.05108861\n",
      "Iteration 10844, loss = 0.05108294\n",
      "Iteration 10845, loss = 0.05107720\n",
      "Iteration 10846, loss = 0.05107142\n",
      "Iteration 10847, loss = 0.05106582\n",
      "Iteration 10848, loss = 0.05106005\n",
      "Iteration 10849, loss = 0.05105444\n",
      "Iteration 10850, loss = 0.05104866\n",
      "Iteration 10851, loss = 0.05104297\n",
      "Iteration 10852, loss = 0.05103728\n",
      "Iteration 10853, loss = 0.05103154\n",
      "Iteration 10854, loss = 0.05102585\n",
      "Iteration 10855, loss = 0.05102014\n",
      "Iteration 10856, loss = 0.05101446\n",
      "Iteration 10857, loss = 0.05100873\n",
      "Iteration 10858, loss = 0.05100310\n",
      "Iteration 10859, loss = 0.05099732\n",
      "Iteration 10860, loss = 0.05099168\n",
      "Iteration 10861, loss = 0.05098588\n",
      "Iteration 10862, loss = 0.05098028\n",
      "Iteration 10863, loss = 0.05097454\n",
      "Iteration 10864, loss = 0.05096891\n",
      "Iteration 10865, loss = 0.05096316\n",
      "Iteration 10866, loss = 0.05095749\n",
      "Iteration 10867, loss = 0.05095177\n",
      "Iteration 10868, loss = 0.05094612\n",
      "Iteration 10869, loss = 0.05094042\n",
      "Iteration 10870, loss = 0.05093471\n",
      "Iteration 10871, loss = 0.05092911\n",
      "Iteration 10872, loss = 0.05092335\n",
      "Iteration 10873, loss = 0.05091777\n",
      "Iteration 10874, loss = 0.05091201\n",
      "Iteration 10875, loss = 0.05090633\n",
      "Iteration 10876, loss = 0.05090064\n",
      "Iteration 10877, loss = 0.05089500\n",
      "Iteration 10878, loss = 0.05088927\n",
      "Iteration 10879, loss = 0.05088360\n",
      "Iteration 10880, loss = 0.05087794\n",
      "Iteration 10881, loss = 0.05087225\n",
      "Iteration 10882, loss = 0.05086660\n",
      "Iteration 10883, loss = 0.05086091\n",
      "Iteration 10884, loss = 0.05085523\n",
      "Iteration 10885, loss = 0.05084953\n",
      "Iteration 10886, loss = 0.05084387\n",
      "Iteration 10887, loss = 0.05083821\n",
      "Iteration 10888, loss = 0.05083253\n",
      "Iteration 10889, loss = 0.05082689\n",
      "Iteration 10890, loss = 0.05082112\n",
      "Iteration 10891, loss = 0.05081547\n",
      "Iteration 10892, loss = 0.05080994\n",
      "Iteration 10893, loss = 0.05080427\n",
      "Iteration 10894, loss = 0.05079849\n",
      "Iteration 10895, loss = 0.05079288\n",
      "Iteration 10896, loss = 0.05078721\n",
      "Iteration 10897, loss = 0.05078156\n",
      "Iteration 10898, loss = 0.05077596\n",
      "Iteration 10899, loss = 0.05077021\n",
      "Iteration 10900, loss = 0.05076463\n",
      "Iteration 10901, loss = 0.05075889\n",
      "Iteration 10902, loss = 0.05075336\n",
      "Iteration 10903, loss = 0.05074761\n",
      "Iteration 10904, loss = 0.05074196\n",
      "Iteration 10905, loss = 0.05073635\n",
      "Iteration 10906, loss = 0.05073072\n",
      "Iteration 10907, loss = 0.05072502\n",
      "Iteration 10908, loss = 0.05071945\n",
      "Iteration 10909, loss = 0.05071374\n",
      "Iteration 10910, loss = 0.05070819\n",
      "Iteration 10911, loss = 0.05070243\n",
      "Iteration 10912, loss = 0.05069686\n",
      "Iteration 10913, loss = 0.05069127\n",
      "Iteration 10914, loss = 0.05068561\n",
      "Iteration 10915, loss = 0.05067991\n",
      "Iteration 10916, loss = 0.05067435\n",
      "Iteration 10917, loss = 0.05066864\n",
      "Iteration 10918, loss = 0.05066315\n",
      "Iteration 10919, loss = 0.05065744\n",
      "Iteration 10920, loss = 0.05065183\n",
      "Iteration 10921, loss = 0.05064616\n",
      "Iteration 10922, loss = 0.05064054\n",
      "Iteration 10923, loss = 0.05063485\n",
      "Iteration 10924, loss = 0.05062935\n",
      "Iteration 10925, loss = 0.05062371\n",
      "Iteration 10926, loss = 0.05061806\n",
      "Iteration 10927, loss = 0.05061249\n",
      "Iteration 10928, loss = 0.05060691\n",
      "Iteration 10929, loss = 0.05060134\n",
      "Iteration 10930, loss = 0.05059565\n",
      "Iteration 10931, loss = 0.05059002\n",
      "Iteration 10932, loss = 0.05058447\n",
      "Iteration 10933, loss = 0.05057891\n",
      "Iteration 10934, loss = 0.05057322\n",
      "Iteration 10935, loss = 0.05056767\n",
      "Iteration 10936, loss = 0.05056205\n",
      "Iteration 10937, loss = 0.05055652\n",
      "Iteration 10938, loss = 0.05055092\n",
      "Iteration 10939, loss = 0.05054526\n",
      "Iteration 10940, loss = 0.05053964\n",
      "Iteration 10941, loss = 0.05053409\n",
      "Iteration 10942, loss = 0.05052849\n",
      "Iteration 10943, loss = 0.05052293\n",
      "Iteration 10944, loss = 0.05051734\n",
      "Iteration 10945, loss = 0.05051175\n",
      "Iteration 10946, loss = 0.05050620\n",
      "Iteration 10947, loss = 0.05050060\n",
      "Iteration 10948, loss = 0.05049500\n",
      "Iteration 10949, loss = 0.05048940\n",
      "Iteration 10950, loss = 0.05048385\n",
      "Iteration 10951, loss = 0.05047823\n",
      "Iteration 10952, loss = 0.05047268\n",
      "Iteration 10953, loss = 0.05046708\n",
      "Iteration 10954, loss = 0.05046149\n",
      "Iteration 10955, loss = 0.05045590\n",
      "Iteration 10956, loss = 0.05045035\n",
      "Iteration 10957, loss = 0.05044472\n",
      "Iteration 10958, loss = 0.05043918\n",
      "Iteration 10959, loss = 0.05043368\n",
      "Iteration 10960, loss = 0.05042808\n",
      "Iteration 10961, loss = 0.05042253\n",
      "Iteration 10962, loss = 0.05041692\n",
      "Iteration 10963, loss = 0.05041136\n",
      "Iteration 10964, loss = 0.05040583\n",
      "Iteration 10965, loss = 0.05040028\n",
      "Iteration 10966, loss = 0.05039472\n",
      "Iteration 10967, loss = 0.05038913\n",
      "Iteration 10968, loss = 0.05038359\n",
      "Iteration 10969, loss = 0.05037801\n",
      "Iteration 10970, loss = 0.05037257\n",
      "Iteration 10971, loss = 0.05036688\n",
      "Iteration 10972, loss = 0.05036140\n",
      "Iteration 10973, loss = 0.05035580\n",
      "Iteration 10974, loss = 0.05035022\n",
      "Iteration 10975, loss = 0.05034471\n",
      "Iteration 10976, loss = 0.05033908\n",
      "Iteration 10977, loss = 0.05033353\n",
      "Iteration 10978, loss = 0.05032804\n",
      "Iteration 10979, loss = 0.05032246\n",
      "Iteration 10980, loss = 0.05031694\n",
      "Iteration 10981, loss = 0.05031134\n",
      "Iteration 10982, loss = 0.05030581\n",
      "Iteration 10983, loss = 0.05030020\n",
      "Iteration 10984, loss = 0.05029465\n",
      "Iteration 10985, loss = 0.05028914\n",
      "Iteration 10986, loss = 0.05028354\n",
      "Iteration 10987, loss = 0.05027808\n",
      "Iteration 10988, loss = 0.05027255\n",
      "Iteration 10989, loss = 0.05026698\n",
      "Iteration 10990, loss = 0.05026143\n",
      "Iteration 10991, loss = 0.05025584\n",
      "Iteration 10992, loss = 0.05025032\n",
      "Iteration 10993, loss = 0.05024482\n",
      "Iteration 10994, loss = 0.05023926\n",
      "Iteration 10995, loss = 0.05023375\n",
      "Iteration 10996, loss = 0.05022821\n",
      "Iteration 10997, loss = 0.05022273\n",
      "Iteration 10998, loss = 0.05021717\n",
      "Iteration 10999, loss = 0.05021161\n",
      "Iteration 11000, loss = 0.05020611\n",
      "Iteration 11001, loss = 0.05020063\n",
      "Iteration 11002, loss = 0.05019501\n",
      "Iteration 11003, loss = 0.05018951\n",
      "Iteration 11004, loss = 0.05018399\n",
      "Iteration 11005, loss = 0.05017847\n",
      "Iteration 11006, loss = 0.05017295\n",
      "Iteration 11007, loss = 0.05016747\n",
      "Iteration 11008, loss = 0.05016197\n",
      "Iteration 11009, loss = 0.05015637\n",
      "Iteration 11010, loss = 0.05015079\n",
      "Iteration 11011, loss = 0.05014531\n",
      "Iteration 11012, loss = 0.05013974\n",
      "Iteration 11013, loss = 0.05013425\n",
      "Iteration 11014, loss = 0.05012872\n",
      "Iteration 11015, loss = 0.05012323\n",
      "Iteration 11016, loss = 0.05011771\n",
      "Iteration 11017, loss = 0.05011221\n",
      "Iteration 11018, loss = 0.05010664\n",
      "Iteration 11019, loss = 0.05010116\n",
      "Iteration 11020, loss = 0.05009564\n",
      "Iteration 11021, loss = 0.05009022\n",
      "Iteration 11022, loss = 0.05008473\n",
      "Iteration 11023, loss = 0.05007912\n",
      "Iteration 11024, loss = 0.05007368\n",
      "Iteration 11025, loss = 0.05006810\n",
      "Iteration 11026, loss = 0.05006273\n",
      "Iteration 11027, loss = 0.05005714\n",
      "Iteration 11028, loss = 0.05005171\n",
      "Iteration 11029, loss = 0.05004619\n",
      "Iteration 11030, loss = 0.05004069\n",
      "Iteration 11031, loss = 0.05003519\n",
      "Iteration 11032, loss = 0.05002972\n",
      "Iteration 11033, loss = 0.05002425\n",
      "Iteration 11034, loss = 0.05001876\n",
      "Iteration 11035, loss = 0.05001324\n",
      "Iteration 11036, loss = 0.05000775\n",
      "Iteration 11037, loss = 0.05000233\n",
      "Iteration 11038, loss = 0.04999677\n",
      "Iteration 11039, loss = 0.04999127\n",
      "Iteration 11040, loss = 0.04998592\n",
      "Iteration 11041, loss = 0.04998037\n",
      "Iteration 11042, loss = 0.04997490\n",
      "Iteration 11043, loss = 0.04996931\n",
      "Iteration 11044, loss = 0.04996394\n",
      "Iteration 11045, loss = 0.04995851\n",
      "Iteration 11046, loss = 0.04995293\n",
      "Iteration 11047, loss = 0.04994750\n",
      "Iteration 11048, loss = 0.04994200\n",
      "Iteration 11049, loss = 0.04993650\n",
      "Iteration 11050, loss = 0.04993112\n",
      "Iteration 11051, loss = 0.04992560\n",
      "Iteration 11052, loss = 0.04992016\n",
      "Iteration 11053, loss = 0.04991461\n",
      "Iteration 11054, loss = 0.04990920\n",
      "Iteration 11055, loss = 0.04990371\n",
      "Iteration 11056, loss = 0.04989831\n",
      "Iteration 11057, loss = 0.04989282\n",
      "Iteration 11058, loss = 0.04988731\n",
      "Iteration 11059, loss = 0.04988192\n",
      "Iteration 11060, loss = 0.04987645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11061, loss = 0.04987106\n",
      "Iteration 11062, loss = 0.04986547\n",
      "Iteration 11063, loss = 0.04986012\n",
      "Iteration 11064, loss = 0.04985468\n",
      "Iteration 11065, loss = 0.04984924\n",
      "Iteration 11066, loss = 0.04984380\n",
      "Iteration 11067, loss = 0.04983831\n",
      "Iteration 11068, loss = 0.04983292\n",
      "Iteration 11069, loss = 0.04982746\n",
      "Iteration 11070, loss = 0.04982208\n",
      "Iteration 11071, loss = 0.04981659\n",
      "Iteration 11072, loss = 0.04981114\n",
      "Iteration 11073, loss = 0.04980568\n",
      "Iteration 11074, loss = 0.04980026\n",
      "Iteration 11075, loss = 0.04979487\n",
      "Iteration 11076, loss = 0.04978948\n",
      "Iteration 11077, loss = 0.04978398\n",
      "Iteration 11078, loss = 0.04977854\n",
      "Iteration 11079, loss = 0.04977313\n",
      "Iteration 11080, loss = 0.04976768\n",
      "Iteration 11081, loss = 0.04976230\n",
      "Iteration 11082, loss = 0.04975688\n",
      "Iteration 11083, loss = 0.04975135\n",
      "Iteration 11084, loss = 0.04974599\n",
      "Iteration 11085, loss = 0.04974050\n",
      "Iteration 11086, loss = 0.04973512\n",
      "Iteration 11087, loss = 0.04972966\n",
      "Iteration 11088, loss = 0.04972428\n",
      "Iteration 11089, loss = 0.04971887\n",
      "Iteration 11090, loss = 0.04971339\n",
      "Iteration 11091, loss = 0.04970797\n",
      "Iteration 11092, loss = 0.04970263\n",
      "Iteration 11093, loss = 0.04969717\n",
      "Iteration 11094, loss = 0.04969185\n",
      "Iteration 11095, loss = 0.04968643\n",
      "Iteration 11096, loss = 0.04968103\n",
      "Iteration 11097, loss = 0.04967556\n",
      "Iteration 11098, loss = 0.04967020\n",
      "Iteration 11099, loss = 0.04966479\n",
      "Iteration 11100, loss = 0.04965944\n",
      "Iteration 11101, loss = 0.04965400\n",
      "Iteration 11102, loss = 0.04964861\n",
      "Iteration 11103, loss = 0.04964322\n",
      "Iteration 11104, loss = 0.04963780\n",
      "Iteration 11105, loss = 0.04963240\n",
      "Iteration 11106, loss = 0.04962700\n",
      "Iteration 11107, loss = 0.04962154\n",
      "Iteration 11108, loss = 0.04961614\n",
      "Iteration 11109, loss = 0.04961079\n",
      "Iteration 11110, loss = 0.04960540\n",
      "Iteration 11111, loss = 0.04959998\n",
      "Iteration 11112, loss = 0.04959462\n",
      "Iteration 11113, loss = 0.04958924\n",
      "Iteration 11114, loss = 0.04958380\n",
      "Iteration 11115, loss = 0.04957847\n",
      "Iteration 11116, loss = 0.04957311\n",
      "Iteration 11117, loss = 0.04956767\n",
      "Iteration 11118, loss = 0.04956228\n",
      "Iteration 11119, loss = 0.04955692\n",
      "Iteration 11120, loss = 0.04955156\n",
      "Iteration 11121, loss = 0.04954624\n",
      "Iteration 11122, loss = 0.04954076\n",
      "Iteration 11123, loss = 0.04953544\n",
      "Iteration 11124, loss = 0.04953010\n",
      "Iteration 11125, loss = 0.04952473\n",
      "Iteration 11126, loss = 0.04951927\n",
      "Iteration 11127, loss = 0.04951403\n",
      "Iteration 11128, loss = 0.04950854\n",
      "Iteration 11129, loss = 0.04950319\n",
      "Iteration 11130, loss = 0.04949781\n",
      "Iteration 11131, loss = 0.04949246\n",
      "Iteration 11132, loss = 0.04948713\n",
      "Iteration 11133, loss = 0.04948176\n",
      "Iteration 11134, loss = 0.04947646\n",
      "Iteration 11135, loss = 0.04947110\n",
      "Iteration 11136, loss = 0.04946568\n",
      "Iteration 11137, loss = 0.04946036\n",
      "Iteration 11138, loss = 0.04945501\n",
      "Iteration 11139, loss = 0.04944963\n",
      "Iteration 11140, loss = 0.04944429\n",
      "Iteration 11141, loss = 0.04943895\n",
      "Iteration 11142, loss = 0.04943366\n",
      "Iteration 11143, loss = 0.04942824\n",
      "Iteration 11144, loss = 0.04942291\n",
      "Iteration 11145, loss = 0.04941750\n",
      "Iteration 11146, loss = 0.04941218\n",
      "Iteration 11147, loss = 0.04940682\n",
      "Iteration 11148, loss = 0.04940150\n",
      "Iteration 11149, loss = 0.04939617\n",
      "Iteration 11150, loss = 0.04939082\n",
      "Iteration 11151, loss = 0.04938544\n",
      "Iteration 11152, loss = 0.04938015\n",
      "Iteration 11153, loss = 0.04937477\n",
      "Iteration 11154, loss = 0.04936940\n",
      "Iteration 11155, loss = 0.04936407\n",
      "Iteration 11156, loss = 0.04935876\n",
      "Iteration 11157, loss = 0.04935344\n",
      "Iteration 11158, loss = 0.04934809\n",
      "Iteration 11159, loss = 0.04934279\n",
      "Iteration 11160, loss = 0.04933736\n",
      "Iteration 11161, loss = 0.04933205\n",
      "Iteration 11162, loss = 0.04932677\n",
      "Iteration 11163, loss = 0.04932137\n",
      "Iteration 11164, loss = 0.04931614\n",
      "Iteration 11165, loss = 0.04931078\n",
      "Iteration 11166, loss = 0.04930542\n",
      "Iteration 11167, loss = 0.04930018\n",
      "Iteration 11168, loss = 0.04929479\n",
      "Iteration 11169, loss = 0.04928943\n",
      "Iteration 11170, loss = 0.04928420\n",
      "Iteration 11171, loss = 0.04927889\n",
      "Iteration 11172, loss = 0.04927362\n",
      "Iteration 11173, loss = 0.04926828\n",
      "Iteration 11174, loss = 0.04926297\n",
      "Iteration 11175, loss = 0.04925768\n",
      "Iteration 11176, loss = 0.04925234\n",
      "Iteration 11177, loss = 0.04924704\n",
      "Iteration 11178, loss = 0.04924183\n",
      "Iteration 11179, loss = 0.04923653\n",
      "Iteration 11180, loss = 0.04923112\n",
      "Iteration 11181, loss = 0.04922580\n",
      "Iteration 11182, loss = 0.04922059\n",
      "Iteration 11183, loss = 0.04921529\n",
      "Iteration 11184, loss = 0.04920994\n",
      "Iteration 11185, loss = 0.04920459\n",
      "Iteration 11186, loss = 0.04919934\n",
      "Iteration 11187, loss = 0.04919394\n",
      "Iteration 11188, loss = 0.04918875\n",
      "Iteration 11189, loss = 0.04918337\n",
      "Iteration 11190, loss = 0.04917806\n",
      "Iteration 11191, loss = 0.04917273\n",
      "Iteration 11192, loss = 0.04916743\n",
      "Iteration 11193, loss = 0.04916218\n",
      "Iteration 11194, loss = 0.04915692\n",
      "Iteration 11195, loss = 0.04915159\n",
      "Iteration 11196, loss = 0.04914629\n",
      "Iteration 11197, loss = 0.04914093\n",
      "Iteration 11198, loss = 0.04913570\n",
      "Iteration 11199, loss = 0.04913051\n",
      "Iteration 11200, loss = 0.04912514\n",
      "Iteration 11201, loss = 0.04911990\n",
      "Iteration 11202, loss = 0.04911455\n",
      "Iteration 11203, loss = 0.04910927\n",
      "Iteration 11204, loss = 0.04910404\n",
      "Iteration 11205, loss = 0.04909873\n",
      "Iteration 11206, loss = 0.04909344\n",
      "Iteration 11207, loss = 0.04908814\n",
      "Iteration 11208, loss = 0.04908287\n",
      "Iteration 11209, loss = 0.04907757\n",
      "Iteration 11210, loss = 0.04907227\n",
      "Iteration 11211, loss = 0.04906698\n",
      "Iteration 11212, loss = 0.04906178\n",
      "Iteration 11213, loss = 0.04905645\n",
      "Iteration 11214, loss = 0.04905109\n",
      "Iteration 11215, loss = 0.04904585\n",
      "Iteration 11216, loss = 0.04904070\n",
      "Iteration 11217, loss = 0.04903536\n",
      "Iteration 11218, loss = 0.04903001\n",
      "Iteration 11219, loss = 0.04902479\n",
      "Iteration 11220, loss = 0.04901950\n",
      "Iteration 11221, loss = 0.04901423\n",
      "Iteration 11222, loss = 0.04900892\n",
      "Iteration 11223, loss = 0.04900374\n",
      "Iteration 11224, loss = 0.04899843\n",
      "Iteration 11225, loss = 0.04899318\n",
      "Iteration 11226, loss = 0.04898798\n",
      "Iteration 11227, loss = 0.04898264\n",
      "Iteration 11228, loss = 0.04897737\n",
      "Iteration 11229, loss = 0.04897217\n",
      "Iteration 11230, loss = 0.04896688\n",
      "Iteration 11231, loss = 0.04896159\n",
      "Iteration 11232, loss = 0.04895632\n",
      "Iteration 11233, loss = 0.04895105\n",
      "Iteration 11234, loss = 0.04894584\n",
      "Iteration 11235, loss = 0.04894054\n",
      "Iteration 11236, loss = 0.04893531\n",
      "Iteration 11237, loss = 0.04893006\n",
      "Iteration 11238, loss = 0.04892482\n",
      "Iteration 11239, loss = 0.04891960\n",
      "Iteration 11240, loss = 0.04891427\n",
      "Iteration 11241, loss = 0.04890908\n",
      "Iteration 11242, loss = 0.04890376\n",
      "Iteration 11243, loss = 0.04889852\n",
      "Iteration 11244, loss = 0.04889339\n",
      "Iteration 11245, loss = 0.04888806\n",
      "Iteration 11246, loss = 0.04888283\n",
      "Iteration 11247, loss = 0.04887767\n",
      "Iteration 11248, loss = 0.04887237\n",
      "Iteration 11249, loss = 0.04886712\n",
      "Iteration 11250, loss = 0.04886194\n",
      "Iteration 11251, loss = 0.04885663\n",
      "Iteration 11252, loss = 0.04885145\n",
      "Iteration 11253, loss = 0.04884622\n",
      "Iteration 11254, loss = 0.04884102\n",
      "Iteration 11255, loss = 0.04883580\n",
      "Iteration 11256, loss = 0.04883056\n",
      "Iteration 11257, loss = 0.04882530\n",
      "Iteration 11258, loss = 0.04882020\n",
      "Iteration 11259, loss = 0.04881490\n",
      "Iteration 11260, loss = 0.04880970\n",
      "Iteration 11261, loss = 0.04880440\n",
      "Iteration 11262, loss = 0.04879918\n",
      "Iteration 11263, loss = 0.04879404\n",
      "Iteration 11264, loss = 0.04878876\n",
      "Iteration 11265, loss = 0.04878353\n",
      "Iteration 11266, loss = 0.04877833\n",
      "Iteration 11267, loss = 0.04877311\n",
      "Iteration 11268, loss = 0.04876788\n",
      "Iteration 11269, loss = 0.04876269\n",
      "Iteration 11270, loss = 0.04875744\n",
      "Iteration 11271, loss = 0.04875223\n",
      "Iteration 11272, loss = 0.04874702\n",
      "Iteration 11273, loss = 0.04874185\n",
      "Iteration 11274, loss = 0.04873661\n",
      "Iteration 11275, loss = 0.04873138\n",
      "Iteration 11276, loss = 0.04872616\n",
      "Iteration 11277, loss = 0.04872095\n",
      "Iteration 11278, loss = 0.04871575\n",
      "Iteration 11279, loss = 0.04871056\n",
      "Iteration 11280, loss = 0.04870531\n",
      "Iteration 11281, loss = 0.04870019\n",
      "Iteration 11282, loss = 0.04869489\n",
      "Iteration 11283, loss = 0.04868973\n",
      "Iteration 11284, loss = 0.04868459\n",
      "Iteration 11285, loss = 0.04867929\n",
      "Iteration 11286, loss = 0.04867419\n",
      "Iteration 11287, loss = 0.04866895\n",
      "Iteration 11288, loss = 0.04866369\n",
      "Iteration 11289, loss = 0.04865858\n",
      "Iteration 11290, loss = 0.04865338\n",
      "Iteration 11291, loss = 0.04864821\n",
      "Iteration 11292, loss = 0.04864302\n",
      "Iteration 11293, loss = 0.04863786\n",
      "Iteration 11294, loss = 0.04863267\n",
      "Iteration 11295, loss = 0.04862752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11296, loss = 0.04862240\n",
      "Iteration 11297, loss = 0.04861720\n",
      "Iteration 11298, loss = 0.04861201\n",
      "Iteration 11299, loss = 0.04860687\n",
      "Iteration 11300, loss = 0.04860176\n",
      "Iteration 11301, loss = 0.04859659\n",
      "Iteration 11302, loss = 0.04859144\n",
      "Iteration 11303, loss = 0.04858630\n",
      "Iteration 11304, loss = 0.04858114\n",
      "Iteration 11305, loss = 0.04857596\n",
      "Iteration 11306, loss = 0.04857079\n",
      "Iteration 11307, loss = 0.04856558\n",
      "Iteration 11308, loss = 0.04856046\n",
      "Iteration 11309, loss = 0.04855529\n",
      "Iteration 11310, loss = 0.04855019\n",
      "Iteration 11311, loss = 0.04854501\n",
      "Iteration 11312, loss = 0.04853984\n",
      "Iteration 11313, loss = 0.04853470\n",
      "Iteration 11314, loss = 0.04852952\n",
      "Iteration 11315, loss = 0.04852435\n",
      "Iteration 11316, loss = 0.04851917\n",
      "Iteration 11317, loss = 0.04851399\n",
      "Iteration 11318, loss = 0.04850892\n",
      "Iteration 11319, loss = 0.04850369\n",
      "Iteration 11320, loss = 0.04849855\n",
      "Iteration 11321, loss = 0.04849337\n",
      "Iteration 11322, loss = 0.04848816\n",
      "Iteration 11323, loss = 0.04848304\n",
      "Iteration 11324, loss = 0.04847785\n",
      "Iteration 11325, loss = 0.04847278\n",
      "Iteration 11326, loss = 0.04846765\n",
      "Iteration 11327, loss = 0.04846243\n",
      "Iteration 11328, loss = 0.04845732\n",
      "Iteration 11329, loss = 0.04845217\n",
      "Iteration 11330, loss = 0.04844698\n",
      "Iteration 11331, loss = 0.04844188\n",
      "Iteration 11332, loss = 0.04843680\n",
      "Iteration 11333, loss = 0.04843162\n",
      "Iteration 11334, loss = 0.04842648\n",
      "Iteration 11335, loss = 0.04842132\n",
      "Iteration 11336, loss = 0.04841622\n",
      "Iteration 11337, loss = 0.04841104\n",
      "Iteration 11338, loss = 0.04840594\n",
      "Iteration 11339, loss = 0.04840082\n",
      "Iteration 11340, loss = 0.04839568\n",
      "Iteration 11341, loss = 0.04839051\n",
      "Iteration 11342, loss = 0.04838537\n",
      "Iteration 11343, loss = 0.04838027\n",
      "Iteration 11344, loss = 0.04837512\n",
      "Iteration 11345, loss = 0.04837004\n",
      "Iteration 11346, loss = 0.04836493\n",
      "Iteration 11347, loss = 0.04835969\n",
      "Iteration 11348, loss = 0.04835468\n",
      "Iteration 11349, loss = 0.04834953\n",
      "Iteration 11350, loss = 0.04834448\n",
      "Iteration 11351, loss = 0.04833924\n",
      "Iteration 11352, loss = 0.04833426\n",
      "Iteration 11353, loss = 0.04832910\n",
      "Iteration 11354, loss = 0.04832395\n",
      "Iteration 11355, loss = 0.04831878\n",
      "Iteration 11356, loss = 0.04831374\n",
      "Iteration 11357, loss = 0.04830869\n",
      "Iteration 11358, loss = 0.04830355\n",
      "Iteration 11359, loss = 0.04829842\n",
      "Iteration 11360, loss = 0.04829331\n",
      "Iteration 11361, loss = 0.04828823\n",
      "Iteration 11362, loss = 0.04828313\n",
      "Iteration 11363, loss = 0.04827800\n",
      "Iteration 11364, loss = 0.04827288\n",
      "Iteration 11365, loss = 0.04826780\n",
      "Iteration 11366, loss = 0.04826261\n",
      "Iteration 11367, loss = 0.04825751\n",
      "Iteration 11368, loss = 0.04825240\n",
      "Iteration 11369, loss = 0.04824737\n",
      "Iteration 11370, loss = 0.04824221\n",
      "Iteration 11371, loss = 0.04823718\n",
      "Iteration 11372, loss = 0.04823201\n",
      "Iteration 11373, loss = 0.04822686\n",
      "Iteration 11374, loss = 0.04822182\n",
      "Iteration 11375, loss = 0.04821681\n",
      "Iteration 11376, loss = 0.04821163\n",
      "Iteration 11377, loss = 0.04820651\n",
      "Iteration 11378, loss = 0.04820149\n",
      "Iteration 11379, loss = 0.04819641\n",
      "Iteration 11380, loss = 0.04819134\n",
      "Iteration 11381, loss = 0.04818619\n",
      "Iteration 11382, loss = 0.04818114\n",
      "Iteration 11383, loss = 0.04817607\n",
      "Iteration 11384, loss = 0.04817096\n",
      "Iteration 11385, loss = 0.04816590\n",
      "Iteration 11386, loss = 0.04816083\n",
      "Iteration 11387, loss = 0.04815571\n",
      "Iteration 11388, loss = 0.04815065\n",
      "Iteration 11389, loss = 0.04814555\n",
      "Iteration 11390, loss = 0.04814054\n",
      "Iteration 11391, loss = 0.04813544\n",
      "Iteration 11392, loss = 0.04813040\n",
      "Iteration 11393, loss = 0.04812528\n",
      "Iteration 11394, loss = 0.04812016\n",
      "Iteration 11395, loss = 0.04811517\n",
      "Iteration 11396, loss = 0.04811008\n",
      "Iteration 11397, loss = 0.04810504\n",
      "Iteration 11398, loss = 0.04809991\n",
      "Iteration 11399, loss = 0.04809483\n",
      "Iteration 11400, loss = 0.04808976\n",
      "Iteration 11401, loss = 0.04808476\n",
      "Iteration 11402, loss = 0.04807967\n",
      "Iteration 11403, loss = 0.04807464\n",
      "Iteration 11404, loss = 0.04806955\n",
      "Iteration 11405, loss = 0.04806452\n",
      "Iteration 11406, loss = 0.04805944\n",
      "Iteration 11407, loss = 0.04805436\n",
      "Iteration 11408, loss = 0.04804936\n",
      "Iteration 11409, loss = 0.04804428\n",
      "Iteration 11410, loss = 0.04803932\n",
      "Iteration 11411, loss = 0.04803416\n",
      "Iteration 11412, loss = 0.04802914\n",
      "Iteration 11413, loss = 0.04802409\n",
      "Iteration 11414, loss = 0.04801900\n",
      "Iteration 11415, loss = 0.04801401\n",
      "Iteration 11416, loss = 0.04800899\n",
      "Iteration 11417, loss = 0.04800394\n",
      "Iteration 11418, loss = 0.04799889\n",
      "Iteration 11419, loss = 0.04799382\n",
      "Iteration 11420, loss = 0.04798873\n",
      "Iteration 11421, loss = 0.04798367\n",
      "Iteration 11422, loss = 0.04797864\n",
      "Iteration 11423, loss = 0.04797360\n",
      "Iteration 11424, loss = 0.04796853\n",
      "Iteration 11425, loss = 0.04796354\n",
      "Iteration 11426, loss = 0.04795845\n",
      "Iteration 11427, loss = 0.04795342\n",
      "Iteration 11428, loss = 0.04794838\n",
      "Iteration 11429, loss = 0.04794337\n",
      "Iteration 11430, loss = 0.04793837\n",
      "Iteration 11431, loss = 0.04793324\n",
      "Iteration 11432, loss = 0.04792829\n",
      "Iteration 11433, loss = 0.04792331\n",
      "Iteration 11434, loss = 0.04791821\n",
      "Iteration 11435, loss = 0.04791320\n",
      "Iteration 11436, loss = 0.04790819\n",
      "Iteration 11437, loss = 0.04790313\n",
      "Iteration 11438, loss = 0.04789812\n",
      "Iteration 11439, loss = 0.04789308\n",
      "Iteration 11440, loss = 0.04788803\n",
      "Iteration 11441, loss = 0.04788316\n",
      "Iteration 11442, loss = 0.04787808\n",
      "Iteration 11443, loss = 0.04787299\n",
      "Iteration 11444, loss = 0.04786806\n",
      "Iteration 11445, loss = 0.04786303\n",
      "Iteration 11446, loss = 0.04785802\n",
      "Iteration 11447, loss = 0.04785304\n",
      "Iteration 11448, loss = 0.04784796\n",
      "Iteration 11449, loss = 0.04784297\n",
      "Iteration 11450, loss = 0.04783792\n",
      "Iteration 11451, loss = 0.04783295\n",
      "Iteration 11452, loss = 0.04782791\n",
      "Iteration 11453, loss = 0.04782297\n",
      "Iteration 11454, loss = 0.04781796\n",
      "Iteration 11455, loss = 0.04781292\n",
      "Iteration 11456, loss = 0.04780790\n",
      "Iteration 11457, loss = 0.04780293\n",
      "Iteration 11458, loss = 0.04779796\n",
      "Iteration 11459, loss = 0.04779291\n",
      "Iteration 11460, loss = 0.04778794\n",
      "Iteration 11461, loss = 0.04778292\n",
      "Iteration 11462, loss = 0.04777789\n",
      "Iteration 11463, loss = 0.04777291\n",
      "Iteration 11464, loss = 0.04776795\n",
      "Iteration 11465, loss = 0.04776291\n",
      "Iteration 11466, loss = 0.04775791\n",
      "Iteration 11467, loss = 0.04775298\n",
      "Iteration 11468, loss = 0.04774797\n",
      "Iteration 11469, loss = 0.04774293\n",
      "Iteration 11470, loss = 0.04773795\n",
      "Iteration 11471, loss = 0.04773296\n",
      "Iteration 11472, loss = 0.04772790\n",
      "Iteration 11473, loss = 0.04772293\n",
      "Iteration 11474, loss = 0.04771795\n",
      "Iteration 11475, loss = 0.04771293\n",
      "Iteration 11476, loss = 0.04770798\n",
      "Iteration 11477, loss = 0.04770300\n",
      "Iteration 11478, loss = 0.04769798\n",
      "Iteration 11479, loss = 0.04769303\n",
      "Iteration 11480, loss = 0.04768800\n",
      "Iteration 11481, loss = 0.04768303\n",
      "Iteration 11482, loss = 0.04767801\n",
      "Iteration 11483, loss = 0.04767315\n",
      "Iteration 11484, loss = 0.04766812\n",
      "Iteration 11485, loss = 0.04766319\n",
      "Iteration 11486, loss = 0.04765819\n",
      "Iteration 11487, loss = 0.04765320\n",
      "Iteration 11488, loss = 0.04764817\n",
      "Iteration 11489, loss = 0.04764318\n",
      "Iteration 11490, loss = 0.04763834\n",
      "Iteration 11491, loss = 0.04763328\n",
      "Iteration 11492, loss = 0.04762830\n",
      "Iteration 11493, loss = 0.04762330\n",
      "Iteration 11494, loss = 0.04761833\n",
      "Iteration 11495, loss = 0.04761339\n",
      "Iteration 11496, loss = 0.04760850\n",
      "Iteration 11497, loss = 0.04760346\n",
      "Iteration 11498, loss = 0.04759848\n",
      "Iteration 11499, loss = 0.04759349\n",
      "Iteration 11500, loss = 0.04758861\n",
      "Iteration 11501, loss = 0.04758364\n",
      "Iteration 11502, loss = 0.04757863\n",
      "Iteration 11503, loss = 0.04757369\n",
      "Iteration 11504, loss = 0.04756880\n",
      "Iteration 11505, loss = 0.04756383\n",
      "Iteration 11506, loss = 0.04755882\n",
      "Iteration 11507, loss = 0.04755383\n",
      "Iteration 11508, loss = 0.04754891\n",
      "Iteration 11509, loss = 0.04754401\n",
      "Iteration 11510, loss = 0.04753905\n",
      "Iteration 11511, loss = 0.04753401\n",
      "Iteration 11512, loss = 0.04752905\n",
      "Iteration 11513, loss = 0.04752414\n",
      "Iteration 11514, loss = 0.04751918\n",
      "Iteration 11515, loss = 0.04751425\n",
      "Iteration 11516, loss = 0.04750930\n",
      "Iteration 11517, loss = 0.04750434\n",
      "Iteration 11518, loss = 0.04749947\n",
      "Iteration 11519, loss = 0.04749442\n",
      "Iteration 11520, loss = 0.04748951\n",
      "Iteration 11521, loss = 0.04748462\n",
      "Iteration 11522, loss = 0.04747964\n",
      "Iteration 11523, loss = 0.04747473\n",
      "Iteration 11524, loss = 0.04746985\n",
      "Iteration 11525, loss = 0.04746486\n",
      "Iteration 11526, loss = 0.04745993\n",
      "Iteration 11527, loss = 0.04745493\n",
      "Iteration 11528, loss = 0.04745009\n",
      "Iteration 11529, loss = 0.04744509\n",
      "Iteration 11530, loss = 0.04744026\n",
      "Iteration 11531, loss = 0.04743525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11532, loss = 0.04743037\n",
      "Iteration 11533, loss = 0.04742546\n",
      "Iteration 11534, loss = 0.04742052\n",
      "Iteration 11535, loss = 0.04741563\n",
      "Iteration 11536, loss = 0.04741066\n",
      "Iteration 11537, loss = 0.04740579\n",
      "Iteration 11538, loss = 0.04740082\n",
      "Iteration 11539, loss = 0.04739594\n",
      "Iteration 11540, loss = 0.04739101\n",
      "Iteration 11541, loss = 0.04738611\n",
      "Iteration 11542, loss = 0.04738118\n",
      "Iteration 11543, loss = 0.04737625\n",
      "Iteration 11544, loss = 0.04737138\n",
      "Iteration 11545, loss = 0.04736643\n",
      "Iteration 11546, loss = 0.04736154\n",
      "Iteration 11547, loss = 0.04735670\n",
      "Iteration 11548, loss = 0.04735169\n",
      "Iteration 11549, loss = 0.04734684\n",
      "Iteration 11550, loss = 0.04734196\n",
      "Iteration 11551, loss = 0.04733706\n",
      "Iteration 11552, loss = 0.04733214\n",
      "Iteration 11553, loss = 0.04732721\n",
      "Iteration 11554, loss = 0.04732235\n",
      "Iteration 11555, loss = 0.04731746\n",
      "Iteration 11556, loss = 0.04731256\n",
      "Iteration 11557, loss = 0.04730764\n",
      "Iteration 11558, loss = 0.04730277\n",
      "Iteration 11559, loss = 0.04729789\n",
      "Iteration 11560, loss = 0.04729299\n",
      "Iteration 11561, loss = 0.04728805\n",
      "Iteration 11562, loss = 0.04728314\n",
      "Iteration 11563, loss = 0.04727827\n",
      "Iteration 11564, loss = 0.04727339\n",
      "Iteration 11565, loss = 0.04726839\n",
      "Iteration 11566, loss = 0.04726354\n",
      "Iteration 11567, loss = 0.04725864\n",
      "Iteration 11568, loss = 0.04725374\n",
      "Iteration 11569, loss = 0.04724888\n",
      "Iteration 11570, loss = 0.04724397\n",
      "Iteration 11571, loss = 0.04723907\n",
      "Iteration 11572, loss = 0.04723418\n",
      "Iteration 11573, loss = 0.04722927\n",
      "Iteration 11574, loss = 0.04722440\n",
      "Iteration 11575, loss = 0.04721955\n",
      "Iteration 11576, loss = 0.04721456\n",
      "Iteration 11577, loss = 0.04720974\n",
      "Iteration 11578, loss = 0.04720491\n",
      "Iteration 11579, loss = 0.04720001\n",
      "Iteration 11580, loss = 0.04719505\n",
      "Iteration 11581, loss = 0.04719021\n",
      "Iteration 11582, loss = 0.04718528\n",
      "Iteration 11583, loss = 0.04718045\n",
      "Iteration 11584, loss = 0.04717558\n",
      "Iteration 11585, loss = 0.04717059\n",
      "Iteration 11586, loss = 0.04716572\n",
      "Iteration 11587, loss = 0.04716080\n",
      "Iteration 11588, loss = 0.04715593\n",
      "Iteration 11589, loss = 0.04715114\n",
      "Iteration 11590, loss = 0.04714620\n",
      "Iteration 11591, loss = 0.04714137\n",
      "Iteration 11592, loss = 0.04713639\n",
      "Iteration 11593, loss = 0.04713160\n",
      "Iteration 11594, loss = 0.04712668\n",
      "Iteration 11595, loss = 0.04712175\n",
      "Iteration 11596, loss = 0.04711696\n",
      "Iteration 11597, loss = 0.04711208\n",
      "Iteration 11598, loss = 0.04710722\n",
      "Iteration 11599, loss = 0.04710235\n",
      "Iteration 11600, loss = 0.04709747\n",
      "Iteration 11601, loss = 0.04709258\n",
      "Iteration 11602, loss = 0.04708782\n",
      "Iteration 11603, loss = 0.04708283\n",
      "Iteration 11604, loss = 0.04707800\n",
      "Iteration 11605, loss = 0.04707326\n",
      "Iteration 11606, loss = 0.04706823\n",
      "Iteration 11607, loss = 0.04706344\n",
      "Iteration 11608, loss = 0.04705860\n",
      "Iteration 11609, loss = 0.04705376\n",
      "Iteration 11610, loss = 0.04704897\n",
      "Iteration 11611, loss = 0.04704411\n",
      "Iteration 11612, loss = 0.04703925\n",
      "Iteration 11613, loss = 0.04703440\n",
      "Iteration 11614, loss = 0.04702953\n",
      "Iteration 11615, loss = 0.04702472\n",
      "Iteration 11616, loss = 0.04701983\n",
      "Iteration 11617, loss = 0.04701506\n",
      "Iteration 11618, loss = 0.04701019\n",
      "Iteration 11619, loss = 0.04700539\n",
      "Iteration 11620, loss = 0.04700051\n",
      "Iteration 11621, loss = 0.04699573\n",
      "Iteration 11622, loss = 0.04699085\n",
      "Iteration 11623, loss = 0.04698603\n",
      "Iteration 11624, loss = 0.04698122\n",
      "Iteration 11625, loss = 0.04697640\n",
      "Iteration 11626, loss = 0.04697159\n",
      "Iteration 11627, loss = 0.04696677\n",
      "Iteration 11628, loss = 0.04696193\n",
      "Iteration 11629, loss = 0.04695708\n",
      "Iteration 11630, loss = 0.04695224\n",
      "Iteration 11631, loss = 0.04694743\n",
      "Iteration 11632, loss = 0.04694261\n",
      "Iteration 11633, loss = 0.04693776\n",
      "Iteration 11634, loss = 0.04693286\n",
      "Iteration 11635, loss = 0.04692806\n",
      "Iteration 11636, loss = 0.04692324\n",
      "Iteration 11637, loss = 0.04691838\n",
      "Iteration 11638, loss = 0.04691360\n",
      "Iteration 11639, loss = 0.04690878\n",
      "Iteration 11640, loss = 0.04690403\n",
      "Iteration 11641, loss = 0.04689917\n",
      "Iteration 11642, loss = 0.04689432\n",
      "Iteration 11643, loss = 0.04688953\n",
      "Iteration 11644, loss = 0.04688471\n",
      "Iteration 11645, loss = 0.04687995\n",
      "Iteration 11646, loss = 0.04687509\n",
      "Iteration 11647, loss = 0.04687034\n",
      "Iteration 11648, loss = 0.04686545\n",
      "Iteration 11649, loss = 0.04686071\n",
      "Iteration 11650, loss = 0.04685581\n",
      "Iteration 11651, loss = 0.04685097\n",
      "Iteration 11652, loss = 0.04684623\n",
      "Iteration 11653, loss = 0.04684135\n",
      "Iteration 11654, loss = 0.04683661\n",
      "Iteration 11655, loss = 0.04683175\n",
      "Iteration 11656, loss = 0.04682694\n",
      "Iteration 11657, loss = 0.04682212\n",
      "Iteration 11658, loss = 0.04681734\n",
      "Iteration 11659, loss = 0.04681252\n",
      "Iteration 11660, loss = 0.04680779\n",
      "Iteration 11661, loss = 0.04680294\n",
      "Iteration 11662, loss = 0.04679809\n",
      "Iteration 11663, loss = 0.04679332\n",
      "Iteration 11664, loss = 0.04678852\n",
      "Iteration 11665, loss = 0.04678373\n",
      "Iteration 11666, loss = 0.04677892\n",
      "Iteration 11667, loss = 0.04677415\n",
      "Iteration 11668, loss = 0.04676940\n",
      "Iteration 11669, loss = 0.04676462\n",
      "Iteration 11670, loss = 0.04675984\n",
      "Iteration 11671, loss = 0.04675505\n",
      "Iteration 11672, loss = 0.04675028\n",
      "Iteration 11673, loss = 0.04674546\n",
      "Iteration 11674, loss = 0.04674073\n",
      "Iteration 11675, loss = 0.04673599\n",
      "Iteration 11676, loss = 0.04673116\n",
      "Iteration 11677, loss = 0.04672640\n",
      "Iteration 11678, loss = 0.04672166\n",
      "Iteration 11679, loss = 0.04671688\n",
      "Iteration 11680, loss = 0.04671208\n",
      "Iteration 11681, loss = 0.04670737\n",
      "Iteration 11682, loss = 0.04670255\n",
      "Iteration 11683, loss = 0.04669773\n",
      "Iteration 11684, loss = 0.04669300\n",
      "Iteration 11685, loss = 0.04668819\n",
      "Iteration 11686, loss = 0.04668341\n",
      "Iteration 11687, loss = 0.04667872\n",
      "Iteration 11688, loss = 0.04667392\n",
      "Iteration 11689, loss = 0.04666915\n",
      "Iteration 11690, loss = 0.04666441\n",
      "Iteration 11691, loss = 0.04665956\n",
      "Iteration 11692, loss = 0.04665486\n",
      "Iteration 11693, loss = 0.04665009\n",
      "Iteration 11694, loss = 0.04664534\n",
      "Iteration 11695, loss = 0.04664051\n",
      "Iteration 11696, loss = 0.04663575\n",
      "Iteration 11697, loss = 0.04663104\n",
      "Iteration 11698, loss = 0.04662623\n",
      "Iteration 11699, loss = 0.04662153\n",
      "Iteration 11700, loss = 0.04661670\n",
      "Iteration 11701, loss = 0.04661197\n",
      "Iteration 11702, loss = 0.04660721\n",
      "Iteration 11703, loss = 0.04660247\n",
      "Iteration 11704, loss = 0.04659764\n",
      "Iteration 11705, loss = 0.04659294\n",
      "Iteration 11706, loss = 0.04658814\n",
      "Iteration 11707, loss = 0.04658340\n",
      "Iteration 11708, loss = 0.04657863\n",
      "Iteration 11709, loss = 0.04657385\n",
      "Iteration 11710, loss = 0.04656911\n",
      "Iteration 11711, loss = 0.04656442\n",
      "Iteration 11712, loss = 0.04655961\n",
      "Iteration 11713, loss = 0.04655491\n",
      "Iteration 11714, loss = 0.04655018\n",
      "Iteration 11715, loss = 0.04654533\n",
      "Iteration 11716, loss = 0.04654075\n",
      "Iteration 11717, loss = 0.04653600\n",
      "Iteration 11718, loss = 0.04653128\n",
      "Iteration 11719, loss = 0.04652655\n",
      "Iteration 11720, loss = 0.04652180\n",
      "Iteration 11721, loss = 0.04651704\n",
      "Iteration 11722, loss = 0.04651235\n",
      "Iteration 11723, loss = 0.04650767\n",
      "Iteration 11724, loss = 0.04650294\n",
      "Iteration 11725, loss = 0.04649821\n",
      "Iteration 11726, loss = 0.04649344\n",
      "Iteration 11727, loss = 0.04648878\n",
      "Iteration 11728, loss = 0.04648405\n",
      "Iteration 11729, loss = 0.04647926\n",
      "Iteration 11730, loss = 0.04647457\n",
      "Iteration 11731, loss = 0.04646983\n",
      "Iteration 11732, loss = 0.04646510\n",
      "Iteration 11733, loss = 0.04646042\n",
      "Iteration 11734, loss = 0.04645561\n",
      "Iteration 11735, loss = 0.04645094\n",
      "Iteration 11736, loss = 0.04644627\n",
      "Iteration 11737, loss = 0.04644149\n",
      "Iteration 11738, loss = 0.04643679\n",
      "Iteration 11739, loss = 0.04643209\n",
      "Iteration 11740, loss = 0.04642733\n",
      "Iteration 11741, loss = 0.04642264\n",
      "Iteration 11742, loss = 0.04641789\n",
      "Iteration 11743, loss = 0.04641319\n",
      "Iteration 11744, loss = 0.04640842\n",
      "Iteration 11745, loss = 0.04640375\n",
      "Iteration 11746, loss = 0.04639904\n",
      "Iteration 11747, loss = 0.04639436\n",
      "Iteration 11748, loss = 0.04638954\n",
      "Iteration 11749, loss = 0.04638490\n",
      "Iteration 11750, loss = 0.04638019\n",
      "Iteration 11751, loss = 0.04637547\n",
      "Iteration 11752, loss = 0.04637075\n",
      "Iteration 11753, loss = 0.04636601\n",
      "Iteration 11754, loss = 0.04636133\n",
      "Iteration 11755, loss = 0.04635671\n",
      "Iteration 11756, loss = 0.04635189\n",
      "Iteration 11757, loss = 0.04634724\n",
      "Iteration 11758, loss = 0.04634253\n",
      "Iteration 11759, loss = 0.04633780\n",
      "Iteration 11760, loss = 0.04633317\n",
      "Iteration 11761, loss = 0.04632843\n",
      "Iteration 11762, loss = 0.04632375\n",
      "Iteration 11763, loss = 0.04631909\n",
      "Iteration 11764, loss = 0.04631435\n",
      "Iteration 11765, loss = 0.04630976\n",
      "Iteration 11766, loss = 0.04630505\n",
      "Iteration 11767, loss = 0.04630033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11768, loss = 0.04629565\n",
      "Iteration 11769, loss = 0.04629091\n",
      "Iteration 11770, loss = 0.04628630\n",
      "Iteration 11771, loss = 0.04628163\n",
      "Iteration 11772, loss = 0.04627695\n",
      "Iteration 11773, loss = 0.04627227\n",
      "Iteration 11774, loss = 0.04626758\n",
      "Iteration 11775, loss = 0.04626286\n",
      "Iteration 11776, loss = 0.04625821\n",
      "Iteration 11777, loss = 0.04625352\n",
      "Iteration 11778, loss = 0.04624882\n",
      "Iteration 11779, loss = 0.04624409\n",
      "Iteration 11780, loss = 0.04623947\n",
      "Iteration 11781, loss = 0.04623478\n",
      "Iteration 11782, loss = 0.04623007\n",
      "Iteration 11783, loss = 0.04622539\n",
      "Iteration 11784, loss = 0.04622074\n",
      "Iteration 11785, loss = 0.04621603\n",
      "Iteration 11786, loss = 0.04621139\n",
      "Iteration 11787, loss = 0.04620667\n",
      "Iteration 11788, loss = 0.04620203\n",
      "Iteration 11789, loss = 0.04619735\n",
      "Iteration 11790, loss = 0.04619265\n",
      "Iteration 11791, loss = 0.04618798\n",
      "Iteration 11792, loss = 0.04618334\n",
      "Iteration 11793, loss = 0.04617865\n",
      "Iteration 11794, loss = 0.04617398\n",
      "Iteration 11795, loss = 0.04616942\n",
      "Iteration 11796, loss = 0.04616468\n",
      "Iteration 11797, loss = 0.04615998\n",
      "Iteration 11798, loss = 0.04615530\n",
      "Iteration 11799, loss = 0.04615066\n",
      "Iteration 11800, loss = 0.04614601\n",
      "Iteration 11801, loss = 0.04614137\n",
      "Iteration 11802, loss = 0.04613670\n",
      "Iteration 11803, loss = 0.04613206\n",
      "Iteration 11804, loss = 0.04612735\n",
      "Iteration 11805, loss = 0.04612280\n",
      "Iteration 11806, loss = 0.04611811\n",
      "Iteration 11807, loss = 0.04611344\n",
      "Iteration 11808, loss = 0.04610874\n",
      "Iteration 11809, loss = 0.04610417\n",
      "Iteration 11810, loss = 0.04609949\n",
      "Iteration 11811, loss = 0.04609488\n",
      "Iteration 11812, loss = 0.04609022\n",
      "Iteration 11813, loss = 0.04608552\n",
      "Iteration 11814, loss = 0.04608090\n",
      "Iteration 11815, loss = 0.04607629\n",
      "Iteration 11816, loss = 0.04607158\n",
      "Iteration 11817, loss = 0.04606696\n",
      "Iteration 11818, loss = 0.04606231\n",
      "Iteration 11819, loss = 0.04605766\n",
      "Iteration 11820, loss = 0.04605306\n",
      "Iteration 11821, loss = 0.04604842\n",
      "Iteration 11822, loss = 0.04604374\n",
      "Iteration 11823, loss = 0.04603916\n",
      "Iteration 11824, loss = 0.04603448\n",
      "Iteration 11825, loss = 0.04602990\n",
      "Iteration 11826, loss = 0.04602526\n",
      "Iteration 11827, loss = 0.04602063\n",
      "Iteration 11828, loss = 0.04601592\n",
      "Iteration 11829, loss = 0.04601130\n",
      "Iteration 11830, loss = 0.04600672\n",
      "Iteration 11831, loss = 0.04600207\n",
      "Iteration 11832, loss = 0.04599746\n",
      "Iteration 11833, loss = 0.04599285\n",
      "Iteration 11834, loss = 0.04598816\n",
      "Iteration 11835, loss = 0.04598354\n",
      "Iteration 11836, loss = 0.04597895\n",
      "Iteration 11837, loss = 0.04597430\n",
      "Iteration 11838, loss = 0.04596966\n",
      "Iteration 11839, loss = 0.04596502\n",
      "Iteration 11840, loss = 0.04596041\n",
      "Iteration 11841, loss = 0.04595580\n",
      "Iteration 11842, loss = 0.04595117\n",
      "Iteration 11843, loss = 0.04594652\n",
      "Iteration 11844, loss = 0.04594190\n",
      "Iteration 11845, loss = 0.04593731\n",
      "Iteration 11846, loss = 0.04593269\n",
      "Iteration 11847, loss = 0.04592809\n",
      "Iteration 11848, loss = 0.04592348\n",
      "Iteration 11849, loss = 0.04591888\n",
      "Iteration 11850, loss = 0.04591420\n",
      "Iteration 11851, loss = 0.04590961\n",
      "Iteration 11852, loss = 0.04590504\n",
      "Iteration 11853, loss = 0.04590042\n",
      "Iteration 11854, loss = 0.04589580\n",
      "Iteration 11855, loss = 0.04589115\n",
      "Iteration 11856, loss = 0.04588661\n",
      "Iteration 11857, loss = 0.04588197\n",
      "Iteration 11858, loss = 0.04587737\n",
      "Iteration 11859, loss = 0.04587278\n",
      "Iteration 11860, loss = 0.04586816\n",
      "Iteration 11861, loss = 0.04586357\n",
      "Iteration 11862, loss = 0.04585896\n",
      "Iteration 11863, loss = 0.04585439\n",
      "Iteration 11864, loss = 0.04584979\n",
      "Iteration 11865, loss = 0.04584518\n",
      "Iteration 11866, loss = 0.04584056\n",
      "Iteration 11867, loss = 0.04583599\n",
      "Iteration 11868, loss = 0.04583140\n",
      "Iteration 11869, loss = 0.04582679\n",
      "Iteration 11870, loss = 0.04582219\n",
      "Iteration 11871, loss = 0.04581760\n",
      "Iteration 11872, loss = 0.04581297\n",
      "Iteration 11873, loss = 0.04580842\n",
      "Iteration 11874, loss = 0.04580381\n",
      "Iteration 11875, loss = 0.04579923\n",
      "Iteration 11876, loss = 0.04579469\n",
      "Iteration 11877, loss = 0.04579005\n",
      "Iteration 11878, loss = 0.04578542\n",
      "Iteration 11879, loss = 0.04578088\n",
      "Iteration 11880, loss = 0.04577632\n",
      "Iteration 11881, loss = 0.04577168\n",
      "Iteration 11882, loss = 0.04576721\n",
      "Iteration 11883, loss = 0.04576250\n",
      "Iteration 11884, loss = 0.04575797\n",
      "Iteration 11885, loss = 0.04575337\n",
      "Iteration 11886, loss = 0.04574879\n",
      "Iteration 11887, loss = 0.04574423\n",
      "Iteration 11888, loss = 0.04573973\n",
      "Iteration 11889, loss = 0.04573508\n",
      "Iteration 11890, loss = 0.04573053\n",
      "Iteration 11891, loss = 0.04572596\n",
      "Iteration 11892, loss = 0.04572134\n",
      "Iteration 11893, loss = 0.04571680\n",
      "Iteration 11894, loss = 0.04571213\n",
      "Iteration 11895, loss = 0.04570765\n",
      "Iteration 11896, loss = 0.04570313\n",
      "Iteration 11897, loss = 0.04569854\n",
      "Iteration 11898, loss = 0.04569386\n",
      "Iteration 11899, loss = 0.04568936\n",
      "Iteration 11900, loss = 0.04568475\n",
      "Iteration 11901, loss = 0.04568017\n",
      "Iteration 11902, loss = 0.04567559\n",
      "Iteration 11903, loss = 0.04567103\n",
      "Iteration 11904, loss = 0.04566648\n",
      "Iteration 11905, loss = 0.04566189\n",
      "Iteration 11906, loss = 0.04565733\n",
      "Iteration 11907, loss = 0.04565273\n",
      "Iteration 11908, loss = 0.04564822\n",
      "Iteration 11909, loss = 0.04564367\n",
      "Iteration 11910, loss = 0.04563905\n",
      "Iteration 11911, loss = 0.04563452\n",
      "Iteration 11912, loss = 0.04562994\n",
      "Iteration 11913, loss = 0.04562547\n",
      "Iteration 11914, loss = 0.04562080\n",
      "Iteration 11915, loss = 0.04561632\n",
      "Iteration 11916, loss = 0.04561173\n",
      "Iteration 11917, loss = 0.04560715\n",
      "Iteration 11918, loss = 0.04560259\n",
      "Iteration 11919, loss = 0.04559808\n",
      "Iteration 11920, loss = 0.04559353\n",
      "Iteration 11921, loss = 0.04558905\n",
      "Iteration 11922, loss = 0.04558443\n",
      "Iteration 11923, loss = 0.04557989\n",
      "Iteration 11924, loss = 0.04557538\n",
      "Iteration 11925, loss = 0.04557081\n",
      "Iteration 11926, loss = 0.04556633\n",
      "Iteration 11927, loss = 0.04556178\n",
      "Iteration 11928, loss = 0.04555733\n",
      "Iteration 11929, loss = 0.04555274\n",
      "Iteration 11930, loss = 0.04554818\n",
      "Iteration 11931, loss = 0.04554365\n",
      "Iteration 11932, loss = 0.04553912\n",
      "Iteration 11933, loss = 0.04553461\n",
      "Iteration 11934, loss = 0.04553009\n",
      "Iteration 11935, loss = 0.04552551\n",
      "Iteration 11936, loss = 0.04552104\n",
      "Iteration 11937, loss = 0.04551647\n",
      "Iteration 11938, loss = 0.04551198\n",
      "Iteration 11939, loss = 0.04550740\n",
      "Iteration 11940, loss = 0.04550282\n",
      "Iteration 11941, loss = 0.04549835\n",
      "Iteration 11942, loss = 0.04549377\n",
      "Iteration 11943, loss = 0.04548922\n",
      "Iteration 11944, loss = 0.04548472\n",
      "Iteration 11945, loss = 0.04548017\n",
      "Iteration 11946, loss = 0.04547565\n",
      "Iteration 11947, loss = 0.04547115\n",
      "Iteration 11948, loss = 0.04546658\n",
      "Iteration 11949, loss = 0.04546208\n",
      "Iteration 11950, loss = 0.04545752\n",
      "Iteration 11951, loss = 0.04545301\n",
      "Iteration 11952, loss = 0.04544843\n",
      "Iteration 11953, loss = 0.04544393\n",
      "Iteration 11954, loss = 0.04543938\n",
      "Iteration 11955, loss = 0.04543485\n",
      "Iteration 11956, loss = 0.04543030\n",
      "Iteration 11957, loss = 0.04542582\n",
      "Iteration 11958, loss = 0.04542125\n",
      "Iteration 11959, loss = 0.04541678\n",
      "Iteration 11960, loss = 0.04541224\n",
      "Iteration 11961, loss = 0.04540766\n",
      "Iteration 11962, loss = 0.04540317\n",
      "Iteration 11963, loss = 0.04539863\n",
      "Iteration 11964, loss = 0.04539416\n",
      "Iteration 11965, loss = 0.04538956\n",
      "Iteration 11966, loss = 0.04538509\n",
      "Iteration 11967, loss = 0.04538056\n",
      "Iteration 11968, loss = 0.04537606\n",
      "Iteration 11969, loss = 0.04537156\n",
      "Iteration 11970, loss = 0.04536699\n",
      "Iteration 11971, loss = 0.04536251\n",
      "Iteration 11972, loss = 0.04535800\n",
      "Iteration 11973, loss = 0.04535349\n",
      "Iteration 11974, loss = 0.04534895\n",
      "Iteration 11975, loss = 0.04534447\n",
      "Iteration 11976, loss = 0.04533996\n",
      "Iteration 11977, loss = 0.04533540\n",
      "Iteration 11978, loss = 0.04533093\n",
      "Iteration 11979, loss = 0.04532638\n",
      "Iteration 11980, loss = 0.04532190\n",
      "Iteration 11981, loss = 0.04531738\n",
      "Iteration 11982, loss = 0.04531285\n",
      "Iteration 11983, loss = 0.04530837\n",
      "Iteration 11984, loss = 0.04530388\n",
      "Iteration 11985, loss = 0.04529936\n",
      "Iteration 11986, loss = 0.04529485\n",
      "Iteration 11987, loss = 0.04529045\n",
      "Iteration 11988, loss = 0.04528587\n",
      "Iteration 11989, loss = 0.04528143\n",
      "Iteration 11990, loss = 0.04527690\n",
      "Iteration 11991, loss = 0.04527248\n",
      "Iteration 11992, loss = 0.04526798\n",
      "Iteration 11993, loss = 0.04526346\n",
      "Iteration 11994, loss = 0.04525897\n",
      "Iteration 11995, loss = 0.04525448\n",
      "Iteration 11996, loss = 0.04524998\n",
      "Iteration 11997, loss = 0.04524554\n",
      "Iteration 11998, loss = 0.04524107\n",
      "Iteration 11999, loss = 0.04523647\n",
      "Iteration 12000, loss = 0.04523206\n",
      "Iteration 12001, loss = 0.04522758\n",
      "Iteration 12002, loss = 0.04522310\n",
      "Iteration 12003, loss = 0.04521860\n",
      "Iteration 12004, loss = 0.04521412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12005, loss = 0.04520962\n",
      "Iteration 12006, loss = 0.04520516\n",
      "Iteration 12007, loss = 0.04520068\n",
      "Iteration 12008, loss = 0.04519617\n",
      "Iteration 12009, loss = 0.04519169\n",
      "Iteration 12010, loss = 0.04518722\n",
      "Iteration 12011, loss = 0.04518275\n",
      "Iteration 12012, loss = 0.04517828\n",
      "Iteration 12013, loss = 0.04517378\n",
      "Iteration 12014, loss = 0.04516936\n",
      "Iteration 12015, loss = 0.04516494\n",
      "Iteration 12016, loss = 0.04516041\n",
      "Iteration 12017, loss = 0.04515600\n",
      "Iteration 12018, loss = 0.04515144\n",
      "Iteration 12019, loss = 0.04514696\n",
      "Iteration 12020, loss = 0.04514246\n",
      "Iteration 12021, loss = 0.04513800\n",
      "Iteration 12022, loss = 0.04513355\n",
      "Iteration 12023, loss = 0.04512908\n",
      "Iteration 12024, loss = 0.04512461\n",
      "Iteration 12025, loss = 0.04512020\n",
      "Iteration 12026, loss = 0.04511570\n",
      "Iteration 12027, loss = 0.04511120\n",
      "Iteration 12028, loss = 0.04510676\n",
      "Iteration 12029, loss = 0.04510234\n",
      "Iteration 12030, loss = 0.04509785\n",
      "Iteration 12031, loss = 0.04509338\n",
      "Iteration 12032, loss = 0.04508894\n",
      "Iteration 12033, loss = 0.04508451\n",
      "Iteration 12034, loss = 0.04508002\n",
      "Iteration 12035, loss = 0.04507561\n",
      "Iteration 12036, loss = 0.04507116\n",
      "Iteration 12037, loss = 0.04506671\n",
      "Iteration 12038, loss = 0.04506224\n",
      "Iteration 12039, loss = 0.04505775\n",
      "Iteration 12040, loss = 0.04505329\n",
      "Iteration 12041, loss = 0.04504882\n",
      "Iteration 12042, loss = 0.04504440\n",
      "Iteration 12043, loss = 0.04503996\n",
      "Iteration 12044, loss = 0.04503553\n",
      "Iteration 12045, loss = 0.04503107\n",
      "Iteration 12046, loss = 0.04502660\n",
      "Iteration 12047, loss = 0.04502218\n",
      "Iteration 12048, loss = 0.04501774\n",
      "Iteration 12049, loss = 0.04501331\n",
      "Iteration 12050, loss = 0.04500882\n",
      "Iteration 12051, loss = 0.04500439\n",
      "Iteration 12052, loss = 0.04499991\n",
      "Iteration 12053, loss = 0.04499547\n",
      "Iteration 12054, loss = 0.04499105\n",
      "Iteration 12055, loss = 0.04498658\n",
      "Iteration 12056, loss = 0.04498215\n",
      "Iteration 12057, loss = 0.04497768\n",
      "Iteration 12058, loss = 0.04497329\n",
      "Iteration 12059, loss = 0.04496881\n",
      "Iteration 12060, loss = 0.04496441\n",
      "Iteration 12061, loss = 0.04495995\n",
      "Iteration 12062, loss = 0.04495556\n",
      "Iteration 12063, loss = 0.04495106\n",
      "Iteration 12064, loss = 0.04494668\n",
      "Iteration 12065, loss = 0.04494222\n",
      "Iteration 12066, loss = 0.04493778\n",
      "Iteration 12067, loss = 0.04493334\n",
      "Iteration 12068, loss = 0.04492896\n",
      "Iteration 12069, loss = 0.04492448\n",
      "Iteration 12070, loss = 0.04492008\n",
      "Iteration 12071, loss = 0.04491571\n",
      "Iteration 12072, loss = 0.04491121\n",
      "Iteration 12073, loss = 0.04490678\n",
      "Iteration 12074, loss = 0.04490241\n",
      "Iteration 12075, loss = 0.04489794\n",
      "Iteration 12076, loss = 0.04489359\n",
      "Iteration 12077, loss = 0.04488912\n",
      "Iteration 12078, loss = 0.04488468\n",
      "Iteration 12079, loss = 0.04488033\n",
      "Iteration 12080, loss = 0.04487582\n",
      "Iteration 12081, loss = 0.04487144\n",
      "Iteration 12082, loss = 0.04486700\n",
      "Iteration 12083, loss = 0.04486265\n",
      "Iteration 12084, loss = 0.04485824\n",
      "Iteration 12085, loss = 0.04485379\n",
      "Iteration 12086, loss = 0.04484936\n",
      "Iteration 12087, loss = 0.04484496\n",
      "Iteration 12088, loss = 0.04484050\n",
      "Iteration 12089, loss = 0.04483611\n",
      "Iteration 12090, loss = 0.04483167\n",
      "Iteration 12091, loss = 0.04482730\n",
      "Iteration 12092, loss = 0.04482290\n",
      "Iteration 12093, loss = 0.04481844\n",
      "Iteration 12094, loss = 0.04481406\n",
      "Iteration 12095, loss = 0.04480964\n",
      "Iteration 12096, loss = 0.04480523\n",
      "Iteration 12097, loss = 0.04480091\n",
      "Iteration 12098, loss = 0.04479639\n",
      "Iteration 12099, loss = 0.04479202\n",
      "Iteration 12100, loss = 0.04478762\n",
      "Iteration 12101, loss = 0.04478325\n",
      "Iteration 12102, loss = 0.04477882\n",
      "Iteration 12103, loss = 0.04477441\n",
      "Iteration 12104, loss = 0.04477005\n",
      "Iteration 12105, loss = 0.04476563\n",
      "Iteration 12106, loss = 0.04476126\n",
      "Iteration 12107, loss = 0.04475685\n",
      "Iteration 12108, loss = 0.04475252\n",
      "Iteration 12109, loss = 0.04474805\n",
      "Iteration 12110, loss = 0.04474369\n",
      "Iteration 12111, loss = 0.04473931\n",
      "Iteration 12112, loss = 0.04473489\n",
      "Iteration 12113, loss = 0.04473054\n",
      "Iteration 12114, loss = 0.04472617\n",
      "Iteration 12115, loss = 0.04472179\n",
      "Iteration 12116, loss = 0.04471737\n",
      "Iteration 12117, loss = 0.04471300\n",
      "Iteration 12118, loss = 0.04470859\n",
      "Iteration 12119, loss = 0.04470424\n",
      "Iteration 12120, loss = 0.04469982\n",
      "Iteration 12121, loss = 0.04469549\n",
      "Iteration 12122, loss = 0.04469112\n",
      "Iteration 12123, loss = 0.04468679\n",
      "Iteration 12124, loss = 0.04468234\n",
      "Iteration 12125, loss = 0.04467799\n",
      "Iteration 12126, loss = 0.04467367\n",
      "Iteration 12127, loss = 0.04466923\n",
      "Iteration 12128, loss = 0.04466494\n",
      "Iteration 12129, loss = 0.04466047\n",
      "Iteration 12130, loss = 0.04465615\n",
      "Iteration 12131, loss = 0.04465176\n",
      "Iteration 12132, loss = 0.04464739\n",
      "Iteration 12133, loss = 0.04464306\n",
      "Iteration 12134, loss = 0.04463869\n",
      "Iteration 12135, loss = 0.04463436\n",
      "Iteration 12136, loss = 0.04462997\n",
      "Iteration 12137, loss = 0.04462559\n",
      "Iteration 12138, loss = 0.04462129\n",
      "Iteration 12139, loss = 0.04461689\n",
      "Iteration 12140, loss = 0.04461257\n",
      "Iteration 12141, loss = 0.04460818\n",
      "Iteration 12142, loss = 0.04460386\n",
      "Iteration 12143, loss = 0.04459944\n",
      "Iteration 12144, loss = 0.04459510\n",
      "Iteration 12145, loss = 0.04459073\n",
      "Iteration 12146, loss = 0.04458637\n",
      "Iteration 12147, loss = 0.04458204\n",
      "Iteration 12148, loss = 0.04457766\n",
      "Iteration 12149, loss = 0.04457333\n",
      "Iteration 12150, loss = 0.04456897\n",
      "Iteration 12151, loss = 0.04456464\n",
      "Iteration 12152, loss = 0.04456030\n",
      "Iteration 12153, loss = 0.04455584\n",
      "Iteration 12154, loss = 0.04455150\n",
      "Iteration 12155, loss = 0.04454720\n",
      "Iteration 12156, loss = 0.04454286\n",
      "Iteration 12157, loss = 0.04453847\n",
      "Iteration 12158, loss = 0.04453416\n",
      "Iteration 12159, loss = 0.04452980\n",
      "Iteration 12160, loss = 0.04452548\n",
      "Iteration 12161, loss = 0.04452112\n",
      "Iteration 12162, loss = 0.04451685\n",
      "Iteration 12163, loss = 0.04451246\n",
      "Iteration 12164, loss = 0.04450813\n",
      "Iteration 12165, loss = 0.04450381\n",
      "Iteration 12166, loss = 0.04449945\n",
      "Iteration 12167, loss = 0.04449518\n",
      "Iteration 12168, loss = 0.04449081\n",
      "Iteration 12169, loss = 0.04448647\n",
      "Iteration 12170, loss = 0.04448215\n",
      "Iteration 12171, loss = 0.04447778\n",
      "Iteration 12172, loss = 0.04447343\n",
      "Iteration 12173, loss = 0.04446916\n",
      "Iteration 12174, loss = 0.04446483\n",
      "Iteration 12175, loss = 0.04446053\n",
      "Iteration 12176, loss = 0.04445612\n",
      "Iteration 12177, loss = 0.04445182\n",
      "Iteration 12178, loss = 0.04444751\n",
      "Iteration 12179, loss = 0.04444314\n",
      "Iteration 12180, loss = 0.04443881\n",
      "Iteration 12181, loss = 0.04443452\n",
      "Iteration 12182, loss = 0.04443015\n",
      "Iteration 12183, loss = 0.04442586\n",
      "Iteration 12184, loss = 0.04442151\n",
      "Iteration 12185, loss = 0.04441719\n",
      "Iteration 12186, loss = 0.04441287\n",
      "Iteration 12187, loss = 0.04440854\n",
      "Iteration 12188, loss = 0.04440421\n",
      "Iteration 12189, loss = 0.04439988\n",
      "Iteration 12190, loss = 0.04439556\n",
      "Iteration 12191, loss = 0.04439120\n",
      "Iteration 12192, loss = 0.04438692\n",
      "Iteration 12193, loss = 0.04438259\n",
      "Iteration 12194, loss = 0.04437825\n",
      "Iteration 12195, loss = 0.04437397\n",
      "Iteration 12196, loss = 0.04436963\n",
      "Iteration 12197, loss = 0.04436530\n",
      "Iteration 12198, loss = 0.04436116\n",
      "Iteration 12199, loss = 0.04435671\n",
      "Iteration 12200, loss = 0.04435239\n",
      "Iteration 12201, loss = 0.04434804\n",
      "Iteration 12202, loss = 0.04434381\n",
      "Iteration 12203, loss = 0.04433948\n",
      "Iteration 12204, loss = 0.04433514\n",
      "Iteration 12205, loss = 0.04433085\n",
      "Iteration 12206, loss = 0.04432660\n",
      "Iteration 12207, loss = 0.04432228\n",
      "Iteration 12208, loss = 0.04431796\n",
      "Iteration 12209, loss = 0.04431364\n",
      "Iteration 12210, loss = 0.04430939\n",
      "Iteration 12211, loss = 0.04430503\n",
      "Iteration 12212, loss = 0.04430079\n",
      "Iteration 12213, loss = 0.04429651\n",
      "Iteration 12214, loss = 0.04429222\n",
      "Iteration 12215, loss = 0.04428789\n",
      "Iteration 12216, loss = 0.04428367\n",
      "Iteration 12217, loss = 0.04427931\n",
      "Iteration 12218, loss = 0.04427502\n",
      "Iteration 12219, loss = 0.04427076\n",
      "Iteration 12220, loss = 0.04426644\n",
      "Iteration 12221, loss = 0.04426211\n",
      "Iteration 12222, loss = 0.04425788\n",
      "Iteration 12223, loss = 0.04425351\n",
      "Iteration 12224, loss = 0.04424922\n",
      "Iteration 12225, loss = 0.04424496\n",
      "Iteration 12226, loss = 0.04424066\n",
      "Iteration 12227, loss = 0.04423639\n",
      "Iteration 12228, loss = 0.04423202\n",
      "Iteration 12229, loss = 0.04422776\n",
      "Iteration 12230, loss = 0.04422350\n",
      "Iteration 12231, loss = 0.04421920\n",
      "Iteration 12232, loss = 0.04421492\n",
      "Iteration 12233, loss = 0.04421062\n",
      "Iteration 12234, loss = 0.04420633\n",
      "Iteration 12235, loss = 0.04420204\n",
      "Iteration 12236, loss = 0.04419778\n",
      "Iteration 12237, loss = 0.04419342\n",
      "Iteration 12238, loss = 0.04418927\n",
      "Iteration 12239, loss = 0.04418490\n",
      "Iteration 12240, loss = 0.04418058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12241, loss = 0.04417634\n",
      "Iteration 12242, loss = 0.04417207\n",
      "Iteration 12243, loss = 0.04416779\n",
      "Iteration 12244, loss = 0.04416351\n",
      "Iteration 12245, loss = 0.04415922\n",
      "Iteration 12246, loss = 0.04415500\n",
      "Iteration 12247, loss = 0.04415070\n",
      "Iteration 12248, loss = 0.04414649\n",
      "Iteration 12249, loss = 0.04414219\n",
      "Iteration 12250, loss = 0.04413789\n",
      "Iteration 12251, loss = 0.04413368\n",
      "Iteration 12252, loss = 0.04412940\n",
      "Iteration 12253, loss = 0.04412517\n",
      "Iteration 12254, loss = 0.04412093\n",
      "Iteration 12255, loss = 0.04411666\n",
      "Iteration 12256, loss = 0.04411238\n",
      "Iteration 12257, loss = 0.04410815\n",
      "Iteration 12258, loss = 0.04410393\n",
      "Iteration 12259, loss = 0.04409960\n",
      "Iteration 12260, loss = 0.04409538\n",
      "Iteration 12261, loss = 0.04409113\n",
      "Iteration 12262, loss = 0.04408688\n",
      "Iteration 12263, loss = 0.04408258\n",
      "Iteration 12264, loss = 0.04407836\n",
      "Iteration 12265, loss = 0.04407409\n",
      "Iteration 12266, loss = 0.04406983\n",
      "Iteration 12267, loss = 0.04406560\n",
      "Iteration 12268, loss = 0.04406134\n",
      "Iteration 12269, loss = 0.04405705\n",
      "Iteration 12270, loss = 0.04405282\n",
      "Iteration 12271, loss = 0.04404861\n",
      "Iteration 12272, loss = 0.04404435\n",
      "Iteration 12273, loss = 0.04404007\n",
      "Iteration 12274, loss = 0.04403587\n",
      "Iteration 12275, loss = 0.04403160\n",
      "Iteration 12276, loss = 0.04402742\n",
      "Iteration 12277, loss = 0.04402314\n",
      "Iteration 12278, loss = 0.04401895\n",
      "Iteration 12279, loss = 0.04401469\n",
      "Iteration 12280, loss = 0.04401052\n",
      "Iteration 12281, loss = 0.04400633\n",
      "Iteration 12282, loss = 0.04400207\n",
      "Iteration 12283, loss = 0.04399781\n",
      "Iteration 12284, loss = 0.04399358\n",
      "Iteration 12285, loss = 0.04398941\n",
      "Iteration 12286, loss = 0.04398515\n",
      "Iteration 12287, loss = 0.04398092\n",
      "Iteration 12288, loss = 0.04397672\n",
      "Iteration 12289, loss = 0.04397255\n",
      "Iteration 12290, loss = 0.04396827\n",
      "Iteration 12291, loss = 0.04396406\n",
      "Iteration 12292, loss = 0.04395983\n",
      "Iteration 12293, loss = 0.04395558\n",
      "Iteration 12294, loss = 0.04395132\n",
      "Iteration 12295, loss = 0.04394715\n",
      "Iteration 12296, loss = 0.04394284\n",
      "Iteration 12297, loss = 0.04393869\n",
      "Iteration 12298, loss = 0.04393439\n",
      "Iteration 12299, loss = 0.04393019\n",
      "Iteration 12300, loss = 0.04392591\n",
      "Iteration 12301, loss = 0.04392175\n",
      "Iteration 12302, loss = 0.04391754\n",
      "Iteration 12303, loss = 0.04391327\n",
      "Iteration 12304, loss = 0.04390902\n",
      "Iteration 12305, loss = 0.04390487\n",
      "Iteration 12306, loss = 0.04390062\n",
      "Iteration 12307, loss = 0.04389630\n",
      "Iteration 12308, loss = 0.04389213\n",
      "Iteration 12309, loss = 0.04388787\n",
      "Iteration 12310, loss = 0.04388366\n",
      "Iteration 12311, loss = 0.04387942\n",
      "Iteration 12312, loss = 0.04387523\n",
      "Iteration 12313, loss = 0.04387097\n",
      "Iteration 12314, loss = 0.04386679\n",
      "Iteration 12315, loss = 0.04386255\n",
      "Iteration 12316, loss = 0.04385833\n",
      "Iteration 12317, loss = 0.04385408\n",
      "Iteration 12318, loss = 0.04384985\n",
      "Iteration 12319, loss = 0.04384568\n",
      "Iteration 12320, loss = 0.04384145\n",
      "Iteration 12321, loss = 0.04383729\n",
      "Iteration 12322, loss = 0.04383300\n",
      "Iteration 12323, loss = 0.04382887\n",
      "Iteration 12324, loss = 0.04382461\n",
      "Iteration 12325, loss = 0.04382039\n",
      "Iteration 12326, loss = 0.04381620\n",
      "Iteration 12327, loss = 0.04381205\n",
      "Iteration 12328, loss = 0.04380782\n",
      "Iteration 12329, loss = 0.04380365\n",
      "Iteration 12330, loss = 0.04379945\n",
      "Iteration 12331, loss = 0.04379528\n",
      "Iteration 12332, loss = 0.04379105\n",
      "Iteration 12333, loss = 0.04378684\n",
      "Iteration 12334, loss = 0.04378270\n",
      "Iteration 12335, loss = 0.04377847\n",
      "Iteration 12336, loss = 0.04377427\n",
      "Iteration 12337, loss = 0.04377008\n",
      "Iteration 12338, loss = 0.04376588\n",
      "Iteration 12339, loss = 0.04376167\n",
      "Iteration 12340, loss = 0.04375750\n",
      "Iteration 12341, loss = 0.04375336\n",
      "Iteration 12342, loss = 0.04374911\n",
      "Iteration 12343, loss = 0.04374490\n",
      "Iteration 12344, loss = 0.04374068\n",
      "Iteration 12345, loss = 0.04373646\n",
      "Iteration 12346, loss = 0.04373233\n",
      "Iteration 12347, loss = 0.04372807\n",
      "Iteration 12348, loss = 0.04372391\n",
      "Iteration 12349, loss = 0.04371964\n",
      "Iteration 12350, loss = 0.04371546\n",
      "Iteration 12351, loss = 0.04371129\n",
      "Iteration 12352, loss = 0.04370713\n",
      "Iteration 12353, loss = 0.04370285\n",
      "Iteration 12354, loss = 0.04369868\n",
      "Iteration 12355, loss = 0.04369452\n",
      "Iteration 12356, loss = 0.04369030\n",
      "Iteration 12357, loss = 0.04368608\n",
      "Iteration 12358, loss = 0.04368197\n",
      "Iteration 12359, loss = 0.04367770\n",
      "Iteration 12360, loss = 0.04367358\n",
      "Iteration 12361, loss = 0.04366938\n",
      "Iteration 12362, loss = 0.04366517\n",
      "Iteration 12363, loss = 0.04366100\n",
      "Iteration 12364, loss = 0.04365683\n",
      "Iteration 12365, loss = 0.04365264\n",
      "Iteration 12366, loss = 0.04364849\n",
      "Iteration 12367, loss = 0.04364430\n",
      "Iteration 12368, loss = 0.04364018\n",
      "Iteration 12369, loss = 0.04363598\n",
      "Iteration 12370, loss = 0.04363176\n",
      "Iteration 12371, loss = 0.04362763\n",
      "Iteration 12372, loss = 0.04362343\n",
      "Iteration 12373, loss = 0.04361926\n",
      "Iteration 12374, loss = 0.04361506\n",
      "Iteration 12375, loss = 0.04361088\n",
      "Iteration 12376, loss = 0.04360677\n",
      "Iteration 12377, loss = 0.04360256\n",
      "Iteration 12378, loss = 0.04359844\n",
      "Iteration 12379, loss = 0.04359424\n",
      "Iteration 12380, loss = 0.04359006\n",
      "Iteration 12381, loss = 0.04358589\n",
      "Iteration 12382, loss = 0.04358174\n",
      "Iteration 12383, loss = 0.04357757\n",
      "Iteration 12384, loss = 0.04357345\n",
      "Iteration 12385, loss = 0.04356925\n",
      "Iteration 12386, loss = 0.04356502\n",
      "Iteration 12387, loss = 0.04356092\n",
      "Iteration 12388, loss = 0.04355676\n",
      "Iteration 12389, loss = 0.04355260\n",
      "Iteration 12390, loss = 0.04354843\n",
      "Iteration 12391, loss = 0.04354423\n",
      "Iteration 12392, loss = 0.04354009\n",
      "Iteration 12393, loss = 0.04353593\n",
      "Iteration 12394, loss = 0.04353180\n",
      "Iteration 12395, loss = 0.04352760\n",
      "Iteration 12396, loss = 0.04352345\n",
      "Iteration 12397, loss = 0.04351936\n",
      "Iteration 12398, loss = 0.04351515\n",
      "Iteration 12399, loss = 0.04351099\n",
      "Iteration 12400, loss = 0.04350690\n",
      "Iteration 12401, loss = 0.04350267\n",
      "Iteration 12402, loss = 0.04349854\n",
      "Iteration 12403, loss = 0.04349442\n",
      "Iteration 12404, loss = 0.04349023\n",
      "Iteration 12405, loss = 0.04348608\n",
      "Iteration 12406, loss = 0.04348204\n",
      "Iteration 12407, loss = 0.04347780\n",
      "Iteration 12408, loss = 0.04347365\n",
      "Iteration 12409, loss = 0.04346951\n",
      "Iteration 12410, loss = 0.04346535\n",
      "Iteration 12411, loss = 0.04346129\n",
      "Iteration 12412, loss = 0.04345712\n",
      "Iteration 12413, loss = 0.04345305\n",
      "Iteration 12414, loss = 0.04344887\n",
      "Iteration 12415, loss = 0.04344470\n",
      "Iteration 12416, loss = 0.04344058\n",
      "Iteration 12417, loss = 0.04343651\n",
      "Iteration 12418, loss = 0.04343230\n",
      "Iteration 12419, loss = 0.04342820\n",
      "Iteration 12420, loss = 0.04342408\n",
      "Iteration 12421, loss = 0.04341992\n",
      "Iteration 12422, loss = 0.04341580\n",
      "Iteration 12423, loss = 0.04341170\n",
      "Iteration 12424, loss = 0.04340754\n",
      "Iteration 12425, loss = 0.04340341\n",
      "Iteration 12426, loss = 0.04339928\n",
      "Iteration 12427, loss = 0.04339518\n",
      "Iteration 12428, loss = 0.04339103\n",
      "Iteration 12429, loss = 0.04338691\n",
      "Iteration 12430, loss = 0.04338279\n",
      "Iteration 12431, loss = 0.04337871\n",
      "Iteration 12432, loss = 0.04337457\n",
      "Iteration 12433, loss = 0.04337042\n",
      "Iteration 12434, loss = 0.04336635\n",
      "Iteration 12435, loss = 0.04336221\n",
      "Iteration 12436, loss = 0.04335810\n",
      "Iteration 12437, loss = 0.04335397\n",
      "Iteration 12438, loss = 0.04334983\n",
      "Iteration 12439, loss = 0.04334577\n",
      "Iteration 12440, loss = 0.04334163\n",
      "Iteration 12441, loss = 0.04333753\n",
      "Iteration 12442, loss = 0.04333337\n",
      "Iteration 12443, loss = 0.04332927\n",
      "Iteration 12444, loss = 0.04332517\n",
      "Iteration 12445, loss = 0.04332104\n",
      "Iteration 12446, loss = 0.04331695\n",
      "Iteration 12447, loss = 0.04331277\n",
      "Iteration 12448, loss = 0.04330873\n",
      "Iteration 12449, loss = 0.04330459\n",
      "Iteration 12450, loss = 0.04330045\n",
      "Iteration 12451, loss = 0.04329637\n",
      "Iteration 12452, loss = 0.04329226\n",
      "Iteration 12453, loss = 0.04328813\n",
      "Iteration 12454, loss = 0.04328403\n",
      "Iteration 12455, loss = 0.04327993\n",
      "Iteration 12456, loss = 0.04327582\n",
      "Iteration 12457, loss = 0.04327175\n",
      "Iteration 12458, loss = 0.04326760\n",
      "Iteration 12459, loss = 0.04326353\n",
      "Iteration 12460, loss = 0.04325940\n",
      "Iteration 12461, loss = 0.04325531\n",
      "Iteration 12462, loss = 0.04325128\n",
      "Iteration 12463, loss = 0.04324720\n",
      "Iteration 12464, loss = 0.04324308\n",
      "Iteration 12465, loss = 0.04323897\n",
      "Iteration 12466, loss = 0.04323485\n",
      "Iteration 12467, loss = 0.04323073\n",
      "Iteration 12468, loss = 0.04322668\n",
      "Iteration 12469, loss = 0.04322263\n",
      "Iteration 12470, loss = 0.04321851\n",
      "Iteration 12471, loss = 0.04321443\n",
      "Iteration 12472, loss = 0.04321033\n",
      "Iteration 12473, loss = 0.04320619\n",
      "Iteration 12474, loss = 0.04320212\n",
      "Iteration 12475, loss = 0.04319806\n",
      "Iteration 12476, loss = 0.04319391\n",
      "Iteration 12477, loss = 0.04318986\n",
      "Iteration 12478, loss = 0.04318579\n",
      "Iteration 12479, loss = 0.04318167\n",
      "Iteration 12480, loss = 0.04317758\n",
      "Iteration 12481, loss = 0.04317357\n",
      "Iteration 12482, loss = 0.04316943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12483, loss = 0.04316534\n",
      "Iteration 12484, loss = 0.04316122\n",
      "Iteration 12485, loss = 0.04315714\n",
      "Iteration 12486, loss = 0.04315308\n",
      "Iteration 12487, loss = 0.04314899\n",
      "Iteration 12488, loss = 0.04314492\n",
      "Iteration 12489, loss = 0.04314083\n",
      "Iteration 12490, loss = 0.04313678\n",
      "Iteration 12491, loss = 0.04313268\n",
      "Iteration 12492, loss = 0.04312860\n",
      "Iteration 12493, loss = 0.04312449\n",
      "Iteration 12494, loss = 0.04312048\n",
      "Iteration 12495, loss = 0.04311636\n",
      "Iteration 12496, loss = 0.04311229\n",
      "Iteration 12497, loss = 0.04310823\n",
      "Iteration 12498, loss = 0.04310416\n",
      "Iteration 12499, loss = 0.04310005\n",
      "Iteration 12500, loss = 0.04309601\n",
      "Iteration 12501, loss = 0.04309193\n",
      "Iteration 12502, loss = 0.04308786\n",
      "Iteration 12503, loss = 0.04308375\n",
      "Iteration 12504, loss = 0.04307971\n",
      "Iteration 12505, loss = 0.04307561\n",
      "Iteration 12506, loss = 0.04307164\n",
      "Iteration 12507, loss = 0.04306747\n",
      "Iteration 12508, loss = 0.04306344\n",
      "Iteration 12509, loss = 0.04305941\n",
      "Iteration 12510, loss = 0.04305532\n",
      "Iteration 12511, loss = 0.04305124\n",
      "Iteration 12512, loss = 0.04304719\n",
      "Iteration 12513, loss = 0.04304312\n",
      "Iteration 12514, loss = 0.04303912\n",
      "Iteration 12515, loss = 0.04303505\n",
      "Iteration 12516, loss = 0.04303101\n",
      "Iteration 12517, loss = 0.04302694\n",
      "Iteration 12518, loss = 0.04302285\n",
      "Iteration 12519, loss = 0.04301881\n",
      "Iteration 12520, loss = 0.04301475\n",
      "Iteration 12521, loss = 0.04301065\n",
      "Iteration 12522, loss = 0.04300660\n",
      "Iteration 12523, loss = 0.04300255\n",
      "Iteration 12524, loss = 0.04299851\n",
      "Iteration 12525, loss = 0.04299442\n",
      "Iteration 12526, loss = 0.04299036\n",
      "Iteration 12527, loss = 0.04298630\n",
      "Iteration 12528, loss = 0.04298221\n",
      "Iteration 12529, loss = 0.04297819\n",
      "Iteration 12530, loss = 0.04297413\n",
      "Iteration 12531, loss = 0.04297011\n",
      "Iteration 12532, loss = 0.04296605\n",
      "Iteration 12533, loss = 0.04296200\n",
      "Iteration 12534, loss = 0.04295792\n",
      "Iteration 12535, loss = 0.04295390\n",
      "Iteration 12536, loss = 0.04294984\n",
      "Iteration 12537, loss = 0.04294580\n",
      "Iteration 12538, loss = 0.04294172\n",
      "Iteration 12539, loss = 0.04293772\n",
      "Iteration 12540, loss = 0.04293367\n",
      "Iteration 12541, loss = 0.04292966\n",
      "Iteration 12542, loss = 0.04292565\n",
      "Iteration 12543, loss = 0.04292168\n",
      "Iteration 12544, loss = 0.04291759\n",
      "Iteration 12545, loss = 0.04291354\n",
      "Iteration 12546, loss = 0.04290946\n",
      "Iteration 12547, loss = 0.04290545\n",
      "Iteration 12548, loss = 0.04290143\n",
      "Iteration 12549, loss = 0.04289742\n",
      "Iteration 12550, loss = 0.04289337\n",
      "Iteration 12551, loss = 0.04288936\n",
      "Iteration 12552, loss = 0.04288534\n",
      "Iteration 12553, loss = 0.04288130\n",
      "Iteration 12554, loss = 0.04287727\n",
      "Iteration 12555, loss = 0.04287324\n",
      "Iteration 12556, loss = 0.04286919\n",
      "Iteration 12557, loss = 0.04286525\n",
      "Iteration 12558, loss = 0.04286116\n",
      "Iteration 12559, loss = 0.04285718\n",
      "Iteration 12560, loss = 0.04285311\n",
      "Iteration 12561, loss = 0.04284914\n",
      "Iteration 12562, loss = 0.04284509\n",
      "Iteration 12563, loss = 0.04284106\n",
      "Iteration 12564, loss = 0.04283707\n",
      "Iteration 12565, loss = 0.04283306\n",
      "Iteration 12566, loss = 0.04282908\n",
      "Iteration 12567, loss = 0.04282504\n",
      "Iteration 12568, loss = 0.04282099\n",
      "Iteration 12569, loss = 0.04281702\n",
      "Iteration 12570, loss = 0.04281301\n",
      "Iteration 12571, loss = 0.04280896\n",
      "Iteration 12572, loss = 0.04280493\n",
      "Iteration 12573, loss = 0.04280096\n",
      "Iteration 12574, loss = 0.04279689\n",
      "Iteration 12575, loss = 0.04279291\n",
      "Iteration 12576, loss = 0.04278889\n",
      "Iteration 12577, loss = 0.04278489\n",
      "Iteration 12578, loss = 0.04278090\n",
      "Iteration 12579, loss = 0.04277678\n",
      "Iteration 12580, loss = 0.04277285\n",
      "Iteration 12581, loss = 0.04276880\n",
      "Iteration 12582, loss = 0.04276487\n",
      "Iteration 12583, loss = 0.04276076\n",
      "Iteration 12584, loss = 0.04275676\n",
      "Iteration 12585, loss = 0.04275277\n",
      "Iteration 12586, loss = 0.04274877\n",
      "Iteration 12587, loss = 0.04274472\n",
      "Iteration 12588, loss = 0.04274074\n",
      "Iteration 12589, loss = 0.04273673\n",
      "Iteration 12590, loss = 0.04273271\n",
      "Iteration 12591, loss = 0.04272874\n",
      "Iteration 12592, loss = 0.04272470\n",
      "Iteration 12593, loss = 0.04272069\n",
      "Iteration 12594, loss = 0.04271670\n",
      "Iteration 12595, loss = 0.04271268\n",
      "Iteration 12596, loss = 0.04270868\n",
      "Iteration 12597, loss = 0.04270468\n",
      "Iteration 12598, loss = 0.04270070\n",
      "Iteration 12599, loss = 0.04269665\n",
      "Iteration 12600, loss = 0.04269269\n",
      "Iteration 12601, loss = 0.04268868\n",
      "Iteration 12602, loss = 0.04268459\n",
      "Iteration 12603, loss = 0.04268065\n",
      "Iteration 12604, loss = 0.04267668\n",
      "Iteration 12605, loss = 0.04267268\n",
      "Iteration 12606, loss = 0.04266871\n",
      "Iteration 12607, loss = 0.04266461\n",
      "Iteration 12608, loss = 0.04266065\n",
      "Iteration 12609, loss = 0.04265672\n",
      "Iteration 12610, loss = 0.04265270\n",
      "Iteration 12611, loss = 0.04264869\n",
      "Iteration 12612, loss = 0.04264473\n",
      "Iteration 12613, loss = 0.04264075\n",
      "Iteration 12614, loss = 0.04263676\n",
      "Iteration 12615, loss = 0.04263277\n",
      "Iteration 12616, loss = 0.04262883\n",
      "Iteration 12617, loss = 0.04262479\n",
      "Iteration 12618, loss = 0.04262088\n",
      "Iteration 12619, loss = 0.04261688\n",
      "Iteration 12620, loss = 0.04261291\n",
      "Iteration 12621, loss = 0.04260892\n",
      "Iteration 12622, loss = 0.04260496\n",
      "Iteration 12623, loss = 0.04260095\n",
      "Iteration 12624, loss = 0.04259708\n",
      "Iteration 12625, loss = 0.04259307\n",
      "Iteration 12626, loss = 0.04258909\n",
      "Iteration 12627, loss = 0.04258512\n",
      "Iteration 12628, loss = 0.04258119\n",
      "Iteration 12629, loss = 0.04257720\n",
      "Iteration 12630, loss = 0.04257322\n",
      "Iteration 12631, loss = 0.04256928\n",
      "Iteration 12632, loss = 0.04256533\n",
      "Iteration 12633, loss = 0.04256136\n",
      "Iteration 12634, loss = 0.04255737\n",
      "Iteration 12635, loss = 0.04255344\n",
      "Iteration 12636, loss = 0.04254946\n",
      "Iteration 12637, loss = 0.04254555\n",
      "Iteration 12638, loss = 0.04254158\n",
      "Iteration 12639, loss = 0.04253756\n",
      "Iteration 12640, loss = 0.04253362\n",
      "Iteration 12641, loss = 0.04252972\n",
      "Iteration 12642, loss = 0.04252566\n",
      "Iteration 12643, loss = 0.04252172\n",
      "Iteration 12644, loss = 0.04251771\n",
      "Iteration 12645, loss = 0.04251378\n",
      "Iteration 12646, loss = 0.04250983\n",
      "Iteration 12647, loss = 0.04250588\n",
      "Iteration 12648, loss = 0.04250187\n",
      "Iteration 12649, loss = 0.04249792\n",
      "Iteration 12650, loss = 0.04249398\n",
      "Iteration 12651, loss = 0.04249005\n",
      "Iteration 12652, loss = 0.04248607\n",
      "Iteration 12653, loss = 0.04248208\n",
      "Iteration 12654, loss = 0.04247814\n",
      "Iteration 12655, loss = 0.04247416\n",
      "Iteration 12656, loss = 0.04247019\n",
      "Iteration 12657, loss = 0.04246629\n",
      "Iteration 12658, loss = 0.04246228\n",
      "Iteration 12659, loss = 0.04245834\n",
      "Iteration 12660, loss = 0.04245441\n",
      "Iteration 12661, loss = 0.04245042\n",
      "Iteration 12662, loss = 0.04244653\n",
      "Iteration 12663, loss = 0.04244247\n",
      "Iteration 12664, loss = 0.04243855\n",
      "Iteration 12665, loss = 0.04243470\n",
      "Iteration 12666, loss = 0.04243069\n",
      "Iteration 12667, loss = 0.04242670\n",
      "Iteration 12668, loss = 0.04242278\n",
      "Iteration 12669, loss = 0.04241886\n",
      "Iteration 12670, loss = 0.04241493\n",
      "Iteration 12671, loss = 0.04241097\n",
      "Iteration 12672, loss = 0.04240701\n",
      "Iteration 12673, loss = 0.04240311\n",
      "Iteration 12674, loss = 0.04239914\n",
      "Iteration 12675, loss = 0.04239520\n",
      "Iteration 12676, loss = 0.04239128\n",
      "Iteration 12677, loss = 0.04238728\n",
      "Iteration 12678, loss = 0.04238339\n",
      "Iteration 12679, loss = 0.04237947\n",
      "Iteration 12680, loss = 0.04237549\n",
      "Iteration 12681, loss = 0.04237160\n",
      "Iteration 12682, loss = 0.04236762\n",
      "Iteration 12683, loss = 0.04236369\n",
      "Iteration 12684, loss = 0.04235976\n",
      "Iteration 12685, loss = 0.04235579\n",
      "Iteration 12686, loss = 0.04235184\n",
      "Iteration 12687, loss = 0.04234794\n",
      "Iteration 12688, loss = 0.04234402\n",
      "Iteration 12689, loss = 0.04234009\n",
      "Iteration 12690, loss = 0.04233613\n",
      "Iteration 12691, loss = 0.04233219\n",
      "Iteration 12692, loss = 0.04232826\n",
      "Iteration 12693, loss = 0.04232436\n",
      "Iteration 12694, loss = 0.04232043\n",
      "Iteration 12695, loss = 0.04231647\n",
      "Iteration 12696, loss = 0.04231256\n",
      "Iteration 12697, loss = 0.04230867\n",
      "Iteration 12698, loss = 0.04230470\n",
      "Iteration 12699, loss = 0.04230077\n",
      "Iteration 12700, loss = 0.04229684\n",
      "Iteration 12701, loss = 0.04229290\n",
      "Iteration 12702, loss = 0.04228902\n",
      "Iteration 12703, loss = 0.04228505\n",
      "Iteration 12704, loss = 0.04228125\n",
      "Iteration 12705, loss = 0.04227726\n",
      "Iteration 12706, loss = 0.04227332\n",
      "Iteration 12707, loss = 0.04226940\n",
      "Iteration 12708, loss = 0.04226548\n",
      "Iteration 12709, loss = 0.04226154\n",
      "Iteration 12710, loss = 0.04225769\n",
      "Iteration 12711, loss = 0.04225374\n",
      "Iteration 12712, loss = 0.04224982\n",
      "Iteration 12713, loss = 0.04224592\n",
      "Iteration 12714, loss = 0.04224199\n",
      "Iteration 12715, loss = 0.04223814\n",
      "Iteration 12716, loss = 0.04223415\n",
      "Iteration 12717, loss = 0.04223028\n",
      "Iteration 12718, loss = 0.04222636\n",
      "Iteration 12719, loss = 0.04222249\n",
      "Iteration 12720, loss = 0.04221854\n",
      "Iteration 12721, loss = 0.04221463\n",
      "Iteration 12722, loss = 0.04221079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12723, loss = 0.04220682\n",
      "Iteration 12724, loss = 0.04220295\n",
      "Iteration 12725, loss = 0.04219904\n",
      "Iteration 12726, loss = 0.04219513\n",
      "Iteration 12727, loss = 0.04219123\n",
      "Iteration 12728, loss = 0.04218733\n",
      "Iteration 12729, loss = 0.04218346\n",
      "Iteration 12730, loss = 0.04217954\n",
      "Iteration 12731, loss = 0.04217566\n",
      "Iteration 12732, loss = 0.04217178\n",
      "Iteration 12733, loss = 0.04216784\n",
      "Iteration 12734, loss = 0.04216401\n",
      "Iteration 12735, loss = 0.04216006\n",
      "Iteration 12736, loss = 0.04215621\n",
      "Iteration 12737, loss = 0.04215227\n",
      "Iteration 12738, loss = 0.04214843\n",
      "Iteration 12739, loss = 0.04214451\n",
      "Iteration 12740, loss = 0.04214063\n",
      "Iteration 12741, loss = 0.04213675\n",
      "Iteration 12742, loss = 0.04213287\n",
      "Iteration 12743, loss = 0.04212893\n",
      "Iteration 12744, loss = 0.04212507\n",
      "Iteration 12745, loss = 0.04212119\n",
      "Iteration 12746, loss = 0.04211727\n",
      "Iteration 12747, loss = 0.04211336\n",
      "Iteration 12748, loss = 0.04210949\n",
      "Iteration 12749, loss = 0.04210560\n",
      "Iteration 12750, loss = 0.04210171\n",
      "Iteration 12751, loss = 0.04209787\n",
      "Iteration 12752, loss = 0.04209397\n",
      "Iteration 12753, loss = 0.04209007\n",
      "Iteration 12754, loss = 0.04208623\n",
      "Iteration 12755, loss = 0.04208232\n",
      "Iteration 12756, loss = 0.04207848\n",
      "Iteration 12757, loss = 0.04207460\n",
      "Iteration 12758, loss = 0.04207074\n",
      "Iteration 12759, loss = 0.04206687\n",
      "Iteration 12760, loss = 0.04206302\n",
      "Iteration 12761, loss = 0.04205916\n",
      "Iteration 12762, loss = 0.04205525\n",
      "Iteration 12763, loss = 0.04205139\n",
      "Iteration 12764, loss = 0.04204752\n",
      "Iteration 12765, loss = 0.04204363\n",
      "Iteration 12766, loss = 0.04203978\n",
      "Iteration 12767, loss = 0.04203595\n",
      "Iteration 12768, loss = 0.04203208\n",
      "Iteration 12769, loss = 0.04202818\n",
      "Iteration 12770, loss = 0.04202434\n",
      "Iteration 12771, loss = 0.04202051\n",
      "Iteration 12772, loss = 0.04201661\n",
      "Iteration 12773, loss = 0.04201270\n",
      "Iteration 12774, loss = 0.04200889\n",
      "Iteration 12775, loss = 0.04200506\n",
      "Iteration 12776, loss = 0.04200115\n",
      "Iteration 12777, loss = 0.04199730\n",
      "Iteration 12778, loss = 0.04199341\n",
      "Iteration 12779, loss = 0.04198952\n",
      "Iteration 12780, loss = 0.04198572\n",
      "Iteration 12781, loss = 0.04198178\n",
      "Iteration 12782, loss = 0.04197795\n",
      "Iteration 12783, loss = 0.04197410\n",
      "Iteration 12784, loss = 0.04197021\n",
      "Iteration 12785, loss = 0.04196639\n",
      "Iteration 12786, loss = 0.04196254\n",
      "Iteration 12787, loss = 0.04195865\n",
      "Iteration 12788, loss = 0.04195480\n",
      "Iteration 12789, loss = 0.04195093\n",
      "Iteration 12790, loss = 0.04194712\n",
      "Iteration 12791, loss = 0.04194323\n",
      "Iteration 12792, loss = 0.04193935\n",
      "Iteration 12793, loss = 0.04193552\n",
      "Iteration 12794, loss = 0.04193163\n",
      "Iteration 12795, loss = 0.04192775\n",
      "Iteration 12796, loss = 0.04192395\n",
      "Iteration 12797, loss = 0.04192006\n",
      "Iteration 12798, loss = 0.04191621\n",
      "Iteration 12799, loss = 0.04191239\n",
      "Iteration 12800, loss = 0.04190852\n",
      "Iteration 12801, loss = 0.04190464\n",
      "Iteration 12802, loss = 0.04190081\n",
      "Iteration 12803, loss = 0.04189699\n",
      "Iteration 12804, loss = 0.04189314\n",
      "Iteration 12805, loss = 0.04188928\n",
      "Iteration 12806, loss = 0.04188546\n",
      "Iteration 12807, loss = 0.04188161\n",
      "Iteration 12808, loss = 0.04187778\n",
      "Iteration 12809, loss = 0.04187384\n",
      "Iteration 12810, loss = 0.04187007\n",
      "Iteration 12811, loss = 0.04186619\n",
      "Iteration 12812, loss = 0.04186235\n",
      "Iteration 12813, loss = 0.04185853\n",
      "Iteration 12814, loss = 0.04185468\n",
      "Iteration 12815, loss = 0.04185085\n",
      "Iteration 12816, loss = 0.04184698\n",
      "Iteration 12817, loss = 0.04184319\n",
      "Iteration 12818, loss = 0.04183929\n",
      "Iteration 12819, loss = 0.04183547\n",
      "Iteration 12820, loss = 0.04183169\n",
      "Iteration 12821, loss = 0.04182784\n",
      "Iteration 12822, loss = 0.04182399\n",
      "Iteration 12823, loss = 0.04182017\n",
      "Iteration 12824, loss = 0.04181631\n",
      "Iteration 12825, loss = 0.04181249\n",
      "Iteration 12826, loss = 0.04180869\n",
      "Iteration 12827, loss = 0.04180484\n",
      "Iteration 12828, loss = 0.04180101\n",
      "Iteration 12829, loss = 0.04179722\n",
      "Iteration 12830, loss = 0.04179340\n",
      "Iteration 12831, loss = 0.04178955\n",
      "Iteration 12832, loss = 0.04178579\n",
      "Iteration 12833, loss = 0.04178191\n",
      "Iteration 12834, loss = 0.04177811\n",
      "Iteration 12835, loss = 0.04177431\n",
      "Iteration 12836, loss = 0.04177046\n",
      "Iteration 12837, loss = 0.04176665\n",
      "Iteration 12838, loss = 0.04176279\n",
      "Iteration 12839, loss = 0.04175902\n",
      "Iteration 12840, loss = 0.04175526\n",
      "Iteration 12841, loss = 0.04175136\n",
      "Iteration 12842, loss = 0.04174758\n",
      "Iteration 12843, loss = 0.04174376\n",
      "Iteration 12844, loss = 0.04173996\n",
      "Iteration 12845, loss = 0.04173607\n",
      "Iteration 12846, loss = 0.04173230\n",
      "Iteration 12847, loss = 0.04172846\n",
      "Iteration 12848, loss = 0.04172460\n",
      "Iteration 12849, loss = 0.04172082\n",
      "Iteration 12850, loss = 0.04171697\n",
      "Iteration 12851, loss = 0.04171321\n",
      "Iteration 12852, loss = 0.04170937\n",
      "Iteration 12853, loss = 0.04170556\n",
      "Iteration 12854, loss = 0.04170171\n",
      "Iteration 12855, loss = 0.04169792\n",
      "Iteration 12856, loss = 0.04169411\n",
      "Iteration 12857, loss = 0.04169031\n",
      "Iteration 12858, loss = 0.04168644\n",
      "Iteration 12859, loss = 0.04168261\n",
      "Iteration 12860, loss = 0.04167886\n",
      "Iteration 12861, loss = 0.04167509\n",
      "Iteration 12862, loss = 0.04167127\n",
      "Iteration 12863, loss = 0.04166741\n",
      "Iteration 12864, loss = 0.04166360\n",
      "Iteration 12865, loss = 0.04165985\n",
      "Iteration 12866, loss = 0.04165602\n",
      "Iteration 12867, loss = 0.04165215\n",
      "Iteration 12868, loss = 0.04164841\n",
      "Iteration 12869, loss = 0.04164459\n",
      "Iteration 12870, loss = 0.04164076\n",
      "Iteration 12871, loss = 0.04163697\n",
      "Iteration 12872, loss = 0.04163322\n",
      "Iteration 12873, loss = 0.04162942\n",
      "Iteration 12874, loss = 0.04162563\n",
      "Iteration 12875, loss = 0.04162182\n",
      "Iteration 12876, loss = 0.04161805\n",
      "Iteration 12877, loss = 0.04161426\n",
      "Iteration 12878, loss = 0.04161053\n",
      "Iteration 12879, loss = 0.04160666\n",
      "Iteration 12880, loss = 0.04160288\n",
      "Iteration 12881, loss = 0.04159912\n",
      "Iteration 12882, loss = 0.04159529\n",
      "Iteration 12883, loss = 0.04159154\n",
      "Iteration 12884, loss = 0.04158776\n",
      "Iteration 12885, loss = 0.04158397\n",
      "Iteration 12886, loss = 0.04158017\n",
      "Iteration 12887, loss = 0.04157638\n",
      "Iteration 12888, loss = 0.04157259\n",
      "Iteration 12889, loss = 0.04156880\n",
      "Iteration 12890, loss = 0.04156501\n",
      "Iteration 12891, loss = 0.04156125\n",
      "Iteration 12892, loss = 0.04155747\n",
      "Iteration 12893, loss = 0.04155366\n",
      "Iteration 12894, loss = 0.04154989\n",
      "Iteration 12895, loss = 0.04154610\n",
      "Iteration 12896, loss = 0.04154233\n",
      "Iteration 12897, loss = 0.04153854\n",
      "Iteration 12898, loss = 0.04153477\n",
      "Iteration 12899, loss = 0.04153097\n",
      "Iteration 12900, loss = 0.04152725\n",
      "Iteration 12901, loss = 0.04152347\n",
      "Iteration 12902, loss = 0.04151965\n",
      "Iteration 12903, loss = 0.04151591\n",
      "Iteration 12904, loss = 0.04151209\n",
      "Iteration 12905, loss = 0.04150833\n",
      "Iteration 12906, loss = 0.04150459\n",
      "Iteration 12907, loss = 0.04150080\n",
      "Iteration 12908, loss = 0.04149706\n",
      "Iteration 12909, loss = 0.04149327\n",
      "Iteration 12910, loss = 0.04148945\n",
      "Iteration 12911, loss = 0.04148574\n",
      "Iteration 12912, loss = 0.04148193\n",
      "Iteration 12913, loss = 0.04147819\n",
      "Iteration 12914, loss = 0.04147439\n",
      "Iteration 12915, loss = 0.04147060\n",
      "Iteration 12916, loss = 0.04146687\n",
      "Iteration 12917, loss = 0.04146305\n",
      "Iteration 12918, loss = 0.04145933\n",
      "Iteration 12919, loss = 0.04145559\n",
      "Iteration 12920, loss = 0.04145176\n",
      "Iteration 12921, loss = 0.04144800\n",
      "Iteration 12922, loss = 0.04144422\n",
      "Iteration 12923, loss = 0.04144053\n",
      "Iteration 12924, loss = 0.04143676\n",
      "Iteration 12925, loss = 0.04143298\n",
      "Iteration 12926, loss = 0.04142923\n",
      "Iteration 12927, loss = 0.04142552\n",
      "Iteration 12928, loss = 0.04142175\n",
      "Iteration 12929, loss = 0.04141799\n",
      "Iteration 12930, loss = 0.04141421\n",
      "Iteration 12931, loss = 0.04141044\n",
      "Iteration 12932, loss = 0.04140669\n",
      "Iteration 12933, loss = 0.04140298\n",
      "Iteration 12934, loss = 0.04139917\n",
      "Iteration 12935, loss = 0.04139542\n",
      "Iteration 12936, loss = 0.04139169\n",
      "Iteration 12937, loss = 0.04138795\n",
      "Iteration 12938, loss = 0.04138415\n",
      "Iteration 12939, loss = 0.04138041\n",
      "Iteration 12940, loss = 0.04137671\n",
      "Iteration 12941, loss = 0.04137290\n",
      "Iteration 12942, loss = 0.04136920\n",
      "Iteration 12943, loss = 0.04136545\n",
      "Iteration 12944, loss = 0.04136171\n",
      "Iteration 12945, loss = 0.04135797\n",
      "Iteration 12946, loss = 0.04135422\n",
      "Iteration 12947, loss = 0.04135049\n",
      "Iteration 12948, loss = 0.04134674\n",
      "Iteration 12949, loss = 0.04134300\n",
      "Iteration 12950, loss = 0.04133928\n",
      "Iteration 12951, loss = 0.04133554\n",
      "Iteration 12952, loss = 0.04133183\n",
      "Iteration 12953, loss = 0.04132806\n",
      "Iteration 12954, loss = 0.04132431\n",
      "Iteration 12955, loss = 0.04132055\n",
      "Iteration 12956, loss = 0.04131681\n",
      "Iteration 12957, loss = 0.04131312\n",
      "Iteration 12958, loss = 0.04130935\n",
      "Iteration 12959, loss = 0.04130558\n",
      "Iteration 12960, loss = 0.04130189\n",
      "Iteration 12961, loss = 0.04129815\n",
      "Iteration 12962, loss = 0.04129441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12963, loss = 0.04129068\n",
      "Iteration 12964, loss = 0.04128694\n",
      "Iteration 12965, loss = 0.04128320\n",
      "Iteration 12966, loss = 0.04127951\n",
      "Iteration 12967, loss = 0.04127573\n",
      "Iteration 12968, loss = 0.04127200\n",
      "Iteration 12969, loss = 0.04126828\n",
      "Iteration 12970, loss = 0.04126458\n",
      "Iteration 12971, loss = 0.04126084\n",
      "Iteration 12972, loss = 0.04125711\n",
      "Iteration 12973, loss = 0.04125334\n",
      "Iteration 12974, loss = 0.04124962\n",
      "Iteration 12975, loss = 0.04124589\n",
      "Iteration 12976, loss = 0.04124215\n",
      "Iteration 12977, loss = 0.04123846\n",
      "Iteration 12978, loss = 0.04123469\n",
      "Iteration 12979, loss = 0.04123095\n",
      "Iteration 12980, loss = 0.04122719\n",
      "Iteration 12981, loss = 0.04122347\n",
      "Iteration 12982, loss = 0.04121975\n",
      "Iteration 12983, loss = 0.04121604\n",
      "Iteration 12984, loss = 0.04121234\n",
      "Iteration 12985, loss = 0.04120858\n",
      "Iteration 12986, loss = 0.04120485\n",
      "Iteration 12987, loss = 0.04120110\n",
      "Iteration 12988, loss = 0.04119741\n",
      "Iteration 12989, loss = 0.04119368\n",
      "Iteration 12990, loss = 0.04118994\n",
      "Iteration 12991, loss = 0.04118628\n",
      "Iteration 12992, loss = 0.04118253\n",
      "Iteration 12993, loss = 0.04117882\n",
      "Iteration 12994, loss = 0.04117510\n",
      "Iteration 12995, loss = 0.04117140\n",
      "Iteration 12996, loss = 0.04116769\n",
      "Iteration 12997, loss = 0.04116396\n",
      "Iteration 12998, loss = 0.04116027\n",
      "Iteration 12999, loss = 0.04115656\n",
      "Iteration 13000, loss = 0.04115290\n",
      "Iteration 13001, loss = 0.04114911\n",
      "Iteration 13002, loss = 0.04114541\n",
      "Iteration 13003, loss = 0.04114172\n",
      "Iteration 13004, loss = 0.04113806\n",
      "Iteration 13005, loss = 0.04113428\n",
      "Iteration 13006, loss = 0.04113065\n",
      "Iteration 13007, loss = 0.04112694\n",
      "Iteration 13008, loss = 0.04112324\n",
      "Iteration 13009, loss = 0.04111952\n",
      "Iteration 13010, loss = 0.04111585\n",
      "Iteration 13011, loss = 0.04111217\n",
      "Iteration 13012, loss = 0.04110841\n",
      "Iteration 13013, loss = 0.04110472\n",
      "Iteration 13014, loss = 0.04110106\n",
      "Iteration 13015, loss = 0.04109732\n",
      "Iteration 13016, loss = 0.04109365\n",
      "Iteration 13017, loss = 0.04108994\n",
      "Iteration 13018, loss = 0.04108622\n",
      "Iteration 13019, loss = 0.04108254\n",
      "Iteration 13020, loss = 0.04107886\n",
      "Iteration 13021, loss = 0.04107517\n",
      "Iteration 13022, loss = 0.04107143\n",
      "Iteration 13023, loss = 0.04106777\n",
      "Iteration 13024, loss = 0.04106406\n",
      "Iteration 13025, loss = 0.04106042\n",
      "Iteration 13026, loss = 0.04105673\n",
      "Iteration 13027, loss = 0.04105302\n",
      "Iteration 13028, loss = 0.04104927\n",
      "Iteration 13029, loss = 0.04104560\n",
      "Iteration 13030, loss = 0.04104196\n",
      "Iteration 13031, loss = 0.04103824\n",
      "Iteration 13032, loss = 0.04103452\n",
      "Iteration 13033, loss = 0.04103084\n",
      "Iteration 13034, loss = 0.04102715\n",
      "Iteration 13035, loss = 0.04102342\n",
      "Iteration 13036, loss = 0.04101974\n",
      "Iteration 13037, loss = 0.04101605\n",
      "Iteration 13038, loss = 0.04101235\n",
      "Iteration 13039, loss = 0.04100869\n",
      "Iteration 13040, loss = 0.04100495\n",
      "Iteration 13041, loss = 0.04100126\n",
      "Iteration 13042, loss = 0.04099760\n",
      "Iteration 13043, loss = 0.04099387\n",
      "Iteration 13044, loss = 0.04099019\n",
      "Iteration 13045, loss = 0.04098655\n",
      "Iteration 13046, loss = 0.04098282\n",
      "Iteration 13047, loss = 0.04097914\n",
      "Iteration 13048, loss = 0.04097551\n",
      "Iteration 13049, loss = 0.04097179\n",
      "Iteration 13050, loss = 0.04096810\n",
      "Iteration 13051, loss = 0.04096440\n",
      "Iteration 13052, loss = 0.04096069\n",
      "Iteration 13053, loss = 0.04095710\n",
      "Iteration 13054, loss = 0.04095335\n",
      "Iteration 13055, loss = 0.04094968\n",
      "Iteration 13056, loss = 0.04094601\n",
      "Iteration 13057, loss = 0.04094236\n",
      "Iteration 13058, loss = 0.04093859\n",
      "Iteration 13059, loss = 0.04093494\n",
      "Iteration 13060, loss = 0.04093129\n",
      "Iteration 13061, loss = 0.04092756\n",
      "Iteration 13062, loss = 0.04092393\n",
      "Iteration 13063, loss = 0.04092026\n",
      "Iteration 13064, loss = 0.04091655\n",
      "Iteration 13065, loss = 0.04091287\n",
      "Iteration 13066, loss = 0.04090918\n",
      "Iteration 13067, loss = 0.04090554\n",
      "Iteration 13068, loss = 0.04090183\n",
      "Iteration 13069, loss = 0.04089817\n",
      "Iteration 13070, loss = 0.04089453\n",
      "Iteration 13071, loss = 0.04089088\n",
      "Iteration 13072, loss = 0.04088715\n",
      "Iteration 13073, loss = 0.04088347\n",
      "Iteration 13074, loss = 0.04087980\n",
      "Iteration 13075, loss = 0.04087617\n",
      "Iteration 13076, loss = 0.04087246\n",
      "Iteration 13077, loss = 0.04086879\n",
      "Iteration 13078, loss = 0.04086513\n",
      "Iteration 13079, loss = 0.04086150\n",
      "Iteration 13080, loss = 0.04085784\n",
      "Iteration 13081, loss = 0.04085414\n",
      "Iteration 13082, loss = 0.04085049\n",
      "Iteration 13083, loss = 0.04084683\n",
      "Iteration 13084, loss = 0.04084317\n",
      "Iteration 13085, loss = 0.04083952\n",
      "Iteration 13086, loss = 0.04083587\n",
      "Iteration 13087, loss = 0.04083223\n",
      "Iteration 13088, loss = 0.04082855\n",
      "Iteration 13089, loss = 0.04082490\n",
      "Iteration 13090, loss = 0.04082124\n",
      "Iteration 13091, loss = 0.04081767\n",
      "Iteration 13092, loss = 0.04081397\n",
      "Iteration 13093, loss = 0.04081034\n",
      "Iteration 13094, loss = 0.04080668\n",
      "Iteration 13095, loss = 0.04080299\n",
      "Iteration 13096, loss = 0.04079937\n",
      "Iteration 13097, loss = 0.04079567\n",
      "Iteration 13098, loss = 0.04079205\n",
      "Iteration 13099, loss = 0.04078841\n",
      "Iteration 13100, loss = 0.04078475\n",
      "Iteration 13101, loss = 0.04078106\n",
      "Iteration 13102, loss = 0.04077743\n",
      "Iteration 13103, loss = 0.04077379\n",
      "Iteration 13104, loss = 0.04077012\n",
      "Iteration 13105, loss = 0.04076646\n",
      "Iteration 13106, loss = 0.04076282\n",
      "Iteration 13107, loss = 0.04075918\n",
      "Iteration 13108, loss = 0.04075554\n",
      "Iteration 13109, loss = 0.04075185\n",
      "Iteration 13110, loss = 0.04074828\n",
      "Iteration 13111, loss = 0.04074456\n",
      "Iteration 13112, loss = 0.04074096\n",
      "Iteration 13113, loss = 0.04073730\n",
      "Iteration 13114, loss = 0.04073367\n",
      "Iteration 13115, loss = 0.04073007\n",
      "Iteration 13116, loss = 0.04072638\n",
      "Iteration 13117, loss = 0.04072277\n",
      "Iteration 13118, loss = 0.04071912\n",
      "Iteration 13119, loss = 0.04071551\n",
      "Iteration 13120, loss = 0.04071187\n",
      "Iteration 13121, loss = 0.04070820\n",
      "Iteration 13122, loss = 0.04070459\n",
      "Iteration 13123, loss = 0.04070095\n",
      "Iteration 13124, loss = 0.04069733\n",
      "Iteration 13125, loss = 0.04069374\n",
      "Iteration 13126, loss = 0.04069007\n",
      "Iteration 13127, loss = 0.04068643\n",
      "Iteration 13128, loss = 0.04068277\n",
      "Iteration 13129, loss = 0.04067915\n",
      "Iteration 13130, loss = 0.04067549\n",
      "Iteration 13131, loss = 0.04067188\n",
      "Iteration 13132, loss = 0.04066826\n",
      "Iteration 13133, loss = 0.04066462\n",
      "Iteration 13134, loss = 0.04066098\n",
      "Iteration 13135, loss = 0.04065734\n",
      "Iteration 13136, loss = 0.04065375\n",
      "Iteration 13137, loss = 0.04065008\n",
      "Iteration 13138, loss = 0.04064653\n",
      "Iteration 13139, loss = 0.04064290\n",
      "Iteration 13140, loss = 0.04063919\n",
      "Iteration 13141, loss = 0.04063564\n",
      "Iteration 13142, loss = 0.04063199\n",
      "Iteration 13143, loss = 0.04062840\n",
      "Iteration 13144, loss = 0.04062481\n",
      "Iteration 13145, loss = 0.04062115\n",
      "Iteration 13146, loss = 0.04061750\n",
      "Iteration 13147, loss = 0.04061398\n",
      "Iteration 13148, loss = 0.04061026\n",
      "Iteration 13149, loss = 0.04060664\n",
      "Iteration 13150, loss = 0.04060305\n",
      "Iteration 13151, loss = 0.04059942\n",
      "Iteration 13152, loss = 0.04059583\n",
      "Iteration 13153, loss = 0.04059217\n",
      "Iteration 13154, loss = 0.04058854\n",
      "Iteration 13155, loss = 0.04058499\n",
      "Iteration 13156, loss = 0.04058135\n",
      "Iteration 13157, loss = 0.04057773\n",
      "Iteration 13158, loss = 0.04057406\n",
      "Iteration 13159, loss = 0.04057054\n",
      "Iteration 13160, loss = 0.04056689\n",
      "Iteration 13161, loss = 0.04056325\n",
      "Iteration 13162, loss = 0.04055965\n",
      "Iteration 13163, loss = 0.04055600\n",
      "Iteration 13164, loss = 0.04055240\n",
      "Iteration 13165, loss = 0.04054883\n",
      "Iteration 13166, loss = 0.04054517\n",
      "Iteration 13167, loss = 0.04054163\n",
      "Iteration 13168, loss = 0.04053798\n",
      "Iteration 13169, loss = 0.04053443\n",
      "Iteration 13170, loss = 0.04053078\n",
      "Iteration 13171, loss = 0.04052723\n",
      "Iteration 13172, loss = 0.04052363\n",
      "Iteration 13173, loss = 0.04052001\n",
      "Iteration 13174, loss = 0.04051641\n",
      "Iteration 13175, loss = 0.04051280\n",
      "Iteration 13176, loss = 0.04050922\n",
      "Iteration 13177, loss = 0.04050565\n",
      "Iteration 13178, loss = 0.04050198\n",
      "Iteration 13179, loss = 0.04049838\n",
      "Iteration 13180, loss = 0.04049483\n",
      "Iteration 13181, loss = 0.04049121\n",
      "Iteration 13182, loss = 0.04048759\n",
      "Iteration 13183, loss = 0.04048406\n",
      "Iteration 13184, loss = 0.04048045\n",
      "Iteration 13185, loss = 0.04047683\n",
      "Iteration 13186, loss = 0.04047324\n",
      "Iteration 13187, loss = 0.04046972\n",
      "Iteration 13188, loss = 0.04046607\n",
      "Iteration 13189, loss = 0.04046248\n",
      "Iteration 13190, loss = 0.04045886\n",
      "Iteration 13191, loss = 0.04045530\n",
      "Iteration 13192, loss = 0.04045171\n",
      "Iteration 13193, loss = 0.04044813\n",
      "Iteration 13194, loss = 0.04044455\n",
      "Iteration 13195, loss = 0.04044096\n",
      "Iteration 13196, loss = 0.04043735\n",
      "Iteration 13197, loss = 0.04043379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13198, loss = 0.04043019\n",
      "Iteration 13199, loss = 0.04042661\n",
      "Iteration 13200, loss = 0.04042304\n",
      "Iteration 13201, loss = 0.04041941\n",
      "Iteration 13202, loss = 0.04041583\n",
      "Iteration 13203, loss = 0.04041225\n",
      "Iteration 13204, loss = 0.04040866\n",
      "Iteration 13205, loss = 0.04040515\n",
      "Iteration 13206, loss = 0.04040152\n",
      "Iteration 13207, loss = 0.04039792\n",
      "Iteration 13208, loss = 0.04039436\n",
      "Iteration 13209, loss = 0.04039083\n",
      "Iteration 13210, loss = 0.04038720\n",
      "Iteration 13211, loss = 0.04038364\n",
      "Iteration 13212, loss = 0.04038006\n",
      "Iteration 13213, loss = 0.04037650\n",
      "Iteration 13214, loss = 0.04037293\n",
      "Iteration 13215, loss = 0.04036934\n",
      "Iteration 13216, loss = 0.04036576\n",
      "Iteration 13217, loss = 0.04036217\n",
      "Iteration 13218, loss = 0.04035863\n",
      "Iteration 13219, loss = 0.04035502\n",
      "Iteration 13220, loss = 0.04035147\n",
      "Iteration 13221, loss = 0.04034787\n",
      "Iteration 13222, loss = 0.04034430\n",
      "Iteration 13223, loss = 0.04034075\n",
      "Iteration 13224, loss = 0.04033716\n",
      "Iteration 13225, loss = 0.04033361\n",
      "Iteration 13226, loss = 0.04033000\n",
      "Iteration 13227, loss = 0.04032646\n",
      "Iteration 13228, loss = 0.04032286\n",
      "Iteration 13229, loss = 0.04031930\n",
      "Iteration 13230, loss = 0.04031573\n",
      "Iteration 13231, loss = 0.04031218\n",
      "Iteration 13232, loss = 0.04030865\n",
      "Iteration 13233, loss = 0.04030501\n",
      "Iteration 13234, loss = 0.04030146\n",
      "Iteration 13235, loss = 0.04029789\n",
      "Iteration 13236, loss = 0.04029431\n",
      "Iteration 13237, loss = 0.04029073\n",
      "Iteration 13238, loss = 0.04028720\n",
      "Iteration 13239, loss = 0.04028368\n",
      "Iteration 13240, loss = 0.04028004\n",
      "Iteration 13241, loss = 0.04027648\n",
      "Iteration 13242, loss = 0.04027294\n",
      "Iteration 13243, loss = 0.04026935\n",
      "Iteration 13244, loss = 0.04026582\n",
      "Iteration 13245, loss = 0.04026222\n",
      "Iteration 13246, loss = 0.04025867\n",
      "Iteration 13247, loss = 0.04025510\n",
      "Iteration 13248, loss = 0.04025161\n",
      "Iteration 13249, loss = 0.04024801\n",
      "Iteration 13250, loss = 0.04024446\n",
      "Iteration 13251, loss = 0.04024092\n",
      "Iteration 13252, loss = 0.04023741\n",
      "Iteration 13253, loss = 0.04023378\n",
      "Iteration 13254, loss = 0.04023026\n",
      "Iteration 13255, loss = 0.04022671\n",
      "Iteration 13256, loss = 0.04022317\n",
      "Iteration 13257, loss = 0.04021962\n",
      "Iteration 13258, loss = 0.04021607\n",
      "Iteration 13259, loss = 0.04021255\n",
      "Iteration 13260, loss = 0.04020897\n",
      "Iteration 13261, loss = 0.04020539\n",
      "Iteration 13262, loss = 0.04020191\n",
      "Iteration 13263, loss = 0.04019830\n",
      "Iteration 13264, loss = 0.04019480\n",
      "Iteration 13265, loss = 0.04019124\n",
      "Iteration 13266, loss = 0.04018766\n",
      "Iteration 13267, loss = 0.04018417\n",
      "Iteration 13268, loss = 0.04018059\n",
      "Iteration 13269, loss = 0.04017708\n",
      "Iteration 13270, loss = 0.04017347\n",
      "Iteration 13271, loss = 0.04016997\n",
      "Iteration 13272, loss = 0.04016644\n",
      "Iteration 13273, loss = 0.04016286\n",
      "Iteration 13274, loss = 0.04015932\n",
      "Iteration 13275, loss = 0.04015578\n",
      "Iteration 13276, loss = 0.04015228\n",
      "Iteration 13277, loss = 0.04014878\n",
      "Iteration 13278, loss = 0.04014515\n",
      "Iteration 13279, loss = 0.04014171\n",
      "Iteration 13280, loss = 0.04013815\n",
      "Iteration 13281, loss = 0.04013459\n",
      "Iteration 13282, loss = 0.04013108\n",
      "Iteration 13283, loss = 0.04012752\n",
      "Iteration 13284, loss = 0.04012405\n",
      "Iteration 13285, loss = 0.04012047\n",
      "Iteration 13286, loss = 0.04011695\n",
      "Iteration 13287, loss = 0.04011343\n",
      "Iteration 13288, loss = 0.04010989\n",
      "Iteration 13289, loss = 0.04010641\n",
      "Iteration 13290, loss = 0.04010280\n",
      "Iteration 13291, loss = 0.04009932\n",
      "Iteration 13292, loss = 0.04009579\n",
      "Iteration 13293, loss = 0.04009227\n",
      "Iteration 13294, loss = 0.04008870\n",
      "Iteration 13295, loss = 0.04008521\n",
      "Iteration 13296, loss = 0.04008170\n",
      "Iteration 13297, loss = 0.04007814\n",
      "Iteration 13298, loss = 0.04007462\n",
      "Iteration 13299, loss = 0.04007111\n",
      "Iteration 13300, loss = 0.04006756\n",
      "Iteration 13301, loss = 0.04006402\n",
      "Iteration 13302, loss = 0.04006059\n",
      "Iteration 13303, loss = 0.04005695\n",
      "Iteration 13304, loss = 0.04005347\n",
      "Iteration 13305, loss = 0.04004993\n",
      "Iteration 13306, loss = 0.04004640\n",
      "Iteration 13307, loss = 0.04004286\n",
      "Iteration 13308, loss = 0.04003942\n",
      "Iteration 13309, loss = 0.04003585\n",
      "Iteration 13310, loss = 0.04003232\n",
      "Iteration 13311, loss = 0.04002882\n",
      "Iteration 13312, loss = 0.04002527\n",
      "Iteration 13313, loss = 0.04002180\n",
      "Iteration 13314, loss = 0.04001825\n",
      "Iteration 13315, loss = 0.04001472\n",
      "Iteration 13316, loss = 0.04001124\n",
      "Iteration 13317, loss = 0.04000772\n",
      "Iteration 13318, loss = 0.04000425\n",
      "Iteration 13319, loss = 0.04000071\n",
      "Iteration 13320, loss = 0.03999722\n",
      "Iteration 13321, loss = 0.03999371\n",
      "Iteration 13322, loss = 0.03999023\n",
      "Iteration 13323, loss = 0.03998672\n",
      "Iteration 13324, loss = 0.03998324\n",
      "Iteration 13325, loss = 0.03997970\n",
      "Iteration 13326, loss = 0.03997619\n",
      "Iteration 13327, loss = 0.03997278\n",
      "Iteration 13328, loss = 0.03996928\n",
      "Iteration 13329, loss = 0.03996570\n",
      "Iteration 13330, loss = 0.03996221\n",
      "Iteration 13331, loss = 0.03995871\n",
      "Iteration 13332, loss = 0.03995522\n",
      "Iteration 13333, loss = 0.03995171\n",
      "Iteration 13334, loss = 0.03994823\n",
      "Iteration 13335, loss = 0.03994475\n",
      "Iteration 13336, loss = 0.03994124\n",
      "Iteration 13337, loss = 0.03993773\n",
      "Iteration 13338, loss = 0.03993426\n",
      "Iteration 13339, loss = 0.03993077\n",
      "Iteration 13340, loss = 0.03992724\n",
      "Iteration 13341, loss = 0.03992373\n",
      "Iteration 13342, loss = 0.03992028\n",
      "Iteration 13343, loss = 0.03991677\n",
      "Iteration 13344, loss = 0.03991326\n",
      "Iteration 13345, loss = 0.03990976\n",
      "Iteration 13346, loss = 0.03990629\n",
      "Iteration 13347, loss = 0.03990276\n",
      "Iteration 13348, loss = 0.03989924\n",
      "Iteration 13349, loss = 0.03989579\n",
      "Iteration 13350, loss = 0.03989227\n",
      "Iteration 13351, loss = 0.03988878\n",
      "Iteration 13352, loss = 0.03988528\n",
      "Iteration 13353, loss = 0.03988180\n",
      "Iteration 13354, loss = 0.03987833\n",
      "Iteration 13355, loss = 0.03987486\n",
      "Iteration 13356, loss = 0.03987133\n",
      "Iteration 13357, loss = 0.03986787\n",
      "Iteration 13358, loss = 0.03986437\n",
      "Iteration 13359, loss = 0.03986085\n",
      "Iteration 13360, loss = 0.03985740\n",
      "Iteration 13361, loss = 0.03985387\n",
      "Iteration 13362, loss = 0.03985035\n",
      "Iteration 13363, loss = 0.03984687\n",
      "Iteration 13364, loss = 0.03984338\n",
      "Iteration 13365, loss = 0.03983996\n",
      "Iteration 13366, loss = 0.03983645\n",
      "Iteration 13367, loss = 0.03983294\n",
      "Iteration 13368, loss = 0.03982948\n",
      "Iteration 13369, loss = 0.03982600\n",
      "Iteration 13370, loss = 0.03982251\n",
      "Iteration 13371, loss = 0.03981906\n",
      "Iteration 13372, loss = 0.03981554\n",
      "Iteration 13373, loss = 0.03981207\n",
      "Iteration 13374, loss = 0.03980860\n",
      "Iteration 13375, loss = 0.03980514\n",
      "Iteration 13376, loss = 0.03980164\n",
      "Iteration 13377, loss = 0.03979819\n",
      "Iteration 13378, loss = 0.03979470\n",
      "Iteration 13379, loss = 0.03979123\n",
      "Iteration 13380, loss = 0.03978777\n",
      "Iteration 13381, loss = 0.03978427\n",
      "Iteration 13382, loss = 0.03978080\n",
      "Iteration 13383, loss = 0.03977736\n",
      "Iteration 13384, loss = 0.03977386\n",
      "Iteration 13385, loss = 0.03977041\n",
      "Iteration 13386, loss = 0.03976696\n",
      "Iteration 13387, loss = 0.03976350\n",
      "Iteration 13388, loss = 0.03975996\n",
      "Iteration 13389, loss = 0.03975649\n",
      "Iteration 13390, loss = 0.03975300\n",
      "Iteration 13391, loss = 0.03974951\n",
      "Iteration 13392, loss = 0.03974602\n",
      "Iteration 13393, loss = 0.03974261\n",
      "Iteration 13394, loss = 0.03973914\n",
      "Iteration 13395, loss = 0.03973564\n",
      "Iteration 13396, loss = 0.03973219\n",
      "Iteration 13397, loss = 0.03972877\n",
      "Iteration 13398, loss = 0.03972522\n",
      "Iteration 13399, loss = 0.03972180\n",
      "Iteration 13400, loss = 0.03971831\n",
      "Iteration 13401, loss = 0.03971487\n",
      "Iteration 13402, loss = 0.03971144\n",
      "Iteration 13403, loss = 0.03970795\n",
      "Iteration 13404, loss = 0.03970444\n",
      "Iteration 13405, loss = 0.03970101\n",
      "Iteration 13406, loss = 0.03969756\n",
      "Iteration 13407, loss = 0.03969410\n",
      "Iteration 13408, loss = 0.03969064\n",
      "Iteration 13409, loss = 0.03968718\n",
      "Iteration 13410, loss = 0.03968372\n",
      "Iteration 13411, loss = 0.03968028\n",
      "Iteration 13412, loss = 0.03967682\n",
      "Iteration 13413, loss = 0.03967338\n",
      "Iteration 13414, loss = 0.03966990\n",
      "Iteration 13415, loss = 0.03966646\n",
      "Iteration 13416, loss = 0.03966299\n",
      "Iteration 13417, loss = 0.03965952\n",
      "Iteration 13418, loss = 0.03965610\n",
      "Iteration 13419, loss = 0.03965261\n",
      "Iteration 13420, loss = 0.03964915\n",
      "Iteration 13421, loss = 0.03964575\n",
      "Iteration 13422, loss = 0.03964227\n",
      "Iteration 13423, loss = 0.03963882\n",
      "Iteration 13424, loss = 0.03963538\n",
      "Iteration 13425, loss = 0.03963192\n",
      "Iteration 13426, loss = 0.03962844\n",
      "Iteration 13427, loss = 0.03962502\n",
      "Iteration 13428, loss = 0.03962159\n",
      "Iteration 13429, loss = 0.03961811\n",
      "Iteration 13430, loss = 0.03961464\n",
      "Iteration 13431, loss = 0.03961131\n",
      "Iteration 13432, loss = 0.03960777\n",
      "Iteration 13433, loss = 0.03960432\n",
      "Iteration 13434, loss = 0.03960086\n",
      "Iteration 13435, loss = 0.03959749\n",
      "Iteration 13436, loss = 0.03959401\n",
      "Iteration 13437, loss = 0.03959062\n",
      "Iteration 13438, loss = 0.03958714\n",
      "Iteration 13439, loss = 0.03958370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13440, loss = 0.03958027\n",
      "Iteration 13441, loss = 0.03957685\n",
      "Iteration 13442, loss = 0.03957338\n",
      "Iteration 13443, loss = 0.03956997\n",
      "Iteration 13444, loss = 0.03956652\n",
      "Iteration 13445, loss = 0.03956314\n",
      "Iteration 13446, loss = 0.03955969\n",
      "Iteration 13447, loss = 0.03955623\n",
      "Iteration 13448, loss = 0.03955283\n",
      "Iteration 13449, loss = 0.03954937\n",
      "Iteration 13450, loss = 0.03954595\n",
      "Iteration 13451, loss = 0.03954251\n",
      "Iteration 13452, loss = 0.03953910\n",
      "Iteration 13453, loss = 0.03953570\n",
      "Iteration 13454, loss = 0.03953224\n",
      "Iteration 13455, loss = 0.03952878\n",
      "Iteration 13456, loss = 0.03952539\n",
      "Iteration 13457, loss = 0.03952196\n",
      "Iteration 13458, loss = 0.03951856\n",
      "Iteration 13459, loss = 0.03951511\n",
      "Iteration 13460, loss = 0.03951167\n",
      "Iteration 13461, loss = 0.03950827\n",
      "Iteration 13462, loss = 0.03950480\n",
      "Iteration 13463, loss = 0.03950138\n",
      "Iteration 13464, loss = 0.03949794\n",
      "Iteration 13465, loss = 0.03949457\n",
      "Iteration 13466, loss = 0.03949107\n",
      "Iteration 13467, loss = 0.03948772\n",
      "Iteration 13468, loss = 0.03948426\n",
      "Iteration 13469, loss = 0.03948081\n",
      "Iteration 13470, loss = 0.03947743\n",
      "Iteration 13471, loss = 0.03947396\n",
      "Iteration 13472, loss = 0.03947054\n",
      "Iteration 13473, loss = 0.03946709\n",
      "Iteration 13474, loss = 0.03946372\n",
      "Iteration 13475, loss = 0.03946029\n",
      "Iteration 13476, loss = 0.03945686\n",
      "Iteration 13477, loss = 0.03945345\n",
      "Iteration 13478, loss = 0.03945004\n",
      "Iteration 13479, loss = 0.03944662\n",
      "Iteration 13480, loss = 0.03944322\n",
      "Iteration 13481, loss = 0.03943982\n",
      "Iteration 13482, loss = 0.03943640\n",
      "Iteration 13483, loss = 0.03943296\n",
      "Iteration 13484, loss = 0.03942954\n",
      "Iteration 13485, loss = 0.03942615\n",
      "Iteration 13486, loss = 0.03942278\n",
      "Iteration 13487, loss = 0.03941934\n",
      "Iteration 13488, loss = 0.03941596\n",
      "Iteration 13489, loss = 0.03941253\n",
      "Iteration 13490, loss = 0.03940917\n",
      "Iteration 13491, loss = 0.03940572\n",
      "Iteration 13492, loss = 0.03940235\n",
      "Iteration 13493, loss = 0.03939894\n",
      "Iteration 13494, loss = 0.03939553\n",
      "Iteration 13495, loss = 0.03939215\n",
      "Iteration 13496, loss = 0.03938877\n",
      "Iteration 13497, loss = 0.03938533\n",
      "Iteration 13498, loss = 0.03938194\n",
      "Iteration 13499, loss = 0.03937856\n",
      "Iteration 13500, loss = 0.03937516\n",
      "Iteration 13501, loss = 0.03937178\n",
      "Iteration 13502, loss = 0.03936837\n",
      "Iteration 13503, loss = 0.03936500\n",
      "Iteration 13504, loss = 0.03936162\n",
      "Iteration 13505, loss = 0.03935820\n",
      "Iteration 13506, loss = 0.03935483\n",
      "Iteration 13507, loss = 0.03935143\n",
      "Iteration 13508, loss = 0.03934801\n",
      "Iteration 13509, loss = 0.03934462\n",
      "Iteration 13510, loss = 0.03934119\n",
      "Iteration 13511, loss = 0.03933782\n",
      "Iteration 13512, loss = 0.03933442\n",
      "Iteration 13513, loss = 0.03933102\n",
      "Iteration 13514, loss = 0.03932762\n",
      "Iteration 13515, loss = 0.03932418\n",
      "Iteration 13516, loss = 0.03932083\n",
      "Iteration 13517, loss = 0.03931745\n",
      "Iteration 13518, loss = 0.03931401\n",
      "Iteration 13519, loss = 0.03931067\n",
      "Iteration 13520, loss = 0.03930723\n",
      "Iteration 13521, loss = 0.03930382\n",
      "Iteration 13522, loss = 0.03930045\n",
      "Iteration 13523, loss = 0.03929699\n",
      "Iteration 13524, loss = 0.03929364\n",
      "Iteration 13525, loss = 0.03929029\n",
      "Iteration 13526, loss = 0.03928686\n",
      "Iteration 13527, loss = 0.03928344\n",
      "Iteration 13528, loss = 0.03928009\n",
      "Iteration 13529, loss = 0.03927668\n",
      "Iteration 13530, loss = 0.03927327\n",
      "Iteration 13531, loss = 0.03926991\n",
      "Iteration 13532, loss = 0.03926657\n",
      "Iteration 13533, loss = 0.03926317\n",
      "Iteration 13534, loss = 0.03925976\n",
      "Iteration 13535, loss = 0.03925638\n",
      "Iteration 13536, loss = 0.03925299\n",
      "Iteration 13537, loss = 0.03924963\n",
      "Iteration 13538, loss = 0.03924623\n",
      "Iteration 13539, loss = 0.03924283\n",
      "Iteration 13540, loss = 0.03923950\n",
      "Iteration 13541, loss = 0.03923611\n",
      "Iteration 13542, loss = 0.03923270\n",
      "Iteration 13543, loss = 0.03922936\n",
      "Iteration 13544, loss = 0.03922600\n",
      "Iteration 13545, loss = 0.03922261\n",
      "Iteration 13546, loss = 0.03921924\n",
      "Iteration 13547, loss = 0.03921588\n",
      "Iteration 13548, loss = 0.03921251\n",
      "Iteration 13549, loss = 0.03920914\n",
      "Iteration 13550, loss = 0.03920575\n",
      "Iteration 13551, loss = 0.03920243\n",
      "Iteration 13552, loss = 0.03919900\n",
      "Iteration 13553, loss = 0.03919567\n",
      "Iteration 13554, loss = 0.03919225\n",
      "Iteration 13555, loss = 0.03918888\n",
      "Iteration 13556, loss = 0.03918557\n",
      "Iteration 13557, loss = 0.03918222\n",
      "Iteration 13558, loss = 0.03917877\n",
      "Iteration 13559, loss = 0.03917544\n",
      "Iteration 13560, loss = 0.03917212\n",
      "Iteration 13561, loss = 0.03916875\n",
      "Iteration 13562, loss = 0.03916538\n",
      "Iteration 13563, loss = 0.03916199\n",
      "Iteration 13564, loss = 0.03915863\n",
      "Iteration 13565, loss = 0.03915528\n",
      "Iteration 13566, loss = 0.03915192\n",
      "Iteration 13567, loss = 0.03914856\n",
      "Iteration 13568, loss = 0.03914524\n",
      "Iteration 13569, loss = 0.03914183\n",
      "Iteration 13570, loss = 0.03913847\n",
      "Iteration 13571, loss = 0.03913511\n",
      "Iteration 13572, loss = 0.03913180\n",
      "Iteration 13573, loss = 0.03912839\n",
      "Iteration 13574, loss = 0.03912505\n",
      "Iteration 13575, loss = 0.03912169\n",
      "Iteration 13576, loss = 0.03911837\n",
      "Iteration 13577, loss = 0.03911507\n",
      "Iteration 13578, loss = 0.03911168\n",
      "Iteration 13579, loss = 0.03910834\n",
      "Iteration 13580, loss = 0.03910494\n",
      "Iteration 13581, loss = 0.03910162\n",
      "Iteration 13582, loss = 0.03909823\n",
      "Iteration 13583, loss = 0.03909491\n",
      "Iteration 13584, loss = 0.03909153\n",
      "Iteration 13585, loss = 0.03908818\n",
      "Iteration 13586, loss = 0.03908486\n",
      "Iteration 13587, loss = 0.03908151\n",
      "Iteration 13588, loss = 0.03907813\n",
      "Iteration 13589, loss = 0.03907486\n",
      "Iteration 13590, loss = 0.03907148\n",
      "Iteration 13591, loss = 0.03906811\n",
      "Iteration 13592, loss = 0.03906477\n",
      "Iteration 13593, loss = 0.03906140\n",
      "Iteration 13594, loss = 0.03905813\n",
      "Iteration 13595, loss = 0.03905472\n",
      "Iteration 13596, loss = 0.03905145\n",
      "Iteration 13597, loss = 0.03904807\n",
      "Iteration 13598, loss = 0.03904475\n",
      "Iteration 13599, loss = 0.03904139\n",
      "Iteration 13600, loss = 0.03903806\n",
      "Iteration 13601, loss = 0.03903473\n",
      "Iteration 13602, loss = 0.03903136\n",
      "Iteration 13603, loss = 0.03902801\n",
      "Iteration 13604, loss = 0.03902468\n",
      "Iteration 13605, loss = 0.03902133\n",
      "Iteration 13606, loss = 0.03901800\n",
      "Iteration 13607, loss = 0.03901464\n",
      "Iteration 13608, loss = 0.03901132\n",
      "Iteration 13609, loss = 0.03900798\n",
      "Iteration 13610, loss = 0.03900463\n",
      "Iteration 13611, loss = 0.03900127\n",
      "Iteration 13612, loss = 0.03899793\n",
      "Iteration 13613, loss = 0.03899463\n",
      "Iteration 13614, loss = 0.03899126\n",
      "Iteration 13615, loss = 0.03898794\n",
      "Iteration 13616, loss = 0.03898459\n",
      "Iteration 13617, loss = 0.03898127\n",
      "Iteration 13618, loss = 0.03897790\n",
      "Iteration 13619, loss = 0.03897460\n",
      "Iteration 13620, loss = 0.03897125\n",
      "Iteration 13621, loss = 0.03896793\n",
      "Iteration 13622, loss = 0.03896456\n",
      "Iteration 13623, loss = 0.03896125\n",
      "Iteration 13624, loss = 0.03895796\n",
      "Iteration 13625, loss = 0.03895460\n",
      "Iteration 13626, loss = 0.03895125\n",
      "Iteration 13627, loss = 0.03894787\n",
      "Iteration 13628, loss = 0.03894461\n",
      "Iteration 13629, loss = 0.03894126\n",
      "Iteration 13630, loss = 0.03893792\n",
      "Iteration 13631, loss = 0.03893463\n",
      "Iteration 13632, loss = 0.03893126\n",
      "Iteration 13633, loss = 0.03892794\n",
      "Iteration 13634, loss = 0.03892461\n",
      "Iteration 13635, loss = 0.03892125\n",
      "Iteration 13636, loss = 0.03891797\n",
      "Iteration 13637, loss = 0.03891462\n",
      "Iteration 13638, loss = 0.03891130\n",
      "Iteration 13639, loss = 0.03890802\n",
      "Iteration 13640, loss = 0.03890470\n",
      "Iteration 13641, loss = 0.03890141\n",
      "Iteration 13642, loss = 0.03889806\n",
      "Iteration 13643, loss = 0.03889470\n",
      "Iteration 13644, loss = 0.03889143\n",
      "Iteration 13645, loss = 0.03888812\n",
      "Iteration 13646, loss = 0.03888479\n",
      "Iteration 13647, loss = 0.03888148\n",
      "Iteration 13648, loss = 0.03887818\n",
      "Iteration 13649, loss = 0.03887485\n",
      "Iteration 13650, loss = 0.03887151\n",
      "Iteration 13651, loss = 0.03886822\n",
      "Iteration 13652, loss = 0.03886493\n",
      "Iteration 13653, loss = 0.03886160\n",
      "Iteration 13654, loss = 0.03885830\n",
      "Iteration 13655, loss = 0.03885502\n",
      "Iteration 13656, loss = 0.03885165\n",
      "Iteration 13657, loss = 0.03884838\n",
      "Iteration 13658, loss = 0.03884507\n",
      "Iteration 13659, loss = 0.03884176\n",
      "Iteration 13660, loss = 0.03883847\n",
      "Iteration 13661, loss = 0.03883514\n",
      "Iteration 13662, loss = 0.03883186\n",
      "Iteration 13663, loss = 0.03882852\n",
      "Iteration 13664, loss = 0.03882520\n",
      "Iteration 13665, loss = 0.03882192\n",
      "Iteration 13666, loss = 0.03881856\n",
      "Iteration 13667, loss = 0.03881532\n",
      "Iteration 13668, loss = 0.03881200\n",
      "Iteration 13669, loss = 0.03880871\n",
      "Iteration 13670, loss = 0.03880543\n",
      "Iteration 13671, loss = 0.03880206\n",
      "Iteration 13672, loss = 0.03879882\n",
      "Iteration 13673, loss = 0.03879544\n",
      "Iteration 13674, loss = 0.03879223\n",
      "Iteration 13675, loss = 0.03878886\n",
      "Iteration 13676, loss = 0.03878566\n",
      "Iteration 13677, loss = 0.03878235\n",
      "Iteration 13678, loss = 0.03877897\n",
      "Iteration 13679, loss = 0.03877573\n",
      "Iteration 13680, loss = 0.03877247\n",
      "Iteration 13681, loss = 0.03876915\n",
      "Iteration 13682, loss = 0.03876587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13683, loss = 0.03876256\n",
      "Iteration 13684, loss = 0.03875925\n",
      "Iteration 13685, loss = 0.03875602\n",
      "Iteration 13686, loss = 0.03875269\n",
      "Iteration 13687, loss = 0.03874942\n",
      "Iteration 13688, loss = 0.03874611\n",
      "Iteration 13689, loss = 0.03874282\n",
      "Iteration 13690, loss = 0.03873951\n",
      "Iteration 13691, loss = 0.03873622\n",
      "Iteration 13692, loss = 0.03873294\n",
      "Iteration 13693, loss = 0.03872963\n",
      "Iteration 13694, loss = 0.03872636\n",
      "Iteration 13695, loss = 0.03872303\n",
      "Iteration 13696, loss = 0.03871974\n",
      "Iteration 13697, loss = 0.03871644\n",
      "Iteration 13698, loss = 0.03871317\n",
      "Iteration 13699, loss = 0.03870991\n",
      "Iteration 13700, loss = 0.03870656\n",
      "Iteration 13701, loss = 0.03870331\n",
      "Iteration 13702, loss = 0.03870000\n",
      "Iteration 13703, loss = 0.03869674\n",
      "Iteration 13704, loss = 0.03869343\n",
      "Iteration 13705, loss = 0.03869011\n",
      "Iteration 13706, loss = 0.03868685\n",
      "Iteration 13707, loss = 0.03868354\n",
      "Iteration 13708, loss = 0.03868027\n",
      "Iteration 13709, loss = 0.03867697\n",
      "Iteration 13710, loss = 0.03867368\n",
      "Iteration 13711, loss = 0.03867040\n",
      "Iteration 13712, loss = 0.03866711\n",
      "Iteration 13713, loss = 0.03866381\n",
      "Iteration 13714, loss = 0.03866056\n",
      "Iteration 13715, loss = 0.03865725\n",
      "Iteration 13716, loss = 0.03865396\n",
      "Iteration 13717, loss = 0.03865070\n",
      "Iteration 13718, loss = 0.03864741\n",
      "Iteration 13719, loss = 0.03864413\n",
      "Iteration 13720, loss = 0.03864087\n",
      "Iteration 13721, loss = 0.03863762\n",
      "Iteration 13722, loss = 0.03863433\n",
      "Iteration 13723, loss = 0.03863108\n",
      "Iteration 13724, loss = 0.03862776\n",
      "Iteration 13725, loss = 0.03862453\n",
      "Iteration 13726, loss = 0.03862122\n",
      "Iteration 13727, loss = 0.03861794\n",
      "Iteration 13728, loss = 0.03861471\n",
      "Iteration 13729, loss = 0.03861140\n",
      "Iteration 13730, loss = 0.03860811\n",
      "Iteration 13731, loss = 0.03860493\n",
      "Iteration 13732, loss = 0.03860161\n",
      "Iteration 13733, loss = 0.03859833\n",
      "Iteration 13734, loss = 0.03859510\n",
      "Iteration 13735, loss = 0.03859179\n",
      "Iteration 13736, loss = 0.03858856\n",
      "Iteration 13737, loss = 0.03858528\n",
      "Iteration 13738, loss = 0.03858205\n",
      "Iteration 13739, loss = 0.03857876\n",
      "Iteration 13740, loss = 0.03857552\n",
      "Iteration 13741, loss = 0.03857224\n",
      "Iteration 13742, loss = 0.03856894\n",
      "Iteration 13743, loss = 0.03856571\n",
      "Iteration 13744, loss = 0.03856246\n",
      "Iteration 13745, loss = 0.03855920\n",
      "Iteration 13746, loss = 0.03855593\n",
      "Iteration 13747, loss = 0.03855271\n",
      "Iteration 13748, loss = 0.03854946\n",
      "Iteration 13749, loss = 0.03854614\n",
      "Iteration 13750, loss = 0.03854286\n",
      "Iteration 13751, loss = 0.03853967\n",
      "Iteration 13752, loss = 0.03853636\n",
      "Iteration 13753, loss = 0.03853311\n",
      "Iteration 13754, loss = 0.03852988\n",
      "Iteration 13755, loss = 0.03852661\n",
      "Iteration 13756, loss = 0.03852335\n",
      "Iteration 13757, loss = 0.03852013\n",
      "Iteration 13758, loss = 0.03851679\n",
      "Iteration 13759, loss = 0.03851359\n",
      "Iteration 13760, loss = 0.03851032\n",
      "Iteration 13761, loss = 0.03850702\n",
      "Iteration 13762, loss = 0.03850375\n",
      "Iteration 13763, loss = 0.03850053\n",
      "Iteration 13764, loss = 0.03849728\n",
      "Iteration 13765, loss = 0.03849405\n",
      "Iteration 13766, loss = 0.03849077\n",
      "Iteration 13767, loss = 0.03848749\n",
      "Iteration 13768, loss = 0.03848427\n",
      "Iteration 13769, loss = 0.03848100\n",
      "Iteration 13770, loss = 0.03847778\n",
      "Iteration 13771, loss = 0.03847455\n",
      "Iteration 13772, loss = 0.03847124\n",
      "Iteration 13773, loss = 0.03846804\n",
      "Iteration 13774, loss = 0.03846482\n",
      "Iteration 13775, loss = 0.03846155\n",
      "Iteration 13776, loss = 0.03845833\n",
      "Iteration 13777, loss = 0.03845506\n",
      "Iteration 13778, loss = 0.03845181\n",
      "Iteration 13779, loss = 0.03844860\n",
      "Iteration 13780, loss = 0.03844533\n",
      "Iteration 13781, loss = 0.03844212\n",
      "Iteration 13782, loss = 0.03843885\n",
      "Iteration 13783, loss = 0.03843560\n",
      "Iteration 13784, loss = 0.03843239\n",
      "Iteration 13785, loss = 0.03842913\n",
      "Iteration 13786, loss = 0.03842592\n",
      "Iteration 13787, loss = 0.03842273\n",
      "Iteration 13788, loss = 0.03841942\n",
      "Iteration 13789, loss = 0.03841619\n",
      "Iteration 13790, loss = 0.03841299\n",
      "Iteration 13791, loss = 0.03840970\n",
      "Iteration 13792, loss = 0.03840647\n",
      "Iteration 13793, loss = 0.03840322\n",
      "Iteration 13794, loss = 0.03840000\n",
      "Iteration 13795, loss = 0.03839674\n",
      "Iteration 13796, loss = 0.03839350\n",
      "Iteration 13797, loss = 0.03839027\n",
      "Iteration 13798, loss = 0.03838703\n",
      "Iteration 13799, loss = 0.03838382\n",
      "Iteration 13800, loss = 0.03838056\n",
      "Iteration 13801, loss = 0.03837733\n",
      "Iteration 13802, loss = 0.03837408\n",
      "Iteration 13803, loss = 0.03837089\n",
      "Iteration 13804, loss = 0.03836769\n",
      "Iteration 13805, loss = 0.03836444\n",
      "Iteration 13806, loss = 0.03836116\n",
      "Iteration 13807, loss = 0.03835797\n",
      "Iteration 13808, loss = 0.03835474\n",
      "Iteration 13809, loss = 0.03835150\n",
      "Iteration 13810, loss = 0.03834832\n",
      "Iteration 13811, loss = 0.03834508\n",
      "Iteration 13812, loss = 0.03834192\n",
      "Iteration 13813, loss = 0.03833862\n",
      "Iteration 13814, loss = 0.03833540\n",
      "Iteration 13815, loss = 0.03833213\n",
      "Iteration 13816, loss = 0.03832892\n",
      "Iteration 13817, loss = 0.03832570\n",
      "Iteration 13818, loss = 0.03832250\n",
      "Iteration 13819, loss = 0.03831925\n",
      "Iteration 13820, loss = 0.03831601\n",
      "Iteration 13821, loss = 0.03831279\n",
      "Iteration 13822, loss = 0.03830957\n",
      "Iteration 13823, loss = 0.03830635\n",
      "Iteration 13824, loss = 0.03830310\n",
      "Iteration 13825, loss = 0.03829992\n",
      "Iteration 13826, loss = 0.03829671\n",
      "Iteration 13827, loss = 0.03829346\n",
      "Iteration 13828, loss = 0.03829025\n",
      "Iteration 13829, loss = 0.03828705\n",
      "Iteration 13830, loss = 0.03828385\n",
      "Iteration 13831, loss = 0.03828059\n",
      "Iteration 13832, loss = 0.03827738\n",
      "Iteration 13833, loss = 0.03827411\n",
      "Iteration 13834, loss = 0.03827098\n",
      "Iteration 13835, loss = 0.03826772\n",
      "Iteration 13836, loss = 0.03826448\n",
      "Iteration 13837, loss = 0.03826132\n",
      "Iteration 13838, loss = 0.03825808\n",
      "Iteration 13839, loss = 0.03825486\n",
      "Iteration 13840, loss = 0.03825160\n",
      "Iteration 13841, loss = 0.03824837\n",
      "Iteration 13842, loss = 0.03824519\n",
      "Iteration 13843, loss = 0.03824197\n",
      "Iteration 13844, loss = 0.03823871\n",
      "Iteration 13845, loss = 0.03823552\n",
      "Iteration 13846, loss = 0.03823236\n",
      "Iteration 13847, loss = 0.03822910\n",
      "Iteration 13848, loss = 0.03822591\n",
      "Iteration 13849, loss = 0.03822269\n",
      "Iteration 13850, loss = 0.03821942\n",
      "Iteration 13851, loss = 0.03821625\n",
      "Iteration 13852, loss = 0.03821304\n",
      "Iteration 13853, loss = 0.03820979\n",
      "Iteration 13854, loss = 0.03820662\n",
      "Iteration 13855, loss = 0.03820338\n",
      "Iteration 13856, loss = 0.03820019\n",
      "Iteration 13857, loss = 0.03819694\n",
      "Iteration 13858, loss = 0.03819377\n",
      "Iteration 13859, loss = 0.03819055\n",
      "Iteration 13860, loss = 0.03818733\n",
      "Iteration 13861, loss = 0.03818416\n",
      "Iteration 13862, loss = 0.03818092\n",
      "Iteration 13863, loss = 0.03817771\n",
      "Iteration 13864, loss = 0.03817453\n",
      "Iteration 13865, loss = 0.03817135\n",
      "Iteration 13866, loss = 0.03816812\n",
      "Iteration 13867, loss = 0.03816492\n",
      "Iteration 13868, loss = 0.03816170\n",
      "Iteration 13869, loss = 0.03815852\n",
      "Iteration 13870, loss = 0.03815534\n",
      "Iteration 13871, loss = 0.03815213\n",
      "Iteration 13872, loss = 0.03814893\n",
      "Iteration 13873, loss = 0.03814573\n",
      "Iteration 13874, loss = 0.03814255\n",
      "Iteration 13875, loss = 0.03813937\n",
      "Iteration 13876, loss = 0.03813618\n",
      "Iteration 13877, loss = 0.03813294\n",
      "Iteration 13878, loss = 0.03812978\n",
      "Iteration 13879, loss = 0.03812656\n",
      "Iteration 13880, loss = 0.03812343\n",
      "Iteration 13881, loss = 0.03812021\n",
      "Iteration 13882, loss = 0.03811703\n",
      "Iteration 13883, loss = 0.03811379\n",
      "Iteration 13884, loss = 0.03811061\n",
      "Iteration 13885, loss = 0.03810741\n",
      "Iteration 13886, loss = 0.03810425\n",
      "Iteration 13887, loss = 0.03810103\n",
      "Iteration 13888, loss = 0.03809784\n",
      "Iteration 13889, loss = 0.03809466\n",
      "Iteration 13890, loss = 0.03809149\n",
      "Iteration 13891, loss = 0.03808832\n",
      "Iteration 13892, loss = 0.03808510\n",
      "Iteration 13893, loss = 0.03808194\n",
      "Iteration 13894, loss = 0.03807872\n",
      "Iteration 13895, loss = 0.03807556\n",
      "Iteration 13896, loss = 0.03807238\n",
      "Iteration 13897, loss = 0.03806920\n",
      "Iteration 13898, loss = 0.03806607\n",
      "Iteration 13899, loss = 0.03806284\n",
      "Iteration 13900, loss = 0.03805965\n",
      "Iteration 13901, loss = 0.03805648\n",
      "Iteration 13902, loss = 0.03805332\n",
      "Iteration 13903, loss = 0.03805013\n",
      "Iteration 13904, loss = 0.03804694\n",
      "Iteration 13905, loss = 0.03804378\n",
      "Iteration 13906, loss = 0.03804060\n",
      "Iteration 13907, loss = 0.03803740\n",
      "Iteration 13908, loss = 0.03803425\n",
      "Iteration 13909, loss = 0.03803104\n",
      "Iteration 13910, loss = 0.03802792\n",
      "Iteration 13911, loss = 0.03802472\n",
      "Iteration 13912, loss = 0.03802154\n",
      "Iteration 13913, loss = 0.03801834\n",
      "Iteration 13914, loss = 0.03801518\n",
      "Iteration 13915, loss = 0.03801206\n",
      "Iteration 13916, loss = 0.03800886\n",
      "Iteration 13917, loss = 0.03800568\n",
      "Iteration 13918, loss = 0.03800249\n",
      "Iteration 13919, loss = 0.03799932\n",
      "Iteration 13920, loss = 0.03799616\n",
      "Iteration 13921, loss = 0.03799300\n",
      "Iteration 13922, loss = 0.03798982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13923, loss = 0.03798665\n",
      "Iteration 13924, loss = 0.03798349\n",
      "Iteration 13925, loss = 0.03798033\n",
      "Iteration 13926, loss = 0.03797715\n",
      "Iteration 13927, loss = 0.03797397\n",
      "Iteration 13928, loss = 0.03797087\n",
      "Iteration 13929, loss = 0.03796768\n",
      "Iteration 13930, loss = 0.03796451\n",
      "Iteration 13931, loss = 0.03796137\n",
      "Iteration 13932, loss = 0.03795814\n",
      "Iteration 13933, loss = 0.03795506\n",
      "Iteration 13934, loss = 0.03795185\n",
      "Iteration 13935, loss = 0.03794871\n",
      "Iteration 13936, loss = 0.03794553\n",
      "Iteration 13937, loss = 0.03794238\n",
      "Iteration 13938, loss = 0.03793919\n",
      "Iteration 13939, loss = 0.03793604\n",
      "Iteration 13940, loss = 0.03793285\n",
      "Iteration 13941, loss = 0.03792968\n",
      "Iteration 13942, loss = 0.03792652\n",
      "Iteration 13943, loss = 0.03792340\n",
      "Iteration 13944, loss = 0.03792018\n",
      "Iteration 13945, loss = 0.03791702\n",
      "Iteration 13946, loss = 0.03791389\n",
      "Iteration 13947, loss = 0.03791072\n",
      "Iteration 13948, loss = 0.03790755\n",
      "Iteration 13949, loss = 0.03790442\n",
      "Iteration 13950, loss = 0.03790126\n",
      "Iteration 13951, loss = 0.03789807\n",
      "Iteration 13952, loss = 0.03789492\n",
      "Iteration 13953, loss = 0.03789176\n",
      "Iteration 13954, loss = 0.03788862\n",
      "Iteration 13955, loss = 0.03788549\n",
      "Iteration 13956, loss = 0.03788230\n",
      "Iteration 13957, loss = 0.03787917\n",
      "Iteration 13958, loss = 0.03787601\n",
      "Iteration 13959, loss = 0.03787285\n",
      "Iteration 13960, loss = 0.03786975\n",
      "Iteration 13961, loss = 0.03786658\n",
      "Iteration 13962, loss = 0.03786341\n",
      "Iteration 13963, loss = 0.03786028\n",
      "Iteration 13964, loss = 0.03785712\n",
      "Iteration 13965, loss = 0.03785397\n",
      "Iteration 13966, loss = 0.03785086\n",
      "Iteration 13967, loss = 0.03784769\n",
      "Iteration 13968, loss = 0.03784453\n",
      "Iteration 13969, loss = 0.03784138\n",
      "Iteration 13970, loss = 0.03783825\n",
      "Iteration 13971, loss = 0.03783513\n",
      "Iteration 13972, loss = 0.03783197\n",
      "Iteration 13973, loss = 0.03782879\n",
      "Iteration 13974, loss = 0.03782572\n",
      "Iteration 13975, loss = 0.03782253\n",
      "Iteration 13976, loss = 0.03781941\n",
      "Iteration 13977, loss = 0.03781625\n",
      "Iteration 13978, loss = 0.03781313\n",
      "Iteration 13979, loss = 0.03780996\n",
      "Iteration 13980, loss = 0.03780684\n",
      "Iteration 13981, loss = 0.03780368\n",
      "Iteration 13982, loss = 0.03780053\n",
      "Iteration 13983, loss = 0.03779741\n",
      "Iteration 13984, loss = 0.03779427\n",
      "Iteration 13985, loss = 0.03779116\n",
      "Iteration 13986, loss = 0.03778797\n",
      "Iteration 13987, loss = 0.03778482\n",
      "Iteration 13988, loss = 0.03778168\n",
      "Iteration 13989, loss = 0.03777855\n",
      "Iteration 13990, loss = 0.03777541\n",
      "Iteration 13991, loss = 0.03777230\n",
      "Iteration 13992, loss = 0.03776912\n",
      "Iteration 13993, loss = 0.03776601\n",
      "Iteration 13994, loss = 0.03776286\n",
      "Iteration 13995, loss = 0.03775971\n",
      "Iteration 13996, loss = 0.03775659\n",
      "Iteration 13997, loss = 0.03775346\n",
      "Iteration 13998, loss = 0.03775033\n",
      "Iteration 13999, loss = 0.03774718\n",
      "Iteration 14000, loss = 0.03774404\n",
      "Iteration 14001, loss = 0.03774091\n",
      "Iteration 14002, loss = 0.03773779\n",
      "Iteration 14003, loss = 0.03773466\n",
      "Iteration 14004, loss = 0.03773149\n",
      "Iteration 14005, loss = 0.03772839\n",
      "Iteration 14006, loss = 0.03772526\n",
      "Iteration 14007, loss = 0.03772215\n",
      "Iteration 14008, loss = 0.03771903\n",
      "Iteration 14009, loss = 0.03771592\n",
      "Iteration 14010, loss = 0.03771276\n",
      "Iteration 14011, loss = 0.03770965\n",
      "Iteration 14012, loss = 0.03770653\n",
      "Iteration 14013, loss = 0.03770337\n",
      "Iteration 14014, loss = 0.03770032\n",
      "Iteration 14015, loss = 0.03769715\n",
      "Iteration 14016, loss = 0.03769401\n",
      "Iteration 14017, loss = 0.03769089\n",
      "Iteration 14018, loss = 0.03768778\n",
      "Iteration 14019, loss = 0.03768464\n",
      "Iteration 14020, loss = 0.03768148\n",
      "Iteration 14021, loss = 0.03767840\n",
      "Iteration 14022, loss = 0.03767526\n",
      "Iteration 14023, loss = 0.03767216\n",
      "Iteration 14024, loss = 0.03766903\n",
      "Iteration 14025, loss = 0.03766592\n",
      "Iteration 14026, loss = 0.03766279\n",
      "Iteration 14027, loss = 0.03765970\n",
      "Iteration 14028, loss = 0.03765654\n",
      "Iteration 14029, loss = 0.03765345\n",
      "Iteration 14030, loss = 0.03765033\n",
      "Iteration 14031, loss = 0.03764719\n",
      "Iteration 14032, loss = 0.03764412\n",
      "Iteration 14033, loss = 0.03764101\n",
      "Iteration 14034, loss = 0.03763791\n",
      "Iteration 14035, loss = 0.03763477\n",
      "Iteration 14036, loss = 0.03763169\n",
      "Iteration 14037, loss = 0.03762852\n",
      "Iteration 14038, loss = 0.03762544\n",
      "Iteration 14039, loss = 0.03762235\n",
      "Iteration 14040, loss = 0.03761924\n",
      "Iteration 14041, loss = 0.03761616\n",
      "Iteration 14042, loss = 0.03761304\n",
      "Iteration 14043, loss = 0.03760992\n",
      "Iteration 14044, loss = 0.03760682\n",
      "Iteration 14045, loss = 0.03760374\n",
      "Iteration 14046, loss = 0.03760061\n",
      "Iteration 14047, loss = 0.03759750\n",
      "Iteration 14048, loss = 0.03759441\n",
      "Iteration 14049, loss = 0.03759128\n",
      "Iteration 14050, loss = 0.03758818\n",
      "Iteration 14051, loss = 0.03758511\n",
      "Iteration 14052, loss = 0.03758200\n",
      "Iteration 14053, loss = 0.03757889\n",
      "Iteration 14054, loss = 0.03757581\n",
      "Iteration 14055, loss = 0.03757269\n",
      "Iteration 14056, loss = 0.03756959\n",
      "Iteration 14057, loss = 0.03756650\n",
      "Iteration 14058, loss = 0.03756337\n",
      "Iteration 14059, loss = 0.03756028\n",
      "Iteration 14060, loss = 0.03755723\n",
      "Iteration 14061, loss = 0.03755413\n",
      "Iteration 14062, loss = 0.03755104\n",
      "Iteration 14063, loss = 0.03754790\n",
      "Iteration 14064, loss = 0.03754483\n",
      "Iteration 14065, loss = 0.03754172\n",
      "Iteration 14066, loss = 0.03753863\n",
      "Iteration 14067, loss = 0.03753555\n",
      "Iteration 14068, loss = 0.03753248\n",
      "Iteration 14069, loss = 0.03752935\n",
      "Iteration 14070, loss = 0.03752632\n",
      "Iteration 14071, loss = 0.03752319\n",
      "Iteration 14072, loss = 0.03752010\n",
      "Iteration 14073, loss = 0.03751699\n",
      "Iteration 14074, loss = 0.03751393\n",
      "Iteration 14075, loss = 0.03751081\n",
      "Iteration 14076, loss = 0.03750775\n",
      "Iteration 14077, loss = 0.03750466\n",
      "Iteration 14078, loss = 0.03750153\n",
      "Iteration 14079, loss = 0.03749849\n",
      "Iteration 14080, loss = 0.03749537\n",
      "Iteration 14081, loss = 0.03749230\n",
      "Iteration 14082, loss = 0.03748923\n",
      "Iteration 14083, loss = 0.03748611\n",
      "Iteration 14084, loss = 0.03748308\n",
      "Iteration 14085, loss = 0.03747994\n",
      "Iteration 14086, loss = 0.03747690\n",
      "Iteration 14087, loss = 0.03747379\n",
      "Iteration 14088, loss = 0.03747071\n",
      "Iteration 14089, loss = 0.03746764\n",
      "Iteration 14090, loss = 0.03746455\n",
      "Iteration 14091, loss = 0.03746146\n",
      "Iteration 14092, loss = 0.03745843\n",
      "Iteration 14093, loss = 0.03745535\n",
      "Iteration 14094, loss = 0.03745224\n",
      "Iteration 14095, loss = 0.03744917\n",
      "Iteration 14096, loss = 0.03744610\n",
      "Iteration 14097, loss = 0.03744306\n",
      "Iteration 14098, loss = 0.03743993\n",
      "Iteration 14099, loss = 0.03743689\n",
      "Iteration 14100, loss = 0.03743382\n",
      "Iteration 14101, loss = 0.03743071\n",
      "Iteration 14102, loss = 0.03742764\n",
      "Iteration 14103, loss = 0.03742455\n",
      "Iteration 14104, loss = 0.03742149\n",
      "Iteration 14105, loss = 0.03741843\n",
      "Iteration 14106, loss = 0.03741532\n",
      "Iteration 14107, loss = 0.03741227\n",
      "Iteration 14108, loss = 0.03740919\n",
      "Iteration 14109, loss = 0.03740616\n",
      "Iteration 14110, loss = 0.03740306\n",
      "Iteration 14111, loss = 0.03739995\n",
      "Iteration 14112, loss = 0.03739693\n",
      "Iteration 14113, loss = 0.03739387\n",
      "Iteration 14114, loss = 0.03739077\n",
      "Iteration 14115, loss = 0.03738774\n",
      "Iteration 14116, loss = 0.03738463\n",
      "Iteration 14117, loss = 0.03738160\n",
      "Iteration 14118, loss = 0.03737852\n",
      "Iteration 14119, loss = 0.03737544\n",
      "Iteration 14120, loss = 0.03737238\n",
      "Iteration 14121, loss = 0.03736931\n",
      "Iteration 14122, loss = 0.03736626\n",
      "Iteration 14123, loss = 0.03736321\n",
      "Iteration 14124, loss = 0.03736014\n",
      "Iteration 14125, loss = 0.03735707\n",
      "Iteration 14126, loss = 0.03735403\n",
      "Iteration 14127, loss = 0.03735097\n",
      "Iteration 14128, loss = 0.03734788\n",
      "Iteration 14129, loss = 0.03734484\n",
      "Iteration 14130, loss = 0.03734180\n",
      "Iteration 14131, loss = 0.03733871\n",
      "Iteration 14132, loss = 0.03733566\n",
      "Iteration 14133, loss = 0.03733259\n",
      "Iteration 14134, loss = 0.03732951\n",
      "Iteration 14135, loss = 0.03732645\n",
      "Iteration 14136, loss = 0.03732339\n",
      "Iteration 14137, loss = 0.03732031\n",
      "Iteration 14138, loss = 0.03731724\n",
      "Iteration 14139, loss = 0.03731419\n",
      "Iteration 14140, loss = 0.03731114\n",
      "Iteration 14141, loss = 0.03730805\n",
      "Iteration 14142, loss = 0.03730501\n",
      "Iteration 14143, loss = 0.03730195\n",
      "Iteration 14144, loss = 0.03729890\n",
      "Iteration 14145, loss = 0.03729585\n",
      "Iteration 14146, loss = 0.03729280\n",
      "Iteration 14147, loss = 0.03728973\n",
      "Iteration 14148, loss = 0.03728666\n",
      "Iteration 14149, loss = 0.03728362\n",
      "Iteration 14150, loss = 0.03728057\n",
      "Iteration 14151, loss = 0.03727754\n",
      "Iteration 14152, loss = 0.03727449\n",
      "Iteration 14153, loss = 0.03727139\n",
      "Iteration 14154, loss = 0.03726835\n",
      "Iteration 14155, loss = 0.03726535\n",
      "Iteration 14156, loss = 0.03726228\n",
      "Iteration 14157, loss = 0.03725921\n",
      "Iteration 14158, loss = 0.03725616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14159, loss = 0.03725314\n",
      "Iteration 14160, loss = 0.03725009\n",
      "Iteration 14161, loss = 0.03724706\n",
      "Iteration 14162, loss = 0.03724399\n",
      "Iteration 14163, loss = 0.03724092\n",
      "Iteration 14164, loss = 0.03723791\n",
      "Iteration 14165, loss = 0.03723487\n",
      "Iteration 14166, loss = 0.03723178\n",
      "Iteration 14167, loss = 0.03722876\n",
      "Iteration 14168, loss = 0.03722575\n",
      "Iteration 14169, loss = 0.03722268\n",
      "Iteration 14170, loss = 0.03721964\n",
      "Iteration 14171, loss = 0.03721663\n",
      "Iteration 14172, loss = 0.03721357\n",
      "Iteration 14173, loss = 0.03721053\n",
      "Iteration 14174, loss = 0.03720751\n",
      "Iteration 14175, loss = 0.03720448\n",
      "Iteration 14176, loss = 0.03720142\n",
      "Iteration 14177, loss = 0.03719836\n",
      "Iteration 14178, loss = 0.03719536\n",
      "Iteration 14179, loss = 0.03719230\n",
      "Iteration 14180, loss = 0.03718925\n",
      "Iteration 14181, loss = 0.03718621\n",
      "Iteration 14182, loss = 0.03718320\n",
      "Iteration 14183, loss = 0.03718014\n",
      "Iteration 14184, loss = 0.03717715\n",
      "Iteration 14185, loss = 0.03717409\n",
      "Iteration 14186, loss = 0.03717107\n",
      "Iteration 14187, loss = 0.03716804\n",
      "Iteration 14188, loss = 0.03716497\n",
      "Iteration 14189, loss = 0.03716195\n",
      "Iteration 14190, loss = 0.03715891\n",
      "Iteration 14191, loss = 0.03715587\n",
      "Iteration 14192, loss = 0.03715286\n",
      "Iteration 14193, loss = 0.03714979\n",
      "Iteration 14194, loss = 0.03714674\n",
      "Iteration 14195, loss = 0.03714371\n",
      "Iteration 14196, loss = 0.03714071\n",
      "Iteration 14197, loss = 0.03713768\n",
      "Iteration 14198, loss = 0.03713465\n",
      "Iteration 14199, loss = 0.03713159\n",
      "Iteration 14200, loss = 0.03712857\n",
      "Iteration 14201, loss = 0.03712555\n",
      "Iteration 14202, loss = 0.03712252\n",
      "Iteration 14203, loss = 0.03711949\n",
      "Iteration 14204, loss = 0.03711644\n",
      "Iteration 14205, loss = 0.03711344\n",
      "Iteration 14206, loss = 0.03711041\n",
      "Iteration 14207, loss = 0.03710739\n",
      "Iteration 14208, loss = 0.03710435\n",
      "Iteration 14209, loss = 0.03710132\n",
      "Iteration 14210, loss = 0.03709831\n",
      "Iteration 14211, loss = 0.03709532\n",
      "Iteration 14212, loss = 0.03709224\n",
      "Iteration 14213, loss = 0.03708923\n",
      "Iteration 14214, loss = 0.03708621\n",
      "Iteration 14215, loss = 0.03708321\n",
      "Iteration 14216, loss = 0.03708017\n",
      "Iteration 14217, loss = 0.03707718\n",
      "Iteration 14218, loss = 0.03707415\n",
      "Iteration 14219, loss = 0.03707114\n",
      "Iteration 14220, loss = 0.03706814\n",
      "Iteration 14221, loss = 0.03706510\n",
      "Iteration 14222, loss = 0.03706208\n",
      "Iteration 14223, loss = 0.03705907\n",
      "Iteration 14224, loss = 0.03705606\n",
      "Iteration 14225, loss = 0.03705308\n",
      "Iteration 14226, loss = 0.03705008\n",
      "Iteration 14227, loss = 0.03704705\n",
      "Iteration 14228, loss = 0.03704400\n",
      "Iteration 14229, loss = 0.03704101\n",
      "Iteration 14230, loss = 0.03703799\n",
      "Iteration 14231, loss = 0.03703499\n",
      "Iteration 14232, loss = 0.03703200\n",
      "Iteration 14233, loss = 0.03702897\n",
      "Iteration 14234, loss = 0.03702600\n",
      "Iteration 14235, loss = 0.03702294\n",
      "Iteration 14236, loss = 0.03701994\n",
      "Iteration 14237, loss = 0.03701693\n",
      "Iteration 14238, loss = 0.03701392\n",
      "Iteration 14239, loss = 0.03701090\n",
      "Iteration 14240, loss = 0.03700790\n",
      "Iteration 14241, loss = 0.03700492\n",
      "Iteration 14242, loss = 0.03700194\n",
      "Iteration 14243, loss = 0.03699886\n",
      "Iteration 14244, loss = 0.03699587\n",
      "Iteration 14245, loss = 0.03699291\n",
      "Iteration 14246, loss = 0.03698988\n",
      "Iteration 14247, loss = 0.03698690\n",
      "Iteration 14248, loss = 0.03698391\n",
      "Iteration 14249, loss = 0.03698088\n",
      "Iteration 14250, loss = 0.03697793\n",
      "Iteration 14251, loss = 0.03697494\n",
      "Iteration 14252, loss = 0.03697190\n",
      "Iteration 14253, loss = 0.03696894\n",
      "Iteration 14254, loss = 0.03696592\n",
      "Iteration 14255, loss = 0.03696292\n",
      "Iteration 14256, loss = 0.03695994\n",
      "Iteration 14257, loss = 0.03695697\n",
      "Iteration 14258, loss = 0.03695395\n",
      "Iteration 14259, loss = 0.03695096\n",
      "Iteration 14260, loss = 0.03694801\n",
      "Iteration 14261, loss = 0.03694499\n",
      "Iteration 14262, loss = 0.03694199\n",
      "Iteration 14263, loss = 0.03693898\n",
      "Iteration 14264, loss = 0.03693599\n",
      "Iteration 14265, loss = 0.03693304\n",
      "Iteration 14266, loss = 0.03693004\n",
      "Iteration 14267, loss = 0.03692704\n",
      "Iteration 14268, loss = 0.03692405\n",
      "Iteration 14269, loss = 0.03692106\n",
      "Iteration 14270, loss = 0.03691807\n",
      "Iteration 14271, loss = 0.03691507\n",
      "Iteration 14272, loss = 0.03691206\n",
      "Iteration 14273, loss = 0.03690906\n",
      "Iteration 14274, loss = 0.03690605\n",
      "Iteration 14275, loss = 0.03690307\n",
      "Iteration 14276, loss = 0.03690007\n",
      "Iteration 14277, loss = 0.03689706\n",
      "Iteration 14278, loss = 0.03689408\n",
      "Iteration 14279, loss = 0.03689110\n",
      "Iteration 14280, loss = 0.03688812\n",
      "Iteration 14281, loss = 0.03688509\n",
      "Iteration 14282, loss = 0.03688214\n",
      "Iteration 14283, loss = 0.03687913\n",
      "Iteration 14284, loss = 0.03687615\n",
      "Iteration 14285, loss = 0.03687317\n",
      "Iteration 14286, loss = 0.03687017\n",
      "Iteration 14287, loss = 0.03686722\n",
      "Iteration 14288, loss = 0.03686424\n",
      "Iteration 14289, loss = 0.03686129\n",
      "Iteration 14290, loss = 0.03685828\n",
      "Iteration 14291, loss = 0.03685526\n",
      "Iteration 14292, loss = 0.03685227\n",
      "Iteration 14293, loss = 0.03684933\n",
      "Iteration 14294, loss = 0.03684629\n",
      "Iteration 14295, loss = 0.03684336\n",
      "Iteration 14296, loss = 0.03684035\n",
      "Iteration 14297, loss = 0.03683738\n",
      "Iteration 14298, loss = 0.03683440\n",
      "Iteration 14299, loss = 0.03683142\n",
      "Iteration 14300, loss = 0.03682841\n",
      "Iteration 14301, loss = 0.03682541\n",
      "Iteration 14302, loss = 0.03682247\n",
      "Iteration 14303, loss = 0.03681950\n",
      "Iteration 14304, loss = 0.03681652\n",
      "Iteration 14305, loss = 0.03681355\n",
      "Iteration 14306, loss = 0.03681053\n",
      "Iteration 14307, loss = 0.03680759\n",
      "Iteration 14308, loss = 0.03680458\n",
      "Iteration 14309, loss = 0.03680164\n",
      "Iteration 14310, loss = 0.03679865\n",
      "Iteration 14311, loss = 0.03679569\n",
      "Iteration 14312, loss = 0.03679273\n",
      "Iteration 14313, loss = 0.03678971\n",
      "Iteration 14314, loss = 0.03678672\n",
      "Iteration 14315, loss = 0.03678374\n",
      "Iteration 14316, loss = 0.03678076\n",
      "Iteration 14317, loss = 0.03677781\n",
      "Iteration 14318, loss = 0.03677480\n",
      "Iteration 14319, loss = 0.03677189\n",
      "Iteration 14320, loss = 0.03676886\n",
      "Iteration 14321, loss = 0.03676590\n",
      "Iteration 14322, loss = 0.03676295\n",
      "Iteration 14323, loss = 0.03676000\n",
      "Iteration 14324, loss = 0.03675700\n",
      "Iteration 14325, loss = 0.03675402\n",
      "Iteration 14326, loss = 0.03675109\n",
      "Iteration 14327, loss = 0.03674813\n",
      "Iteration 14328, loss = 0.03674514\n",
      "Iteration 14329, loss = 0.03674214\n",
      "Iteration 14330, loss = 0.03673924\n",
      "Iteration 14331, loss = 0.03673625\n",
      "Iteration 14332, loss = 0.03673326\n",
      "Iteration 14333, loss = 0.03673032\n",
      "Iteration 14334, loss = 0.03672734\n",
      "Iteration 14335, loss = 0.03672437\n",
      "Iteration 14336, loss = 0.03672144\n",
      "Iteration 14337, loss = 0.03671849\n",
      "Iteration 14338, loss = 0.03671550\n",
      "Iteration 14339, loss = 0.03671256\n",
      "Iteration 14340, loss = 0.03670958\n",
      "Iteration 14341, loss = 0.03670661\n",
      "Iteration 14342, loss = 0.03670363\n",
      "Iteration 14343, loss = 0.03670069\n",
      "Iteration 14344, loss = 0.03669772\n",
      "Iteration 14345, loss = 0.03669478\n",
      "Iteration 14346, loss = 0.03669182\n",
      "Iteration 14347, loss = 0.03668886\n",
      "Iteration 14348, loss = 0.03668592\n",
      "Iteration 14349, loss = 0.03668296\n",
      "Iteration 14350, loss = 0.03668000\n",
      "Iteration 14351, loss = 0.03667705\n",
      "Iteration 14352, loss = 0.03667410\n",
      "Iteration 14353, loss = 0.03667118\n",
      "Iteration 14354, loss = 0.03666821\n",
      "Iteration 14355, loss = 0.03666524\n",
      "Iteration 14356, loss = 0.03666232\n",
      "Iteration 14357, loss = 0.03665933\n",
      "Iteration 14358, loss = 0.03665643\n",
      "Iteration 14359, loss = 0.03665346\n",
      "Iteration 14360, loss = 0.03665050\n",
      "Iteration 14361, loss = 0.03664758\n",
      "Iteration 14362, loss = 0.03664462\n",
      "Iteration 14363, loss = 0.03664166\n",
      "Iteration 14364, loss = 0.03663874\n",
      "Iteration 14365, loss = 0.03663575\n",
      "Iteration 14366, loss = 0.03663284\n",
      "Iteration 14367, loss = 0.03662989\n",
      "Iteration 14368, loss = 0.03662693\n",
      "Iteration 14369, loss = 0.03662397\n",
      "Iteration 14370, loss = 0.03662105\n",
      "Iteration 14371, loss = 0.03661809\n",
      "Iteration 14372, loss = 0.03661516\n",
      "Iteration 14373, loss = 0.03661221\n",
      "Iteration 14374, loss = 0.03660929\n",
      "Iteration 14375, loss = 0.03660634\n",
      "Iteration 14376, loss = 0.03660339\n",
      "Iteration 14377, loss = 0.03660044\n",
      "Iteration 14378, loss = 0.03659760\n",
      "Iteration 14379, loss = 0.03659459\n",
      "Iteration 14380, loss = 0.03659167\n",
      "Iteration 14381, loss = 0.03658874\n",
      "Iteration 14382, loss = 0.03658581\n",
      "Iteration 14383, loss = 0.03658287\n",
      "Iteration 14384, loss = 0.03657993\n",
      "Iteration 14385, loss = 0.03657700\n",
      "Iteration 14386, loss = 0.03657405\n",
      "Iteration 14387, loss = 0.03657110\n",
      "Iteration 14388, loss = 0.03656819\n",
      "Iteration 14389, loss = 0.03656526\n",
      "Iteration 14390, loss = 0.03656234\n",
      "Iteration 14391, loss = 0.03655943\n",
      "Iteration 14392, loss = 0.03655645\n",
      "Iteration 14393, loss = 0.03655350\n",
      "Iteration 14394, loss = 0.03655060\n",
      "Iteration 14395, loss = 0.03654767\n",
      "Iteration 14396, loss = 0.03654473\n",
      "Iteration 14397, loss = 0.03654182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14398, loss = 0.03653890\n",
      "Iteration 14399, loss = 0.03653592\n",
      "Iteration 14400, loss = 0.03653303\n",
      "Iteration 14401, loss = 0.03653012\n",
      "Iteration 14402, loss = 0.03652721\n",
      "Iteration 14403, loss = 0.03652422\n",
      "Iteration 14404, loss = 0.03652131\n",
      "Iteration 14405, loss = 0.03651839\n",
      "Iteration 14406, loss = 0.03651552\n",
      "Iteration 14407, loss = 0.03651254\n",
      "Iteration 14408, loss = 0.03650963\n",
      "Iteration 14409, loss = 0.03650672\n",
      "Iteration 14410, loss = 0.03650376\n",
      "Iteration 14411, loss = 0.03650086\n",
      "Iteration 14412, loss = 0.03649793\n",
      "Iteration 14413, loss = 0.03649499\n",
      "Iteration 14414, loss = 0.03649209\n",
      "Iteration 14415, loss = 0.03648916\n",
      "Iteration 14416, loss = 0.03648623\n",
      "Iteration 14417, loss = 0.03648331\n",
      "Iteration 14418, loss = 0.03648038\n",
      "Iteration 14419, loss = 0.03647748\n",
      "Iteration 14420, loss = 0.03647455\n",
      "Iteration 14421, loss = 0.03647160\n",
      "Iteration 14422, loss = 0.03646869\n",
      "Iteration 14423, loss = 0.03646576\n",
      "Iteration 14424, loss = 0.03646287\n",
      "Iteration 14425, loss = 0.03645996\n",
      "Iteration 14426, loss = 0.03645698\n",
      "Iteration 14427, loss = 0.03645409\n",
      "Iteration 14428, loss = 0.03645115\n",
      "Iteration 14429, loss = 0.03644825\n",
      "Iteration 14430, loss = 0.03644529\n",
      "Iteration 14431, loss = 0.03644242\n",
      "Iteration 14432, loss = 0.03643951\n",
      "Iteration 14433, loss = 0.03643659\n",
      "Iteration 14434, loss = 0.03643362\n",
      "Iteration 14435, loss = 0.03643074\n",
      "Iteration 14436, loss = 0.03642785\n",
      "Iteration 14437, loss = 0.03642494\n",
      "Iteration 14438, loss = 0.03642202\n",
      "Iteration 14439, loss = 0.03641905\n",
      "Iteration 14440, loss = 0.03641617\n",
      "Iteration 14441, loss = 0.03641322\n",
      "Iteration 14442, loss = 0.03641031\n",
      "Iteration 14443, loss = 0.03640738\n",
      "Iteration 14444, loss = 0.03640451\n",
      "Iteration 14445, loss = 0.03640156\n",
      "Iteration 14446, loss = 0.03639867\n",
      "Iteration 14447, loss = 0.03639577\n",
      "Iteration 14448, loss = 0.03639288\n",
      "Iteration 14449, loss = 0.03638992\n",
      "Iteration 14450, loss = 0.03638703\n",
      "Iteration 14451, loss = 0.03638416\n",
      "Iteration 14452, loss = 0.03638124\n",
      "Iteration 14453, loss = 0.03637834\n",
      "Iteration 14454, loss = 0.03637543\n",
      "Iteration 14455, loss = 0.03637254\n",
      "Iteration 14456, loss = 0.03636962\n",
      "Iteration 14457, loss = 0.03636673\n",
      "Iteration 14458, loss = 0.03636386\n",
      "Iteration 14459, loss = 0.03636090\n",
      "Iteration 14460, loss = 0.03635805\n",
      "Iteration 14461, loss = 0.03635512\n",
      "Iteration 14462, loss = 0.03635223\n",
      "Iteration 14463, loss = 0.03634934\n",
      "Iteration 14464, loss = 0.03634642\n",
      "Iteration 14465, loss = 0.03634357\n",
      "Iteration 14466, loss = 0.03634065\n",
      "Iteration 14467, loss = 0.03633773\n",
      "Iteration 14468, loss = 0.03633485\n",
      "Iteration 14469, loss = 0.03633195\n",
      "Iteration 14470, loss = 0.03632905\n",
      "Iteration 14471, loss = 0.03632615\n",
      "Iteration 14472, loss = 0.03632322\n",
      "Iteration 14473, loss = 0.03632035\n",
      "Iteration 14474, loss = 0.03631745\n",
      "Iteration 14475, loss = 0.03631453\n",
      "Iteration 14476, loss = 0.03631166\n",
      "Iteration 14477, loss = 0.03630874\n",
      "Iteration 14478, loss = 0.03630587\n",
      "Iteration 14479, loss = 0.03630297\n",
      "Iteration 14480, loss = 0.03630007\n",
      "Iteration 14481, loss = 0.03629717\n",
      "Iteration 14482, loss = 0.03629429\n",
      "Iteration 14483, loss = 0.03629139\n",
      "Iteration 14484, loss = 0.03628848\n",
      "Iteration 14485, loss = 0.03628564\n",
      "Iteration 14486, loss = 0.03628273\n",
      "Iteration 14487, loss = 0.03627980\n",
      "Iteration 14488, loss = 0.03627694\n",
      "Iteration 14489, loss = 0.03627403\n",
      "Iteration 14490, loss = 0.03627115\n",
      "Iteration 14491, loss = 0.03626828\n",
      "Iteration 14492, loss = 0.03626534\n",
      "Iteration 14493, loss = 0.03626247\n",
      "Iteration 14494, loss = 0.03625957\n",
      "Iteration 14495, loss = 0.03625666\n",
      "Iteration 14496, loss = 0.03625378\n",
      "Iteration 14497, loss = 0.03625088\n",
      "Iteration 14498, loss = 0.03624802\n",
      "Iteration 14499, loss = 0.03624513\n",
      "Iteration 14500, loss = 0.03624223\n",
      "Iteration 14501, loss = 0.03623937\n",
      "Iteration 14502, loss = 0.03623648\n",
      "Iteration 14503, loss = 0.03623354\n",
      "Iteration 14504, loss = 0.03623073\n",
      "Iteration 14505, loss = 0.03622780\n",
      "Iteration 14506, loss = 0.03622493\n",
      "Iteration 14507, loss = 0.03622204\n",
      "Iteration 14508, loss = 0.03621916\n",
      "Iteration 14509, loss = 0.03621630\n",
      "Iteration 14510, loss = 0.03621338\n",
      "Iteration 14511, loss = 0.03621053\n",
      "Iteration 14512, loss = 0.03620767\n",
      "Iteration 14513, loss = 0.03620477\n",
      "Iteration 14514, loss = 0.03620192\n",
      "Iteration 14515, loss = 0.03619903\n",
      "Iteration 14516, loss = 0.03619616\n",
      "Iteration 14517, loss = 0.03619325\n",
      "Iteration 14518, loss = 0.03619037\n",
      "Iteration 14519, loss = 0.03618753\n",
      "Iteration 14520, loss = 0.03618465\n",
      "Iteration 14521, loss = 0.03618177\n",
      "Iteration 14522, loss = 0.03617890\n",
      "Iteration 14523, loss = 0.03617603\n",
      "Iteration 14524, loss = 0.03617317\n",
      "Iteration 14525, loss = 0.03617027\n",
      "Iteration 14526, loss = 0.03616739\n",
      "Iteration 14527, loss = 0.03616451\n",
      "Iteration 14528, loss = 0.03616164\n",
      "Iteration 14529, loss = 0.03615877\n",
      "Iteration 14530, loss = 0.03615589\n",
      "Iteration 14531, loss = 0.03615305\n",
      "Iteration 14532, loss = 0.03615011\n",
      "Iteration 14533, loss = 0.03614727\n",
      "Iteration 14534, loss = 0.03614441\n",
      "Iteration 14535, loss = 0.03614148\n",
      "Iteration 14536, loss = 0.03613866\n",
      "Iteration 14537, loss = 0.03613576\n",
      "Iteration 14538, loss = 0.03613287\n",
      "Iteration 14539, loss = 0.03613000\n",
      "Iteration 14540, loss = 0.03612715\n",
      "Iteration 14541, loss = 0.03612430\n",
      "Iteration 14542, loss = 0.03612142\n",
      "Iteration 14543, loss = 0.03611854\n",
      "Iteration 14544, loss = 0.03611570\n",
      "Iteration 14545, loss = 0.03611278\n",
      "Iteration 14546, loss = 0.03610994\n",
      "Iteration 14547, loss = 0.03610707\n",
      "Iteration 14548, loss = 0.03610422\n",
      "Iteration 14549, loss = 0.03610135\n",
      "Iteration 14550, loss = 0.03609849\n",
      "Iteration 14551, loss = 0.03609562\n",
      "Iteration 14552, loss = 0.03609278\n",
      "Iteration 14553, loss = 0.03608991\n",
      "Iteration 14554, loss = 0.03608706\n",
      "Iteration 14555, loss = 0.03608420\n",
      "Iteration 14556, loss = 0.03608134\n",
      "Iteration 14557, loss = 0.03607851\n",
      "Iteration 14558, loss = 0.03607560\n",
      "Iteration 14559, loss = 0.03607272\n",
      "Iteration 14560, loss = 0.03606991\n",
      "Iteration 14561, loss = 0.03606704\n",
      "Iteration 14562, loss = 0.03606416\n",
      "Iteration 14563, loss = 0.03606132\n",
      "Iteration 14564, loss = 0.03605851\n",
      "Iteration 14565, loss = 0.03605559\n",
      "Iteration 14566, loss = 0.03605277\n",
      "Iteration 14567, loss = 0.03604988\n",
      "Iteration 14568, loss = 0.03604707\n",
      "Iteration 14569, loss = 0.03604418\n",
      "Iteration 14570, loss = 0.03604134\n",
      "Iteration 14571, loss = 0.03603852\n",
      "Iteration 14572, loss = 0.03603565\n",
      "Iteration 14573, loss = 0.03603278\n",
      "Iteration 14574, loss = 0.03602992\n",
      "Iteration 14575, loss = 0.03602708\n",
      "Iteration 14576, loss = 0.03602422\n",
      "Iteration 14577, loss = 0.03602137\n",
      "Iteration 14578, loss = 0.03601855\n",
      "Iteration 14579, loss = 0.03601568\n",
      "Iteration 14580, loss = 0.03601281\n",
      "Iteration 14581, loss = 0.03600996\n",
      "Iteration 14582, loss = 0.03600717\n",
      "Iteration 14583, loss = 0.03600425\n",
      "Iteration 14584, loss = 0.03600142\n",
      "Iteration 14585, loss = 0.03599858\n",
      "Iteration 14586, loss = 0.03599569\n",
      "Iteration 14587, loss = 0.03599289\n",
      "Iteration 14588, loss = 0.03599003\n",
      "Iteration 14589, loss = 0.03598716\n",
      "Iteration 14590, loss = 0.03598438\n",
      "Iteration 14591, loss = 0.03598150\n",
      "Iteration 14592, loss = 0.03597866\n",
      "Iteration 14593, loss = 0.03597578\n",
      "Iteration 14594, loss = 0.03597295\n",
      "Iteration 14595, loss = 0.03597014\n",
      "Iteration 14596, loss = 0.03596728\n",
      "Iteration 14597, loss = 0.03596443\n",
      "Iteration 14598, loss = 0.03596158\n",
      "Iteration 14599, loss = 0.03595875\n",
      "Iteration 14600, loss = 0.03595590\n",
      "Iteration 14601, loss = 0.03595308\n",
      "Iteration 14602, loss = 0.03595023\n",
      "Iteration 14603, loss = 0.03594741\n",
      "Iteration 14604, loss = 0.03594455\n",
      "Iteration 14605, loss = 0.03594174\n",
      "Iteration 14606, loss = 0.03593893\n",
      "Iteration 14607, loss = 0.03593604\n",
      "Iteration 14608, loss = 0.03593324\n",
      "Iteration 14609, loss = 0.03593040\n",
      "Iteration 14610, loss = 0.03592755\n",
      "Iteration 14611, loss = 0.03592472\n",
      "Iteration 14612, loss = 0.03592188\n",
      "Iteration 14613, loss = 0.03591908\n",
      "Iteration 14614, loss = 0.03591624\n",
      "Iteration 14615, loss = 0.03591340\n",
      "Iteration 14616, loss = 0.03591060\n",
      "Iteration 14617, loss = 0.03590777\n",
      "Iteration 14618, loss = 0.03590492\n",
      "Iteration 14619, loss = 0.03590211\n",
      "Iteration 14620, loss = 0.03589931\n",
      "Iteration 14621, loss = 0.03589650\n",
      "Iteration 14622, loss = 0.03589364\n",
      "Iteration 14623, loss = 0.03589081\n",
      "Iteration 14624, loss = 0.03588798\n",
      "Iteration 14625, loss = 0.03588518\n",
      "Iteration 14626, loss = 0.03588229\n",
      "Iteration 14627, loss = 0.03587954\n",
      "Iteration 14628, loss = 0.03587668\n",
      "Iteration 14629, loss = 0.03587383\n",
      "Iteration 14630, loss = 0.03587102\n",
      "Iteration 14631, loss = 0.03586817\n",
      "Iteration 14632, loss = 0.03586534\n",
      "Iteration 14633, loss = 0.03586254\n",
      "Iteration 14634, loss = 0.03585972\n",
      "Iteration 14635, loss = 0.03585689\n",
      "Iteration 14636, loss = 0.03585407\n",
      "Iteration 14637, loss = 0.03585122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14638, loss = 0.03584843\n",
      "Iteration 14639, loss = 0.03584559\n",
      "Iteration 14640, loss = 0.03584275\n",
      "Iteration 14641, loss = 0.03583993\n",
      "Iteration 14642, loss = 0.03583709\n",
      "Iteration 14643, loss = 0.03583429\n",
      "Iteration 14644, loss = 0.03583145\n",
      "Iteration 14645, loss = 0.03582862\n",
      "Iteration 14646, loss = 0.03582582\n",
      "Iteration 14647, loss = 0.03582300\n",
      "Iteration 14648, loss = 0.03582015\n",
      "Iteration 14649, loss = 0.03581735\n",
      "Iteration 14650, loss = 0.03581454\n",
      "Iteration 14651, loss = 0.03581171\n",
      "Iteration 14652, loss = 0.03580892\n",
      "Iteration 14653, loss = 0.03580606\n",
      "Iteration 14654, loss = 0.03580326\n",
      "Iteration 14655, loss = 0.03580041\n",
      "Iteration 14656, loss = 0.03579757\n",
      "Iteration 14657, loss = 0.03579482\n",
      "Iteration 14658, loss = 0.03579195\n",
      "Iteration 14659, loss = 0.03578914\n",
      "Iteration 14660, loss = 0.03578635\n",
      "Iteration 14661, loss = 0.03578353\n",
      "Iteration 14662, loss = 0.03578069\n",
      "Iteration 14663, loss = 0.03577787\n",
      "Iteration 14664, loss = 0.03577505\n",
      "Iteration 14665, loss = 0.03577227\n",
      "Iteration 14666, loss = 0.03576943\n",
      "Iteration 14667, loss = 0.03576664\n",
      "Iteration 14668, loss = 0.03576382\n",
      "Iteration 14669, loss = 0.03576101\n",
      "Iteration 14670, loss = 0.03575820\n",
      "Iteration 14671, loss = 0.03575538\n",
      "Iteration 14672, loss = 0.03575259\n",
      "Iteration 14673, loss = 0.03574977\n",
      "Iteration 14674, loss = 0.03574697\n",
      "Iteration 14675, loss = 0.03574418\n",
      "Iteration 14676, loss = 0.03574133\n",
      "Iteration 14677, loss = 0.03573850\n",
      "Iteration 14678, loss = 0.03573574\n",
      "Iteration 14679, loss = 0.03573289\n",
      "Iteration 14680, loss = 0.03573014\n",
      "Iteration 14681, loss = 0.03572732\n",
      "Iteration 14682, loss = 0.03572450\n",
      "Iteration 14683, loss = 0.03572171\n",
      "Iteration 14684, loss = 0.03571890\n",
      "Iteration 14685, loss = 0.03571608\n",
      "Iteration 14686, loss = 0.03571326\n",
      "Iteration 14687, loss = 0.03571051\n",
      "Iteration 14688, loss = 0.03570766\n",
      "Iteration 14689, loss = 0.03570482\n",
      "Iteration 14690, loss = 0.03570202\n",
      "Iteration 14691, loss = 0.03569923\n",
      "Iteration 14692, loss = 0.03569641\n",
      "Iteration 14693, loss = 0.03569362\n",
      "Iteration 14694, loss = 0.03569079\n",
      "Iteration 14695, loss = 0.03568803\n",
      "Iteration 14696, loss = 0.03568520\n",
      "Iteration 14697, loss = 0.03568241\n",
      "Iteration 14698, loss = 0.03567963\n",
      "Iteration 14699, loss = 0.03567683\n",
      "Iteration 14700, loss = 0.03567402\n",
      "Iteration 14701, loss = 0.03567123\n",
      "Iteration 14702, loss = 0.03566844\n",
      "Iteration 14703, loss = 0.03566562\n",
      "Iteration 14704, loss = 0.03566284\n",
      "Iteration 14705, loss = 0.03566006\n",
      "Iteration 14706, loss = 0.03565727\n",
      "Iteration 14707, loss = 0.03565447\n",
      "Iteration 14708, loss = 0.03565169\n",
      "Iteration 14709, loss = 0.03564890\n",
      "Iteration 14710, loss = 0.03564610\n",
      "Iteration 14711, loss = 0.03564334\n",
      "Iteration 14712, loss = 0.03564047\n",
      "Iteration 14713, loss = 0.03563773\n",
      "Iteration 14714, loss = 0.03563491\n",
      "Iteration 14715, loss = 0.03563215\n",
      "Iteration 14716, loss = 0.03562934\n",
      "Iteration 14717, loss = 0.03562655\n",
      "Iteration 14718, loss = 0.03562373\n",
      "Iteration 14719, loss = 0.03562098\n",
      "Iteration 14720, loss = 0.03561823\n",
      "Iteration 14721, loss = 0.03561538\n",
      "Iteration 14722, loss = 0.03561263\n",
      "Iteration 14723, loss = 0.03560984\n",
      "Iteration 14724, loss = 0.03560705\n",
      "Iteration 14725, loss = 0.03560427\n",
      "Iteration 14726, loss = 0.03560149\n",
      "Iteration 14727, loss = 0.03559873\n",
      "Iteration 14728, loss = 0.03559590\n",
      "Iteration 14729, loss = 0.03559313\n",
      "Iteration 14730, loss = 0.03559036\n",
      "Iteration 14731, loss = 0.03558757\n",
      "Iteration 14732, loss = 0.03558480\n",
      "Iteration 14733, loss = 0.03558196\n",
      "Iteration 14734, loss = 0.03557918\n",
      "Iteration 14735, loss = 0.03557643\n",
      "Iteration 14736, loss = 0.03557365\n",
      "Iteration 14737, loss = 0.03557084\n",
      "Iteration 14738, loss = 0.03556806\n",
      "Iteration 14739, loss = 0.03556526\n",
      "Iteration 14740, loss = 0.03556248\n",
      "Iteration 14741, loss = 0.03555971\n",
      "Iteration 14742, loss = 0.03555693\n",
      "Iteration 14743, loss = 0.03555413\n",
      "Iteration 14744, loss = 0.03555137\n",
      "Iteration 14745, loss = 0.03554858\n",
      "Iteration 14746, loss = 0.03554583\n",
      "Iteration 14747, loss = 0.03554303\n",
      "Iteration 14748, loss = 0.03554026\n",
      "Iteration 14749, loss = 0.03553748\n",
      "Iteration 14750, loss = 0.03553475\n",
      "Iteration 14751, loss = 0.03553192\n",
      "Iteration 14752, loss = 0.03552914\n",
      "Iteration 14753, loss = 0.03552639\n",
      "Iteration 14754, loss = 0.03552360\n",
      "Iteration 14755, loss = 0.03552084\n",
      "Iteration 14756, loss = 0.03551807\n",
      "Iteration 14757, loss = 0.03551531\n",
      "Iteration 14758, loss = 0.03551252\n",
      "Iteration 14759, loss = 0.03550979\n",
      "Iteration 14760, loss = 0.03550694\n",
      "Iteration 14761, loss = 0.03550420\n",
      "Iteration 14762, loss = 0.03550144\n",
      "Iteration 14763, loss = 0.03549868\n",
      "Iteration 14764, loss = 0.03549587\n",
      "Iteration 14765, loss = 0.03549312\n",
      "Iteration 14766, loss = 0.03549038\n",
      "Iteration 14767, loss = 0.03548756\n",
      "Iteration 14768, loss = 0.03548478\n",
      "Iteration 14769, loss = 0.03548202\n",
      "Iteration 14770, loss = 0.03547925\n",
      "Iteration 14771, loss = 0.03547652\n",
      "Iteration 14772, loss = 0.03547373\n",
      "Iteration 14773, loss = 0.03547093\n",
      "Iteration 14774, loss = 0.03546818\n",
      "Iteration 14775, loss = 0.03546542\n",
      "Iteration 14776, loss = 0.03546268\n",
      "Iteration 14777, loss = 0.03545987\n",
      "Iteration 14778, loss = 0.03545715\n",
      "Iteration 14779, loss = 0.03545438\n",
      "Iteration 14780, loss = 0.03545158\n",
      "Iteration 14781, loss = 0.03544883\n",
      "Iteration 14782, loss = 0.03544604\n",
      "Iteration 14783, loss = 0.03544329\n",
      "Iteration 14784, loss = 0.03544055\n",
      "Iteration 14785, loss = 0.03543775\n",
      "Iteration 14786, loss = 0.03543502\n",
      "Iteration 14787, loss = 0.03543220\n",
      "Iteration 14788, loss = 0.03542948\n",
      "Iteration 14789, loss = 0.03542672\n",
      "Iteration 14790, loss = 0.03542395\n",
      "Iteration 14791, loss = 0.03542119\n",
      "Iteration 14792, loss = 0.03541845\n",
      "Iteration 14793, loss = 0.03541572\n",
      "Iteration 14794, loss = 0.03541292\n",
      "Iteration 14795, loss = 0.03541017\n",
      "Iteration 14796, loss = 0.03540740\n",
      "Iteration 14797, loss = 0.03540464\n",
      "Iteration 14798, loss = 0.03540190\n",
      "Iteration 14799, loss = 0.03539914\n",
      "Iteration 14800, loss = 0.03539637\n",
      "Iteration 14801, loss = 0.03539363\n",
      "Iteration 14802, loss = 0.03539086\n",
      "Iteration 14803, loss = 0.03538810\n",
      "Iteration 14804, loss = 0.03538534\n",
      "Iteration 14805, loss = 0.03538263\n",
      "Iteration 14806, loss = 0.03537983\n",
      "Iteration 14807, loss = 0.03537708\n",
      "Iteration 14808, loss = 0.03537436\n",
      "Iteration 14809, loss = 0.03537161\n",
      "Iteration 14810, loss = 0.03536883\n",
      "Iteration 14811, loss = 0.03536610\n",
      "Iteration 14812, loss = 0.03536335\n",
      "Iteration 14813, loss = 0.03536060\n",
      "Iteration 14814, loss = 0.03535787\n",
      "Iteration 14815, loss = 0.03535510\n",
      "Iteration 14816, loss = 0.03535239\n",
      "Iteration 14817, loss = 0.03534958\n",
      "Iteration 14818, loss = 0.03534693\n",
      "Iteration 14819, loss = 0.03534412\n",
      "Iteration 14820, loss = 0.03534136\n",
      "Iteration 14821, loss = 0.03533867\n",
      "Iteration 14822, loss = 0.03533588\n",
      "Iteration 14823, loss = 0.03533318\n",
      "Iteration 14824, loss = 0.03533046\n",
      "Iteration 14825, loss = 0.03532771\n",
      "Iteration 14826, loss = 0.03532495\n",
      "Iteration 14827, loss = 0.03532227\n",
      "Iteration 14828, loss = 0.03531949\n",
      "Iteration 14829, loss = 0.03531675\n",
      "Iteration 14830, loss = 0.03531398\n",
      "Iteration 14831, loss = 0.03531129\n",
      "Iteration 14832, loss = 0.03530853\n",
      "Iteration 14833, loss = 0.03530581\n",
      "Iteration 14834, loss = 0.03530310\n",
      "Iteration 14835, loss = 0.03530035\n",
      "Iteration 14836, loss = 0.03529761\n",
      "Iteration 14837, loss = 0.03529489\n",
      "Iteration 14838, loss = 0.03529213\n",
      "Iteration 14839, loss = 0.03528940\n",
      "Iteration 14840, loss = 0.03528667\n",
      "Iteration 14841, loss = 0.03528390\n",
      "Iteration 14842, loss = 0.03528118\n",
      "Iteration 14843, loss = 0.03527844\n",
      "Iteration 14844, loss = 0.03527573\n",
      "Iteration 14845, loss = 0.03527296\n",
      "Iteration 14846, loss = 0.03527023\n",
      "Iteration 14847, loss = 0.03526753\n",
      "Iteration 14848, loss = 0.03526475\n",
      "Iteration 14849, loss = 0.03526204\n",
      "Iteration 14850, loss = 0.03525928\n",
      "Iteration 14851, loss = 0.03525661\n",
      "Iteration 14852, loss = 0.03525387\n",
      "Iteration 14853, loss = 0.03525110\n",
      "Iteration 14854, loss = 0.03524834\n",
      "Iteration 14855, loss = 0.03524563\n",
      "Iteration 14856, loss = 0.03524292\n",
      "Iteration 14857, loss = 0.03524017\n",
      "Iteration 14858, loss = 0.03523746\n",
      "Iteration 14859, loss = 0.03523470\n",
      "Iteration 14860, loss = 0.03523202\n",
      "Iteration 14861, loss = 0.03522929\n",
      "Iteration 14862, loss = 0.03522656\n",
      "Iteration 14863, loss = 0.03522383\n",
      "Iteration 14864, loss = 0.03522112\n",
      "Iteration 14865, loss = 0.03521837\n",
      "Iteration 14866, loss = 0.03521566\n",
      "Iteration 14867, loss = 0.03521291\n",
      "Iteration 14868, loss = 0.03521026\n",
      "Iteration 14869, loss = 0.03520748\n",
      "Iteration 14870, loss = 0.03520475\n",
      "Iteration 14871, loss = 0.03520203\n",
      "Iteration 14872, loss = 0.03519930\n",
      "Iteration 14873, loss = 0.03519659\n",
      "Iteration 14874, loss = 0.03519387\n",
      "Iteration 14875, loss = 0.03519117\n",
      "Iteration 14876, loss = 0.03518843\n",
      "Iteration 14877, loss = 0.03518572\n",
      "Iteration 14878, loss = 0.03518296\n",
      "Iteration 14879, loss = 0.03518028\n",
      "Iteration 14880, loss = 0.03517752\n",
      "Iteration 14881, loss = 0.03517482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14882, loss = 0.03517211\n",
      "Iteration 14883, loss = 0.03516938\n",
      "Iteration 14884, loss = 0.03516669\n",
      "Iteration 14885, loss = 0.03516395\n",
      "Iteration 14886, loss = 0.03516122\n",
      "Iteration 14887, loss = 0.03515851\n",
      "Iteration 14888, loss = 0.03515579\n",
      "Iteration 14889, loss = 0.03515309\n",
      "Iteration 14890, loss = 0.03515036\n",
      "Iteration 14891, loss = 0.03514767\n",
      "Iteration 14892, loss = 0.03514495\n",
      "Iteration 14893, loss = 0.03514227\n",
      "Iteration 14894, loss = 0.03513954\n",
      "Iteration 14895, loss = 0.03513685\n",
      "Iteration 14896, loss = 0.03513417\n",
      "Iteration 14897, loss = 0.03513140\n",
      "Iteration 14898, loss = 0.03512870\n",
      "Iteration 14899, loss = 0.03512601\n",
      "Iteration 14900, loss = 0.03512331\n",
      "Iteration 14901, loss = 0.03512057\n",
      "Iteration 14902, loss = 0.03511787\n",
      "Iteration 14903, loss = 0.03511517\n",
      "Iteration 14904, loss = 0.03511245\n",
      "Iteration 14905, loss = 0.03510973\n",
      "Iteration 14906, loss = 0.03510702\n",
      "Iteration 14907, loss = 0.03510434\n",
      "Iteration 14908, loss = 0.03510162\n",
      "Iteration 14909, loss = 0.03509890\n",
      "Iteration 14910, loss = 0.03509621\n",
      "Iteration 14911, loss = 0.03509352\n",
      "Iteration 14912, loss = 0.03509080\n",
      "Iteration 14913, loss = 0.03508808\n",
      "Iteration 14914, loss = 0.03508544\n",
      "Iteration 14915, loss = 0.03508271\n",
      "Iteration 14916, loss = 0.03508000\n",
      "Iteration 14917, loss = 0.03507729\n",
      "Iteration 14918, loss = 0.03507457\n",
      "Iteration 14919, loss = 0.03507189\n",
      "Iteration 14920, loss = 0.03506919\n",
      "Iteration 14921, loss = 0.03506651\n",
      "Iteration 14922, loss = 0.03506377\n",
      "Iteration 14923, loss = 0.03506110\n",
      "Iteration 14924, loss = 0.03505840\n",
      "Iteration 14925, loss = 0.03505572\n",
      "Iteration 14926, loss = 0.03505300\n",
      "Iteration 14927, loss = 0.03505032\n",
      "Iteration 14928, loss = 0.03504764\n",
      "Iteration 14929, loss = 0.03504492\n",
      "Iteration 14930, loss = 0.03504223\n",
      "Iteration 14931, loss = 0.03503953\n",
      "Iteration 14932, loss = 0.03503681\n",
      "Iteration 14933, loss = 0.03503412\n",
      "Iteration 14934, loss = 0.03503141\n",
      "Iteration 14935, loss = 0.03502869\n",
      "Iteration 14936, loss = 0.03502599\n",
      "Iteration 14937, loss = 0.03502328\n",
      "Iteration 14938, loss = 0.03502059\n",
      "Iteration 14939, loss = 0.03501792\n",
      "Iteration 14940, loss = 0.03501516\n",
      "Iteration 14941, loss = 0.03501249\n",
      "Iteration 14942, loss = 0.03500980\n",
      "Iteration 14943, loss = 0.03500704\n",
      "Iteration 14944, loss = 0.03500441\n",
      "Iteration 14945, loss = 0.03500167\n",
      "Iteration 14946, loss = 0.03499898\n",
      "Iteration 14947, loss = 0.03499631\n",
      "Iteration 14948, loss = 0.03499360\n",
      "Iteration 14949, loss = 0.03499092\n",
      "Iteration 14950, loss = 0.03498818\n",
      "Iteration 14951, loss = 0.03498557\n",
      "Iteration 14952, loss = 0.03498286\n",
      "Iteration 14953, loss = 0.03498015\n",
      "Iteration 14954, loss = 0.03497744\n",
      "Iteration 14955, loss = 0.03497478\n",
      "Iteration 14956, loss = 0.03497207\n",
      "Iteration 14957, loss = 0.03496943\n",
      "Iteration 14958, loss = 0.03496671\n",
      "Iteration 14959, loss = 0.03496400\n",
      "Iteration 14960, loss = 0.03496129\n",
      "Iteration 14961, loss = 0.03495864\n",
      "Iteration 14962, loss = 0.03495595\n",
      "Iteration 14963, loss = 0.03495326\n",
      "Iteration 14964, loss = 0.03495058\n",
      "Iteration 14965, loss = 0.03494791\n",
      "Iteration 14966, loss = 0.03494518\n",
      "Iteration 14967, loss = 0.03494251\n",
      "Iteration 14968, loss = 0.03493985\n",
      "Iteration 14969, loss = 0.03493716\n",
      "Iteration 14970, loss = 0.03493447\n",
      "Iteration 14971, loss = 0.03493178\n",
      "Iteration 14972, loss = 0.03492909\n",
      "Iteration 14973, loss = 0.03492641\n",
      "Iteration 14974, loss = 0.03492374\n",
      "Iteration 14975, loss = 0.03492104\n",
      "Iteration 14976, loss = 0.03491838\n",
      "Iteration 14977, loss = 0.03491570\n",
      "Iteration 14978, loss = 0.03491302\n",
      "Iteration 14979, loss = 0.03491032\n",
      "Iteration 14980, loss = 0.03490763\n",
      "Iteration 14981, loss = 0.03490494\n",
      "Iteration 14982, loss = 0.03490232\n",
      "Iteration 14983, loss = 0.03489961\n",
      "Iteration 14984, loss = 0.03489696\n",
      "Iteration 14985, loss = 0.03489427\n",
      "Iteration 14986, loss = 0.03489160\n",
      "Iteration 14987, loss = 0.03488890\n",
      "Iteration 14988, loss = 0.03488627\n",
      "Iteration 14989, loss = 0.03488357\n",
      "Iteration 14990, loss = 0.03488090\n",
      "Iteration 14991, loss = 0.03487823\n",
      "Iteration 14992, loss = 0.03487556\n",
      "Iteration 14993, loss = 0.03487287\n",
      "Iteration 14994, loss = 0.03487020\n",
      "Iteration 14995, loss = 0.03486755\n",
      "Iteration 14996, loss = 0.03486489\n",
      "Iteration 14997, loss = 0.03486221\n",
      "Iteration 14998, loss = 0.03485952\n",
      "Iteration 14999, loss = 0.03485690\n",
      "Iteration 15000, loss = 0.03485420\n",
      "Iteration 15001, loss = 0.03485154\n",
      "Iteration 15002, loss = 0.03484888\n",
      "Iteration 15003, loss = 0.03484623\n",
      "Iteration 15004, loss = 0.03484354\n",
      "Iteration 15005, loss = 0.03484088\n",
      "Iteration 15006, loss = 0.03483821\n",
      "Iteration 15007, loss = 0.03483559\n",
      "Iteration 15008, loss = 0.03483290\n",
      "Iteration 15009, loss = 0.03483024\n",
      "Iteration 15010, loss = 0.03482757\n",
      "Iteration 15011, loss = 0.03482492\n",
      "Iteration 15012, loss = 0.03482229\n",
      "Iteration 15013, loss = 0.03481961\n",
      "Iteration 15014, loss = 0.03481692\n",
      "Iteration 15015, loss = 0.03481427\n",
      "Iteration 15016, loss = 0.03481160\n",
      "Iteration 15017, loss = 0.03480898\n",
      "Iteration 15018, loss = 0.03480635\n",
      "Iteration 15019, loss = 0.03480364\n",
      "Iteration 15020, loss = 0.03480094\n",
      "Iteration 15021, loss = 0.03479828\n",
      "Iteration 15022, loss = 0.03479566\n",
      "Iteration 15023, loss = 0.03479295\n",
      "Iteration 15024, loss = 0.03479029\n",
      "Iteration 15025, loss = 0.03478766\n",
      "Iteration 15026, loss = 0.03478500\n",
      "Iteration 15027, loss = 0.03478230\n",
      "Iteration 15028, loss = 0.03477963\n",
      "Iteration 15029, loss = 0.03477700\n",
      "Iteration 15030, loss = 0.03477433\n",
      "Iteration 15031, loss = 0.03477162\n",
      "Iteration 15032, loss = 0.03476900\n",
      "Iteration 15033, loss = 0.03476634\n",
      "Iteration 15034, loss = 0.03476368\n",
      "Iteration 15035, loss = 0.03476101\n",
      "Iteration 15036, loss = 0.03475837\n",
      "Iteration 15037, loss = 0.03475571\n",
      "Iteration 15038, loss = 0.03475308\n",
      "Iteration 15039, loss = 0.03475041\n",
      "Iteration 15040, loss = 0.03474774\n",
      "Iteration 15041, loss = 0.03474509\n",
      "Iteration 15042, loss = 0.03474241\n",
      "Iteration 15043, loss = 0.03473978\n",
      "Iteration 15044, loss = 0.03473710\n",
      "Iteration 15045, loss = 0.03473442\n",
      "Iteration 15046, loss = 0.03473180\n",
      "Iteration 15047, loss = 0.03472912\n",
      "Iteration 15048, loss = 0.03472647\n",
      "Iteration 15049, loss = 0.03472379\n",
      "Iteration 15050, loss = 0.03472118\n",
      "Iteration 15051, loss = 0.03471849\n",
      "Iteration 15052, loss = 0.03471585\n",
      "Iteration 15053, loss = 0.03471319\n",
      "Iteration 15054, loss = 0.03471052\n",
      "Iteration 15055, loss = 0.03470787\n",
      "Iteration 15056, loss = 0.03470522\n",
      "Iteration 15057, loss = 0.03470256\n",
      "Iteration 15058, loss = 0.03469993\n",
      "Iteration 15059, loss = 0.03469727\n",
      "Iteration 15060, loss = 0.03469461\n",
      "Iteration 15061, loss = 0.03469195\n",
      "Iteration 15062, loss = 0.03468934\n",
      "Iteration 15063, loss = 0.03468669\n",
      "Iteration 15064, loss = 0.03468400\n",
      "Iteration 15065, loss = 0.03468136\n",
      "Iteration 15066, loss = 0.03467869\n",
      "Iteration 15067, loss = 0.03467607\n",
      "Iteration 15068, loss = 0.03467343\n",
      "Iteration 15069, loss = 0.03467080\n",
      "Iteration 15070, loss = 0.03466812\n",
      "Iteration 15071, loss = 0.03466553\n",
      "Iteration 15072, loss = 0.03466283\n",
      "Iteration 15073, loss = 0.03466019\n",
      "Iteration 15074, loss = 0.03465756\n",
      "Iteration 15075, loss = 0.03465493\n",
      "Iteration 15076, loss = 0.03465227\n",
      "Iteration 15077, loss = 0.03464965\n",
      "Iteration 15078, loss = 0.03464698\n",
      "Iteration 15079, loss = 0.03464439\n",
      "Iteration 15080, loss = 0.03464170\n",
      "Iteration 15081, loss = 0.03463907\n",
      "Iteration 15082, loss = 0.03463643\n",
      "Iteration 15083, loss = 0.03463379\n",
      "Iteration 15084, loss = 0.03463112\n",
      "Iteration 15085, loss = 0.03462855\n",
      "Iteration 15086, loss = 0.03462589\n",
      "Iteration 15087, loss = 0.03462325\n",
      "Iteration 15088, loss = 0.03462062\n",
      "Iteration 15089, loss = 0.03461795\n",
      "Iteration 15090, loss = 0.03461535\n",
      "Iteration 15091, loss = 0.03461271\n",
      "Iteration 15092, loss = 0.03461007\n",
      "Iteration 15093, loss = 0.03460744\n",
      "Iteration 15094, loss = 0.03460481\n",
      "Iteration 15095, loss = 0.03460217\n",
      "Iteration 15096, loss = 0.03459957\n",
      "Iteration 15097, loss = 0.03459691\n",
      "Iteration 15098, loss = 0.03459431\n",
      "Iteration 15099, loss = 0.03459166\n",
      "Iteration 15100, loss = 0.03458905\n",
      "Iteration 15101, loss = 0.03458639\n",
      "Iteration 15102, loss = 0.03458377\n",
      "Iteration 15103, loss = 0.03458113\n",
      "Iteration 15104, loss = 0.03457851\n",
      "Iteration 15105, loss = 0.03457589\n",
      "Iteration 15106, loss = 0.03457327\n",
      "Iteration 15107, loss = 0.03457061\n",
      "Iteration 15108, loss = 0.03456800\n",
      "Iteration 15109, loss = 0.03456537\n",
      "Iteration 15110, loss = 0.03456272\n",
      "Iteration 15111, loss = 0.03456013\n",
      "Iteration 15112, loss = 0.03455750\n",
      "Iteration 15113, loss = 0.03455484\n",
      "Iteration 15114, loss = 0.03455224\n",
      "Iteration 15115, loss = 0.03454960\n",
      "Iteration 15116, loss = 0.03454696\n",
      "Iteration 15117, loss = 0.03454433\n",
      "Iteration 15118, loss = 0.03454171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15119, loss = 0.03453910\n",
      "Iteration 15120, loss = 0.03453644\n",
      "Iteration 15121, loss = 0.03453380\n",
      "Iteration 15122, loss = 0.03453116\n",
      "Iteration 15123, loss = 0.03452852\n",
      "Iteration 15124, loss = 0.03452593\n",
      "Iteration 15125, loss = 0.03452331\n",
      "Iteration 15126, loss = 0.03452066\n",
      "Iteration 15127, loss = 0.03451805\n",
      "Iteration 15128, loss = 0.03451544\n",
      "Iteration 15129, loss = 0.03451280\n",
      "Iteration 15130, loss = 0.03451018\n",
      "Iteration 15131, loss = 0.03450756\n",
      "Iteration 15132, loss = 0.03450494\n",
      "Iteration 15133, loss = 0.03450230\n",
      "Iteration 15134, loss = 0.03449972\n",
      "Iteration 15135, loss = 0.03449710\n",
      "Iteration 15136, loss = 0.03449444\n",
      "Iteration 15137, loss = 0.03449186\n",
      "Iteration 15138, loss = 0.03448927\n",
      "Iteration 15139, loss = 0.03448666\n",
      "Iteration 15140, loss = 0.03448406\n",
      "Iteration 15141, loss = 0.03448143\n",
      "Iteration 15142, loss = 0.03447879\n",
      "Iteration 15143, loss = 0.03447618\n",
      "Iteration 15144, loss = 0.03447356\n",
      "Iteration 15145, loss = 0.03447095\n",
      "Iteration 15146, loss = 0.03446833\n",
      "Iteration 15147, loss = 0.03446569\n",
      "Iteration 15148, loss = 0.03446307\n",
      "Iteration 15149, loss = 0.03446045\n",
      "Iteration 15150, loss = 0.03445784\n",
      "Iteration 15151, loss = 0.03445518\n",
      "Iteration 15152, loss = 0.03445258\n",
      "Iteration 15153, loss = 0.03444997\n",
      "Iteration 15154, loss = 0.03444734\n",
      "Iteration 15155, loss = 0.03444472\n",
      "Iteration 15156, loss = 0.03444207\n",
      "Iteration 15157, loss = 0.03443944\n",
      "Iteration 15158, loss = 0.03443684\n",
      "Iteration 15159, loss = 0.03443424\n",
      "Iteration 15160, loss = 0.03443164\n",
      "Iteration 15161, loss = 0.03442900\n",
      "Iteration 15162, loss = 0.03442641\n",
      "Iteration 15163, loss = 0.03442376\n",
      "Iteration 15164, loss = 0.03442118\n",
      "Iteration 15165, loss = 0.03441855\n",
      "Iteration 15166, loss = 0.03441595\n",
      "Iteration 15167, loss = 0.03441336\n",
      "Iteration 15168, loss = 0.03441075\n",
      "Iteration 15169, loss = 0.03440812\n",
      "Iteration 15170, loss = 0.03440553\n",
      "Iteration 15171, loss = 0.03440290\n",
      "Iteration 15172, loss = 0.03440029\n",
      "Iteration 15173, loss = 0.03439770\n",
      "Iteration 15174, loss = 0.03439509\n",
      "Iteration 15175, loss = 0.03439247\n",
      "Iteration 15176, loss = 0.03438990\n",
      "Iteration 15177, loss = 0.03438725\n",
      "Iteration 15178, loss = 0.03438465\n",
      "Iteration 15179, loss = 0.03438204\n",
      "Iteration 15180, loss = 0.03437947\n",
      "Iteration 15181, loss = 0.03437685\n",
      "Iteration 15182, loss = 0.03437426\n",
      "Iteration 15183, loss = 0.03437163\n",
      "Iteration 15184, loss = 0.03436901\n",
      "Iteration 15185, loss = 0.03436643\n",
      "Iteration 15186, loss = 0.03436383\n",
      "Iteration 15187, loss = 0.03436125\n",
      "Iteration 15188, loss = 0.03435865\n",
      "Iteration 15189, loss = 0.03435601\n",
      "Iteration 15190, loss = 0.03435343\n",
      "Iteration 15191, loss = 0.03435082\n",
      "Iteration 15192, loss = 0.03434826\n",
      "Iteration 15193, loss = 0.03434563\n",
      "Iteration 15194, loss = 0.03434304\n",
      "Iteration 15195, loss = 0.03434045\n",
      "Iteration 15196, loss = 0.03433786\n",
      "Iteration 15197, loss = 0.03433526\n",
      "Iteration 15198, loss = 0.03433268\n",
      "Iteration 15199, loss = 0.03433004\n",
      "Iteration 15200, loss = 0.03432747\n",
      "Iteration 15201, loss = 0.03432486\n",
      "Iteration 15202, loss = 0.03432227\n",
      "Iteration 15203, loss = 0.03431965\n",
      "Iteration 15204, loss = 0.03431711\n",
      "Iteration 15205, loss = 0.03431445\n",
      "Iteration 15206, loss = 0.03431187\n",
      "Iteration 15207, loss = 0.03430927\n",
      "Iteration 15208, loss = 0.03430666\n",
      "Iteration 15209, loss = 0.03430406\n",
      "Iteration 15210, loss = 0.03430145\n",
      "Iteration 15211, loss = 0.03429889\n",
      "Iteration 15212, loss = 0.03429631\n",
      "Iteration 15213, loss = 0.03429372\n",
      "Iteration 15214, loss = 0.03429109\n",
      "Iteration 15215, loss = 0.03428852\n",
      "Iteration 15216, loss = 0.03428595\n",
      "Iteration 15217, loss = 0.03428336\n",
      "Iteration 15218, loss = 0.03428077\n",
      "Iteration 15219, loss = 0.03427819\n",
      "Iteration 15220, loss = 0.03427557\n",
      "Iteration 15221, loss = 0.03427299\n",
      "Iteration 15222, loss = 0.03427041\n",
      "Iteration 15223, loss = 0.03426785\n",
      "Iteration 15224, loss = 0.03426524\n",
      "Iteration 15225, loss = 0.03426264\n",
      "Iteration 15226, loss = 0.03426004\n",
      "Iteration 15227, loss = 0.03425750\n",
      "Iteration 15228, loss = 0.03425491\n",
      "Iteration 15229, loss = 0.03425228\n",
      "Iteration 15230, loss = 0.03424972\n",
      "Iteration 15231, loss = 0.03424717\n",
      "Iteration 15232, loss = 0.03424455\n",
      "Iteration 15233, loss = 0.03424196\n",
      "Iteration 15234, loss = 0.03423937\n",
      "Iteration 15235, loss = 0.03423681\n",
      "Iteration 15236, loss = 0.03423423\n",
      "Iteration 15237, loss = 0.03423164\n",
      "Iteration 15238, loss = 0.03422910\n",
      "Iteration 15239, loss = 0.03422648\n",
      "Iteration 15240, loss = 0.03422392\n",
      "Iteration 15241, loss = 0.03422135\n",
      "Iteration 15242, loss = 0.03421875\n",
      "Iteration 15243, loss = 0.03421624\n",
      "Iteration 15244, loss = 0.03421361\n",
      "Iteration 15245, loss = 0.03421103\n",
      "Iteration 15246, loss = 0.03420848\n",
      "Iteration 15247, loss = 0.03420590\n",
      "Iteration 15248, loss = 0.03420330\n",
      "Iteration 15249, loss = 0.03420074\n",
      "Iteration 15250, loss = 0.03419814\n",
      "Iteration 15251, loss = 0.03419560\n",
      "Iteration 15252, loss = 0.03419301\n",
      "Iteration 15253, loss = 0.03419042\n",
      "Iteration 15254, loss = 0.03418786\n",
      "Iteration 15255, loss = 0.03418529\n",
      "Iteration 15256, loss = 0.03418273\n",
      "Iteration 15257, loss = 0.03418015\n",
      "Iteration 15258, loss = 0.03417760\n",
      "Iteration 15259, loss = 0.03417501\n",
      "Iteration 15260, loss = 0.03417247\n",
      "Iteration 15261, loss = 0.03416987\n",
      "Iteration 15262, loss = 0.03416731\n",
      "Iteration 15263, loss = 0.03416474\n",
      "Iteration 15264, loss = 0.03416218\n",
      "Iteration 15265, loss = 0.03415960\n",
      "Iteration 15266, loss = 0.03415706\n",
      "Iteration 15267, loss = 0.03415449\n",
      "Iteration 15268, loss = 0.03415195\n",
      "Iteration 15269, loss = 0.03414938\n",
      "Iteration 15270, loss = 0.03414678\n",
      "Iteration 15271, loss = 0.03414423\n",
      "Iteration 15272, loss = 0.03414168\n",
      "Iteration 15273, loss = 0.03413912\n",
      "Iteration 15274, loss = 0.03413654\n",
      "Iteration 15275, loss = 0.03413400\n",
      "Iteration 15276, loss = 0.03413142\n",
      "Iteration 15277, loss = 0.03412885\n",
      "Iteration 15278, loss = 0.03412631\n",
      "Iteration 15279, loss = 0.03412378\n",
      "Iteration 15280, loss = 0.03412120\n",
      "Iteration 15281, loss = 0.03411866\n",
      "Iteration 15282, loss = 0.03411609\n",
      "Iteration 15283, loss = 0.03411353\n",
      "Iteration 15284, loss = 0.03411100\n",
      "Iteration 15285, loss = 0.03410840\n",
      "Iteration 15286, loss = 0.03410585\n",
      "Iteration 15287, loss = 0.03410327\n",
      "Iteration 15288, loss = 0.03410072\n",
      "Iteration 15289, loss = 0.03409818\n",
      "Iteration 15290, loss = 0.03409564\n",
      "Iteration 15291, loss = 0.03409308\n",
      "Iteration 15292, loss = 0.03409050\n",
      "Iteration 15293, loss = 0.03408793\n",
      "Iteration 15294, loss = 0.03408539\n",
      "Iteration 15295, loss = 0.03408285\n",
      "Iteration 15296, loss = 0.03408028\n",
      "Iteration 15297, loss = 0.03407773\n",
      "Iteration 15298, loss = 0.03407519\n",
      "Iteration 15299, loss = 0.03407260\n",
      "Iteration 15300, loss = 0.03407008\n",
      "Iteration 15301, loss = 0.03406752\n",
      "Iteration 15302, loss = 0.03406499\n",
      "Iteration 15303, loss = 0.03406237\n",
      "Iteration 15304, loss = 0.03405986\n",
      "Iteration 15305, loss = 0.03405732\n",
      "Iteration 15306, loss = 0.03405474\n",
      "Iteration 15307, loss = 0.03405220\n",
      "Iteration 15308, loss = 0.03404967\n",
      "Iteration 15309, loss = 0.03404712\n",
      "Iteration 15310, loss = 0.03404452\n",
      "Iteration 15311, loss = 0.03404200\n",
      "Iteration 15312, loss = 0.03403944\n",
      "Iteration 15313, loss = 0.03403689\n",
      "Iteration 15314, loss = 0.03403434\n",
      "Iteration 15315, loss = 0.03403178\n",
      "Iteration 15316, loss = 0.03402921\n",
      "Iteration 15317, loss = 0.03402666\n",
      "Iteration 15318, loss = 0.03402412\n",
      "Iteration 15319, loss = 0.03402159\n",
      "Iteration 15320, loss = 0.03401904\n",
      "Iteration 15321, loss = 0.03401648\n",
      "Iteration 15322, loss = 0.03401393\n",
      "Iteration 15323, loss = 0.03401136\n",
      "Iteration 15324, loss = 0.03400885\n",
      "Iteration 15325, loss = 0.03400631\n",
      "Iteration 15326, loss = 0.03400374\n",
      "Iteration 15327, loss = 0.03400116\n",
      "Iteration 15328, loss = 0.03399863\n",
      "Iteration 15329, loss = 0.03399606\n",
      "Iteration 15330, loss = 0.03399353\n",
      "Iteration 15331, loss = 0.03399097\n",
      "Iteration 15332, loss = 0.03398843\n",
      "Iteration 15333, loss = 0.03398585\n",
      "Iteration 15334, loss = 0.03398338\n",
      "Iteration 15335, loss = 0.03398080\n",
      "Iteration 15336, loss = 0.03397822\n",
      "Iteration 15337, loss = 0.03397570\n",
      "Iteration 15338, loss = 0.03397318\n",
      "Iteration 15339, loss = 0.03397060\n",
      "Iteration 15340, loss = 0.03396806\n",
      "Iteration 15341, loss = 0.03396556\n",
      "Iteration 15342, loss = 0.03396300\n",
      "Iteration 15343, loss = 0.03396049\n",
      "Iteration 15344, loss = 0.03395794\n",
      "Iteration 15345, loss = 0.03395545\n",
      "Iteration 15346, loss = 0.03395287\n",
      "Iteration 15347, loss = 0.03395032\n",
      "Iteration 15348, loss = 0.03394779\n",
      "Iteration 15349, loss = 0.03394531\n",
      "Iteration 15350, loss = 0.03394276\n",
      "Iteration 15351, loss = 0.03394022\n",
      "Iteration 15352, loss = 0.03393767\n",
      "Iteration 15353, loss = 0.03393512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15354, loss = 0.03393264\n",
      "Iteration 15355, loss = 0.03393008\n",
      "Iteration 15356, loss = 0.03392755\n",
      "Iteration 15357, loss = 0.03392502\n",
      "Iteration 15358, loss = 0.03392250\n",
      "Iteration 15359, loss = 0.03391999\n",
      "Iteration 15360, loss = 0.03391745\n",
      "Iteration 15361, loss = 0.03391495\n",
      "Iteration 15362, loss = 0.03391238\n",
      "Iteration 15363, loss = 0.03390990\n",
      "Iteration 15364, loss = 0.03390738\n",
      "Iteration 15365, loss = 0.03390483\n",
      "Iteration 15366, loss = 0.03390229\n",
      "Iteration 15367, loss = 0.03389977\n",
      "Iteration 15368, loss = 0.03389726\n",
      "Iteration 15369, loss = 0.03389472\n",
      "Iteration 15370, loss = 0.03389224\n",
      "Iteration 15371, loss = 0.03388966\n",
      "Iteration 15372, loss = 0.03388717\n",
      "Iteration 15373, loss = 0.03388462\n",
      "Iteration 15374, loss = 0.03388212\n",
      "Iteration 15375, loss = 0.03387958\n",
      "Iteration 15376, loss = 0.03387708\n",
      "Iteration 15377, loss = 0.03387455\n",
      "Iteration 15378, loss = 0.03387202\n",
      "Iteration 15379, loss = 0.03386949\n",
      "Iteration 15380, loss = 0.03386696\n",
      "Iteration 15381, loss = 0.03386443\n",
      "Iteration 15382, loss = 0.03386190\n",
      "Iteration 15383, loss = 0.03385939\n",
      "Iteration 15384, loss = 0.03385689\n",
      "Iteration 15385, loss = 0.03385433\n",
      "Iteration 15386, loss = 0.03385183\n",
      "Iteration 15387, loss = 0.03384934\n",
      "Iteration 15388, loss = 0.03384678\n",
      "Iteration 15389, loss = 0.03384429\n",
      "Iteration 15390, loss = 0.03384176\n",
      "Iteration 15391, loss = 0.03383925\n",
      "Iteration 15392, loss = 0.03383675\n",
      "Iteration 15393, loss = 0.03383422\n",
      "Iteration 15394, loss = 0.03383173\n",
      "Iteration 15395, loss = 0.03382921\n",
      "Iteration 15396, loss = 0.03382666\n",
      "Iteration 15397, loss = 0.03382416\n",
      "Iteration 15398, loss = 0.03382165\n",
      "Iteration 15399, loss = 0.03381912\n",
      "Iteration 15400, loss = 0.03381662\n",
      "Iteration 15401, loss = 0.03381411\n",
      "Iteration 15402, loss = 0.03381158\n",
      "Iteration 15403, loss = 0.03380908\n",
      "Iteration 15404, loss = 0.03380657\n",
      "Iteration 15405, loss = 0.03380407\n",
      "Iteration 15406, loss = 0.03380151\n",
      "Iteration 15407, loss = 0.03379900\n",
      "Iteration 15408, loss = 0.03379646\n",
      "Iteration 15409, loss = 0.03379400\n",
      "Iteration 15410, loss = 0.03379147\n",
      "Iteration 15411, loss = 0.03378897\n",
      "Iteration 15412, loss = 0.03378644\n",
      "Iteration 15413, loss = 0.03378395\n",
      "Iteration 15414, loss = 0.03378140\n",
      "Iteration 15415, loss = 0.03377890\n",
      "Iteration 15416, loss = 0.03377637\n",
      "Iteration 15417, loss = 0.03377388\n",
      "Iteration 15418, loss = 0.03377137\n",
      "Iteration 15419, loss = 0.03376885\n",
      "Iteration 15420, loss = 0.03376635\n",
      "Iteration 15421, loss = 0.03376383\n",
      "Iteration 15422, loss = 0.03376131\n",
      "Iteration 15423, loss = 0.03375881\n",
      "Iteration 15424, loss = 0.03375630\n",
      "Iteration 15425, loss = 0.03375382\n",
      "Iteration 15426, loss = 0.03375128\n",
      "Iteration 15427, loss = 0.03374876\n",
      "Iteration 15428, loss = 0.03374627\n",
      "Iteration 15429, loss = 0.03374376\n",
      "Iteration 15430, loss = 0.03374126\n",
      "Iteration 15431, loss = 0.03373876\n",
      "Iteration 15432, loss = 0.03373624\n",
      "Iteration 15433, loss = 0.03373377\n",
      "Iteration 15434, loss = 0.03373126\n",
      "Iteration 15435, loss = 0.03372875\n",
      "Iteration 15436, loss = 0.03372625\n",
      "Iteration 15437, loss = 0.03372374\n",
      "Iteration 15438, loss = 0.03372122\n",
      "Iteration 15439, loss = 0.03371876\n",
      "Iteration 15440, loss = 0.03371625\n",
      "Iteration 15441, loss = 0.03371372\n",
      "Iteration 15442, loss = 0.03371127\n",
      "Iteration 15443, loss = 0.03370871\n",
      "Iteration 15444, loss = 0.03370623\n",
      "Iteration 15445, loss = 0.03370372\n",
      "Iteration 15446, loss = 0.03370125\n",
      "Iteration 15447, loss = 0.03369872\n",
      "Iteration 15448, loss = 0.03369621\n",
      "Iteration 15449, loss = 0.03369371\n",
      "Iteration 15450, loss = 0.03369121\n",
      "Iteration 15451, loss = 0.03368870\n",
      "Iteration 15452, loss = 0.03368618\n",
      "Iteration 15453, loss = 0.03368369\n",
      "Iteration 15454, loss = 0.03368119\n",
      "Iteration 15455, loss = 0.03367865\n",
      "Iteration 15456, loss = 0.03367619\n",
      "Iteration 15457, loss = 0.03367369\n",
      "Iteration 15458, loss = 0.03367117\n",
      "Iteration 15459, loss = 0.03366871\n",
      "Iteration 15460, loss = 0.03366616\n",
      "Iteration 15461, loss = 0.03366366\n",
      "Iteration 15462, loss = 0.03366118\n",
      "Iteration 15463, loss = 0.03365874\n",
      "Iteration 15464, loss = 0.03365619\n",
      "Iteration 15465, loss = 0.03365371\n",
      "Iteration 15466, loss = 0.03365121\n",
      "Iteration 15467, loss = 0.03364873\n",
      "Iteration 15468, loss = 0.03364627\n",
      "Iteration 15469, loss = 0.03364374\n",
      "Iteration 15470, loss = 0.03364126\n",
      "Iteration 15471, loss = 0.03363875\n",
      "Iteration 15472, loss = 0.03363630\n",
      "Iteration 15473, loss = 0.03363377\n",
      "Iteration 15474, loss = 0.03363130\n",
      "Iteration 15475, loss = 0.03362881\n",
      "Iteration 15476, loss = 0.03362634\n",
      "Iteration 15477, loss = 0.03362385\n",
      "Iteration 15478, loss = 0.03362140\n",
      "Iteration 15479, loss = 0.03361887\n",
      "Iteration 15480, loss = 0.03361638\n",
      "Iteration 15481, loss = 0.03361391\n",
      "Iteration 15482, loss = 0.03361145\n",
      "Iteration 15483, loss = 0.03360895\n",
      "Iteration 15484, loss = 0.03360645\n",
      "Iteration 15485, loss = 0.03360397\n",
      "Iteration 15486, loss = 0.03360150\n",
      "Iteration 15487, loss = 0.03359902\n",
      "Iteration 15488, loss = 0.03359655\n",
      "Iteration 15489, loss = 0.03359406\n",
      "Iteration 15490, loss = 0.03359159\n",
      "Iteration 15491, loss = 0.03358910\n",
      "Iteration 15492, loss = 0.03358658\n",
      "Iteration 15493, loss = 0.03358410\n",
      "Iteration 15494, loss = 0.03358164\n",
      "Iteration 15495, loss = 0.03357915\n",
      "Iteration 15496, loss = 0.03357666\n",
      "Iteration 15497, loss = 0.03357418\n",
      "Iteration 15498, loss = 0.03357167\n",
      "Iteration 15499, loss = 0.03356920\n",
      "Iteration 15500, loss = 0.03356672\n",
      "Iteration 15501, loss = 0.03356422\n",
      "Iteration 15502, loss = 0.03356175\n",
      "Iteration 15503, loss = 0.03355925\n",
      "Iteration 15504, loss = 0.03355679\n",
      "Iteration 15505, loss = 0.03355430\n",
      "Iteration 15506, loss = 0.03355181\n",
      "Iteration 15507, loss = 0.03354936\n",
      "Iteration 15508, loss = 0.03354685\n",
      "Iteration 15509, loss = 0.03354438\n",
      "Iteration 15510, loss = 0.03354191\n",
      "Iteration 15511, loss = 0.03353940\n",
      "Iteration 15512, loss = 0.03353693\n",
      "Iteration 15513, loss = 0.03353448\n",
      "Iteration 15514, loss = 0.03353201\n",
      "Iteration 15515, loss = 0.03352953\n",
      "Iteration 15516, loss = 0.03352703\n",
      "Iteration 15517, loss = 0.03352457\n",
      "Iteration 15518, loss = 0.03352210\n",
      "Iteration 15519, loss = 0.03351964\n",
      "Iteration 15520, loss = 0.03351718\n",
      "Iteration 15521, loss = 0.03351470\n",
      "Iteration 15522, loss = 0.03351225\n",
      "Iteration 15523, loss = 0.03350975\n",
      "Iteration 15524, loss = 0.03350726\n",
      "Iteration 15525, loss = 0.03350480\n",
      "Iteration 15526, loss = 0.03350232\n",
      "Iteration 15527, loss = 0.03349988\n",
      "Iteration 15528, loss = 0.03349742\n",
      "Iteration 15529, loss = 0.03349495\n",
      "Iteration 15530, loss = 0.03349247\n",
      "Iteration 15531, loss = 0.03349004\n",
      "Iteration 15532, loss = 0.03348755\n",
      "Iteration 15533, loss = 0.03348507\n",
      "Iteration 15534, loss = 0.03348260\n",
      "Iteration 15535, loss = 0.03348014\n",
      "Iteration 15536, loss = 0.03347766\n",
      "Iteration 15537, loss = 0.03347516\n",
      "Iteration 15538, loss = 0.03347275\n",
      "Iteration 15539, loss = 0.03347023\n",
      "Iteration 15540, loss = 0.03346778\n",
      "Iteration 15541, loss = 0.03346532\n",
      "Iteration 15542, loss = 0.03346280\n",
      "Iteration 15543, loss = 0.03346038\n",
      "Iteration 15544, loss = 0.03345790\n",
      "Iteration 15545, loss = 0.03345544\n",
      "Iteration 15546, loss = 0.03345295\n",
      "Iteration 15547, loss = 0.03345048\n",
      "Iteration 15548, loss = 0.03344802\n",
      "Iteration 15549, loss = 0.03344555\n",
      "Iteration 15550, loss = 0.03344311\n",
      "Iteration 15551, loss = 0.03344064\n",
      "Iteration 15552, loss = 0.03343812\n",
      "Iteration 15553, loss = 0.03343568\n",
      "Iteration 15554, loss = 0.03343321\n",
      "Iteration 15555, loss = 0.03343076\n",
      "Iteration 15556, loss = 0.03342827\n",
      "Iteration 15557, loss = 0.03342585\n",
      "Iteration 15558, loss = 0.03342338\n",
      "Iteration 15559, loss = 0.03342092\n",
      "Iteration 15560, loss = 0.03341843\n",
      "Iteration 15561, loss = 0.03341597\n",
      "Iteration 15562, loss = 0.03341355\n",
      "Iteration 15563, loss = 0.03341105\n",
      "Iteration 15564, loss = 0.03340861\n",
      "Iteration 15565, loss = 0.03340615\n",
      "Iteration 15566, loss = 0.03340370\n",
      "Iteration 15567, loss = 0.03340124\n",
      "Iteration 15568, loss = 0.03339881\n",
      "Iteration 15569, loss = 0.03339636\n",
      "Iteration 15570, loss = 0.03339390\n",
      "Iteration 15571, loss = 0.03339145\n",
      "Iteration 15572, loss = 0.03338901\n",
      "Iteration 15573, loss = 0.03338650\n",
      "Iteration 15574, loss = 0.03338409\n",
      "Iteration 15575, loss = 0.03338162\n",
      "Iteration 15576, loss = 0.03337913\n",
      "Iteration 15577, loss = 0.03337672\n",
      "Iteration 15578, loss = 0.03337426\n",
      "Iteration 15579, loss = 0.03337180\n",
      "Iteration 15580, loss = 0.03336932\n",
      "Iteration 15581, loss = 0.03336687\n",
      "Iteration 15582, loss = 0.03336444\n",
      "Iteration 15583, loss = 0.03336198\n",
      "Iteration 15584, loss = 0.03335955\n",
      "Iteration 15585, loss = 0.03335712\n",
      "Iteration 15586, loss = 0.03335466\n",
      "Iteration 15587, loss = 0.03335220\n",
      "Iteration 15588, loss = 0.03334974\n",
      "Iteration 15589, loss = 0.03334731\n",
      "Iteration 15590, loss = 0.03334487\n",
      "Iteration 15591, loss = 0.03334243\n",
      "Iteration 15592, loss = 0.03333996\n",
      "Iteration 15593, loss = 0.03333751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15594, loss = 0.03333507\n",
      "Iteration 15595, loss = 0.03333264\n",
      "Iteration 15596, loss = 0.03333019\n",
      "Iteration 15597, loss = 0.03332778\n",
      "Iteration 15598, loss = 0.03332534\n",
      "Iteration 15599, loss = 0.03332290\n",
      "Iteration 15600, loss = 0.03332044\n",
      "Iteration 15601, loss = 0.03331804\n",
      "Iteration 15602, loss = 0.03331554\n",
      "Iteration 15603, loss = 0.03331313\n",
      "Iteration 15604, loss = 0.03331068\n",
      "Iteration 15605, loss = 0.03330824\n",
      "Iteration 15606, loss = 0.03330579\n",
      "Iteration 15607, loss = 0.03330336\n",
      "Iteration 15608, loss = 0.03330091\n",
      "Iteration 15609, loss = 0.03329848\n",
      "Iteration 15610, loss = 0.03329603\n",
      "Iteration 15611, loss = 0.03329362\n",
      "Iteration 15612, loss = 0.03329115\n",
      "Iteration 15613, loss = 0.03328872\n",
      "Iteration 15614, loss = 0.03328630\n",
      "Iteration 15615, loss = 0.03328384\n",
      "Iteration 15616, loss = 0.03328141\n",
      "Iteration 15617, loss = 0.03327896\n",
      "Iteration 15618, loss = 0.03327654\n",
      "Iteration 15619, loss = 0.03327409\n",
      "Iteration 15620, loss = 0.03327167\n",
      "Iteration 15621, loss = 0.03326921\n",
      "Iteration 15622, loss = 0.03326678\n",
      "Iteration 15623, loss = 0.03326436\n",
      "Iteration 15624, loss = 0.03326188\n",
      "Iteration 15625, loss = 0.03325948\n",
      "Iteration 15626, loss = 0.03325705\n",
      "Iteration 15627, loss = 0.03325460\n",
      "Iteration 15628, loss = 0.03325215\n",
      "Iteration 15629, loss = 0.03324974\n",
      "Iteration 15630, loss = 0.03324731\n",
      "Iteration 15631, loss = 0.03324485\n",
      "Iteration 15632, loss = 0.03324243\n",
      "Iteration 15633, loss = 0.03324003\n",
      "Iteration 15634, loss = 0.03323757\n",
      "Iteration 15635, loss = 0.03323514\n",
      "Iteration 15636, loss = 0.03323273\n",
      "Iteration 15637, loss = 0.03323027\n",
      "Iteration 15638, loss = 0.03322786\n",
      "Iteration 15639, loss = 0.03322542\n",
      "Iteration 15640, loss = 0.03322299\n",
      "Iteration 15641, loss = 0.03322058\n",
      "Iteration 15642, loss = 0.03321811\n",
      "Iteration 15643, loss = 0.03321570\n",
      "Iteration 15644, loss = 0.03321330\n",
      "Iteration 15645, loss = 0.03321086\n",
      "Iteration 15646, loss = 0.03320843\n",
      "Iteration 15647, loss = 0.03320599\n",
      "Iteration 15648, loss = 0.03320357\n",
      "Iteration 15649, loss = 0.03320117\n",
      "Iteration 15650, loss = 0.03319874\n",
      "Iteration 15651, loss = 0.03319631\n",
      "Iteration 15652, loss = 0.03319390\n",
      "Iteration 15653, loss = 0.03319147\n",
      "Iteration 15654, loss = 0.03318906\n",
      "Iteration 15655, loss = 0.03318661\n",
      "Iteration 15656, loss = 0.03318420\n",
      "Iteration 15657, loss = 0.03318175\n",
      "Iteration 15658, loss = 0.03317932\n",
      "Iteration 15659, loss = 0.03317690\n",
      "Iteration 15660, loss = 0.03317446\n",
      "Iteration 15661, loss = 0.03317205\n",
      "Iteration 15662, loss = 0.03316963\n",
      "Iteration 15663, loss = 0.03316722\n",
      "Iteration 15664, loss = 0.03316476\n",
      "Iteration 15665, loss = 0.03316236\n",
      "Iteration 15666, loss = 0.03315991\n",
      "Iteration 15667, loss = 0.03315746\n",
      "Iteration 15668, loss = 0.03315505\n",
      "Iteration 15669, loss = 0.03315267\n",
      "Iteration 15670, loss = 0.03315020\n",
      "Iteration 15671, loss = 0.03314780\n",
      "Iteration 15672, loss = 0.03314535\n",
      "Iteration 15673, loss = 0.03314297\n",
      "Iteration 15674, loss = 0.03314052\n",
      "Iteration 15675, loss = 0.03313811\n",
      "Iteration 15676, loss = 0.03313566\n",
      "Iteration 15677, loss = 0.03313322\n",
      "Iteration 15678, loss = 0.03313080\n",
      "Iteration 15679, loss = 0.03312837\n",
      "Iteration 15680, loss = 0.03312597\n",
      "Iteration 15681, loss = 0.03312355\n",
      "Iteration 15682, loss = 0.03312112\n",
      "Iteration 15683, loss = 0.03311870\n",
      "Iteration 15684, loss = 0.03311633\n",
      "Iteration 15685, loss = 0.03311388\n",
      "Iteration 15686, loss = 0.03311147\n",
      "Iteration 15687, loss = 0.03310907\n",
      "Iteration 15688, loss = 0.03310665\n",
      "Iteration 15689, loss = 0.03310421\n",
      "Iteration 15690, loss = 0.03310181\n",
      "Iteration 15691, loss = 0.03309941\n",
      "Iteration 15692, loss = 0.03309700\n",
      "Iteration 15693, loss = 0.03309458\n",
      "Iteration 15694, loss = 0.03309215\n",
      "Iteration 15695, loss = 0.03308978\n",
      "Iteration 15696, loss = 0.03308732\n",
      "Iteration 15697, loss = 0.03308492\n",
      "Iteration 15698, loss = 0.03308255\n",
      "Iteration 15699, loss = 0.03308008\n",
      "Iteration 15700, loss = 0.03307770\n",
      "Iteration 15701, loss = 0.03307526\n",
      "Iteration 15702, loss = 0.03307285\n",
      "Iteration 15703, loss = 0.03307043\n",
      "Iteration 15704, loss = 0.03306805\n",
      "Iteration 15705, loss = 0.03306560\n",
      "Iteration 15706, loss = 0.03306320\n",
      "Iteration 15707, loss = 0.03306080\n",
      "Iteration 15708, loss = 0.03305840\n",
      "Iteration 15709, loss = 0.03305601\n",
      "Iteration 15710, loss = 0.03305358\n",
      "Iteration 15711, loss = 0.03305116\n",
      "Iteration 15712, loss = 0.03304879\n",
      "Iteration 15713, loss = 0.03304638\n",
      "Iteration 15714, loss = 0.03304396\n",
      "Iteration 15715, loss = 0.03304156\n",
      "Iteration 15716, loss = 0.03303918\n",
      "Iteration 15717, loss = 0.03303676\n",
      "Iteration 15718, loss = 0.03303435\n",
      "Iteration 15719, loss = 0.03303197\n",
      "Iteration 15720, loss = 0.03302956\n",
      "Iteration 15721, loss = 0.03302720\n",
      "Iteration 15722, loss = 0.03302480\n",
      "Iteration 15723, loss = 0.03302238\n",
      "Iteration 15724, loss = 0.03301996\n",
      "Iteration 15725, loss = 0.03301762\n",
      "Iteration 15726, loss = 0.03301520\n",
      "Iteration 15727, loss = 0.03301280\n",
      "Iteration 15728, loss = 0.03301038\n",
      "Iteration 15729, loss = 0.03300798\n",
      "Iteration 15730, loss = 0.03300558\n",
      "Iteration 15731, loss = 0.03300321\n",
      "Iteration 15732, loss = 0.03300082\n",
      "Iteration 15733, loss = 0.03299841\n",
      "Iteration 15734, loss = 0.03299603\n",
      "Iteration 15735, loss = 0.03299362\n",
      "Iteration 15736, loss = 0.03299120\n",
      "Iteration 15737, loss = 0.03298881\n",
      "Iteration 15738, loss = 0.03298642\n",
      "Iteration 15739, loss = 0.03298399\n",
      "Iteration 15740, loss = 0.03298161\n",
      "Iteration 15741, loss = 0.03297920\n",
      "Iteration 15742, loss = 0.03297684\n",
      "Iteration 15743, loss = 0.03297443\n",
      "Iteration 15744, loss = 0.03297205\n",
      "Iteration 15745, loss = 0.03296962\n",
      "Iteration 15746, loss = 0.03296725\n",
      "Iteration 15747, loss = 0.03296487\n",
      "Iteration 15748, loss = 0.03296245\n",
      "Iteration 15749, loss = 0.03296010\n",
      "Iteration 15750, loss = 0.03295767\n",
      "Iteration 15751, loss = 0.03295534\n",
      "Iteration 15752, loss = 0.03295291\n",
      "Iteration 15753, loss = 0.03295052\n",
      "Iteration 15754, loss = 0.03294813\n",
      "Iteration 15755, loss = 0.03294573\n",
      "Iteration 15756, loss = 0.03294334\n",
      "Iteration 15757, loss = 0.03294101\n",
      "Iteration 15758, loss = 0.03293857\n",
      "Iteration 15759, loss = 0.03293619\n",
      "Iteration 15760, loss = 0.03293380\n",
      "Iteration 15761, loss = 0.03293139\n",
      "Iteration 15762, loss = 0.03292901\n",
      "Iteration 15763, loss = 0.03292663\n",
      "Iteration 15764, loss = 0.03292425\n",
      "Iteration 15765, loss = 0.03292186\n",
      "Iteration 15766, loss = 0.03291947\n",
      "Iteration 15767, loss = 0.03291709\n",
      "Iteration 15768, loss = 0.03291467\n",
      "Iteration 15769, loss = 0.03291229\n",
      "Iteration 15770, loss = 0.03290990\n",
      "Iteration 15771, loss = 0.03290751\n",
      "Iteration 15772, loss = 0.03290513\n",
      "Iteration 15773, loss = 0.03290273\n",
      "Iteration 15774, loss = 0.03290039\n",
      "Iteration 15775, loss = 0.03289796\n",
      "Iteration 15776, loss = 0.03289559\n",
      "Iteration 15777, loss = 0.03289321\n",
      "Iteration 15778, loss = 0.03289084\n",
      "Iteration 15779, loss = 0.03288840\n",
      "Iteration 15780, loss = 0.03288603\n",
      "Iteration 15781, loss = 0.03288365\n",
      "Iteration 15782, loss = 0.03288126\n",
      "Iteration 15783, loss = 0.03287888\n",
      "Iteration 15784, loss = 0.03287652\n",
      "Iteration 15785, loss = 0.03287410\n",
      "Iteration 15786, loss = 0.03287173\n",
      "Iteration 15787, loss = 0.03286934\n",
      "Iteration 15788, loss = 0.03286696\n",
      "Iteration 15789, loss = 0.03286458\n",
      "Iteration 15790, loss = 0.03286218\n",
      "Iteration 15791, loss = 0.03285981\n",
      "Iteration 15792, loss = 0.03285743\n",
      "Iteration 15793, loss = 0.03285507\n",
      "Iteration 15794, loss = 0.03285269\n",
      "Iteration 15795, loss = 0.03285031\n",
      "Iteration 15796, loss = 0.03284796\n",
      "Iteration 15797, loss = 0.03284554\n",
      "Iteration 15798, loss = 0.03284313\n",
      "Iteration 15799, loss = 0.03284078\n",
      "Iteration 15800, loss = 0.03283840\n",
      "Iteration 15801, loss = 0.03283601\n",
      "Iteration 15802, loss = 0.03283363\n",
      "Iteration 15803, loss = 0.03283125\n",
      "Iteration 15804, loss = 0.03282890\n",
      "Iteration 15805, loss = 0.03282651\n",
      "Iteration 15806, loss = 0.03282413\n",
      "Iteration 15807, loss = 0.03282175\n",
      "Iteration 15808, loss = 0.03281939\n",
      "Iteration 15809, loss = 0.03281700\n",
      "Iteration 15810, loss = 0.03281461\n",
      "Iteration 15811, loss = 0.03281225\n",
      "Iteration 15812, loss = 0.03280988\n",
      "Iteration 15813, loss = 0.03280752\n",
      "Iteration 15814, loss = 0.03280513\n",
      "Iteration 15815, loss = 0.03280276\n",
      "Iteration 15816, loss = 0.03280037\n",
      "Iteration 15817, loss = 0.03279801\n",
      "Iteration 15818, loss = 0.03279568\n",
      "Iteration 15819, loss = 0.03279327\n",
      "Iteration 15820, loss = 0.03279091\n",
      "Iteration 15821, loss = 0.03278853\n",
      "Iteration 15822, loss = 0.03278621\n",
      "Iteration 15823, loss = 0.03278384\n",
      "Iteration 15824, loss = 0.03278148\n",
      "Iteration 15825, loss = 0.03277911\n",
      "Iteration 15826, loss = 0.03277672\n",
      "Iteration 15827, loss = 0.03277436\n",
      "Iteration 15828, loss = 0.03277199\n",
      "Iteration 15829, loss = 0.03276965\n",
      "Iteration 15830, loss = 0.03276726\n",
      "Iteration 15831, loss = 0.03276489\n",
      "Iteration 15832, loss = 0.03276253\n",
      "Iteration 15833, loss = 0.03276017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15834, loss = 0.03275783\n",
      "Iteration 15835, loss = 0.03275542\n",
      "Iteration 15836, loss = 0.03275310\n",
      "Iteration 15837, loss = 0.03275071\n",
      "Iteration 15838, loss = 0.03274835\n",
      "Iteration 15839, loss = 0.03274599\n",
      "Iteration 15840, loss = 0.03274363\n",
      "Iteration 15841, loss = 0.03274128\n",
      "Iteration 15842, loss = 0.03273889\n",
      "Iteration 15843, loss = 0.03273654\n",
      "Iteration 15844, loss = 0.03273418\n",
      "Iteration 15845, loss = 0.03273184\n",
      "Iteration 15846, loss = 0.03272946\n",
      "Iteration 15847, loss = 0.03272711\n",
      "Iteration 15848, loss = 0.03272473\n",
      "Iteration 15849, loss = 0.03272243\n",
      "Iteration 15850, loss = 0.03272000\n",
      "Iteration 15851, loss = 0.03271767\n",
      "Iteration 15852, loss = 0.03271531\n",
      "Iteration 15853, loss = 0.03271298\n",
      "Iteration 15854, loss = 0.03271063\n",
      "Iteration 15855, loss = 0.03270826\n",
      "Iteration 15856, loss = 0.03270590\n",
      "Iteration 15857, loss = 0.03270355\n",
      "Iteration 15858, loss = 0.03270120\n",
      "Iteration 15859, loss = 0.03269886\n",
      "Iteration 15860, loss = 0.03269651\n",
      "Iteration 15861, loss = 0.03269414\n",
      "Iteration 15862, loss = 0.03269178\n",
      "Iteration 15863, loss = 0.03268946\n",
      "Iteration 15864, loss = 0.03268709\n",
      "Iteration 15865, loss = 0.03268475\n",
      "Iteration 15866, loss = 0.03268243\n",
      "Iteration 15867, loss = 0.03268004\n",
      "Iteration 15868, loss = 0.03267768\n",
      "Iteration 15869, loss = 0.03267535\n",
      "Iteration 15870, loss = 0.03267296\n",
      "Iteration 15871, loss = 0.03267063\n",
      "Iteration 15872, loss = 0.03266827\n",
      "Iteration 15873, loss = 0.03266596\n",
      "Iteration 15874, loss = 0.03266358\n",
      "Iteration 15875, loss = 0.03266125\n",
      "Iteration 15876, loss = 0.03265891\n",
      "Iteration 15877, loss = 0.03265651\n",
      "Iteration 15878, loss = 0.03265418\n",
      "Iteration 15879, loss = 0.03265187\n",
      "Iteration 15880, loss = 0.03264950\n",
      "Iteration 15881, loss = 0.03264713\n",
      "Iteration 15882, loss = 0.03264483\n",
      "Iteration 15883, loss = 0.03264248\n",
      "Iteration 15884, loss = 0.03264013\n",
      "Iteration 15885, loss = 0.03263777\n",
      "Iteration 15886, loss = 0.03263543\n",
      "Iteration 15887, loss = 0.03263307\n",
      "Iteration 15888, loss = 0.03263074\n",
      "Iteration 15889, loss = 0.03262840\n",
      "Iteration 15890, loss = 0.03262606\n",
      "Iteration 15891, loss = 0.03262371\n",
      "Iteration 15892, loss = 0.03262137\n",
      "Iteration 15893, loss = 0.03261904\n",
      "Iteration 15894, loss = 0.03261670\n",
      "Iteration 15895, loss = 0.03261433\n",
      "Iteration 15896, loss = 0.03261200\n",
      "Iteration 15897, loss = 0.03260965\n",
      "Iteration 15898, loss = 0.03260732\n",
      "Iteration 15899, loss = 0.03260496\n",
      "Iteration 15900, loss = 0.03260262\n",
      "Iteration 15901, loss = 0.03260025\n",
      "Iteration 15902, loss = 0.03259794\n",
      "Iteration 15903, loss = 0.03259560\n",
      "Iteration 15904, loss = 0.03259324\n",
      "Iteration 15905, loss = 0.03259091\n",
      "Iteration 15906, loss = 0.03258860\n",
      "Iteration 15907, loss = 0.03258624\n",
      "Iteration 15908, loss = 0.03258388\n",
      "Iteration 15909, loss = 0.03258156\n",
      "Iteration 15910, loss = 0.03257927\n",
      "Iteration 15911, loss = 0.03257691\n",
      "Iteration 15912, loss = 0.03257455\n",
      "Iteration 15913, loss = 0.03257224\n",
      "Iteration 15914, loss = 0.03256991\n",
      "Iteration 15915, loss = 0.03256755\n",
      "Iteration 15916, loss = 0.03256521\n",
      "Iteration 15917, loss = 0.03256288\n",
      "Iteration 15918, loss = 0.03256054\n",
      "Iteration 15919, loss = 0.03255824\n",
      "Iteration 15920, loss = 0.03255587\n",
      "Iteration 15921, loss = 0.03255356\n",
      "Iteration 15922, loss = 0.03255121\n",
      "Iteration 15923, loss = 0.03254887\n",
      "Iteration 15924, loss = 0.03254652\n",
      "Iteration 15925, loss = 0.03254420\n",
      "Iteration 15926, loss = 0.03254186\n",
      "Iteration 15927, loss = 0.03253955\n",
      "Iteration 15928, loss = 0.03253719\n",
      "Iteration 15929, loss = 0.03253485\n",
      "Iteration 15930, loss = 0.03253256\n",
      "Iteration 15931, loss = 0.03253020\n",
      "Iteration 15932, loss = 0.03252787\n",
      "Iteration 15933, loss = 0.03252552\n",
      "Iteration 15934, loss = 0.03252323\n",
      "Iteration 15935, loss = 0.03252086\n",
      "Iteration 15936, loss = 0.03251856\n",
      "Iteration 15937, loss = 0.03251622\n",
      "Iteration 15938, loss = 0.03251388\n",
      "Iteration 15939, loss = 0.03251155\n",
      "Iteration 15940, loss = 0.03250925\n",
      "Iteration 15941, loss = 0.03250694\n",
      "Iteration 15942, loss = 0.03250461\n",
      "Iteration 15943, loss = 0.03250229\n",
      "Iteration 15944, loss = 0.03249996\n",
      "Iteration 15945, loss = 0.03249763\n",
      "Iteration 15946, loss = 0.03249530\n",
      "Iteration 15947, loss = 0.03249297\n",
      "Iteration 15948, loss = 0.03249064\n",
      "Iteration 15949, loss = 0.03248834\n",
      "Iteration 15950, loss = 0.03248599\n",
      "Iteration 15951, loss = 0.03248367\n",
      "Iteration 15952, loss = 0.03248135\n",
      "Iteration 15953, loss = 0.03247903\n",
      "Iteration 15954, loss = 0.03247671\n",
      "Iteration 15955, loss = 0.03247438\n",
      "Iteration 15956, loss = 0.03247206\n",
      "Iteration 15957, loss = 0.03246974\n",
      "Iteration 15958, loss = 0.03246742\n",
      "Iteration 15959, loss = 0.03246510\n",
      "Iteration 15960, loss = 0.03246277\n",
      "Iteration 15961, loss = 0.03246046\n",
      "Iteration 15962, loss = 0.03245811\n",
      "Iteration 15963, loss = 0.03245582\n",
      "Iteration 15964, loss = 0.03245350\n",
      "Iteration 15965, loss = 0.03245116\n",
      "Iteration 15966, loss = 0.03244885\n",
      "Iteration 15967, loss = 0.03244653\n",
      "Iteration 15968, loss = 0.03244425\n",
      "Iteration 15969, loss = 0.03244191\n",
      "Iteration 15970, loss = 0.03243960\n",
      "Iteration 15971, loss = 0.03243729\n",
      "Iteration 15972, loss = 0.03243499\n",
      "Iteration 15973, loss = 0.03243264\n",
      "Iteration 15974, loss = 0.03243037\n",
      "Iteration 15975, loss = 0.03242805\n",
      "Iteration 15976, loss = 0.03242572\n",
      "Iteration 15977, loss = 0.03242341\n",
      "Iteration 15978, loss = 0.03242108\n",
      "Iteration 15979, loss = 0.03241875\n",
      "Iteration 15980, loss = 0.03241648\n",
      "Iteration 15981, loss = 0.03241417\n",
      "Iteration 15982, loss = 0.03241182\n",
      "Iteration 15983, loss = 0.03240950\n",
      "Iteration 15984, loss = 0.03240721\n",
      "Iteration 15985, loss = 0.03240487\n",
      "Iteration 15986, loss = 0.03240258\n",
      "Iteration 15987, loss = 0.03240025\n",
      "Iteration 15988, loss = 0.03239791\n",
      "Iteration 15989, loss = 0.03239563\n",
      "Iteration 15990, loss = 0.03239332\n",
      "Iteration 15991, loss = 0.03239101\n",
      "Iteration 15992, loss = 0.03238868\n",
      "Iteration 15993, loss = 0.03238638\n",
      "Iteration 15994, loss = 0.03238407\n",
      "Iteration 15995, loss = 0.03238175\n",
      "Iteration 15996, loss = 0.03237944\n",
      "Iteration 15997, loss = 0.03237711\n",
      "Iteration 15998, loss = 0.03237485\n",
      "Iteration 15999, loss = 0.03237250\n",
      "Iteration 16000, loss = 0.03237022\n",
      "Iteration 16001, loss = 0.03236788\n",
      "Iteration 16002, loss = 0.03236557\n",
      "Iteration 16003, loss = 0.03236324\n",
      "Iteration 16004, loss = 0.03236099\n",
      "Iteration 16005, loss = 0.03235866\n",
      "Iteration 16006, loss = 0.03235633\n",
      "Iteration 16007, loss = 0.03235402\n",
      "Iteration 16008, loss = 0.03235171\n",
      "Iteration 16009, loss = 0.03234941\n",
      "Iteration 16010, loss = 0.03234711\n",
      "Iteration 16011, loss = 0.03234477\n",
      "Iteration 16012, loss = 0.03234249\n",
      "Iteration 16013, loss = 0.03234024\n",
      "Iteration 16014, loss = 0.03233788\n",
      "Iteration 16015, loss = 0.03233556\n",
      "Iteration 16016, loss = 0.03233327\n",
      "Iteration 16017, loss = 0.03233096\n",
      "Iteration 16018, loss = 0.03232868\n",
      "Iteration 16019, loss = 0.03232637\n",
      "Iteration 16020, loss = 0.03232408\n",
      "Iteration 16021, loss = 0.03232177\n",
      "Iteration 16022, loss = 0.03231947\n",
      "Iteration 16023, loss = 0.03231718\n",
      "Iteration 16024, loss = 0.03231488\n",
      "Iteration 16025, loss = 0.03231256\n",
      "Iteration 16026, loss = 0.03231028\n",
      "Iteration 16027, loss = 0.03230797\n",
      "Iteration 16028, loss = 0.03230564\n",
      "Iteration 16029, loss = 0.03230338\n",
      "Iteration 16030, loss = 0.03230105\n",
      "Iteration 16031, loss = 0.03229872\n",
      "Iteration 16032, loss = 0.03229645\n",
      "Iteration 16033, loss = 0.03229415\n",
      "Iteration 16034, loss = 0.03229184\n",
      "Iteration 16035, loss = 0.03228955\n",
      "Iteration 16036, loss = 0.03228724\n",
      "Iteration 16037, loss = 0.03228493\n",
      "Iteration 16038, loss = 0.03228267\n",
      "Iteration 16039, loss = 0.03228038\n",
      "Iteration 16040, loss = 0.03227806\n",
      "Iteration 16041, loss = 0.03227575\n",
      "Iteration 16042, loss = 0.03227349\n",
      "Iteration 16043, loss = 0.03227118\n",
      "Iteration 16044, loss = 0.03226891\n",
      "Iteration 16045, loss = 0.03226663\n",
      "Iteration 16046, loss = 0.03226435\n",
      "Iteration 16047, loss = 0.03226204\n",
      "Iteration 16048, loss = 0.03225975\n",
      "Iteration 16049, loss = 0.03225748\n",
      "Iteration 16050, loss = 0.03225519\n",
      "Iteration 16051, loss = 0.03225289\n",
      "Iteration 16052, loss = 0.03225063\n",
      "Iteration 16053, loss = 0.03224832\n",
      "Iteration 16054, loss = 0.03224606\n",
      "Iteration 16055, loss = 0.03224377\n",
      "Iteration 16056, loss = 0.03224145\n",
      "Iteration 16057, loss = 0.03223918\n",
      "Iteration 16058, loss = 0.03223689\n",
      "Iteration 16059, loss = 0.03223460\n",
      "Iteration 16060, loss = 0.03223233\n",
      "Iteration 16061, loss = 0.03223003\n",
      "Iteration 16062, loss = 0.03222773\n",
      "Iteration 16063, loss = 0.03222544\n",
      "Iteration 16064, loss = 0.03222317\n",
      "Iteration 16065, loss = 0.03222087\n",
      "Iteration 16066, loss = 0.03221858\n",
      "Iteration 16067, loss = 0.03221631\n",
      "Iteration 16068, loss = 0.03221403\n",
      "Iteration 16069, loss = 0.03221174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16070, loss = 0.03220943\n",
      "Iteration 16071, loss = 0.03220713\n",
      "Iteration 16072, loss = 0.03220489\n",
      "Iteration 16073, loss = 0.03220260\n",
      "Iteration 16074, loss = 0.03220030\n",
      "Iteration 16075, loss = 0.03219801\n",
      "Iteration 16076, loss = 0.03219573\n",
      "Iteration 16077, loss = 0.03219345\n",
      "Iteration 16078, loss = 0.03219119\n",
      "Iteration 16079, loss = 0.03218889\n",
      "Iteration 16080, loss = 0.03218660\n",
      "Iteration 16081, loss = 0.03218435\n",
      "Iteration 16082, loss = 0.03218204\n",
      "Iteration 16083, loss = 0.03217976\n",
      "Iteration 16084, loss = 0.03217746\n",
      "Iteration 16085, loss = 0.03217520\n",
      "Iteration 16086, loss = 0.03217292\n",
      "Iteration 16087, loss = 0.03217062\n",
      "Iteration 16088, loss = 0.03216836\n",
      "Iteration 16089, loss = 0.03216605\n",
      "Iteration 16090, loss = 0.03216378\n",
      "Iteration 16091, loss = 0.03216155\n",
      "Iteration 16092, loss = 0.03215924\n",
      "Iteration 16093, loss = 0.03215690\n",
      "Iteration 16094, loss = 0.03215468\n",
      "Iteration 16095, loss = 0.03215240\n",
      "Iteration 16096, loss = 0.03215009\n",
      "Iteration 16097, loss = 0.03214782\n",
      "Iteration 16098, loss = 0.03214554\n",
      "Iteration 16099, loss = 0.03214328\n",
      "Iteration 16100, loss = 0.03214102\n",
      "Iteration 16101, loss = 0.03213871\n",
      "Iteration 16102, loss = 0.03213645\n",
      "Iteration 16103, loss = 0.03213419\n",
      "Iteration 16104, loss = 0.03213190\n",
      "Iteration 16105, loss = 0.03212962\n",
      "Iteration 16106, loss = 0.03212736\n",
      "Iteration 16107, loss = 0.03212511\n",
      "Iteration 16108, loss = 0.03212284\n",
      "Iteration 16109, loss = 0.03212054\n",
      "Iteration 16110, loss = 0.03211828\n",
      "Iteration 16111, loss = 0.03211600\n",
      "Iteration 16112, loss = 0.03211373\n",
      "Iteration 16113, loss = 0.03211146\n",
      "Iteration 16114, loss = 0.03210918\n",
      "Iteration 16115, loss = 0.03210693\n",
      "Iteration 16116, loss = 0.03210462\n",
      "Iteration 16117, loss = 0.03210238\n",
      "Iteration 16118, loss = 0.03210010\n",
      "Iteration 16119, loss = 0.03209786\n",
      "Iteration 16120, loss = 0.03209559\n",
      "Iteration 16121, loss = 0.03209330\n",
      "Iteration 16122, loss = 0.03209105\n",
      "Iteration 16123, loss = 0.03208879\n",
      "Iteration 16124, loss = 0.03208652\n",
      "Iteration 16125, loss = 0.03208425\n",
      "Iteration 16126, loss = 0.03208199\n",
      "Iteration 16127, loss = 0.03207973\n",
      "Iteration 16128, loss = 0.03207747\n",
      "Iteration 16129, loss = 0.03207518\n",
      "Iteration 16130, loss = 0.03207295\n",
      "Iteration 16131, loss = 0.03207067\n",
      "Iteration 16132, loss = 0.03206841\n",
      "Iteration 16133, loss = 0.03206614\n",
      "Iteration 16134, loss = 0.03206390\n",
      "Iteration 16135, loss = 0.03206160\n",
      "Iteration 16136, loss = 0.03205937\n",
      "Iteration 16137, loss = 0.03205712\n",
      "Iteration 16138, loss = 0.03205483\n",
      "Iteration 16139, loss = 0.03205255\n",
      "Iteration 16140, loss = 0.03205031\n",
      "Iteration 16141, loss = 0.03204805\n",
      "Iteration 16142, loss = 0.03204579\n",
      "Iteration 16143, loss = 0.03204352\n",
      "Iteration 16144, loss = 0.03204126\n",
      "Iteration 16145, loss = 0.03203903\n",
      "Iteration 16146, loss = 0.03203676\n",
      "Iteration 16147, loss = 0.03203452\n",
      "Iteration 16148, loss = 0.03203226\n",
      "Iteration 16149, loss = 0.03203002\n",
      "Iteration 16150, loss = 0.03202774\n",
      "Iteration 16151, loss = 0.03202549\n",
      "Iteration 16152, loss = 0.03202323\n",
      "Iteration 16153, loss = 0.03202100\n",
      "Iteration 16154, loss = 0.03201874\n",
      "Iteration 16155, loss = 0.03201648\n",
      "Iteration 16156, loss = 0.03201423\n",
      "Iteration 16157, loss = 0.03201197\n",
      "Iteration 16158, loss = 0.03200973\n",
      "Iteration 16159, loss = 0.03200745\n",
      "Iteration 16160, loss = 0.03200518\n",
      "Iteration 16161, loss = 0.03200294\n",
      "Iteration 16162, loss = 0.03200069\n",
      "Iteration 16163, loss = 0.03199842\n",
      "Iteration 16164, loss = 0.03199619\n",
      "Iteration 16165, loss = 0.03199395\n",
      "Iteration 16166, loss = 0.03199166\n",
      "Iteration 16167, loss = 0.03198940\n",
      "Iteration 16168, loss = 0.03198714\n",
      "Iteration 16169, loss = 0.03198489\n",
      "Iteration 16170, loss = 0.03198264\n",
      "Iteration 16171, loss = 0.03198041\n",
      "Iteration 16172, loss = 0.03197812\n",
      "Iteration 16173, loss = 0.03197589\n",
      "Iteration 16174, loss = 0.03197364\n",
      "Iteration 16175, loss = 0.03197138\n",
      "Iteration 16176, loss = 0.03196911\n",
      "Iteration 16177, loss = 0.03196686\n",
      "Iteration 16178, loss = 0.03196464\n",
      "Iteration 16179, loss = 0.03196238\n",
      "Iteration 16180, loss = 0.03196012\n",
      "Iteration 16181, loss = 0.03195789\n",
      "Iteration 16182, loss = 0.03195566\n",
      "Iteration 16183, loss = 0.03195336\n",
      "Iteration 16184, loss = 0.03195112\n",
      "Iteration 16185, loss = 0.03194887\n",
      "Iteration 16186, loss = 0.03194666\n",
      "Iteration 16187, loss = 0.03194440\n",
      "Iteration 16188, loss = 0.03194215\n",
      "Iteration 16189, loss = 0.03193989\n",
      "Iteration 16190, loss = 0.03193766\n",
      "Iteration 16191, loss = 0.03193543\n",
      "Iteration 16192, loss = 0.03193318\n",
      "Iteration 16193, loss = 0.03193093\n",
      "Iteration 16194, loss = 0.03192869\n",
      "Iteration 16195, loss = 0.03192645\n",
      "Iteration 16196, loss = 0.03192419\n",
      "Iteration 16197, loss = 0.03192196\n",
      "Iteration 16198, loss = 0.03191971\n",
      "Iteration 16199, loss = 0.03191748\n",
      "Iteration 16200, loss = 0.03191524\n",
      "Iteration 16201, loss = 0.03191301\n",
      "Iteration 16202, loss = 0.03191075\n",
      "Iteration 16203, loss = 0.03190852\n",
      "Iteration 16204, loss = 0.03190626\n",
      "Iteration 16205, loss = 0.03190402\n",
      "Iteration 16206, loss = 0.03190178\n",
      "Iteration 16207, loss = 0.03189951\n",
      "Iteration 16208, loss = 0.03189731\n",
      "Iteration 16209, loss = 0.03189504\n",
      "Iteration 16210, loss = 0.03189279\n",
      "Iteration 16211, loss = 0.03189059\n",
      "Iteration 16212, loss = 0.03188833\n",
      "Iteration 16213, loss = 0.03188608\n",
      "Iteration 16214, loss = 0.03188383\n",
      "Iteration 16215, loss = 0.03188158\n",
      "Iteration 16216, loss = 0.03187933\n",
      "Iteration 16217, loss = 0.03187711\n",
      "Iteration 16218, loss = 0.03187485\n",
      "Iteration 16219, loss = 0.03187263\n",
      "Iteration 16220, loss = 0.03187036\n",
      "Iteration 16221, loss = 0.03186812\n",
      "Iteration 16222, loss = 0.03186587\n",
      "Iteration 16223, loss = 0.03186365\n",
      "Iteration 16224, loss = 0.03186141\n",
      "Iteration 16225, loss = 0.03185919\n",
      "Iteration 16226, loss = 0.03185692\n",
      "Iteration 16227, loss = 0.03185470\n",
      "Iteration 16228, loss = 0.03185247\n",
      "Iteration 16229, loss = 0.03185023\n",
      "Iteration 16230, loss = 0.03184801\n",
      "Iteration 16231, loss = 0.03184577\n",
      "Iteration 16232, loss = 0.03184352\n",
      "Iteration 16233, loss = 0.03184130\n",
      "Iteration 16234, loss = 0.03183905\n",
      "Iteration 16235, loss = 0.03183683\n",
      "Iteration 16236, loss = 0.03183459\n",
      "Iteration 16237, loss = 0.03183237\n",
      "Iteration 16238, loss = 0.03183013\n",
      "Iteration 16239, loss = 0.03182787\n",
      "Iteration 16240, loss = 0.03182565\n",
      "Iteration 16241, loss = 0.03182346\n",
      "Iteration 16242, loss = 0.03182118\n",
      "Iteration 16243, loss = 0.03181892\n",
      "Iteration 16244, loss = 0.03181670\n",
      "Iteration 16245, loss = 0.03181448\n",
      "Iteration 16246, loss = 0.03181226\n",
      "Iteration 16247, loss = 0.03181000\n",
      "Iteration 16248, loss = 0.03180780\n",
      "Iteration 16249, loss = 0.03180556\n",
      "Iteration 16250, loss = 0.03180332\n",
      "Iteration 16251, loss = 0.03180111\n",
      "Iteration 16252, loss = 0.03179888\n",
      "Iteration 16253, loss = 0.03179669\n",
      "Iteration 16254, loss = 0.03179445\n",
      "Iteration 16255, loss = 0.03179221\n",
      "Iteration 16256, loss = 0.03178997\n",
      "Iteration 16257, loss = 0.03178775\n",
      "Iteration 16258, loss = 0.03178555\n",
      "Iteration 16259, loss = 0.03178333\n",
      "Iteration 16260, loss = 0.03178108\n",
      "Iteration 16261, loss = 0.03177886\n",
      "Iteration 16262, loss = 0.03177665\n",
      "Iteration 16263, loss = 0.03177442\n",
      "Iteration 16264, loss = 0.03177219\n",
      "Iteration 16265, loss = 0.03177002\n",
      "Iteration 16266, loss = 0.03176778\n",
      "Iteration 16267, loss = 0.03176554\n",
      "Iteration 16268, loss = 0.03176332\n",
      "Iteration 16269, loss = 0.03176110\n",
      "Iteration 16270, loss = 0.03175887\n",
      "Iteration 16271, loss = 0.03175665\n",
      "Iteration 16272, loss = 0.03175446\n",
      "Iteration 16273, loss = 0.03175221\n",
      "Iteration 16274, loss = 0.03175002\n",
      "Iteration 16275, loss = 0.03174778\n",
      "Iteration 16276, loss = 0.03174558\n",
      "Iteration 16277, loss = 0.03174335\n",
      "Iteration 16278, loss = 0.03174113\n",
      "Iteration 16279, loss = 0.03173893\n",
      "Iteration 16280, loss = 0.03173669\n",
      "Iteration 16281, loss = 0.03173451\n",
      "Iteration 16282, loss = 0.03173226\n",
      "Iteration 16283, loss = 0.03173003\n",
      "Iteration 16284, loss = 0.03172783\n",
      "Iteration 16285, loss = 0.03172562\n",
      "Iteration 16286, loss = 0.03172342\n",
      "Iteration 16287, loss = 0.03172118\n",
      "Iteration 16288, loss = 0.03171896\n",
      "Iteration 16289, loss = 0.03171674\n",
      "Iteration 16290, loss = 0.03171455\n",
      "Iteration 16291, loss = 0.03171232\n",
      "Iteration 16292, loss = 0.03171011\n",
      "Iteration 16293, loss = 0.03170791\n",
      "Iteration 16294, loss = 0.03170568\n",
      "Iteration 16295, loss = 0.03170348\n",
      "Iteration 16296, loss = 0.03170127\n",
      "Iteration 16297, loss = 0.03169907\n",
      "Iteration 16298, loss = 0.03169686\n",
      "Iteration 16299, loss = 0.03169464\n",
      "Iteration 16300, loss = 0.03169241\n",
      "Iteration 16301, loss = 0.03169022\n",
      "Iteration 16302, loss = 0.03168798\n",
      "Iteration 16303, loss = 0.03168578\n",
      "Iteration 16304, loss = 0.03168359\n",
      "Iteration 16305, loss = 0.03168136\n",
      "Iteration 16306, loss = 0.03167915\n",
      "Iteration 16307, loss = 0.03167693\n",
      "Iteration 16308, loss = 0.03167476\n",
      "Iteration 16309, loss = 0.03167252\n",
      "Iteration 16310, loss = 0.03167032\n",
      "Iteration 16311, loss = 0.03166814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16312, loss = 0.03166592\n",
      "Iteration 16313, loss = 0.03166371\n",
      "Iteration 16314, loss = 0.03166152\n",
      "Iteration 16315, loss = 0.03165930\n",
      "Iteration 16316, loss = 0.03165710\n",
      "Iteration 16317, loss = 0.03165490\n",
      "Iteration 16318, loss = 0.03165270\n",
      "Iteration 16319, loss = 0.03165051\n",
      "Iteration 16320, loss = 0.03164828\n",
      "Iteration 16321, loss = 0.03164606\n",
      "Iteration 16322, loss = 0.03164386\n",
      "Iteration 16323, loss = 0.03164166\n",
      "Iteration 16324, loss = 0.03163947\n",
      "Iteration 16325, loss = 0.03163724\n",
      "Iteration 16326, loss = 0.03163508\n",
      "Iteration 16327, loss = 0.03163286\n",
      "Iteration 16328, loss = 0.03163068\n",
      "Iteration 16329, loss = 0.03162842\n",
      "Iteration 16330, loss = 0.03162627\n",
      "Iteration 16331, loss = 0.03162405\n",
      "Iteration 16332, loss = 0.03162188\n",
      "Iteration 16333, loss = 0.03161965\n",
      "Iteration 16334, loss = 0.03161745\n",
      "Iteration 16335, loss = 0.03161523\n",
      "Iteration 16336, loss = 0.03161304\n",
      "Iteration 16337, loss = 0.03161083\n",
      "Iteration 16338, loss = 0.03160863\n",
      "Iteration 16339, loss = 0.03160646\n",
      "Iteration 16340, loss = 0.03160424\n",
      "Iteration 16341, loss = 0.03160206\n",
      "Iteration 16342, loss = 0.03159984\n",
      "Iteration 16343, loss = 0.03159767\n",
      "Iteration 16344, loss = 0.03159547\n",
      "Iteration 16345, loss = 0.03159325\n",
      "Iteration 16346, loss = 0.03159107\n",
      "Iteration 16347, loss = 0.03158887\n",
      "Iteration 16348, loss = 0.03158669\n",
      "Iteration 16349, loss = 0.03158451\n",
      "Iteration 16350, loss = 0.03158229\n",
      "Iteration 16351, loss = 0.03158012\n",
      "Iteration 16352, loss = 0.03157790\n",
      "Iteration 16353, loss = 0.03157572\n",
      "Iteration 16354, loss = 0.03157352\n",
      "Iteration 16355, loss = 0.03157134\n",
      "Iteration 16356, loss = 0.03156916\n",
      "Iteration 16357, loss = 0.03156692\n",
      "Iteration 16358, loss = 0.03156473\n",
      "Iteration 16359, loss = 0.03156255\n",
      "Iteration 16360, loss = 0.03156036\n",
      "Iteration 16361, loss = 0.03155815\n",
      "Iteration 16362, loss = 0.03155597\n",
      "Iteration 16363, loss = 0.03155377\n",
      "Iteration 16364, loss = 0.03155156\n",
      "Iteration 16365, loss = 0.03154939\n",
      "Iteration 16366, loss = 0.03154719\n",
      "Iteration 16367, loss = 0.03154502\n",
      "Iteration 16368, loss = 0.03154280\n",
      "Iteration 16369, loss = 0.03154060\n",
      "Iteration 16370, loss = 0.03153843\n",
      "Iteration 16371, loss = 0.03153623\n",
      "Iteration 16372, loss = 0.03153402\n",
      "Iteration 16373, loss = 0.03153185\n",
      "Iteration 16374, loss = 0.03152965\n",
      "Iteration 16375, loss = 0.03152748\n",
      "Iteration 16376, loss = 0.03152527\n",
      "Iteration 16377, loss = 0.03152309\n",
      "Iteration 16378, loss = 0.03152089\n",
      "Iteration 16379, loss = 0.03151876\n",
      "Iteration 16380, loss = 0.03151652\n",
      "Iteration 16381, loss = 0.03151435\n",
      "Iteration 16382, loss = 0.03151217\n",
      "Iteration 16383, loss = 0.03150996\n",
      "Iteration 16384, loss = 0.03150778\n",
      "Iteration 16385, loss = 0.03150562\n",
      "Iteration 16386, loss = 0.03150343\n",
      "Iteration 16387, loss = 0.03150123\n",
      "Iteration 16388, loss = 0.03149906\n",
      "Iteration 16389, loss = 0.03149688\n",
      "Iteration 16390, loss = 0.03149471\n",
      "Iteration 16391, loss = 0.03149253\n",
      "Iteration 16392, loss = 0.03149034\n",
      "Iteration 16393, loss = 0.03148815\n",
      "Iteration 16394, loss = 0.03148597\n",
      "Iteration 16395, loss = 0.03148379\n",
      "Iteration 16396, loss = 0.03148162\n",
      "Iteration 16397, loss = 0.03147945\n",
      "Iteration 16398, loss = 0.03147727\n",
      "Iteration 16399, loss = 0.03147506\n",
      "Iteration 16400, loss = 0.03147291\n",
      "Iteration 16401, loss = 0.03147071\n",
      "Iteration 16402, loss = 0.03146853\n",
      "Iteration 16403, loss = 0.03146636\n",
      "Iteration 16404, loss = 0.03146423\n",
      "Iteration 16405, loss = 0.03146199\n",
      "Iteration 16406, loss = 0.03145985\n",
      "Iteration 16407, loss = 0.03145764\n",
      "Iteration 16408, loss = 0.03145547\n",
      "Iteration 16409, loss = 0.03145329\n",
      "Iteration 16410, loss = 0.03145109\n",
      "Iteration 16411, loss = 0.03144895\n",
      "Iteration 16412, loss = 0.03144672\n",
      "Iteration 16413, loss = 0.03144457\n",
      "Iteration 16414, loss = 0.03144238\n",
      "Iteration 16415, loss = 0.03144022\n",
      "Iteration 16416, loss = 0.03143801\n",
      "Iteration 16417, loss = 0.03143587\n",
      "Iteration 16418, loss = 0.03143366\n",
      "Iteration 16419, loss = 0.03143149\n",
      "Iteration 16420, loss = 0.03142930\n",
      "Iteration 16421, loss = 0.03142713\n",
      "Iteration 16422, loss = 0.03142497\n",
      "Iteration 16423, loss = 0.03142281\n",
      "Iteration 16424, loss = 0.03142063\n",
      "Iteration 16425, loss = 0.03141843\n",
      "Iteration 16426, loss = 0.03141628\n",
      "Iteration 16427, loss = 0.03141410\n",
      "Iteration 16428, loss = 0.03141194\n",
      "Iteration 16429, loss = 0.03140972\n",
      "Iteration 16430, loss = 0.03140759\n",
      "Iteration 16431, loss = 0.03140540\n",
      "Iteration 16432, loss = 0.03140323\n",
      "Iteration 16433, loss = 0.03140105\n",
      "Iteration 16434, loss = 0.03139888\n",
      "Iteration 16435, loss = 0.03139668\n",
      "Iteration 16436, loss = 0.03139453\n",
      "Iteration 16437, loss = 0.03139236\n",
      "Iteration 16438, loss = 0.03139017\n",
      "Iteration 16439, loss = 0.03138800\n",
      "Iteration 16440, loss = 0.03138584\n",
      "Iteration 16441, loss = 0.03138367\n",
      "Iteration 16442, loss = 0.03138148\n",
      "Iteration 16443, loss = 0.03137933\n",
      "Iteration 16444, loss = 0.03137717\n",
      "Iteration 16445, loss = 0.03137499\n",
      "Iteration 16446, loss = 0.03137281\n",
      "Iteration 16447, loss = 0.03137067\n",
      "Iteration 16448, loss = 0.03136849\n",
      "Iteration 16449, loss = 0.03136632\n",
      "Iteration 16450, loss = 0.03136414\n",
      "Iteration 16451, loss = 0.03136199\n",
      "Iteration 16452, loss = 0.03135982\n",
      "Iteration 16453, loss = 0.03135765\n",
      "Iteration 16454, loss = 0.03135548\n",
      "Iteration 16455, loss = 0.03135331\n",
      "Iteration 16456, loss = 0.03135113\n",
      "Iteration 16457, loss = 0.03134900\n",
      "Iteration 16458, loss = 0.03134683\n",
      "Iteration 16459, loss = 0.03134465\n",
      "Iteration 16460, loss = 0.03134248\n",
      "Iteration 16461, loss = 0.03134032\n",
      "Iteration 16462, loss = 0.03133815\n",
      "Iteration 16463, loss = 0.03133599\n",
      "Iteration 16464, loss = 0.03133378\n",
      "Iteration 16465, loss = 0.03133164\n",
      "Iteration 16466, loss = 0.03132947\n",
      "Iteration 16467, loss = 0.03132729\n",
      "Iteration 16468, loss = 0.03132512\n",
      "Iteration 16469, loss = 0.03132297\n",
      "Iteration 16470, loss = 0.03132080\n",
      "Iteration 16471, loss = 0.03131864\n",
      "Iteration 16472, loss = 0.03131649\n",
      "Iteration 16473, loss = 0.03131432\n",
      "Iteration 16474, loss = 0.03131219\n",
      "Iteration 16475, loss = 0.03131001\n",
      "Iteration 16476, loss = 0.03130782\n",
      "Iteration 16477, loss = 0.03130566\n",
      "Iteration 16478, loss = 0.03130349\n",
      "Iteration 16479, loss = 0.03130135\n",
      "Iteration 16480, loss = 0.03129917\n",
      "Iteration 16481, loss = 0.03129701\n",
      "Iteration 16482, loss = 0.03129484\n",
      "Iteration 16483, loss = 0.03129265\n",
      "Iteration 16484, loss = 0.03129052\n",
      "Iteration 16485, loss = 0.03128835\n",
      "Iteration 16486, loss = 0.03128622\n",
      "Iteration 16487, loss = 0.03128404\n",
      "Iteration 16488, loss = 0.03128187\n",
      "Iteration 16489, loss = 0.03127972\n",
      "Iteration 16490, loss = 0.03127756\n",
      "Iteration 16491, loss = 0.03127541\n",
      "Iteration 16492, loss = 0.03127326\n",
      "Iteration 16493, loss = 0.03127109\n",
      "Iteration 16494, loss = 0.03126896\n",
      "Iteration 16495, loss = 0.03126680\n",
      "Iteration 16496, loss = 0.03126463\n",
      "Iteration 16497, loss = 0.03126248\n",
      "Iteration 16498, loss = 0.03126032\n",
      "Iteration 16499, loss = 0.03125816\n",
      "Iteration 16500, loss = 0.03125604\n",
      "Iteration 16501, loss = 0.03125389\n",
      "Iteration 16502, loss = 0.03125177\n",
      "Iteration 16503, loss = 0.03124960\n",
      "Iteration 16504, loss = 0.03124747\n",
      "Iteration 16505, loss = 0.03124530\n",
      "Iteration 16506, loss = 0.03124315\n",
      "Iteration 16507, loss = 0.03124099\n",
      "Iteration 16508, loss = 0.03123885\n",
      "Iteration 16509, loss = 0.03123667\n",
      "Iteration 16510, loss = 0.03123451\n",
      "Iteration 16511, loss = 0.03123241\n",
      "Iteration 16512, loss = 0.03123023\n",
      "Iteration 16513, loss = 0.03122808\n",
      "Iteration 16514, loss = 0.03122593\n",
      "Iteration 16515, loss = 0.03122379\n",
      "Iteration 16516, loss = 0.03122162\n",
      "Iteration 16517, loss = 0.03121950\n",
      "Iteration 16518, loss = 0.03121738\n",
      "Iteration 16519, loss = 0.03121520\n",
      "Iteration 16520, loss = 0.03121307\n",
      "Iteration 16521, loss = 0.03121092\n",
      "Iteration 16522, loss = 0.03120878\n",
      "Iteration 16523, loss = 0.03120666\n",
      "Iteration 16524, loss = 0.03120452\n",
      "Iteration 16525, loss = 0.03120238\n",
      "Iteration 16526, loss = 0.03120022\n",
      "Iteration 16527, loss = 0.03119805\n",
      "Iteration 16528, loss = 0.03119595\n",
      "Iteration 16529, loss = 0.03119381\n",
      "Iteration 16530, loss = 0.03119163\n",
      "Iteration 16531, loss = 0.03118953\n",
      "Iteration 16532, loss = 0.03118734\n",
      "Iteration 16533, loss = 0.03118519\n",
      "Iteration 16534, loss = 0.03118305\n",
      "Iteration 16535, loss = 0.03118093\n",
      "Iteration 16536, loss = 0.03117876\n",
      "Iteration 16537, loss = 0.03117666\n",
      "Iteration 16538, loss = 0.03117451\n",
      "Iteration 16539, loss = 0.03117235\n",
      "Iteration 16540, loss = 0.03117021\n",
      "Iteration 16541, loss = 0.03116810\n",
      "Iteration 16542, loss = 0.03116593\n",
      "Iteration 16543, loss = 0.03116378\n",
      "Iteration 16544, loss = 0.03116167\n",
      "Iteration 16545, loss = 0.03115950\n",
      "Iteration 16546, loss = 0.03115736\n",
      "Iteration 16547, loss = 0.03115526\n",
      "Iteration 16548, loss = 0.03115309\n",
      "Iteration 16549, loss = 0.03115097\n",
      "Iteration 16550, loss = 0.03114883\n",
      "Iteration 16551, loss = 0.03114670\n",
      "Iteration 16552, loss = 0.03114457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16553, loss = 0.03114240\n",
      "Iteration 16554, loss = 0.03114029\n",
      "Iteration 16555, loss = 0.03113817\n",
      "Iteration 16556, loss = 0.03113601\n",
      "Iteration 16557, loss = 0.03113391\n",
      "Iteration 16558, loss = 0.03113175\n",
      "Iteration 16559, loss = 0.03112962\n",
      "Iteration 16560, loss = 0.03112750\n",
      "Iteration 16561, loss = 0.03112537\n",
      "Iteration 16562, loss = 0.03112323\n",
      "Iteration 16563, loss = 0.03112111\n",
      "Iteration 16564, loss = 0.03111896\n",
      "Iteration 16565, loss = 0.03111682\n",
      "Iteration 16566, loss = 0.03111469\n",
      "Iteration 16567, loss = 0.03111257\n",
      "Iteration 16568, loss = 0.03111043\n",
      "Iteration 16569, loss = 0.03110830\n",
      "Iteration 16570, loss = 0.03110617\n",
      "Iteration 16571, loss = 0.03110401\n",
      "Iteration 16572, loss = 0.03110192\n",
      "Iteration 16573, loss = 0.03109980\n",
      "Iteration 16574, loss = 0.03109763\n",
      "Iteration 16575, loss = 0.03109551\n",
      "Iteration 16576, loss = 0.03109336\n",
      "Iteration 16577, loss = 0.03109124\n",
      "Iteration 16578, loss = 0.03108911\n",
      "Iteration 16579, loss = 0.03108698\n",
      "Iteration 16580, loss = 0.03108487\n",
      "Iteration 16581, loss = 0.03108277\n",
      "Iteration 16582, loss = 0.03108059\n",
      "Iteration 16583, loss = 0.03107847\n",
      "Iteration 16584, loss = 0.03107634\n",
      "Iteration 16585, loss = 0.03107419\n",
      "Iteration 16586, loss = 0.03107207\n",
      "Iteration 16587, loss = 0.03106994\n",
      "Iteration 16588, loss = 0.03106779\n",
      "Iteration 16589, loss = 0.03106567\n",
      "Iteration 16590, loss = 0.03106356\n",
      "Iteration 16591, loss = 0.03106139\n",
      "Iteration 16592, loss = 0.03105926\n",
      "Iteration 16593, loss = 0.03105714\n",
      "Iteration 16594, loss = 0.03105499\n",
      "Iteration 16595, loss = 0.03105288\n",
      "Iteration 16596, loss = 0.03105072\n",
      "Iteration 16597, loss = 0.03104861\n",
      "Iteration 16598, loss = 0.03104651\n",
      "Iteration 16599, loss = 0.03104435\n",
      "Iteration 16600, loss = 0.03104222\n",
      "Iteration 16601, loss = 0.03104007\n",
      "Iteration 16602, loss = 0.03103795\n",
      "Iteration 16603, loss = 0.03103587\n",
      "Iteration 16604, loss = 0.03103369\n",
      "Iteration 16605, loss = 0.03103158\n",
      "Iteration 16606, loss = 0.03102945\n",
      "Iteration 16607, loss = 0.03102735\n",
      "Iteration 16608, loss = 0.03102520\n",
      "Iteration 16609, loss = 0.03102307\n",
      "Iteration 16610, loss = 0.03102095\n",
      "Iteration 16611, loss = 0.03101885\n",
      "Iteration 16612, loss = 0.03101673\n",
      "Iteration 16613, loss = 0.03101460\n",
      "Iteration 16614, loss = 0.03101246\n",
      "Iteration 16615, loss = 0.03101037\n",
      "Iteration 16616, loss = 0.03100823\n",
      "Iteration 16617, loss = 0.03100612\n",
      "Iteration 16618, loss = 0.03100402\n",
      "Iteration 16619, loss = 0.03100190\n",
      "Iteration 16620, loss = 0.03099976\n",
      "Iteration 16621, loss = 0.03099765\n",
      "Iteration 16622, loss = 0.03099552\n",
      "Iteration 16623, loss = 0.03099341\n",
      "Iteration 16624, loss = 0.03099129\n",
      "Iteration 16625, loss = 0.03098919\n",
      "Iteration 16626, loss = 0.03098703\n",
      "Iteration 16627, loss = 0.03098495\n",
      "Iteration 16628, loss = 0.03098284\n",
      "Iteration 16629, loss = 0.03098071\n",
      "Iteration 16630, loss = 0.03097861\n",
      "Iteration 16631, loss = 0.03097650\n",
      "Iteration 16632, loss = 0.03097436\n",
      "Iteration 16633, loss = 0.03097224\n",
      "Iteration 16634, loss = 0.03097014\n",
      "Iteration 16635, loss = 0.03096803\n",
      "Iteration 16636, loss = 0.03096594\n",
      "Iteration 16637, loss = 0.03096378\n",
      "Iteration 16638, loss = 0.03096168\n",
      "Iteration 16639, loss = 0.03095956\n",
      "Iteration 16640, loss = 0.03095748\n",
      "Iteration 16641, loss = 0.03095533\n",
      "Iteration 16642, loss = 0.03095324\n",
      "Iteration 16643, loss = 0.03095112\n",
      "Iteration 16644, loss = 0.03094900\n",
      "Iteration 16645, loss = 0.03094689\n",
      "Iteration 16646, loss = 0.03094483\n",
      "Iteration 16647, loss = 0.03094265\n",
      "Iteration 16648, loss = 0.03094054\n",
      "Iteration 16649, loss = 0.03093844\n",
      "Iteration 16650, loss = 0.03093630\n",
      "Iteration 16651, loss = 0.03093419\n",
      "Iteration 16652, loss = 0.03093208\n",
      "Iteration 16653, loss = 0.03092996\n",
      "Iteration 16654, loss = 0.03092783\n",
      "Iteration 16655, loss = 0.03092575\n",
      "Iteration 16656, loss = 0.03092361\n",
      "Iteration 16657, loss = 0.03092151\n",
      "Iteration 16658, loss = 0.03091938\n",
      "Iteration 16659, loss = 0.03091725\n",
      "Iteration 16660, loss = 0.03091516\n",
      "Iteration 16661, loss = 0.03091301\n",
      "Iteration 16662, loss = 0.03091094\n",
      "Iteration 16663, loss = 0.03090880\n",
      "Iteration 16664, loss = 0.03090671\n",
      "Iteration 16665, loss = 0.03090458\n",
      "Iteration 16666, loss = 0.03090249\n",
      "Iteration 16667, loss = 0.03090034\n",
      "Iteration 16668, loss = 0.03089826\n",
      "Iteration 16669, loss = 0.03089613\n",
      "Iteration 16670, loss = 0.03089405\n",
      "Iteration 16671, loss = 0.03089193\n",
      "Iteration 16672, loss = 0.03088981\n",
      "Iteration 16673, loss = 0.03088773\n",
      "Iteration 16674, loss = 0.03088563\n",
      "Iteration 16675, loss = 0.03088351\n",
      "Iteration 16676, loss = 0.03088140\n",
      "Iteration 16677, loss = 0.03087933\n",
      "Iteration 16678, loss = 0.03087721\n",
      "Iteration 16679, loss = 0.03087511\n",
      "Iteration 16680, loss = 0.03087301\n",
      "Iteration 16681, loss = 0.03087088\n",
      "Iteration 16682, loss = 0.03086878\n",
      "Iteration 16683, loss = 0.03086667\n",
      "Iteration 16684, loss = 0.03086458\n",
      "Iteration 16685, loss = 0.03086248\n",
      "Iteration 16686, loss = 0.03086038\n",
      "Iteration 16687, loss = 0.03085830\n",
      "Iteration 16688, loss = 0.03085615\n",
      "Iteration 16689, loss = 0.03085406\n",
      "Iteration 16690, loss = 0.03085195\n",
      "Iteration 16691, loss = 0.03084985\n",
      "Iteration 16692, loss = 0.03084776\n",
      "Iteration 16693, loss = 0.03084567\n",
      "Iteration 16694, loss = 0.03084356\n",
      "Iteration 16695, loss = 0.03084143\n",
      "Iteration 16696, loss = 0.03083935\n",
      "Iteration 16697, loss = 0.03083727\n",
      "Iteration 16698, loss = 0.03083517\n",
      "Iteration 16699, loss = 0.03083305\n",
      "Iteration 16700, loss = 0.03083096\n",
      "Iteration 16701, loss = 0.03082887\n",
      "Iteration 16702, loss = 0.03082676\n",
      "Iteration 16703, loss = 0.03082469\n",
      "Iteration 16704, loss = 0.03082258\n",
      "Iteration 16705, loss = 0.03082047\n",
      "Iteration 16706, loss = 0.03081840\n",
      "Iteration 16707, loss = 0.03081630\n",
      "Iteration 16708, loss = 0.03081421\n",
      "Iteration 16709, loss = 0.03081213\n",
      "Iteration 16710, loss = 0.03081004\n",
      "Iteration 16711, loss = 0.03080792\n",
      "Iteration 16712, loss = 0.03080583\n",
      "Iteration 16713, loss = 0.03080375\n",
      "Iteration 16714, loss = 0.03080165\n",
      "Iteration 16715, loss = 0.03079956\n",
      "Iteration 16716, loss = 0.03079747\n",
      "Iteration 16717, loss = 0.03079539\n",
      "Iteration 16718, loss = 0.03079329\n",
      "Iteration 16719, loss = 0.03079121\n",
      "Iteration 16720, loss = 0.03078912\n",
      "Iteration 16721, loss = 0.03078702\n",
      "Iteration 16722, loss = 0.03078495\n",
      "Iteration 16723, loss = 0.03078285\n",
      "Iteration 16724, loss = 0.03078076\n",
      "Iteration 16725, loss = 0.03077868\n",
      "Iteration 16726, loss = 0.03077659\n",
      "Iteration 16727, loss = 0.03077448\n",
      "Iteration 16728, loss = 0.03077244\n",
      "Iteration 16729, loss = 0.03077035\n",
      "Iteration 16730, loss = 0.03076825\n",
      "Iteration 16731, loss = 0.03076616\n",
      "Iteration 16732, loss = 0.03076410\n",
      "Iteration 16733, loss = 0.03076199\n",
      "Iteration 16734, loss = 0.03075994\n",
      "Iteration 16735, loss = 0.03075784\n",
      "Iteration 16736, loss = 0.03075578\n",
      "Iteration 16737, loss = 0.03075369\n",
      "Iteration 16738, loss = 0.03075162\n",
      "Iteration 16739, loss = 0.03074954\n",
      "Iteration 16740, loss = 0.03074745\n",
      "Iteration 16741, loss = 0.03074537\n",
      "Iteration 16742, loss = 0.03074329\n",
      "Iteration 16743, loss = 0.03074123\n",
      "Iteration 16744, loss = 0.03073915\n",
      "Iteration 16745, loss = 0.03073707\n",
      "Iteration 16746, loss = 0.03073496\n",
      "Iteration 16747, loss = 0.03073291\n",
      "Iteration 16748, loss = 0.03073082\n",
      "Iteration 16749, loss = 0.03072875\n",
      "Iteration 16750, loss = 0.03072667\n",
      "Iteration 16751, loss = 0.03072461\n",
      "Iteration 16752, loss = 0.03072251\n",
      "Iteration 16753, loss = 0.03072041\n",
      "Iteration 16754, loss = 0.03071835\n",
      "Iteration 16755, loss = 0.03071626\n",
      "Iteration 16756, loss = 0.03071421\n",
      "Iteration 16757, loss = 0.03071213\n",
      "Iteration 16758, loss = 0.03071004\n",
      "Iteration 16759, loss = 0.03070794\n",
      "Iteration 16760, loss = 0.03070588\n",
      "Iteration 16761, loss = 0.03070380\n",
      "Iteration 16762, loss = 0.03070172\n",
      "Iteration 16763, loss = 0.03069963\n",
      "Iteration 16764, loss = 0.03069755\n",
      "Iteration 16765, loss = 0.03069550\n",
      "Iteration 16766, loss = 0.03069340\n",
      "Iteration 16767, loss = 0.03069134\n",
      "Iteration 16768, loss = 0.03068925\n",
      "Iteration 16769, loss = 0.03068718\n",
      "Iteration 16770, loss = 0.03068510\n",
      "Iteration 16771, loss = 0.03068303\n",
      "Iteration 16772, loss = 0.03068097\n",
      "Iteration 16773, loss = 0.03067887\n",
      "Iteration 16774, loss = 0.03067678\n",
      "Iteration 16775, loss = 0.03067473\n",
      "Iteration 16776, loss = 0.03067262\n",
      "Iteration 16777, loss = 0.03067057\n",
      "Iteration 16778, loss = 0.03066848\n",
      "Iteration 16779, loss = 0.03066641\n",
      "Iteration 16780, loss = 0.03066431\n",
      "Iteration 16781, loss = 0.03066228\n",
      "Iteration 16782, loss = 0.03066019\n",
      "Iteration 16783, loss = 0.03065811\n",
      "Iteration 16784, loss = 0.03065604\n",
      "Iteration 16785, loss = 0.03065399\n",
      "Iteration 16786, loss = 0.03065189\n",
      "Iteration 16787, loss = 0.03064982\n",
      "Iteration 16788, loss = 0.03064776\n",
      "Iteration 16789, loss = 0.03064569\n",
      "Iteration 16790, loss = 0.03064360\n",
      "Iteration 16791, loss = 0.03064153\n",
      "Iteration 16792, loss = 0.03063948\n",
      "Iteration 16793, loss = 0.03063740\n",
      "Iteration 16794, loss = 0.03063534\n",
      "Iteration 16795, loss = 0.03063328\n",
      "Iteration 16796, loss = 0.03063120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16797, loss = 0.03062915\n",
      "Iteration 16798, loss = 0.03062707\n",
      "Iteration 16799, loss = 0.03062501\n",
      "Iteration 16800, loss = 0.03062296\n",
      "Iteration 16801, loss = 0.03062088\n",
      "Iteration 16802, loss = 0.03061883\n",
      "Iteration 16803, loss = 0.03061676\n",
      "Iteration 16804, loss = 0.03061470\n",
      "Iteration 16805, loss = 0.03061263\n",
      "Iteration 16806, loss = 0.03061057\n",
      "Iteration 16807, loss = 0.03060852\n",
      "Iteration 16808, loss = 0.03060644\n",
      "Iteration 16809, loss = 0.03060437\n",
      "Iteration 16810, loss = 0.03060230\n",
      "Iteration 16811, loss = 0.03060025\n",
      "Iteration 16812, loss = 0.03059820\n",
      "Iteration 16813, loss = 0.03059612\n",
      "Iteration 16814, loss = 0.03059408\n",
      "Iteration 16815, loss = 0.03059202\n",
      "Iteration 16816, loss = 0.03058994\n",
      "Iteration 16817, loss = 0.03058788\n",
      "Iteration 16818, loss = 0.03058584\n",
      "Iteration 16819, loss = 0.03058375\n",
      "Iteration 16820, loss = 0.03058168\n",
      "Iteration 16821, loss = 0.03057964\n",
      "Iteration 16822, loss = 0.03057756\n",
      "Iteration 16823, loss = 0.03057550\n",
      "Iteration 16824, loss = 0.03057341\n",
      "Iteration 16825, loss = 0.03057140\n",
      "Iteration 16826, loss = 0.03056934\n",
      "Iteration 16827, loss = 0.03056726\n",
      "Iteration 16828, loss = 0.03056520\n",
      "Iteration 16829, loss = 0.03056312\n",
      "Iteration 16830, loss = 0.03056110\n",
      "Iteration 16831, loss = 0.03055902\n",
      "Iteration 16832, loss = 0.03055696\n",
      "Iteration 16833, loss = 0.03055491\n",
      "Iteration 16834, loss = 0.03055288\n",
      "Iteration 16835, loss = 0.03055081\n",
      "Iteration 16836, loss = 0.03054876\n",
      "Iteration 16837, loss = 0.03054673\n",
      "Iteration 16838, loss = 0.03054465\n",
      "Iteration 16839, loss = 0.03054259\n",
      "Iteration 16840, loss = 0.03054055\n",
      "Iteration 16841, loss = 0.03053853\n",
      "Iteration 16842, loss = 0.03053645\n",
      "Iteration 16843, loss = 0.03053441\n",
      "Iteration 16844, loss = 0.03053235\n",
      "Iteration 16845, loss = 0.03053028\n",
      "Iteration 16846, loss = 0.03052822\n",
      "Iteration 16847, loss = 0.03052616\n",
      "Iteration 16848, loss = 0.03052413\n",
      "Iteration 16849, loss = 0.03052208\n",
      "Iteration 16850, loss = 0.03052001\n",
      "Iteration 16851, loss = 0.03051796\n",
      "Iteration 16852, loss = 0.03051595\n",
      "Iteration 16853, loss = 0.03051388\n",
      "Iteration 16854, loss = 0.03051179\n",
      "Iteration 16855, loss = 0.03050977\n",
      "Iteration 16856, loss = 0.03050771\n",
      "Iteration 16857, loss = 0.03050567\n",
      "Iteration 16858, loss = 0.03050359\n",
      "Iteration 16859, loss = 0.03050159\n",
      "Iteration 16860, loss = 0.03049950\n",
      "Iteration 16861, loss = 0.03049747\n",
      "Iteration 16862, loss = 0.03049540\n",
      "Iteration 16863, loss = 0.03049337\n",
      "Iteration 16864, loss = 0.03049132\n",
      "Iteration 16865, loss = 0.03048925\n",
      "Iteration 16866, loss = 0.03048722\n",
      "Iteration 16867, loss = 0.03048517\n",
      "Iteration 16868, loss = 0.03048312\n",
      "Iteration 16869, loss = 0.03048107\n",
      "Iteration 16870, loss = 0.03047900\n",
      "Iteration 16871, loss = 0.03047697\n",
      "Iteration 16872, loss = 0.03047489\n",
      "Iteration 16873, loss = 0.03047288\n",
      "Iteration 16874, loss = 0.03047083\n",
      "Iteration 16875, loss = 0.03046880\n",
      "Iteration 16876, loss = 0.03046673\n",
      "Iteration 16877, loss = 0.03046469\n",
      "Iteration 16878, loss = 0.03046266\n",
      "Iteration 16879, loss = 0.03046061\n",
      "Iteration 16880, loss = 0.03045857\n",
      "Iteration 16881, loss = 0.03045651\n",
      "Iteration 16882, loss = 0.03045447\n",
      "Iteration 16883, loss = 0.03045242\n",
      "Iteration 16884, loss = 0.03045037\n",
      "Iteration 16885, loss = 0.03044834\n",
      "Iteration 16886, loss = 0.03044628\n",
      "Iteration 16887, loss = 0.03044423\n",
      "Iteration 16888, loss = 0.03044219\n",
      "Iteration 16889, loss = 0.03044016\n",
      "Iteration 16890, loss = 0.03043813\n",
      "Iteration 16891, loss = 0.03043607\n",
      "Iteration 16892, loss = 0.03043403\n",
      "Iteration 16893, loss = 0.03043199\n",
      "Iteration 16894, loss = 0.03042993\n",
      "Iteration 16895, loss = 0.03042788\n",
      "Iteration 16896, loss = 0.03042586\n",
      "Iteration 16897, loss = 0.03042381\n",
      "Iteration 16898, loss = 0.03042175\n",
      "Iteration 16899, loss = 0.03041972\n",
      "Iteration 16900, loss = 0.03041771\n",
      "Iteration 16901, loss = 0.03041565\n",
      "Iteration 16902, loss = 0.03041363\n",
      "Iteration 16903, loss = 0.03041157\n",
      "Iteration 16904, loss = 0.03040954\n",
      "Iteration 16905, loss = 0.03040749\n",
      "Iteration 16906, loss = 0.03040547\n",
      "Iteration 16907, loss = 0.03040339\n",
      "Iteration 16908, loss = 0.03040137\n",
      "Iteration 16909, loss = 0.03039933\n",
      "Iteration 16910, loss = 0.03039729\n",
      "Iteration 16911, loss = 0.03039527\n",
      "Iteration 16912, loss = 0.03039322\n",
      "Iteration 16913, loss = 0.03039118\n",
      "Iteration 16914, loss = 0.03038914\n",
      "Iteration 16915, loss = 0.03038714\n",
      "Iteration 16916, loss = 0.03038508\n",
      "Iteration 16917, loss = 0.03038304\n",
      "Iteration 16918, loss = 0.03038099\n",
      "Iteration 16919, loss = 0.03037895\n",
      "Iteration 16920, loss = 0.03037699\n",
      "Iteration 16921, loss = 0.03037490\n",
      "Iteration 16922, loss = 0.03037285\n",
      "Iteration 16923, loss = 0.03037086\n",
      "Iteration 16924, loss = 0.03036882\n",
      "Iteration 16925, loss = 0.03036678\n",
      "Iteration 16926, loss = 0.03036476\n",
      "Iteration 16927, loss = 0.03036274\n",
      "Iteration 16928, loss = 0.03036069\n",
      "Iteration 16929, loss = 0.03035865\n",
      "Iteration 16930, loss = 0.03035662\n",
      "Iteration 16931, loss = 0.03035461\n",
      "Iteration 16932, loss = 0.03035255\n",
      "Iteration 16933, loss = 0.03035052\n",
      "Iteration 16934, loss = 0.03034849\n",
      "Iteration 16935, loss = 0.03034646\n",
      "Iteration 16936, loss = 0.03034441\n",
      "Iteration 16937, loss = 0.03034238\n",
      "Iteration 16938, loss = 0.03034036\n",
      "Iteration 16939, loss = 0.03033831\n",
      "Iteration 16940, loss = 0.03033627\n",
      "Iteration 16941, loss = 0.03033426\n",
      "Iteration 16942, loss = 0.03033222\n",
      "Iteration 16943, loss = 0.03033018\n",
      "Iteration 16944, loss = 0.03032817\n",
      "Iteration 16945, loss = 0.03032612\n",
      "Iteration 16946, loss = 0.03032410\n",
      "Iteration 16947, loss = 0.03032207\n",
      "Iteration 16948, loss = 0.03032006\n",
      "Iteration 16949, loss = 0.03031801\n",
      "Iteration 16950, loss = 0.03031600\n",
      "Iteration 16951, loss = 0.03031395\n",
      "Iteration 16952, loss = 0.03031194\n",
      "Iteration 16953, loss = 0.03030993\n",
      "Iteration 16954, loss = 0.03030787\n",
      "Iteration 16955, loss = 0.03030586\n",
      "Iteration 16956, loss = 0.03030384\n",
      "Iteration 16957, loss = 0.03030179\n",
      "Iteration 16958, loss = 0.03029975\n",
      "Iteration 16959, loss = 0.03029774\n",
      "Iteration 16960, loss = 0.03029570\n",
      "Iteration 16961, loss = 0.03029369\n",
      "Iteration 16962, loss = 0.03029168\n",
      "Iteration 16963, loss = 0.03028961\n",
      "Iteration 16964, loss = 0.03028761\n",
      "Iteration 16965, loss = 0.03028559\n",
      "Iteration 16966, loss = 0.03028356\n",
      "Iteration 16967, loss = 0.03028152\n",
      "Iteration 16968, loss = 0.03027951\n",
      "Iteration 16969, loss = 0.03027752\n",
      "Iteration 16970, loss = 0.03027548\n",
      "Iteration 16971, loss = 0.03027346\n",
      "Iteration 16972, loss = 0.03027144\n",
      "Iteration 16973, loss = 0.03026943\n",
      "Iteration 16974, loss = 0.03026742\n",
      "Iteration 16975, loss = 0.03026541\n",
      "Iteration 16976, loss = 0.03026338\n",
      "Iteration 16977, loss = 0.03026135\n",
      "Iteration 16978, loss = 0.03025935\n",
      "Iteration 16979, loss = 0.03025735\n",
      "Iteration 16980, loss = 0.03025536\n",
      "Iteration 16981, loss = 0.03025329\n",
      "Iteration 16982, loss = 0.03025130\n",
      "Iteration 16983, loss = 0.03024927\n",
      "Iteration 16984, loss = 0.03024727\n",
      "Iteration 16985, loss = 0.03024524\n",
      "Iteration 16986, loss = 0.03024321\n",
      "Iteration 16987, loss = 0.03024122\n",
      "Iteration 16988, loss = 0.03023920\n",
      "Iteration 16989, loss = 0.03023720\n",
      "Iteration 16990, loss = 0.03023517\n",
      "Iteration 16991, loss = 0.03023315\n",
      "Iteration 16992, loss = 0.03023118\n",
      "Iteration 16993, loss = 0.03022912\n",
      "Iteration 16994, loss = 0.03022711\n",
      "Iteration 16995, loss = 0.03022508\n",
      "Iteration 16996, loss = 0.03022307\n",
      "Iteration 16997, loss = 0.03022109\n",
      "Iteration 16998, loss = 0.03021906\n",
      "Iteration 16999, loss = 0.03021703\n",
      "Iteration 17000, loss = 0.03021504\n",
      "Iteration 17001, loss = 0.03021302\n",
      "Iteration 17002, loss = 0.03021101\n",
      "Iteration 17003, loss = 0.03020899\n",
      "Iteration 17004, loss = 0.03020699\n",
      "Iteration 17005, loss = 0.03020495\n",
      "Iteration 17006, loss = 0.03020295\n",
      "Iteration 17007, loss = 0.03020092\n",
      "Iteration 17008, loss = 0.03019891\n",
      "Iteration 17009, loss = 0.03019691\n",
      "Iteration 17010, loss = 0.03019490\n",
      "Iteration 17011, loss = 0.03019291\n",
      "Iteration 17012, loss = 0.03019088\n",
      "Iteration 17013, loss = 0.03018888\n",
      "Iteration 17014, loss = 0.03018688\n",
      "Iteration 17015, loss = 0.03018486\n",
      "Iteration 17016, loss = 0.03018286\n",
      "Iteration 17017, loss = 0.03018085\n",
      "Iteration 17018, loss = 0.03017887\n",
      "Iteration 17019, loss = 0.03017683\n",
      "Iteration 17020, loss = 0.03017483\n",
      "Iteration 17021, loss = 0.03017282\n",
      "Iteration 17022, loss = 0.03017084\n",
      "Iteration 17023, loss = 0.03016881\n",
      "Iteration 17024, loss = 0.03016683\n",
      "Iteration 17025, loss = 0.03016481\n",
      "Iteration 17026, loss = 0.03016281\n",
      "Iteration 17027, loss = 0.03016081\n",
      "Iteration 17028, loss = 0.03015884\n",
      "Iteration 17029, loss = 0.03015680\n",
      "Iteration 17030, loss = 0.03015479\n",
      "Iteration 17031, loss = 0.03015279\n",
      "Iteration 17032, loss = 0.03015077\n",
      "Iteration 17033, loss = 0.03014877\n",
      "Iteration 17034, loss = 0.03014675\n",
      "Iteration 17035, loss = 0.03014474\n",
      "Iteration 17036, loss = 0.03014276\n",
      "Iteration 17037, loss = 0.03014072\n",
      "Iteration 17038, loss = 0.03013871\n",
      "Iteration 17039, loss = 0.03013675\n",
      "Iteration 17040, loss = 0.03013470\n",
      "Iteration 17041, loss = 0.03013273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17042, loss = 0.03013071\n",
      "Iteration 17043, loss = 0.03012869\n",
      "Iteration 17044, loss = 0.03012670\n",
      "Iteration 17045, loss = 0.03012469\n",
      "Iteration 17046, loss = 0.03012268\n",
      "Iteration 17047, loss = 0.03012069\n",
      "Iteration 17048, loss = 0.03011869\n",
      "Iteration 17049, loss = 0.03011670\n",
      "Iteration 17050, loss = 0.03011467\n",
      "Iteration 17051, loss = 0.03011270\n",
      "Iteration 17052, loss = 0.03011068\n",
      "Iteration 17053, loss = 0.03010867\n",
      "Iteration 17054, loss = 0.03010670\n",
      "Iteration 17055, loss = 0.03010470\n",
      "Iteration 17056, loss = 0.03010272\n",
      "Iteration 17057, loss = 0.03010073\n",
      "Iteration 17058, loss = 0.03009873\n",
      "Iteration 17059, loss = 0.03009675\n",
      "Iteration 17060, loss = 0.03009473\n",
      "Iteration 17061, loss = 0.03009277\n",
      "Iteration 17062, loss = 0.03009077\n",
      "Iteration 17063, loss = 0.03008875\n",
      "Iteration 17064, loss = 0.03008677\n",
      "Iteration 17065, loss = 0.03008479\n",
      "Iteration 17066, loss = 0.03008280\n",
      "Iteration 17067, loss = 0.03008081\n",
      "Iteration 17068, loss = 0.03007882\n",
      "Iteration 17069, loss = 0.03007682\n",
      "Iteration 17070, loss = 0.03007483\n",
      "Iteration 17071, loss = 0.03007284\n",
      "Iteration 17072, loss = 0.03007083\n",
      "Iteration 17073, loss = 0.03006888\n",
      "Iteration 17074, loss = 0.03006683\n",
      "Iteration 17075, loss = 0.03006486\n",
      "Iteration 17076, loss = 0.03006290\n",
      "Iteration 17077, loss = 0.03006087\n",
      "Iteration 17078, loss = 0.03005887\n",
      "Iteration 17079, loss = 0.03005690\n",
      "Iteration 17080, loss = 0.03005490\n",
      "Iteration 17081, loss = 0.03005289\n",
      "Iteration 17082, loss = 0.03005091\n",
      "Iteration 17083, loss = 0.03004890\n",
      "Iteration 17084, loss = 0.03004691\n",
      "Iteration 17085, loss = 0.03004493\n",
      "Iteration 17086, loss = 0.03004292\n",
      "Iteration 17087, loss = 0.03004093\n",
      "Iteration 17088, loss = 0.03003898\n",
      "Iteration 17089, loss = 0.03003695\n",
      "Iteration 17090, loss = 0.03003495\n",
      "Iteration 17091, loss = 0.03003298\n",
      "Iteration 17092, loss = 0.03003097\n",
      "Iteration 17093, loss = 0.03002897\n",
      "Iteration 17094, loss = 0.03002701\n",
      "Iteration 17095, loss = 0.03002504\n",
      "Iteration 17096, loss = 0.03002301\n",
      "Iteration 17097, loss = 0.03002104\n",
      "Iteration 17098, loss = 0.03001904\n",
      "Iteration 17099, loss = 0.03001704\n",
      "Iteration 17100, loss = 0.03001505\n",
      "Iteration 17101, loss = 0.03001306\n",
      "Iteration 17102, loss = 0.03001104\n",
      "Iteration 17103, loss = 0.03000907\n",
      "Iteration 17104, loss = 0.03000707\n",
      "Iteration 17105, loss = 0.03000507\n",
      "Iteration 17106, loss = 0.03000308\n",
      "Iteration 17107, loss = 0.03000109\n",
      "Iteration 17108, loss = 0.02999911\n",
      "Iteration 17109, loss = 0.02999710\n",
      "Iteration 17110, loss = 0.02999512\n",
      "Iteration 17111, loss = 0.02999313\n",
      "Iteration 17112, loss = 0.02999118\n",
      "Iteration 17113, loss = 0.02998918\n",
      "Iteration 17114, loss = 0.02998722\n",
      "Iteration 17115, loss = 0.02998521\n",
      "Iteration 17116, loss = 0.02998321\n",
      "Iteration 17117, loss = 0.02998123\n",
      "Iteration 17118, loss = 0.02997925\n",
      "Iteration 17119, loss = 0.02997730\n",
      "Iteration 17120, loss = 0.02997529\n",
      "Iteration 17121, loss = 0.02997334\n",
      "Iteration 17122, loss = 0.02997134\n",
      "Iteration 17123, loss = 0.02996936\n",
      "Iteration 17124, loss = 0.02996737\n",
      "Iteration 17125, loss = 0.02996539\n",
      "Iteration 17126, loss = 0.02996345\n",
      "Iteration 17127, loss = 0.02996143\n",
      "Iteration 17128, loss = 0.02995946\n",
      "Iteration 17129, loss = 0.02995750\n",
      "Iteration 17130, loss = 0.02995551\n",
      "Iteration 17131, loss = 0.02995352\n",
      "Iteration 17132, loss = 0.02995154\n",
      "Iteration 17133, loss = 0.02994959\n",
      "Iteration 17134, loss = 0.02994759\n",
      "Iteration 17135, loss = 0.02994564\n",
      "Iteration 17136, loss = 0.02994363\n",
      "Iteration 17137, loss = 0.02994167\n",
      "Iteration 17138, loss = 0.02993972\n",
      "Iteration 17139, loss = 0.02993769\n",
      "Iteration 17140, loss = 0.02993573\n",
      "Iteration 17141, loss = 0.02993376\n",
      "Iteration 17142, loss = 0.02993178\n",
      "Iteration 17143, loss = 0.02992981\n",
      "Iteration 17144, loss = 0.02992784\n",
      "Iteration 17145, loss = 0.02992587\n",
      "Iteration 17146, loss = 0.02992389\n",
      "Iteration 17147, loss = 0.02992193\n",
      "Iteration 17148, loss = 0.02991992\n",
      "Iteration 17149, loss = 0.02991798\n",
      "Iteration 17150, loss = 0.02991600\n",
      "Iteration 17151, loss = 0.02991403\n",
      "Iteration 17152, loss = 0.02991205\n",
      "Iteration 17153, loss = 0.02991009\n",
      "Iteration 17154, loss = 0.02990810\n",
      "Iteration 17155, loss = 0.02990612\n",
      "Iteration 17156, loss = 0.02990418\n",
      "Iteration 17157, loss = 0.02990220\n",
      "Iteration 17158, loss = 0.02990025\n",
      "Iteration 17159, loss = 0.02989825\n",
      "Iteration 17160, loss = 0.02989630\n",
      "Iteration 17161, loss = 0.02989432\n",
      "Iteration 17162, loss = 0.02989238\n",
      "Iteration 17163, loss = 0.02989039\n",
      "Iteration 17164, loss = 0.02988842\n",
      "Iteration 17165, loss = 0.02988644\n",
      "Iteration 17166, loss = 0.02988448\n",
      "Iteration 17167, loss = 0.02988253\n",
      "Iteration 17168, loss = 0.02988057\n",
      "Iteration 17169, loss = 0.02987859\n",
      "Iteration 17170, loss = 0.02987665\n",
      "Iteration 17171, loss = 0.02987464\n",
      "Iteration 17172, loss = 0.02987268\n",
      "Iteration 17173, loss = 0.02987072\n",
      "Iteration 17174, loss = 0.02986875\n",
      "Iteration 17175, loss = 0.02986678\n",
      "Iteration 17176, loss = 0.02986481\n",
      "Iteration 17177, loss = 0.02986284\n",
      "Iteration 17178, loss = 0.02986086\n",
      "Iteration 17179, loss = 0.02985894\n",
      "Iteration 17180, loss = 0.02985696\n",
      "Iteration 17181, loss = 0.02985499\n",
      "Iteration 17182, loss = 0.02985301\n",
      "Iteration 17183, loss = 0.02985107\n",
      "Iteration 17184, loss = 0.02984910\n",
      "Iteration 17185, loss = 0.02984713\n",
      "Iteration 17186, loss = 0.02984517\n",
      "Iteration 17187, loss = 0.02984323\n",
      "Iteration 17188, loss = 0.02984126\n",
      "Iteration 17189, loss = 0.02983929\n",
      "Iteration 17190, loss = 0.02983733\n",
      "Iteration 17191, loss = 0.02983536\n",
      "Iteration 17192, loss = 0.02983340\n",
      "Iteration 17193, loss = 0.02983143\n",
      "Iteration 17194, loss = 0.02982948\n",
      "Iteration 17195, loss = 0.02982752\n",
      "Iteration 17196, loss = 0.02982553\n",
      "Iteration 17197, loss = 0.02982360\n",
      "Iteration 17198, loss = 0.02982164\n",
      "Iteration 17199, loss = 0.02981966\n",
      "Iteration 17200, loss = 0.02981771\n",
      "Iteration 17201, loss = 0.02981576\n",
      "Iteration 17202, loss = 0.02981379\n",
      "Iteration 17203, loss = 0.02981184\n",
      "Iteration 17204, loss = 0.02980988\n",
      "Iteration 17205, loss = 0.02980794\n",
      "Iteration 17206, loss = 0.02980597\n",
      "Iteration 17207, loss = 0.02980401\n",
      "Iteration 17208, loss = 0.02980206\n",
      "Iteration 17209, loss = 0.02980010\n",
      "Iteration 17210, loss = 0.02979815\n",
      "Iteration 17211, loss = 0.02979617\n",
      "Iteration 17212, loss = 0.02979423\n",
      "Iteration 17213, loss = 0.02979231\n",
      "Iteration 17214, loss = 0.02979032\n",
      "Iteration 17215, loss = 0.02978836\n",
      "Iteration 17216, loss = 0.02978641\n",
      "Iteration 17217, loss = 0.02978446\n",
      "Iteration 17218, loss = 0.02978251\n",
      "Iteration 17219, loss = 0.02978056\n",
      "Iteration 17220, loss = 0.02977862\n",
      "Iteration 17221, loss = 0.02977666\n",
      "Iteration 17222, loss = 0.02977471\n",
      "Iteration 17223, loss = 0.02977273\n",
      "Iteration 17224, loss = 0.02977079\n",
      "Iteration 17225, loss = 0.02976884\n",
      "Iteration 17226, loss = 0.02976687\n",
      "Iteration 17227, loss = 0.02976492\n",
      "Iteration 17228, loss = 0.02976297\n",
      "Iteration 17229, loss = 0.02976101\n",
      "Iteration 17230, loss = 0.02975906\n",
      "Iteration 17231, loss = 0.02975708\n",
      "Iteration 17232, loss = 0.02975513\n",
      "Iteration 17233, loss = 0.02975317\n",
      "Iteration 17234, loss = 0.02975121\n",
      "Iteration 17235, loss = 0.02974925\n",
      "Iteration 17236, loss = 0.02974732\n",
      "Iteration 17237, loss = 0.02974538\n",
      "Iteration 17238, loss = 0.02974342\n",
      "Iteration 17239, loss = 0.02974144\n",
      "Iteration 17240, loss = 0.02973948\n",
      "Iteration 17241, loss = 0.02973754\n",
      "Iteration 17242, loss = 0.02973560\n",
      "Iteration 17243, loss = 0.02973364\n",
      "Iteration 17244, loss = 0.02973169\n",
      "Iteration 17245, loss = 0.02972974\n",
      "Iteration 17246, loss = 0.02972781\n",
      "Iteration 17247, loss = 0.02972584\n",
      "Iteration 17248, loss = 0.02972391\n",
      "Iteration 17249, loss = 0.02972195\n",
      "Iteration 17250, loss = 0.02972003\n",
      "Iteration 17251, loss = 0.02971806\n",
      "Iteration 17252, loss = 0.02971613\n",
      "Iteration 17253, loss = 0.02971419\n",
      "Iteration 17254, loss = 0.02971225\n",
      "Iteration 17255, loss = 0.02971032\n",
      "Iteration 17256, loss = 0.02970834\n",
      "Iteration 17257, loss = 0.02970640\n",
      "Iteration 17258, loss = 0.02970446\n",
      "Iteration 17259, loss = 0.02970253\n",
      "Iteration 17260, loss = 0.02970057\n",
      "Iteration 17261, loss = 0.02969863\n",
      "Iteration 17262, loss = 0.02969667\n",
      "Iteration 17263, loss = 0.02969474\n",
      "Iteration 17264, loss = 0.02969278\n",
      "Iteration 17265, loss = 0.02969085\n",
      "Iteration 17266, loss = 0.02968890\n",
      "Iteration 17267, loss = 0.02968693\n",
      "Iteration 17268, loss = 0.02968503\n",
      "Iteration 17269, loss = 0.02968307\n",
      "Iteration 17270, loss = 0.02968111\n",
      "Iteration 17271, loss = 0.02967920\n",
      "Iteration 17272, loss = 0.02967722\n",
      "Iteration 17273, loss = 0.02967529\n",
      "Iteration 17274, loss = 0.02967337\n",
      "Iteration 17275, loss = 0.02967143\n",
      "Iteration 17276, loss = 0.02966948\n",
      "Iteration 17277, loss = 0.02966753\n",
      "Iteration 17278, loss = 0.02966560\n",
      "Iteration 17279, loss = 0.02966366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17280, loss = 0.02966174\n",
      "Iteration 17281, loss = 0.02965980\n",
      "Iteration 17282, loss = 0.02965787\n",
      "Iteration 17283, loss = 0.02965592\n",
      "Iteration 17284, loss = 0.02965399\n",
      "Iteration 17285, loss = 0.02965207\n",
      "Iteration 17286, loss = 0.02965014\n",
      "Iteration 17287, loss = 0.02964819\n",
      "Iteration 17288, loss = 0.02964623\n",
      "Iteration 17289, loss = 0.02964433\n",
      "Iteration 17290, loss = 0.02964239\n",
      "Iteration 17291, loss = 0.02964047\n",
      "Iteration 17292, loss = 0.02963854\n",
      "Iteration 17293, loss = 0.02963658\n",
      "Iteration 17294, loss = 0.02963467\n",
      "Iteration 17295, loss = 0.02963269\n",
      "Iteration 17296, loss = 0.02963078\n",
      "Iteration 17297, loss = 0.02962887\n",
      "Iteration 17298, loss = 0.02962692\n",
      "Iteration 17299, loss = 0.02962498\n",
      "Iteration 17300, loss = 0.02962304\n",
      "Iteration 17301, loss = 0.02962110\n",
      "Iteration 17302, loss = 0.02961917\n",
      "Iteration 17303, loss = 0.02961725\n",
      "Iteration 17304, loss = 0.02961532\n",
      "Iteration 17305, loss = 0.02961337\n",
      "Iteration 17306, loss = 0.02961144\n",
      "Iteration 17307, loss = 0.02960953\n",
      "Iteration 17308, loss = 0.02960758\n",
      "Iteration 17309, loss = 0.02960564\n",
      "Iteration 17310, loss = 0.02960373\n",
      "Iteration 17311, loss = 0.02960180\n",
      "Iteration 17312, loss = 0.02959985\n",
      "Iteration 17313, loss = 0.02959794\n",
      "Iteration 17314, loss = 0.02959599\n",
      "Iteration 17315, loss = 0.02959405\n",
      "Iteration 17316, loss = 0.02959216\n",
      "Iteration 17317, loss = 0.02959021\n",
      "Iteration 17318, loss = 0.02958829\n",
      "Iteration 17319, loss = 0.02958636\n",
      "Iteration 17320, loss = 0.02958444\n",
      "Iteration 17321, loss = 0.02958251\n",
      "Iteration 17322, loss = 0.02958060\n",
      "Iteration 17323, loss = 0.02957868\n",
      "Iteration 17324, loss = 0.02957674\n",
      "Iteration 17325, loss = 0.02957486\n",
      "Iteration 17326, loss = 0.02957290\n",
      "Iteration 17327, loss = 0.02957098\n",
      "Iteration 17328, loss = 0.02956905\n",
      "Iteration 17329, loss = 0.02956711\n",
      "Iteration 17330, loss = 0.02956520\n",
      "Iteration 17331, loss = 0.02956325\n",
      "Iteration 17332, loss = 0.02956133\n",
      "Iteration 17333, loss = 0.02955940\n",
      "Iteration 17334, loss = 0.02955749\n",
      "Iteration 17335, loss = 0.02955554\n",
      "Iteration 17336, loss = 0.02955363\n",
      "Iteration 17337, loss = 0.02955169\n",
      "Iteration 17338, loss = 0.02954976\n",
      "Iteration 17339, loss = 0.02954787\n",
      "Iteration 17340, loss = 0.02954591\n",
      "Iteration 17341, loss = 0.02954399\n",
      "Iteration 17342, loss = 0.02954207\n",
      "Iteration 17343, loss = 0.02954014\n",
      "Iteration 17344, loss = 0.02953821\n",
      "Iteration 17345, loss = 0.02953632\n",
      "Iteration 17346, loss = 0.02953440\n",
      "Iteration 17347, loss = 0.02953246\n",
      "Iteration 17348, loss = 0.02953055\n",
      "Iteration 17349, loss = 0.02952861\n",
      "Iteration 17350, loss = 0.02952669\n",
      "Iteration 17351, loss = 0.02952479\n",
      "Iteration 17352, loss = 0.02952286\n",
      "Iteration 17353, loss = 0.02952092\n",
      "Iteration 17354, loss = 0.02951904\n",
      "Iteration 17355, loss = 0.02951708\n",
      "Iteration 17356, loss = 0.02951517\n",
      "Iteration 17357, loss = 0.02951326\n",
      "Iteration 17358, loss = 0.02951133\n",
      "Iteration 17359, loss = 0.02950943\n",
      "Iteration 17360, loss = 0.02950751\n",
      "Iteration 17361, loss = 0.02950558\n",
      "Iteration 17362, loss = 0.02950366\n",
      "Iteration 17363, loss = 0.02950173\n",
      "Iteration 17364, loss = 0.02949982\n",
      "Iteration 17365, loss = 0.02949787\n",
      "Iteration 17366, loss = 0.02949597\n",
      "Iteration 17367, loss = 0.02949404\n",
      "Iteration 17368, loss = 0.02949212\n",
      "Iteration 17369, loss = 0.02949019\n",
      "Iteration 17370, loss = 0.02948827\n",
      "Iteration 17371, loss = 0.02948635\n",
      "Iteration 17372, loss = 0.02948443\n",
      "Iteration 17373, loss = 0.02948251\n",
      "Iteration 17374, loss = 0.02948060\n",
      "Iteration 17375, loss = 0.02947868\n",
      "Iteration 17376, loss = 0.02947674\n",
      "Iteration 17377, loss = 0.02947483\n",
      "Iteration 17378, loss = 0.02947292\n",
      "Iteration 17379, loss = 0.02947098\n",
      "Iteration 17380, loss = 0.02946905\n",
      "Iteration 17381, loss = 0.02946713\n",
      "Iteration 17382, loss = 0.02946523\n",
      "Iteration 17383, loss = 0.02946333\n",
      "Iteration 17384, loss = 0.02946140\n",
      "Iteration 17385, loss = 0.02945948\n",
      "Iteration 17386, loss = 0.02945757\n",
      "Iteration 17387, loss = 0.02945565\n",
      "Iteration 17388, loss = 0.02945375\n",
      "Iteration 17389, loss = 0.02945183\n",
      "Iteration 17390, loss = 0.02944992\n",
      "Iteration 17391, loss = 0.02944798\n",
      "Iteration 17392, loss = 0.02944606\n",
      "Iteration 17393, loss = 0.02944414\n",
      "Iteration 17394, loss = 0.02944224\n",
      "Iteration 17395, loss = 0.02944033\n",
      "Iteration 17396, loss = 0.02943841\n",
      "Iteration 17397, loss = 0.02943647\n",
      "Iteration 17398, loss = 0.02943457\n",
      "Iteration 17399, loss = 0.02943266\n",
      "Iteration 17400, loss = 0.02943075\n",
      "Iteration 17401, loss = 0.02942883\n",
      "Iteration 17402, loss = 0.02942693\n",
      "Iteration 17403, loss = 0.02942501\n",
      "Iteration 17404, loss = 0.02942309\n",
      "Iteration 17405, loss = 0.02942118\n",
      "Iteration 17406, loss = 0.02941928\n",
      "Iteration 17407, loss = 0.02941738\n",
      "Iteration 17408, loss = 0.02941545\n",
      "Iteration 17409, loss = 0.02941356\n",
      "Iteration 17410, loss = 0.02941162\n",
      "Iteration 17411, loss = 0.02940975\n",
      "Iteration 17412, loss = 0.02940782\n",
      "Iteration 17413, loss = 0.02940593\n",
      "Iteration 17414, loss = 0.02940401\n",
      "Iteration 17415, loss = 0.02940212\n",
      "Iteration 17416, loss = 0.02940021\n",
      "Iteration 17417, loss = 0.02939829\n",
      "Iteration 17418, loss = 0.02939640\n",
      "Iteration 17419, loss = 0.02939450\n",
      "Iteration 17420, loss = 0.02939258\n",
      "Iteration 17421, loss = 0.02939071\n",
      "Iteration 17422, loss = 0.02938876\n",
      "Iteration 17423, loss = 0.02938688\n",
      "Iteration 17424, loss = 0.02938496\n",
      "Iteration 17425, loss = 0.02938307\n",
      "Iteration 17426, loss = 0.02938118\n",
      "Iteration 17427, loss = 0.02937926\n",
      "Iteration 17428, loss = 0.02937735\n",
      "Iteration 17429, loss = 0.02937544\n",
      "Iteration 17430, loss = 0.02937355\n",
      "Iteration 17431, loss = 0.02937165\n",
      "Iteration 17432, loss = 0.02936975\n",
      "Iteration 17433, loss = 0.02936786\n",
      "Iteration 17434, loss = 0.02936596\n",
      "Iteration 17435, loss = 0.02936406\n",
      "Iteration 17436, loss = 0.02936216\n",
      "Iteration 17437, loss = 0.02936025\n",
      "Iteration 17438, loss = 0.02935836\n",
      "Iteration 17439, loss = 0.02935645\n",
      "Iteration 17440, loss = 0.02935454\n",
      "Iteration 17441, loss = 0.02935265\n",
      "Iteration 17442, loss = 0.02935075\n",
      "Iteration 17443, loss = 0.02934888\n",
      "Iteration 17444, loss = 0.02934697\n",
      "Iteration 17445, loss = 0.02934506\n",
      "Iteration 17446, loss = 0.02934314\n",
      "Iteration 17447, loss = 0.02934125\n",
      "Iteration 17448, loss = 0.02933937\n",
      "Iteration 17449, loss = 0.02933746\n",
      "Iteration 17450, loss = 0.02933556\n",
      "Iteration 17451, loss = 0.02933366\n",
      "Iteration 17452, loss = 0.02933178\n",
      "Iteration 17453, loss = 0.02932986\n",
      "Iteration 17454, loss = 0.02932795\n",
      "Iteration 17455, loss = 0.02932606\n",
      "Iteration 17456, loss = 0.02932413\n",
      "Iteration 17457, loss = 0.02932226\n",
      "Iteration 17458, loss = 0.02932034\n",
      "Iteration 17459, loss = 0.02931844\n",
      "Iteration 17460, loss = 0.02931653\n",
      "Iteration 17461, loss = 0.02931464\n",
      "Iteration 17462, loss = 0.02931272\n",
      "Iteration 17463, loss = 0.02931085\n",
      "Iteration 17464, loss = 0.02930895\n",
      "Iteration 17465, loss = 0.02930704\n",
      "Iteration 17466, loss = 0.02930516\n",
      "Iteration 17467, loss = 0.02930327\n",
      "Iteration 17468, loss = 0.02930136\n",
      "Iteration 17469, loss = 0.02929944\n",
      "Iteration 17470, loss = 0.02929757\n",
      "Iteration 17471, loss = 0.02929568\n",
      "Iteration 17472, loss = 0.02929377\n",
      "Iteration 17473, loss = 0.02929190\n",
      "Iteration 17474, loss = 0.02928999\n",
      "Iteration 17475, loss = 0.02928811\n",
      "Iteration 17476, loss = 0.02928621\n",
      "Iteration 17477, loss = 0.02928431\n",
      "Iteration 17478, loss = 0.02928243\n",
      "Iteration 17479, loss = 0.02928054\n",
      "Iteration 17480, loss = 0.02927863\n",
      "Iteration 17481, loss = 0.02927676\n",
      "Iteration 17482, loss = 0.02927483\n",
      "Iteration 17483, loss = 0.02927291\n",
      "Iteration 17484, loss = 0.02927103\n",
      "Iteration 17485, loss = 0.02926914\n",
      "Iteration 17486, loss = 0.02926724\n",
      "Iteration 17487, loss = 0.02926535\n",
      "Iteration 17488, loss = 0.02926343\n",
      "Iteration 17489, loss = 0.02926154\n",
      "Iteration 17490, loss = 0.02925965\n",
      "Iteration 17491, loss = 0.02925778\n",
      "Iteration 17492, loss = 0.02925588\n",
      "Iteration 17493, loss = 0.02925398\n",
      "Iteration 17494, loss = 0.02925209\n",
      "Iteration 17495, loss = 0.02925019\n",
      "Iteration 17496, loss = 0.02924830\n",
      "Iteration 17497, loss = 0.02924644\n",
      "Iteration 17498, loss = 0.02924452\n",
      "Iteration 17499, loss = 0.02924263\n",
      "Iteration 17500, loss = 0.02924076\n",
      "Iteration 17501, loss = 0.02923883\n",
      "Iteration 17502, loss = 0.02923698\n",
      "Iteration 17503, loss = 0.02923508\n",
      "Iteration 17504, loss = 0.02923321\n",
      "Iteration 17505, loss = 0.02923132\n",
      "Iteration 17506, loss = 0.02922944\n",
      "Iteration 17507, loss = 0.02922756\n",
      "Iteration 17508, loss = 0.02922566\n",
      "Iteration 17509, loss = 0.02922380\n",
      "Iteration 17510, loss = 0.02922190\n",
      "Iteration 17511, loss = 0.02922004\n",
      "Iteration 17512, loss = 0.02921812\n",
      "Iteration 17513, loss = 0.02921624\n",
      "Iteration 17514, loss = 0.02921434\n",
      "Iteration 17515, loss = 0.02921246\n",
      "Iteration 17516, loss = 0.02921058\n",
      "Iteration 17517, loss = 0.02920869\n",
      "Iteration 17518, loss = 0.02920682\n",
      "Iteration 17519, loss = 0.02920493\n",
      "Iteration 17520, loss = 0.02920305\n",
      "Iteration 17521, loss = 0.02920118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17522, loss = 0.02919928\n",
      "Iteration 17523, loss = 0.02919740\n",
      "Iteration 17524, loss = 0.02919551\n",
      "Iteration 17525, loss = 0.02919363\n",
      "Iteration 17526, loss = 0.02919177\n",
      "Iteration 17527, loss = 0.02918988\n",
      "Iteration 17528, loss = 0.02918800\n",
      "Iteration 17529, loss = 0.02918615\n",
      "Iteration 17530, loss = 0.02918428\n",
      "Iteration 17531, loss = 0.02918238\n",
      "Iteration 17532, loss = 0.02918053\n",
      "Iteration 17533, loss = 0.02917864\n",
      "Iteration 17534, loss = 0.02917677\n",
      "Iteration 17535, loss = 0.02917489\n",
      "Iteration 17536, loss = 0.02917302\n",
      "Iteration 17537, loss = 0.02917114\n",
      "Iteration 17538, loss = 0.02916925\n",
      "Iteration 17539, loss = 0.02916739\n",
      "Iteration 17540, loss = 0.02916550\n",
      "Iteration 17541, loss = 0.02916366\n",
      "Iteration 17542, loss = 0.02916176\n",
      "Iteration 17543, loss = 0.02915991\n",
      "Iteration 17544, loss = 0.02915802\n",
      "Iteration 17545, loss = 0.02915616\n",
      "Iteration 17546, loss = 0.02915428\n",
      "Iteration 17547, loss = 0.02915244\n",
      "Iteration 17548, loss = 0.02915057\n",
      "Iteration 17549, loss = 0.02914868\n",
      "Iteration 17550, loss = 0.02914683\n",
      "Iteration 17551, loss = 0.02914496\n",
      "Iteration 17552, loss = 0.02914307\n",
      "Iteration 17553, loss = 0.02914121\n",
      "Iteration 17554, loss = 0.02913933\n",
      "Iteration 17555, loss = 0.02913747\n",
      "Iteration 17556, loss = 0.02913559\n",
      "Iteration 17557, loss = 0.02913373\n",
      "Iteration 17558, loss = 0.02913187\n",
      "Iteration 17559, loss = 0.02913000\n",
      "Iteration 17560, loss = 0.02912813\n",
      "Iteration 17561, loss = 0.02912624\n",
      "Iteration 17562, loss = 0.02912436\n",
      "Iteration 17563, loss = 0.02912252\n",
      "Iteration 17564, loss = 0.02912066\n",
      "Iteration 17565, loss = 0.02911878\n",
      "Iteration 17566, loss = 0.02911689\n",
      "Iteration 17567, loss = 0.02911503\n",
      "Iteration 17568, loss = 0.02911317\n",
      "Iteration 17569, loss = 0.02911128\n",
      "Iteration 17570, loss = 0.02910945\n",
      "Iteration 17571, loss = 0.02910756\n",
      "Iteration 17572, loss = 0.02910569\n",
      "Iteration 17573, loss = 0.02910380\n",
      "Iteration 17574, loss = 0.02910194\n",
      "Iteration 17575, loss = 0.02910007\n",
      "Iteration 17576, loss = 0.02909821\n",
      "Iteration 17577, loss = 0.02909632\n",
      "Iteration 17578, loss = 0.02909447\n",
      "Iteration 17579, loss = 0.02909264\n",
      "Iteration 17580, loss = 0.02909072\n",
      "Iteration 17581, loss = 0.02908886\n",
      "Iteration 17582, loss = 0.02908702\n",
      "Iteration 17583, loss = 0.02908516\n",
      "Iteration 17584, loss = 0.02908327\n",
      "Iteration 17585, loss = 0.02908141\n",
      "Iteration 17586, loss = 0.02907956\n",
      "Iteration 17587, loss = 0.02907769\n",
      "Iteration 17588, loss = 0.02907584\n",
      "Iteration 17589, loss = 0.02907396\n",
      "Iteration 17590, loss = 0.02907211\n",
      "Iteration 17591, loss = 0.02907023\n",
      "Iteration 17592, loss = 0.02906839\n",
      "Iteration 17593, loss = 0.02906651\n",
      "Iteration 17594, loss = 0.02906465\n",
      "Iteration 17595, loss = 0.02906278\n",
      "Iteration 17596, loss = 0.02906090\n",
      "Iteration 17597, loss = 0.02905905\n",
      "Iteration 17598, loss = 0.02905719\n",
      "Iteration 17599, loss = 0.02905534\n",
      "Iteration 17600, loss = 0.02905345\n",
      "Iteration 17601, loss = 0.02905160\n",
      "Iteration 17602, loss = 0.02904972\n",
      "Iteration 17603, loss = 0.02904785\n",
      "Iteration 17604, loss = 0.02904602\n",
      "Iteration 17605, loss = 0.02904415\n",
      "Iteration 17606, loss = 0.02904228\n",
      "Iteration 17607, loss = 0.02904042\n",
      "Iteration 17608, loss = 0.02903856\n",
      "Iteration 17609, loss = 0.02903668\n",
      "Iteration 17610, loss = 0.02903483\n",
      "Iteration 17611, loss = 0.02903295\n",
      "Iteration 17612, loss = 0.02903112\n",
      "Iteration 17613, loss = 0.02902927\n",
      "Iteration 17614, loss = 0.02902737\n",
      "Iteration 17615, loss = 0.02902555\n",
      "Iteration 17616, loss = 0.02902367\n",
      "Iteration 17617, loss = 0.02902184\n",
      "Iteration 17618, loss = 0.02901999\n",
      "Iteration 17619, loss = 0.02901812\n",
      "Iteration 17620, loss = 0.02901626\n",
      "Iteration 17621, loss = 0.02901443\n",
      "Iteration 17622, loss = 0.02901258\n",
      "Iteration 17623, loss = 0.02901073\n",
      "Iteration 17624, loss = 0.02900888\n",
      "Iteration 17625, loss = 0.02900702\n",
      "Iteration 17626, loss = 0.02900519\n",
      "Iteration 17627, loss = 0.02900332\n",
      "Iteration 17628, loss = 0.02900150\n",
      "Iteration 17629, loss = 0.02899962\n",
      "Iteration 17630, loss = 0.02899776\n",
      "Iteration 17631, loss = 0.02899590\n",
      "Iteration 17632, loss = 0.02899407\n",
      "Iteration 17633, loss = 0.02899218\n",
      "Iteration 17634, loss = 0.02899037\n",
      "Iteration 17635, loss = 0.02898849\n",
      "Iteration 17636, loss = 0.02898662\n",
      "Iteration 17637, loss = 0.02898479\n",
      "Iteration 17638, loss = 0.02898294\n",
      "Iteration 17639, loss = 0.02898107\n",
      "Iteration 17640, loss = 0.02897923\n",
      "Iteration 17641, loss = 0.02897736\n",
      "Iteration 17642, loss = 0.02897550\n",
      "Iteration 17643, loss = 0.02897365\n",
      "Iteration 17644, loss = 0.02897182\n",
      "Iteration 17645, loss = 0.02896996\n",
      "Iteration 17646, loss = 0.02896810\n",
      "Iteration 17647, loss = 0.02896625\n",
      "Iteration 17648, loss = 0.02896442\n",
      "Iteration 17649, loss = 0.02896255\n",
      "Iteration 17650, loss = 0.02896069\n",
      "Iteration 17651, loss = 0.02895883\n",
      "Iteration 17652, loss = 0.02895698\n",
      "Iteration 17653, loss = 0.02895514\n",
      "Iteration 17654, loss = 0.02895332\n",
      "Iteration 17655, loss = 0.02895145\n",
      "Iteration 17656, loss = 0.02894959\n",
      "Iteration 17657, loss = 0.02894775\n",
      "Iteration 17658, loss = 0.02894592\n",
      "Iteration 17659, loss = 0.02894407\n",
      "Iteration 17660, loss = 0.02894221\n",
      "Iteration 17661, loss = 0.02894038\n",
      "Iteration 17662, loss = 0.02893852\n",
      "Iteration 17663, loss = 0.02893670\n",
      "Iteration 17664, loss = 0.02893484\n",
      "Iteration 17665, loss = 0.02893297\n",
      "Iteration 17666, loss = 0.02893113\n",
      "Iteration 17667, loss = 0.02892929\n",
      "Iteration 17668, loss = 0.02892743\n",
      "Iteration 17669, loss = 0.02892556\n",
      "Iteration 17670, loss = 0.02892373\n",
      "Iteration 17671, loss = 0.02892190\n",
      "Iteration 17672, loss = 0.02892004\n",
      "Iteration 17673, loss = 0.02891820\n",
      "Iteration 17674, loss = 0.02891635\n",
      "Iteration 17675, loss = 0.02891450\n",
      "Iteration 17676, loss = 0.02891263\n",
      "Iteration 17677, loss = 0.02891084\n",
      "Iteration 17678, loss = 0.02890897\n",
      "Iteration 17679, loss = 0.02890710\n",
      "Iteration 17680, loss = 0.02890527\n",
      "Iteration 17681, loss = 0.02890340\n",
      "Iteration 17682, loss = 0.02890159\n",
      "Iteration 17683, loss = 0.02889975\n",
      "Iteration 17684, loss = 0.02889789\n",
      "Iteration 17685, loss = 0.02889605\n",
      "Iteration 17686, loss = 0.02889421\n",
      "Iteration 17687, loss = 0.02889238\n",
      "Iteration 17688, loss = 0.02889055\n",
      "Iteration 17689, loss = 0.02888872\n",
      "Iteration 17690, loss = 0.02888686\n",
      "Iteration 17691, loss = 0.02888504\n",
      "Iteration 17692, loss = 0.02888320\n",
      "Iteration 17693, loss = 0.02888136\n",
      "Iteration 17694, loss = 0.02887951\n",
      "Iteration 17695, loss = 0.02887769\n",
      "Iteration 17696, loss = 0.02887585\n",
      "Iteration 17697, loss = 0.02887402\n",
      "Iteration 17698, loss = 0.02887218\n",
      "Iteration 17699, loss = 0.02887035\n",
      "Iteration 17700, loss = 0.02886851\n",
      "Iteration 17701, loss = 0.02886667\n",
      "Iteration 17702, loss = 0.02886484\n",
      "Iteration 17703, loss = 0.02886301\n",
      "Iteration 17704, loss = 0.02886116\n",
      "Iteration 17705, loss = 0.02885934\n",
      "Iteration 17706, loss = 0.02885750\n",
      "Iteration 17707, loss = 0.02885566\n",
      "Iteration 17708, loss = 0.02885383\n",
      "Iteration 17709, loss = 0.02885197\n",
      "Iteration 17710, loss = 0.02885016\n",
      "Iteration 17711, loss = 0.02884832\n",
      "Iteration 17712, loss = 0.02884649\n",
      "Iteration 17713, loss = 0.02884465\n",
      "Iteration 17714, loss = 0.02884280\n",
      "Iteration 17715, loss = 0.02884096\n",
      "Iteration 17716, loss = 0.02883913\n",
      "Iteration 17717, loss = 0.02883730\n",
      "Iteration 17718, loss = 0.02883545\n",
      "Iteration 17719, loss = 0.02883362\n",
      "Iteration 17720, loss = 0.02883178\n",
      "Iteration 17721, loss = 0.02882995\n",
      "Iteration 17722, loss = 0.02882811\n",
      "Iteration 17723, loss = 0.02882628\n",
      "Iteration 17724, loss = 0.02882444\n",
      "Iteration 17725, loss = 0.02882264\n",
      "Iteration 17726, loss = 0.02882078\n",
      "Iteration 17727, loss = 0.02881894\n",
      "Iteration 17728, loss = 0.02881711\n",
      "Iteration 17729, loss = 0.02881529\n",
      "Iteration 17730, loss = 0.02881344\n",
      "Iteration 17731, loss = 0.02881161\n",
      "Iteration 17732, loss = 0.02880980\n",
      "Iteration 17733, loss = 0.02880797\n",
      "Iteration 17734, loss = 0.02880611\n",
      "Iteration 17735, loss = 0.02880427\n",
      "Iteration 17736, loss = 0.02880244\n",
      "Iteration 17737, loss = 0.02880062\n",
      "Iteration 17738, loss = 0.02879878\n",
      "Iteration 17739, loss = 0.02879693\n",
      "Iteration 17740, loss = 0.02879511\n",
      "Iteration 17741, loss = 0.02879327\n",
      "Iteration 17742, loss = 0.02879142\n",
      "Iteration 17743, loss = 0.02878958\n",
      "Iteration 17744, loss = 0.02878777\n",
      "Iteration 17745, loss = 0.02878594\n",
      "Iteration 17746, loss = 0.02878411\n",
      "Iteration 17747, loss = 0.02878228\n",
      "Iteration 17748, loss = 0.02878045\n",
      "Iteration 17749, loss = 0.02877862\n",
      "Iteration 17750, loss = 0.02877682\n",
      "Iteration 17751, loss = 0.02877497\n",
      "Iteration 17752, loss = 0.02877317\n",
      "Iteration 17753, loss = 0.02877133\n",
      "Iteration 17754, loss = 0.02876949\n",
      "Iteration 17755, loss = 0.02876769\n",
      "Iteration 17756, loss = 0.02876586\n",
      "Iteration 17757, loss = 0.02876403\n",
      "Iteration 17758, loss = 0.02876224\n",
      "Iteration 17759, loss = 0.02876039\n",
      "Iteration 17760, loss = 0.02875857\n",
      "Iteration 17761, loss = 0.02875672\n",
      "Iteration 17762, loss = 0.02875490\n",
      "Iteration 17763, loss = 0.02875310\n",
      "Iteration 17764, loss = 0.02875126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17765, loss = 0.02874946\n",
      "Iteration 17766, loss = 0.02874763\n",
      "Iteration 17767, loss = 0.02874578\n",
      "Iteration 17768, loss = 0.02874397\n",
      "Iteration 17769, loss = 0.02874216\n",
      "Iteration 17770, loss = 0.02874031\n",
      "Iteration 17771, loss = 0.02873849\n",
      "Iteration 17772, loss = 0.02873666\n",
      "Iteration 17773, loss = 0.02873485\n",
      "Iteration 17774, loss = 0.02873303\n",
      "Iteration 17775, loss = 0.02873119\n",
      "Iteration 17776, loss = 0.02872937\n",
      "Iteration 17777, loss = 0.02872753\n",
      "Iteration 17778, loss = 0.02872572\n",
      "Iteration 17779, loss = 0.02872393\n",
      "Iteration 17780, loss = 0.02872208\n",
      "Iteration 17781, loss = 0.02872028\n",
      "Iteration 17782, loss = 0.02871845\n",
      "Iteration 17783, loss = 0.02871664\n",
      "Iteration 17784, loss = 0.02871480\n",
      "Iteration 17785, loss = 0.02871297\n",
      "Iteration 17786, loss = 0.02871118\n",
      "Iteration 17787, loss = 0.02870934\n",
      "Iteration 17788, loss = 0.02870752\n",
      "Iteration 17789, loss = 0.02870571\n",
      "Iteration 17790, loss = 0.02870390\n",
      "Iteration 17791, loss = 0.02870204\n",
      "Iteration 17792, loss = 0.02870024\n",
      "Iteration 17793, loss = 0.02869844\n",
      "Iteration 17794, loss = 0.02869661\n",
      "Iteration 17795, loss = 0.02869480\n",
      "Iteration 17796, loss = 0.02869300\n",
      "Iteration 17797, loss = 0.02869118\n",
      "Iteration 17798, loss = 0.02868937\n",
      "Iteration 17799, loss = 0.02868756\n",
      "Iteration 17800, loss = 0.02868573\n",
      "Iteration 17801, loss = 0.02868392\n",
      "Iteration 17802, loss = 0.02868212\n",
      "Iteration 17803, loss = 0.02868029\n",
      "Iteration 17804, loss = 0.02867848\n",
      "Iteration 17805, loss = 0.02867665\n",
      "Iteration 17806, loss = 0.02867487\n",
      "Iteration 17807, loss = 0.02867306\n",
      "Iteration 17808, loss = 0.02867122\n",
      "Iteration 17809, loss = 0.02866943\n",
      "Iteration 17810, loss = 0.02866763\n",
      "Iteration 17811, loss = 0.02866580\n",
      "Iteration 17812, loss = 0.02866402\n",
      "Iteration 17813, loss = 0.02866217\n",
      "Iteration 17814, loss = 0.02866038\n",
      "Iteration 17815, loss = 0.02865857\n",
      "Iteration 17816, loss = 0.02865675\n",
      "Iteration 17817, loss = 0.02865497\n",
      "Iteration 17818, loss = 0.02865313\n",
      "Iteration 17819, loss = 0.02865134\n",
      "Iteration 17820, loss = 0.02864953\n",
      "Iteration 17821, loss = 0.02864772\n",
      "Iteration 17822, loss = 0.02864593\n",
      "Iteration 17823, loss = 0.02864411\n",
      "Iteration 17824, loss = 0.02864228\n",
      "Iteration 17825, loss = 0.02864049\n",
      "Iteration 17826, loss = 0.02863868\n",
      "Iteration 17827, loss = 0.02863688\n",
      "Iteration 17828, loss = 0.02863507\n",
      "Iteration 17829, loss = 0.02863327\n",
      "Iteration 17830, loss = 0.02863148\n",
      "Iteration 17831, loss = 0.02862964\n",
      "Iteration 17832, loss = 0.02862786\n",
      "Iteration 17833, loss = 0.02862603\n",
      "Iteration 17834, loss = 0.02862423\n",
      "Iteration 17835, loss = 0.02862243\n",
      "Iteration 17836, loss = 0.02862063\n",
      "Iteration 17837, loss = 0.02861881\n",
      "Iteration 17838, loss = 0.02861701\n",
      "Iteration 17839, loss = 0.02861522\n",
      "Iteration 17840, loss = 0.02861342\n",
      "Iteration 17841, loss = 0.02861160\n",
      "Iteration 17842, loss = 0.02860977\n",
      "Iteration 17843, loss = 0.02860799\n",
      "Iteration 17844, loss = 0.02860618\n",
      "Iteration 17845, loss = 0.02860436\n",
      "Iteration 17846, loss = 0.02860258\n",
      "Iteration 17847, loss = 0.02860076\n",
      "Iteration 17848, loss = 0.02859899\n",
      "Iteration 17849, loss = 0.02859718\n",
      "Iteration 17850, loss = 0.02859536\n",
      "Iteration 17851, loss = 0.02859357\n",
      "Iteration 17852, loss = 0.02859174\n",
      "Iteration 17853, loss = 0.02858995\n",
      "Iteration 17854, loss = 0.02858814\n",
      "Iteration 17855, loss = 0.02858633\n",
      "Iteration 17856, loss = 0.02858454\n",
      "Iteration 17857, loss = 0.02858273\n",
      "Iteration 17858, loss = 0.02858095\n",
      "Iteration 17859, loss = 0.02857914\n",
      "Iteration 17860, loss = 0.02857733\n",
      "Iteration 17861, loss = 0.02857553\n",
      "Iteration 17862, loss = 0.02857373\n",
      "Iteration 17863, loss = 0.02857195\n",
      "Iteration 17864, loss = 0.02857011\n",
      "Iteration 17865, loss = 0.02856832\n",
      "Iteration 17866, loss = 0.02856652\n",
      "Iteration 17867, loss = 0.02856472\n",
      "Iteration 17868, loss = 0.02856293\n",
      "Iteration 17869, loss = 0.02856114\n",
      "Iteration 17870, loss = 0.02855934\n",
      "Iteration 17871, loss = 0.02855753\n",
      "Iteration 17872, loss = 0.02855574\n",
      "Iteration 17873, loss = 0.02855393\n",
      "Iteration 17874, loss = 0.02855214\n",
      "Iteration 17875, loss = 0.02855035\n",
      "Iteration 17876, loss = 0.02854853\n",
      "Iteration 17877, loss = 0.02854673\n",
      "Iteration 17878, loss = 0.02854494\n",
      "Iteration 17879, loss = 0.02854314\n",
      "Iteration 17880, loss = 0.02854134\n",
      "Iteration 17881, loss = 0.02853956\n",
      "Iteration 17882, loss = 0.02853775\n",
      "Iteration 17883, loss = 0.02853597\n",
      "Iteration 17884, loss = 0.02853416\n",
      "Iteration 17885, loss = 0.02853238\n",
      "Iteration 17886, loss = 0.02853057\n",
      "Iteration 17887, loss = 0.02852877\n",
      "Iteration 17888, loss = 0.02852699\n",
      "Iteration 17889, loss = 0.02852520\n",
      "Iteration 17890, loss = 0.02852340\n",
      "Iteration 17891, loss = 0.02852162\n",
      "Iteration 17892, loss = 0.02851981\n",
      "Iteration 17893, loss = 0.02851804\n",
      "Iteration 17894, loss = 0.02851626\n",
      "Iteration 17895, loss = 0.02851445\n",
      "Iteration 17896, loss = 0.02851267\n",
      "Iteration 17897, loss = 0.02851089\n",
      "Iteration 17898, loss = 0.02850911\n",
      "Iteration 17899, loss = 0.02850731\n",
      "Iteration 17900, loss = 0.02850552\n",
      "Iteration 17901, loss = 0.02850371\n",
      "Iteration 17902, loss = 0.02850196\n",
      "Iteration 17903, loss = 0.02850016\n",
      "Iteration 17904, loss = 0.02849837\n",
      "Iteration 17905, loss = 0.02849658\n",
      "Iteration 17906, loss = 0.02849480\n",
      "Iteration 17907, loss = 0.02849301\n",
      "Iteration 17908, loss = 0.02849122\n",
      "Iteration 17909, loss = 0.02848945\n",
      "Iteration 17910, loss = 0.02848765\n",
      "Iteration 17911, loss = 0.02848586\n",
      "Iteration 17912, loss = 0.02848409\n",
      "Iteration 17913, loss = 0.02848228\n",
      "Iteration 17914, loss = 0.02848051\n",
      "Iteration 17915, loss = 0.02847869\n",
      "Iteration 17916, loss = 0.02847692\n",
      "Iteration 17917, loss = 0.02847514\n",
      "Iteration 17918, loss = 0.02847336\n",
      "Iteration 17919, loss = 0.02847155\n",
      "Iteration 17920, loss = 0.02846977\n",
      "Iteration 17921, loss = 0.02846799\n",
      "Iteration 17922, loss = 0.02846621\n",
      "Iteration 17923, loss = 0.02846441\n",
      "Iteration 17924, loss = 0.02846261\n",
      "Iteration 17925, loss = 0.02846083\n",
      "Iteration 17926, loss = 0.02845904\n",
      "Iteration 17927, loss = 0.02845725\n",
      "Iteration 17928, loss = 0.02845548\n",
      "Iteration 17929, loss = 0.02845368\n",
      "Iteration 17930, loss = 0.02845191\n",
      "Iteration 17931, loss = 0.02845011\n",
      "Iteration 17932, loss = 0.02844831\n",
      "Iteration 17933, loss = 0.02844655\n",
      "Iteration 17934, loss = 0.02844475\n",
      "Iteration 17935, loss = 0.02844296\n",
      "Iteration 17936, loss = 0.02844118\n",
      "Iteration 17937, loss = 0.02843939\n",
      "Iteration 17938, loss = 0.02843763\n",
      "Iteration 17939, loss = 0.02843580\n",
      "Iteration 17940, loss = 0.02843404\n",
      "Iteration 17941, loss = 0.02843225\n",
      "Iteration 17942, loss = 0.02843048\n",
      "Iteration 17943, loss = 0.02842868\n",
      "Iteration 17944, loss = 0.02842688\n",
      "Iteration 17945, loss = 0.02842511\n",
      "Iteration 17946, loss = 0.02842333\n",
      "Iteration 17947, loss = 0.02842154\n",
      "Iteration 17948, loss = 0.02841977\n",
      "Iteration 17949, loss = 0.02841797\n",
      "Iteration 17950, loss = 0.02841618\n",
      "Iteration 17951, loss = 0.02841440\n",
      "Iteration 17952, loss = 0.02841266\n",
      "Iteration 17953, loss = 0.02841084\n",
      "Iteration 17954, loss = 0.02840909\n",
      "Iteration 17955, loss = 0.02840728\n",
      "Iteration 17956, loss = 0.02840550\n",
      "Iteration 17957, loss = 0.02840372\n",
      "Iteration 17958, loss = 0.02840196\n",
      "Iteration 17959, loss = 0.02840017\n",
      "Iteration 17960, loss = 0.02839841\n",
      "Iteration 17961, loss = 0.02839661\n",
      "Iteration 17962, loss = 0.02839486\n",
      "Iteration 17963, loss = 0.02839305\n",
      "Iteration 17964, loss = 0.02839127\n",
      "Iteration 17965, loss = 0.02838950\n",
      "Iteration 17966, loss = 0.02838774\n",
      "Iteration 17967, loss = 0.02838595\n",
      "Iteration 17968, loss = 0.02838416\n",
      "Iteration 17969, loss = 0.02838240\n",
      "Iteration 17970, loss = 0.02838060\n",
      "Iteration 17971, loss = 0.02837884\n",
      "Iteration 17972, loss = 0.02837708\n",
      "Iteration 17973, loss = 0.02837526\n",
      "Iteration 17974, loss = 0.02837349\n",
      "Iteration 17975, loss = 0.02837174\n",
      "Iteration 17976, loss = 0.02836996\n",
      "Iteration 17977, loss = 0.02836819\n",
      "Iteration 17978, loss = 0.02836641\n",
      "Iteration 17979, loss = 0.02836464\n",
      "Iteration 17980, loss = 0.02836287\n",
      "Iteration 17981, loss = 0.02836110\n",
      "Iteration 17982, loss = 0.02835933\n",
      "Iteration 17983, loss = 0.02835759\n",
      "Iteration 17984, loss = 0.02835579\n",
      "Iteration 17985, loss = 0.02835401\n",
      "Iteration 17986, loss = 0.02835224\n",
      "Iteration 17987, loss = 0.02835048\n",
      "Iteration 17988, loss = 0.02834869\n",
      "Iteration 17989, loss = 0.02834694\n",
      "Iteration 17990, loss = 0.02834515\n",
      "Iteration 17991, loss = 0.02834338\n",
      "Iteration 17992, loss = 0.02834163\n",
      "Iteration 17993, loss = 0.02833982\n",
      "Iteration 17994, loss = 0.02833807\n",
      "Iteration 17995, loss = 0.02833629\n",
      "Iteration 17996, loss = 0.02833451\n",
      "Iteration 17997, loss = 0.02833275\n",
      "Iteration 17998, loss = 0.02833098\n",
      "Iteration 17999, loss = 0.02832922\n",
      "Iteration 18000, loss = 0.02832746\n",
      "Iteration 18001, loss = 0.02832567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18002, loss = 0.02832392\n",
      "Iteration 18003, loss = 0.02832215\n",
      "Iteration 18004, loss = 0.02832038\n",
      "Iteration 18005, loss = 0.02831863\n",
      "Iteration 18006, loss = 0.02831685\n",
      "Iteration 18007, loss = 0.02831508\n",
      "Iteration 18008, loss = 0.02831333\n",
      "Iteration 18009, loss = 0.02831159\n",
      "Iteration 18010, loss = 0.02830980\n",
      "Iteration 18011, loss = 0.02830802\n",
      "Iteration 18012, loss = 0.02830625\n",
      "Iteration 18013, loss = 0.02830452\n",
      "Iteration 18014, loss = 0.02830275\n",
      "Iteration 18015, loss = 0.02830098\n",
      "Iteration 18016, loss = 0.02829922\n",
      "Iteration 18017, loss = 0.02829746\n",
      "Iteration 18018, loss = 0.02829567\n",
      "Iteration 18019, loss = 0.02829392\n",
      "Iteration 18020, loss = 0.02829217\n",
      "Iteration 18021, loss = 0.02829039\n",
      "Iteration 18022, loss = 0.02828864\n",
      "Iteration 18023, loss = 0.02828685\n",
      "Iteration 18024, loss = 0.02828510\n",
      "Iteration 18025, loss = 0.02828334\n",
      "Iteration 18026, loss = 0.02828157\n",
      "Iteration 18027, loss = 0.02827983\n",
      "Iteration 18028, loss = 0.02827805\n",
      "Iteration 18029, loss = 0.02827628\n",
      "Iteration 18030, loss = 0.02827451\n",
      "Iteration 18031, loss = 0.02827275\n",
      "Iteration 18032, loss = 0.02827101\n",
      "Iteration 18033, loss = 0.02826924\n",
      "Iteration 18034, loss = 0.02826748\n",
      "Iteration 18035, loss = 0.02826572\n",
      "Iteration 18036, loss = 0.02826394\n",
      "Iteration 18037, loss = 0.02826218\n",
      "Iteration 18038, loss = 0.02826043\n",
      "Iteration 18039, loss = 0.02825866\n",
      "Iteration 18040, loss = 0.02825692\n",
      "Iteration 18041, loss = 0.02825514\n",
      "Iteration 18042, loss = 0.02825340\n",
      "Iteration 18043, loss = 0.02825165\n",
      "Iteration 18044, loss = 0.02824988\n",
      "Iteration 18045, loss = 0.02824811\n",
      "Iteration 18046, loss = 0.02824638\n",
      "Iteration 18047, loss = 0.02824461\n",
      "Iteration 18048, loss = 0.02824285\n",
      "Iteration 18049, loss = 0.02824110\n",
      "Iteration 18050, loss = 0.02823936\n",
      "Iteration 18051, loss = 0.02823758\n",
      "Iteration 18052, loss = 0.02823584\n",
      "Iteration 18053, loss = 0.02823409\n",
      "Iteration 18054, loss = 0.02823230\n",
      "Iteration 18055, loss = 0.02823056\n",
      "Iteration 18056, loss = 0.02822882\n",
      "Iteration 18057, loss = 0.02822705\n",
      "Iteration 18058, loss = 0.02822532\n",
      "Iteration 18059, loss = 0.02822355\n",
      "Iteration 18060, loss = 0.02822180\n",
      "Iteration 18061, loss = 0.02822004\n",
      "Iteration 18062, loss = 0.02821827\n",
      "Iteration 18063, loss = 0.02821652\n",
      "Iteration 18064, loss = 0.02821477\n",
      "Iteration 18065, loss = 0.02821300\n",
      "Iteration 18066, loss = 0.02821126\n",
      "Iteration 18067, loss = 0.02820949\n",
      "Iteration 18068, loss = 0.02820773\n",
      "Iteration 18069, loss = 0.02820597\n",
      "Iteration 18070, loss = 0.02820422\n",
      "Iteration 18071, loss = 0.02820246\n",
      "Iteration 18072, loss = 0.02820071\n",
      "Iteration 18073, loss = 0.02819895\n",
      "Iteration 18074, loss = 0.02819719\n",
      "Iteration 18075, loss = 0.02819545\n",
      "Iteration 18076, loss = 0.02819368\n",
      "Iteration 18077, loss = 0.02819193\n",
      "Iteration 18078, loss = 0.02819017\n",
      "Iteration 18079, loss = 0.02818843\n",
      "Iteration 18080, loss = 0.02818668\n",
      "Iteration 18081, loss = 0.02818492\n",
      "Iteration 18082, loss = 0.02818317\n",
      "Iteration 18083, loss = 0.02818141\n",
      "Iteration 18084, loss = 0.02817964\n",
      "Iteration 18085, loss = 0.02817792\n",
      "Iteration 18086, loss = 0.02817618\n",
      "Iteration 18087, loss = 0.02817443\n",
      "Iteration 18088, loss = 0.02817266\n",
      "Iteration 18089, loss = 0.02817092\n",
      "Iteration 18090, loss = 0.02816915\n",
      "Iteration 18091, loss = 0.02816742\n",
      "Iteration 18092, loss = 0.02816566\n",
      "Iteration 18093, loss = 0.02816394\n",
      "Iteration 18094, loss = 0.02816217\n",
      "Iteration 18095, loss = 0.02816042\n",
      "Iteration 18096, loss = 0.02815868\n",
      "Iteration 18097, loss = 0.02815694\n",
      "Iteration 18098, loss = 0.02815520\n",
      "Iteration 18099, loss = 0.02815343\n",
      "Iteration 18100, loss = 0.02815171\n",
      "Iteration 18101, loss = 0.02814994\n",
      "Iteration 18102, loss = 0.02814818\n",
      "Iteration 18103, loss = 0.02814645\n",
      "Iteration 18104, loss = 0.02814472\n",
      "Iteration 18105, loss = 0.02814296\n",
      "Iteration 18106, loss = 0.02814120\n",
      "Iteration 18107, loss = 0.02813948\n",
      "Iteration 18108, loss = 0.02813770\n",
      "Iteration 18109, loss = 0.02813596\n",
      "Iteration 18110, loss = 0.02813422\n",
      "Iteration 18111, loss = 0.02813247\n",
      "Iteration 18112, loss = 0.02813071\n",
      "Iteration 18113, loss = 0.02812900\n",
      "Iteration 18114, loss = 0.02812724\n",
      "Iteration 18115, loss = 0.02812549\n",
      "Iteration 18116, loss = 0.02812375\n",
      "Iteration 18117, loss = 0.02812201\n",
      "Iteration 18118, loss = 0.02812026\n",
      "Iteration 18119, loss = 0.02811851\n",
      "Iteration 18120, loss = 0.02811676\n",
      "Iteration 18121, loss = 0.02811505\n",
      "Iteration 18122, loss = 0.02811330\n",
      "Iteration 18123, loss = 0.02811155\n",
      "Iteration 18124, loss = 0.02810983\n",
      "Iteration 18125, loss = 0.02810808\n",
      "Iteration 18126, loss = 0.02810633\n",
      "Iteration 18127, loss = 0.02810462\n",
      "Iteration 18128, loss = 0.02810286\n",
      "Iteration 18129, loss = 0.02810109\n",
      "Iteration 18130, loss = 0.02809938\n",
      "Iteration 18131, loss = 0.02809761\n",
      "Iteration 18132, loss = 0.02809588\n",
      "Iteration 18133, loss = 0.02809416\n",
      "Iteration 18134, loss = 0.02809241\n",
      "Iteration 18135, loss = 0.02809065\n",
      "Iteration 18136, loss = 0.02808890\n",
      "Iteration 18137, loss = 0.02808717\n",
      "Iteration 18138, loss = 0.02808543\n",
      "Iteration 18139, loss = 0.02808368\n",
      "Iteration 18140, loss = 0.02808194\n",
      "Iteration 18141, loss = 0.02808020\n",
      "Iteration 18142, loss = 0.02807847\n",
      "Iteration 18143, loss = 0.02807675\n",
      "Iteration 18144, loss = 0.02807500\n",
      "Iteration 18145, loss = 0.02807326\n",
      "Iteration 18146, loss = 0.02807151\n",
      "Iteration 18147, loss = 0.02806979\n",
      "Iteration 18148, loss = 0.02806805\n",
      "Iteration 18149, loss = 0.02806629\n",
      "Iteration 18150, loss = 0.02806455\n",
      "Iteration 18151, loss = 0.02806283\n",
      "Iteration 18152, loss = 0.02806110\n",
      "Iteration 18153, loss = 0.02805937\n",
      "Iteration 18154, loss = 0.02805763\n",
      "Iteration 18155, loss = 0.02805588\n",
      "Iteration 18156, loss = 0.02805414\n",
      "Iteration 18157, loss = 0.02805240\n",
      "Iteration 18158, loss = 0.02805068\n",
      "Iteration 18159, loss = 0.02804894\n",
      "Iteration 18160, loss = 0.02804721\n",
      "Iteration 18161, loss = 0.02804549\n",
      "Iteration 18162, loss = 0.02804375\n",
      "Iteration 18163, loss = 0.02804200\n",
      "Iteration 18164, loss = 0.02804029\n",
      "Iteration 18165, loss = 0.02803855\n",
      "Iteration 18166, loss = 0.02803681\n",
      "Iteration 18167, loss = 0.02803509\n",
      "Iteration 18168, loss = 0.02803336\n",
      "Iteration 18169, loss = 0.02803162\n",
      "Iteration 18170, loss = 0.02802988\n",
      "Iteration 18171, loss = 0.02802817\n",
      "Iteration 18172, loss = 0.02802640\n",
      "Iteration 18173, loss = 0.02802470\n",
      "Iteration 18174, loss = 0.02802297\n",
      "Iteration 18175, loss = 0.02802121\n",
      "Iteration 18176, loss = 0.02801948\n",
      "Iteration 18177, loss = 0.02801777\n",
      "Iteration 18178, loss = 0.02801601\n",
      "Iteration 18179, loss = 0.02801428\n",
      "Iteration 18180, loss = 0.02801256\n",
      "Iteration 18181, loss = 0.02801082\n",
      "Iteration 18182, loss = 0.02800909\n",
      "Iteration 18183, loss = 0.02800736\n",
      "Iteration 18184, loss = 0.02800565\n",
      "Iteration 18185, loss = 0.02800389\n",
      "Iteration 18186, loss = 0.02800220\n",
      "Iteration 18187, loss = 0.02800045\n",
      "Iteration 18188, loss = 0.02799874\n",
      "Iteration 18189, loss = 0.02799700\n",
      "Iteration 18190, loss = 0.02799528\n",
      "Iteration 18191, loss = 0.02799357\n",
      "Iteration 18192, loss = 0.02799184\n",
      "Iteration 18193, loss = 0.02799011\n",
      "Iteration 18194, loss = 0.02798839\n",
      "Iteration 18195, loss = 0.02798666\n",
      "Iteration 18196, loss = 0.02798494\n",
      "Iteration 18197, loss = 0.02798323\n",
      "Iteration 18198, loss = 0.02798148\n",
      "Iteration 18199, loss = 0.02797981\n",
      "Iteration 18200, loss = 0.02797807\n",
      "Iteration 18201, loss = 0.02797633\n",
      "Iteration 18202, loss = 0.02797461\n",
      "Iteration 18203, loss = 0.02797288\n",
      "Iteration 18204, loss = 0.02797116\n",
      "Iteration 18205, loss = 0.02796942\n",
      "Iteration 18206, loss = 0.02796771\n",
      "Iteration 18207, loss = 0.02796599\n",
      "Iteration 18208, loss = 0.02796424\n",
      "Iteration 18209, loss = 0.02796252\n",
      "Iteration 18210, loss = 0.02796081\n",
      "Iteration 18211, loss = 0.02795908\n",
      "Iteration 18212, loss = 0.02795735\n",
      "Iteration 18213, loss = 0.02795562\n",
      "Iteration 18214, loss = 0.02795390\n",
      "Iteration 18215, loss = 0.02795220\n",
      "Iteration 18216, loss = 0.02795047\n",
      "Iteration 18217, loss = 0.02794876\n",
      "Iteration 18218, loss = 0.02794702\n",
      "Iteration 18219, loss = 0.02794529\n",
      "Iteration 18220, loss = 0.02794359\n",
      "Iteration 18221, loss = 0.02794187\n",
      "Iteration 18222, loss = 0.02794011\n",
      "Iteration 18223, loss = 0.02793840\n",
      "Iteration 18224, loss = 0.02793668\n",
      "Iteration 18225, loss = 0.02793494\n",
      "Iteration 18226, loss = 0.02793325\n",
      "Iteration 18227, loss = 0.02793153\n",
      "Iteration 18228, loss = 0.02792981\n",
      "Iteration 18229, loss = 0.02792808\n",
      "Iteration 18230, loss = 0.02792637\n",
      "Iteration 18231, loss = 0.02792465\n",
      "Iteration 18232, loss = 0.02792293\n",
      "Iteration 18233, loss = 0.02792121\n",
      "Iteration 18234, loss = 0.02791951\n",
      "Iteration 18235, loss = 0.02791776\n",
      "Iteration 18236, loss = 0.02791607\n",
      "Iteration 18237, loss = 0.02791435\n",
      "Iteration 18238, loss = 0.02791263\n",
      "Iteration 18239, loss = 0.02791090\n",
      "Iteration 18240, loss = 0.02790922\n",
      "Iteration 18241, loss = 0.02790747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18242, loss = 0.02790576\n",
      "Iteration 18243, loss = 0.02790404\n",
      "Iteration 18244, loss = 0.02790234\n",
      "Iteration 18245, loss = 0.02790059\n",
      "Iteration 18246, loss = 0.02789887\n",
      "Iteration 18247, loss = 0.02789715\n",
      "Iteration 18248, loss = 0.02789545\n",
      "Iteration 18249, loss = 0.02789374\n",
      "Iteration 18250, loss = 0.02789200\n",
      "Iteration 18251, loss = 0.02789029\n",
      "Iteration 18252, loss = 0.02788856\n",
      "Iteration 18253, loss = 0.02788683\n",
      "Iteration 18254, loss = 0.02788512\n",
      "Iteration 18255, loss = 0.02788343\n",
      "Iteration 18256, loss = 0.02788169\n",
      "Iteration 18257, loss = 0.02787997\n",
      "Iteration 18258, loss = 0.02787823\n",
      "Iteration 18259, loss = 0.02787654\n",
      "Iteration 18260, loss = 0.02787485\n",
      "Iteration 18261, loss = 0.02787311\n",
      "Iteration 18262, loss = 0.02787138\n",
      "Iteration 18263, loss = 0.02786969\n",
      "Iteration 18264, loss = 0.02786797\n",
      "Iteration 18265, loss = 0.02786624\n",
      "Iteration 18266, loss = 0.02786452\n",
      "Iteration 18267, loss = 0.02786281\n",
      "Iteration 18268, loss = 0.02786110\n",
      "Iteration 18269, loss = 0.02785938\n",
      "Iteration 18270, loss = 0.02785765\n",
      "Iteration 18271, loss = 0.02785595\n",
      "Iteration 18272, loss = 0.02785422\n",
      "Iteration 18273, loss = 0.02785251\n",
      "Iteration 18274, loss = 0.02785082\n",
      "Iteration 18275, loss = 0.02784907\n",
      "Iteration 18276, loss = 0.02784738\n",
      "Iteration 18277, loss = 0.02784564\n",
      "Iteration 18278, loss = 0.02784397\n",
      "Iteration 18279, loss = 0.02784227\n",
      "Iteration 18280, loss = 0.02784052\n",
      "Iteration 18281, loss = 0.02783882\n",
      "Iteration 18282, loss = 0.02783711\n",
      "Iteration 18283, loss = 0.02783541\n",
      "Iteration 18284, loss = 0.02783369\n",
      "Iteration 18285, loss = 0.02783197\n",
      "Iteration 18286, loss = 0.02783028\n",
      "Iteration 18287, loss = 0.02782855\n",
      "Iteration 18288, loss = 0.02782684\n",
      "Iteration 18289, loss = 0.02782515\n",
      "Iteration 18290, loss = 0.02782345\n",
      "Iteration 18291, loss = 0.02782171\n",
      "Iteration 18292, loss = 0.02782003\n",
      "Iteration 18293, loss = 0.02781830\n",
      "Iteration 18294, loss = 0.02781663\n",
      "Iteration 18295, loss = 0.02781491\n",
      "Iteration 18296, loss = 0.02781320\n",
      "Iteration 18297, loss = 0.02781147\n",
      "Iteration 18298, loss = 0.02780980\n",
      "Iteration 18299, loss = 0.02780809\n",
      "Iteration 18300, loss = 0.02780638\n",
      "Iteration 18301, loss = 0.02780466\n",
      "Iteration 18302, loss = 0.02780297\n",
      "Iteration 18303, loss = 0.02780125\n",
      "Iteration 18304, loss = 0.02779958\n",
      "Iteration 18305, loss = 0.02779787\n",
      "Iteration 18306, loss = 0.02779614\n",
      "Iteration 18307, loss = 0.02779445\n",
      "Iteration 18308, loss = 0.02779274\n",
      "Iteration 18309, loss = 0.02779106\n",
      "Iteration 18310, loss = 0.02778933\n",
      "Iteration 18311, loss = 0.02778764\n",
      "Iteration 18312, loss = 0.02778593\n",
      "Iteration 18313, loss = 0.02778422\n",
      "Iteration 18314, loss = 0.02778254\n",
      "Iteration 18315, loss = 0.02778083\n",
      "Iteration 18316, loss = 0.02777912\n",
      "Iteration 18317, loss = 0.02777741\n",
      "Iteration 18318, loss = 0.02777573\n",
      "Iteration 18319, loss = 0.02777400\n",
      "Iteration 18320, loss = 0.02777235\n",
      "Iteration 18321, loss = 0.02777060\n",
      "Iteration 18322, loss = 0.02776892\n",
      "Iteration 18323, loss = 0.02776721\n",
      "Iteration 18324, loss = 0.02776551\n",
      "Iteration 18325, loss = 0.02776379\n",
      "Iteration 18326, loss = 0.02776212\n",
      "Iteration 18327, loss = 0.02776041\n",
      "Iteration 18328, loss = 0.02775870\n",
      "Iteration 18329, loss = 0.02775702\n",
      "Iteration 18330, loss = 0.02775532\n",
      "Iteration 18331, loss = 0.02775363\n",
      "Iteration 18332, loss = 0.02775190\n",
      "Iteration 18333, loss = 0.02775022\n",
      "Iteration 18334, loss = 0.02774853\n",
      "Iteration 18335, loss = 0.02774683\n",
      "Iteration 18336, loss = 0.02774512\n",
      "Iteration 18337, loss = 0.02774340\n",
      "Iteration 18338, loss = 0.02774172\n",
      "Iteration 18339, loss = 0.02774003\n",
      "Iteration 18340, loss = 0.02773833\n",
      "Iteration 18341, loss = 0.02773664\n",
      "Iteration 18342, loss = 0.02773493\n",
      "Iteration 18343, loss = 0.02773323\n",
      "Iteration 18344, loss = 0.02773154\n",
      "Iteration 18345, loss = 0.02772983\n",
      "Iteration 18346, loss = 0.02772813\n",
      "Iteration 18347, loss = 0.02772644\n",
      "Iteration 18348, loss = 0.02772472\n",
      "Iteration 18349, loss = 0.02772305\n",
      "Iteration 18350, loss = 0.02772134\n",
      "Iteration 18351, loss = 0.02771964\n",
      "Iteration 18352, loss = 0.02771794\n",
      "Iteration 18353, loss = 0.02771627\n",
      "Iteration 18354, loss = 0.02771455\n",
      "Iteration 18355, loss = 0.02771287\n",
      "Iteration 18356, loss = 0.02771116\n",
      "Iteration 18357, loss = 0.02770949\n",
      "Iteration 18358, loss = 0.02770777\n",
      "Iteration 18359, loss = 0.02770608\n",
      "Iteration 18360, loss = 0.02770438\n",
      "Iteration 18361, loss = 0.02770272\n",
      "Iteration 18362, loss = 0.02770100\n",
      "Iteration 18363, loss = 0.02769932\n",
      "Iteration 18364, loss = 0.02769762\n",
      "Iteration 18365, loss = 0.02769592\n",
      "Iteration 18366, loss = 0.02769423\n",
      "Iteration 18367, loss = 0.02769254\n",
      "Iteration 18368, loss = 0.02769086\n",
      "Iteration 18369, loss = 0.02768916\n",
      "Iteration 18370, loss = 0.02768746\n",
      "Iteration 18371, loss = 0.02768576\n",
      "Iteration 18372, loss = 0.02768411\n",
      "Iteration 18373, loss = 0.02768241\n",
      "Iteration 18374, loss = 0.02768068\n",
      "Iteration 18375, loss = 0.02767900\n",
      "Iteration 18376, loss = 0.02767732\n",
      "Iteration 18377, loss = 0.02767561\n",
      "Iteration 18378, loss = 0.02767392\n",
      "Iteration 18379, loss = 0.02767223\n",
      "Iteration 18380, loss = 0.02767055\n",
      "Iteration 18381, loss = 0.02766887\n",
      "Iteration 18382, loss = 0.02766717\n",
      "Iteration 18383, loss = 0.02766547\n",
      "Iteration 18384, loss = 0.02766379\n",
      "Iteration 18385, loss = 0.02766213\n",
      "Iteration 18386, loss = 0.02766043\n",
      "Iteration 18387, loss = 0.02765875\n",
      "Iteration 18388, loss = 0.02765705\n",
      "Iteration 18389, loss = 0.02765535\n",
      "Iteration 18390, loss = 0.02765367\n",
      "Iteration 18391, loss = 0.02765202\n",
      "Iteration 18392, loss = 0.02765030\n",
      "Iteration 18393, loss = 0.02764860\n",
      "Iteration 18394, loss = 0.02764695\n",
      "Iteration 18395, loss = 0.02764524\n",
      "Iteration 18396, loss = 0.02764356\n",
      "Iteration 18397, loss = 0.02764189\n",
      "Iteration 18398, loss = 0.02764019\n",
      "Iteration 18399, loss = 0.02763850\n",
      "Iteration 18400, loss = 0.02763684\n",
      "Iteration 18401, loss = 0.02763515\n",
      "Iteration 18402, loss = 0.02763347\n",
      "Iteration 18403, loss = 0.02763179\n",
      "Iteration 18404, loss = 0.02763010\n",
      "Iteration 18405, loss = 0.02762843\n",
      "Iteration 18406, loss = 0.02762672\n",
      "Iteration 18407, loss = 0.02762506\n",
      "Iteration 18408, loss = 0.02762335\n",
      "Iteration 18409, loss = 0.02762168\n",
      "Iteration 18410, loss = 0.02762002\n",
      "Iteration 18411, loss = 0.02761831\n",
      "Iteration 18412, loss = 0.02761664\n",
      "Iteration 18413, loss = 0.02761496\n",
      "Iteration 18414, loss = 0.02761327\n",
      "Iteration 18415, loss = 0.02761159\n",
      "Iteration 18416, loss = 0.02760991\n",
      "Iteration 18417, loss = 0.02760822\n",
      "Iteration 18418, loss = 0.02760653\n",
      "Iteration 18419, loss = 0.02760485\n",
      "Iteration 18420, loss = 0.02760317\n",
      "Iteration 18421, loss = 0.02760148\n",
      "Iteration 18422, loss = 0.02759982\n",
      "Iteration 18423, loss = 0.02759813\n",
      "Iteration 18424, loss = 0.02759644\n",
      "Iteration 18425, loss = 0.02759479\n",
      "Iteration 18426, loss = 0.02759311\n",
      "Iteration 18427, loss = 0.02759144\n",
      "Iteration 18428, loss = 0.02758976\n",
      "Iteration 18429, loss = 0.02758810\n",
      "Iteration 18430, loss = 0.02758641\n",
      "Iteration 18431, loss = 0.02758473\n",
      "Iteration 18432, loss = 0.02758306\n",
      "Iteration 18433, loss = 0.02758138\n",
      "Iteration 18434, loss = 0.02757972\n",
      "Iteration 18435, loss = 0.02757803\n",
      "Iteration 18436, loss = 0.02757634\n",
      "Iteration 18437, loss = 0.02757470\n",
      "Iteration 18438, loss = 0.02757299\n",
      "Iteration 18439, loss = 0.02757134\n",
      "Iteration 18440, loss = 0.02756964\n",
      "Iteration 18441, loss = 0.02756796\n",
      "Iteration 18442, loss = 0.02756630\n",
      "Iteration 18443, loss = 0.02756463\n",
      "Iteration 18444, loss = 0.02756295\n",
      "Iteration 18445, loss = 0.02756127\n",
      "Iteration 18446, loss = 0.02755961\n",
      "Iteration 18447, loss = 0.02755793\n",
      "Iteration 18448, loss = 0.02755623\n",
      "Iteration 18449, loss = 0.02755459\n",
      "Iteration 18450, loss = 0.02755289\n",
      "Iteration 18451, loss = 0.02755122\n",
      "Iteration 18452, loss = 0.02754955\n",
      "Iteration 18453, loss = 0.02754788\n",
      "Iteration 18454, loss = 0.02754619\n",
      "Iteration 18455, loss = 0.02754453\n",
      "Iteration 18456, loss = 0.02754285\n",
      "Iteration 18457, loss = 0.02754118\n",
      "Iteration 18458, loss = 0.02753952\n",
      "Iteration 18459, loss = 0.02753783\n",
      "Iteration 18460, loss = 0.02753620\n",
      "Iteration 18461, loss = 0.02753451\n",
      "Iteration 18462, loss = 0.02753283\n",
      "Iteration 18463, loss = 0.02753118\n",
      "Iteration 18464, loss = 0.02752949\n",
      "Iteration 18465, loss = 0.02752782\n",
      "Iteration 18466, loss = 0.02752617\n",
      "Iteration 18467, loss = 0.02752447\n",
      "Iteration 18468, loss = 0.02752280\n",
      "Iteration 18469, loss = 0.02752114\n",
      "Iteration 18470, loss = 0.02751946\n",
      "Iteration 18471, loss = 0.02751780\n",
      "Iteration 18472, loss = 0.02751611\n",
      "Iteration 18473, loss = 0.02751441\n",
      "Iteration 18474, loss = 0.02751276\n",
      "Iteration 18475, loss = 0.02751108\n",
      "Iteration 18476, loss = 0.02750940\n",
      "Iteration 18477, loss = 0.02750775\n",
      "Iteration 18478, loss = 0.02750607\n",
      "Iteration 18479, loss = 0.02750439\n",
      "Iteration 18480, loss = 0.02750271\n",
      "Iteration 18481, loss = 0.02750108\n",
      "Iteration 18482, loss = 0.02749939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18483, loss = 0.02749771\n",
      "Iteration 18484, loss = 0.02749608\n",
      "Iteration 18485, loss = 0.02749438\n",
      "Iteration 18486, loss = 0.02749272\n",
      "Iteration 18487, loss = 0.02749109\n",
      "Iteration 18488, loss = 0.02748940\n",
      "Iteration 18489, loss = 0.02748773\n",
      "Iteration 18490, loss = 0.02748607\n",
      "Iteration 18491, loss = 0.02748443\n",
      "Iteration 18492, loss = 0.02748275\n",
      "Iteration 18493, loss = 0.02748109\n",
      "Iteration 18494, loss = 0.02747944\n",
      "Iteration 18495, loss = 0.02747776\n",
      "Iteration 18496, loss = 0.02747609\n",
      "Iteration 18497, loss = 0.02747442\n",
      "Iteration 18498, loss = 0.02747278\n",
      "Iteration 18499, loss = 0.02747110\n",
      "Iteration 18500, loss = 0.02746944\n",
      "Iteration 18501, loss = 0.02746778\n",
      "Iteration 18502, loss = 0.02746609\n",
      "Iteration 18503, loss = 0.02746443\n",
      "Iteration 18504, loss = 0.02746278\n",
      "Iteration 18505, loss = 0.02746112\n",
      "Iteration 18506, loss = 0.02745943\n",
      "Iteration 18507, loss = 0.02745777\n",
      "Iteration 18508, loss = 0.02745611\n",
      "Iteration 18509, loss = 0.02745445\n",
      "Iteration 18510, loss = 0.02745278\n",
      "Iteration 18511, loss = 0.02745112\n",
      "Iteration 18512, loss = 0.02744945\n",
      "Iteration 18513, loss = 0.02744777\n",
      "Iteration 18514, loss = 0.02744614\n",
      "Iteration 18515, loss = 0.02744450\n",
      "Iteration 18516, loss = 0.02744281\n",
      "Iteration 18517, loss = 0.02744114\n",
      "Iteration 18518, loss = 0.02743951\n",
      "Iteration 18519, loss = 0.02743783\n",
      "Iteration 18520, loss = 0.02743618\n",
      "Iteration 18521, loss = 0.02743452\n",
      "Iteration 18522, loss = 0.02743287\n",
      "Iteration 18523, loss = 0.02743120\n",
      "Iteration 18524, loss = 0.02742955\n",
      "Iteration 18525, loss = 0.02742790\n",
      "Iteration 18526, loss = 0.02742623\n",
      "Iteration 18527, loss = 0.02742455\n",
      "Iteration 18528, loss = 0.02742291\n",
      "Iteration 18529, loss = 0.02742124\n",
      "Iteration 18530, loss = 0.02741958\n",
      "Iteration 18531, loss = 0.02741792\n",
      "Iteration 18532, loss = 0.02741627\n",
      "Iteration 18533, loss = 0.02741459\n",
      "Iteration 18534, loss = 0.02741296\n",
      "Iteration 18535, loss = 0.02741131\n",
      "Iteration 18536, loss = 0.02740965\n",
      "Iteration 18537, loss = 0.02740800\n",
      "Iteration 18538, loss = 0.02740633\n",
      "Iteration 18539, loss = 0.02740467\n",
      "Iteration 18540, loss = 0.02740301\n",
      "Iteration 18541, loss = 0.02740135\n",
      "Iteration 18542, loss = 0.02739970\n",
      "Iteration 18543, loss = 0.02739808\n",
      "Iteration 18544, loss = 0.02739639\n",
      "Iteration 18545, loss = 0.02739475\n",
      "Iteration 18546, loss = 0.02739310\n",
      "Iteration 18547, loss = 0.02739143\n",
      "Iteration 18548, loss = 0.02738977\n",
      "Iteration 18549, loss = 0.02738811\n",
      "Iteration 18550, loss = 0.02738646\n",
      "Iteration 18551, loss = 0.02738482\n",
      "Iteration 18552, loss = 0.02738315\n",
      "Iteration 18553, loss = 0.02738149\n",
      "Iteration 18554, loss = 0.02737982\n",
      "Iteration 18555, loss = 0.02737817\n",
      "Iteration 18556, loss = 0.02737653\n",
      "Iteration 18557, loss = 0.02737487\n",
      "Iteration 18558, loss = 0.02737322\n",
      "Iteration 18559, loss = 0.02737156\n",
      "Iteration 18560, loss = 0.02736992\n",
      "Iteration 18561, loss = 0.02736827\n",
      "Iteration 18562, loss = 0.02736663\n",
      "Iteration 18563, loss = 0.02736496\n",
      "Iteration 18564, loss = 0.02736331\n",
      "Iteration 18565, loss = 0.02736167\n",
      "Iteration 18566, loss = 0.02736001\n",
      "Iteration 18567, loss = 0.02735837\n",
      "Iteration 18568, loss = 0.02735671\n",
      "Iteration 18569, loss = 0.02735506\n",
      "Iteration 18570, loss = 0.02735341\n",
      "Iteration 18571, loss = 0.02735176\n",
      "Iteration 18572, loss = 0.02735013\n",
      "Iteration 18573, loss = 0.02734848\n",
      "Iteration 18574, loss = 0.02734682\n",
      "Iteration 18575, loss = 0.02734516\n",
      "Iteration 18576, loss = 0.02734353\n",
      "Iteration 18577, loss = 0.02734188\n",
      "Iteration 18578, loss = 0.02734026\n",
      "Iteration 18579, loss = 0.02733858\n",
      "Iteration 18580, loss = 0.02733697\n",
      "Iteration 18581, loss = 0.02733533\n",
      "Iteration 18582, loss = 0.02733368\n",
      "Iteration 18583, loss = 0.02733201\n",
      "Iteration 18584, loss = 0.02733036\n",
      "Iteration 18585, loss = 0.02732874\n",
      "Iteration 18586, loss = 0.02732707\n",
      "Iteration 18587, loss = 0.02732544\n",
      "Iteration 18588, loss = 0.02732380\n",
      "Iteration 18589, loss = 0.02732215\n",
      "Iteration 18590, loss = 0.02732051\n",
      "Iteration 18591, loss = 0.02731886\n",
      "Iteration 18592, loss = 0.02731722\n",
      "Iteration 18593, loss = 0.02731558\n",
      "Iteration 18594, loss = 0.02731392\n",
      "Iteration 18595, loss = 0.02731228\n",
      "Iteration 18596, loss = 0.02731064\n",
      "Iteration 18597, loss = 0.02730900\n",
      "Iteration 18598, loss = 0.02730734\n",
      "Iteration 18599, loss = 0.02730570\n",
      "Iteration 18600, loss = 0.02730407\n",
      "Iteration 18601, loss = 0.02730243\n",
      "Iteration 18602, loss = 0.02730077\n",
      "Iteration 18603, loss = 0.02729913\n",
      "Iteration 18604, loss = 0.02729749\n",
      "Iteration 18605, loss = 0.02729584\n",
      "Iteration 18606, loss = 0.02729421\n",
      "Iteration 18607, loss = 0.02729256\n",
      "Iteration 18608, loss = 0.02729091\n",
      "Iteration 18609, loss = 0.02728930\n",
      "Iteration 18610, loss = 0.02728763\n",
      "Iteration 18611, loss = 0.02728599\n",
      "Iteration 18612, loss = 0.02728434\n",
      "Iteration 18613, loss = 0.02728272\n",
      "Iteration 18614, loss = 0.02728107\n",
      "Iteration 18615, loss = 0.02727944\n",
      "Iteration 18616, loss = 0.02727776\n",
      "Iteration 18617, loss = 0.02727614\n",
      "Iteration 18618, loss = 0.02727447\n",
      "Iteration 18619, loss = 0.02727283\n",
      "Iteration 18620, loss = 0.02727120\n",
      "Iteration 18621, loss = 0.02726955\n",
      "Iteration 18622, loss = 0.02726790\n",
      "Iteration 18623, loss = 0.02726626\n",
      "Iteration 18624, loss = 0.02726463\n",
      "Iteration 18625, loss = 0.02726297\n",
      "Iteration 18626, loss = 0.02726134\n",
      "Iteration 18627, loss = 0.02725973\n",
      "Iteration 18628, loss = 0.02725805\n",
      "Iteration 18629, loss = 0.02725643\n",
      "Iteration 18630, loss = 0.02725478\n",
      "Iteration 18631, loss = 0.02725315\n",
      "Iteration 18632, loss = 0.02725151\n",
      "Iteration 18633, loss = 0.02724986\n",
      "Iteration 18634, loss = 0.02724825\n",
      "Iteration 18635, loss = 0.02724659\n",
      "Iteration 18636, loss = 0.02724497\n",
      "Iteration 18637, loss = 0.02724332\n",
      "Iteration 18638, loss = 0.02724168\n",
      "Iteration 18639, loss = 0.02724006\n",
      "Iteration 18640, loss = 0.02723841\n",
      "Iteration 18641, loss = 0.02723678\n",
      "Iteration 18642, loss = 0.02723514\n",
      "Iteration 18643, loss = 0.02723352\n",
      "Iteration 18644, loss = 0.02723185\n",
      "Iteration 18645, loss = 0.02723023\n",
      "Iteration 18646, loss = 0.02722860\n",
      "Iteration 18647, loss = 0.02722696\n",
      "Iteration 18648, loss = 0.02722532\n",
      "Iteration 18649, loss = 0.02722370\n",
      "Iteration 18650, loss = 0.02722206\n",
      "Iteration 18651, loss = 0.02722043\n",
      "Iteration 18652, loss = 0.02721879\n",
      "Iteration 18653, loss = 0.02721715\n",
      "Iteration 18654, loss = 0.02721553\n",
      "Iteration 18655, loss = 0.02721388\n",
      "Iteration 18656, loss = 0.02721226\n",
      "Iteration 18657, loss = 0.02721064\n",
      "Iteration 18658, loss = 0.02720899\n",
      "Iteration 18659, loss = 0.02720737\n",
      "Iteration 18660, loss = 0.02720575\n",
      "Iteration 18661, loss = 0.02720411\n",
      "Iteration 18662, loss = 0.02720249\n",
      "Iteration 18663, loss = 0.02720085\n",
      "Iteration 18664, loss = 0.02719924\n",
      "Iteration 18665, loss = 0.02719759\n",
      "Iteration 18666, loss = 0.02719598\n",
      "Iteration 18667, loss = 0.02719437\n",
      "Iteration 18668, loss = 0.02719273\n",
      "Iteration 18669, loss = 0.02719109\n",
      "Iteration 18670, loss = 0.02718948\n",
      "Iteration 18671, loss = 0.02718783\n",
      "Iteration 18672, loss = 0.02718619\n",
      "Iteration 18673, loss = 0.02718459\n",
      "Iteration 18674, loss = 0.02718295\n",
      "Iteration 18675, loss = 0.02718132\n",
      "Iteration 18676, loss = 0.02717970\n",
      "Iteration 18677, loss = 0.02717807\n",
      "Iteration 18678, loss = 0.02717645\n",
      "Iteration 18679, loss = 0.02717481\n",
      "Iteration 18680, loss = 0.02717318\n",
      "Iteration 18681, loss = 0.02717157\n",
      "Iteration 18682, loss = 0.02716994\n",
      "Iteration 18683, loss = 0.02716831\n",
      "Iteration 18684, loss = 0.02716667\n",
      "Iteration 18685, loss = 0.02716503\n",
      "Iteration 18686, loss = 0.02716341\n",
      "Iteration 18687, loss = 0.02716178\n",
      "Iteration 18688, loss = 0.02716015\n",
      "Iteration 18689, loss = 0.02715853\n",
      "Iteration 18690, loss = 0.02715690\n",
      "Iteration 18691, loss = 0.02715527\n",
      "Iteration 18692, loss = 0.02715364\n",
      "Iteration 18693, loss = 0.02715201\n",
      "Iteration 18694, loss = 0.02715037\n",
      "Iteration 18695, loss = 0.02714876\n",
      "Iteration 18696, loss = 0.02714712\n",
      "Iteration 18697, loss = 0.02714549\n",
      "Iteration 18698, loss = 0.02714390\n",
      "Iteration 18699, loss = 0.02714225\n",
      "Iteration 18700, loss = 0.02714061\n",
      "Iteration 18701, loss = 0.02713901\n",
      "Iteration 18702, loss = 0.02713738\n",
      "Iteration 18703, loss = 0.02713575\n",
      "Iteration 18704, loss = 0.02713411\n",
      "Iteration 18705, loss = 0.02713249\n",
      "Iteration 18706, loss = 0.02713089\n",
      "Iteration 18707, loss = 0.02712926\n",
      "Iteration 18708, loss = 0.02712764\n",
      "Iteration 18709, loss = 0.02712600\n",
      "Iteration 18710, loss = 0.02712438\n",
      "Iteration 18711, loss = 0.02712276\n",
      "Iteration 18712, loss = 0.02712114\n",
      "Iteration 18713, loss = 0.02711952\n",
      "Iteration 18714, loss = 0.02711787\n",
      "Iteration 18715, loss = 0.02711628\n",
      "Iteration 18716, loss = 0.02711465\n",
      "Iteration 18717, loss = 0.02711303\n",
      "Iteration 18718, loss = 0.02711141\n",
      "Iteration 18719, loss = 0.02710978\n",
      "Iteration 18720, loss = 0.02710818\n",
      "Iteration 18721, loss = 0.02710654\n",
      "Iteration 18722, loss = 0.02710495\n",
      "Iteration 18723, loss = 0.02710331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18724, loss = 0.02710169\n",
      "Iteration 18725, loss = 0.02710008\n",
      "Iteration 18726, loss = 0.02709845\n",
      "Iteration 18727, loss = 0.02709682\n",
      "Iteration 18728, loss = 0.02709520\n",
      "Iteration 18729, loss = 0.02709358\n",
      "Iteration 18730, loss = 0.02709194\n",
      "Iteration 18731, loss = 0.02709036\n",
      "Iteration 18732, loss = 0.02708871\n",
      "Iteration 18733, loss = 0.02708711\n",
      "Iteration 18734, loss = 0.02708549\n",
      "Iteration 18735, loss = 0.02708387\n",
      "Iteration 18736, loss = 0.02708225\n",
      "Iteration 18737, loss = 0.02708063\n",
      "Iteration 18738, loss = 0.02707903\n",
      "Iteration 18739, loss = 0.02707741\n",
      "Iteration 18740, loss = 0.02707581\n",
      "Iteration 18741, loss = 0.02707418\n",
      "Iteration 18742, loss = 0.02707258\n",
      "Iteration 18743, loss = 0.02707096\n",
      "Iteration 18744, loss = 0.02706934\n",
      "Iteration 18745, loss = 0.02706773\n",
      "Iteration 18746, loss = 0.02706613\n",
      "Iteration 18747, loss = 0.02706450\n",
      "Iteration 18748, loss = 0.02706291\n",
      "Iteration 18749, loss = 0.02706128\n",
      "Iteration 18750, loss = 0.02705967\n",
      "Iteration 18751, loss = 0.02705805\n",
      "Iteration 18752, loss = 0.02705645\n",
      "Iteration 18753, loss = 0.02705483\n",
      "Iteration 18754, loss = 0.02705321\n",
      "Iteration 18755, loss = 0.02705160\n",
      "Iteration 18756, loss = 0.02704999\n",
      "Iteration 18757, loss = 0.02704839\n",
      "Iteration 18758, loss = 0.02704676\n",
      "Iteration 18759, loss = 0.02704516\n",
      "Iteration 18760, loss = 0.02704354\n",
      "Iteration 18761, loss = 0.02704195\n",
      "Iteration 18762, loss = 0.02704034\n",
      "Iteration 18763, loss = 0.02703872\n",
      "Iteration 18764, loss = 0.02703711\n",
      "Iteration 18765, loss = 0.02703550\n",
      "Iteration 18766, loss = 0.02703390\n",
      "Iteration 18767, loss = 0.02703227\n",
      "Iteration 18768, loss = 0.02703066\n",
      "Iteration 18769, loss = 0.02702906\n",
      "Iteration 18770, loss = 0.02702745\n",
      "Iteration 18771, loss = 0.02702585\n",
      "Iteration 18772, loss = 0.02702423\n",
      "Iteration 18773, loss = 0.02702262\n",
      "Iteration 18774, loss = 0.02702101\n",
      "Iteration 18775, loss = 0.02701939\n",
      "Iteration 18776, loss = 0.02701779\n",
      "Iteration 18777, loss = 0.02701615\n",
      "Iteration 18778, loss = 0.02701458\n",
      "Iteration 18779, loss = 0.02701296\n",
      "Iteration 18780, loss = 0.02701135\n",
      "Iteration 18781, loss = 0.02700974\n",
      "Iteration 18782, loss = 0.02700814\n",
      "Iteration 18783, loss = 0.02700652\n",
      "Iteration 18784, loss = 0.02700492\n",
      "Iteration 18785, loss = 0.02700334\n",
      "Iteration 18786, loss = 0.02700170\n",
      "Iteration 18787, loss = 0.02700012\n",
      "Iteration 18788, loss = 0.02699850\n",
      "Iteration 18789, loss = 0.02699687\n",
      "Iteration 18790, loss = 0.02699530\n",
      "Iteration 18791, loss = 0.02699366\n",
      "Iteration 18792, loss = 0.02699208\n",
      "Iteration 18793, loss = 0.02699046\n",
      "Iteration 18794, loss = 0.02698885\n",
      "Iteration 18795, loss = 0.02698726\n",
      "Iteration 18796, loss = 0.02698564\n",
      "Iteration 18797, loss = 0.02698404\n",
      "Iteration 18798, loss = 0.02698242\n",
      "Iteration 18799, loss = 0.02698081\n",
      "Iteration 18800, loss = 0.02697920\n",
      "Iteration 18801, loss = 0.02697758\n",
      "Iteration 18802, loss = 0.02697599\n",
      "Iteration 18803, loss = 0.02697440\n",
      "Iteration 18804, loss = 0.02697276\n",
      "Iteration 18805, loss = 0.02697116\n",
      "Iteration 18806, loss = 0.02696956\n",
      "Iteration 18807, loss = 0.02696796\n",
      "Iteration 18808, loss = 0.02696636\n",
      "Iteration 18809, loss = 0.02696476\n",
      "Iteration 18810, loss = 0.02696313\n",
      "Iteration 18811, loss = 0.02696154\n",
      "Iteration 18812, loss = 0.02695994\n",
      "Iteration 18813, loss = 0.02695834\n",
      "Iteration 18814, loss = 0.02695675\n",
      "Iteration 18815, loss = 0.02695512\n",
      "Iteration 18816, loss = 0.02695353\n",
      "Iteration 18817, loss = 0.02695193\n",
      "Iteration 18818, loss = 0.02695033\n",
      "Iteration 18819, loss = 0.02694872\n",
      "Iteration 18820, loss = 0.02694713\n",
      "Iteration 18821, loss = 0.02694553\n",
      "Iteration 18822, loss = 0.02694393\n",
      "Iteration 18823, loss = 0.02694231\n",
      "Iteration 18824, loss = 0.02694073\n",
      "Iteration 18825, loss = 0.02693911\n",
      "Iteration 18826, loss = 0.02693752\n",
      "Iteration 18827, loss = 0.02693591\n",
      "Iteration 18828, loss = 0.02693431\n",
      "Iteration 18829, loss = 0.02693272\n",
      "Iteration 18830, loss = 0.02693111\n",
      "Iteration 18831, loss = 0.02692950\n",
      "Iteration 18832, loss = 0.02692792\n",
      "Iteration 18833, loss = 0.02692632\n",
      "Iteration 18834, loss = 0.02692471\n",
      "Iteration 18835, loss = 0.02692312\n",
      "Iteration 18836, loss = 0.02692152\n",
      "Iteration 18837, loss = 0.02691992\n",
      "Iteration 18838, loss = 0.02691832\n",
      "Iteration 18839, loss = 0.02691672\n",
      "Iteration 18840, loss = 0.02691512\n",
      "Iteration 18841, loss = 0.02691352\n",
      "Iteration 18842, loss = 0.02691193\n",
      "Iteration 18843, loss = 0.02691032\n",
      "Iteration 18844, loss = 0.02690873\n",
      "Iteration 18845, loss = 0.02690711\n",
      "Iteration 18846, loss = 0.02690553\n",
      "Iteration 18847, loss = 0.02690393\n",
      "Iteration 18848, loss = 0.02690235\n",
      "Iteration 18849, loss = 0.02690073\n",
      "Iteration 18850, loss = 0.02689915\n",
      "Iteration 18851, loss = 0.02689755\n",
      "Iteration 18852, loss = 0.02689594\n",
      "Iteration 18853, loss = 0.02689436\n",
      "Iteration 18854, loss = 0.02689275\n",
      "Iteration 18855, loss = 0.02689114\n",
      "Iteration 18856, loss = 0.02688956\n",
      "Iteration 18857, loss = 0.02688795\n",
      "Iteration 18858, loss = 0.02688635\n",
      "Iteration 18859, loss = 0.02688475\n",
      "Iteration 18860, loss = 0.02688317\n",
      "Iteration 18861, loss = 0.02688156\n",
      "Iteration 18862, loss = 0.02687998\n",
      "Iteration 18863, loss = 0.02687838\n",
      "Iteration 18864, loss = 0.02687680\n",
      "Iteration 18865, loss = 0.02687518\n",
      "Iteration 18866, loss = 0.02687359\n",
      "Iteration 18867, loss = 0.02687200\n",
      "Iteration 18868, loss = 0.02687041\n",
      "Iteration 18869, loss = 0.02686881\n",
      "Iteration 18870, loss = 0.02686722\n",
      "Iteration 18871, loss = 0.02686562\n",
      "Iteration 18872, loss = 0.02686404\n",
      "Iteration 18873, loss = 0.02686246\n",
      "Iteration 18874, loss = 0.02686085\n",
      "Iteration 18875, loss = 0.02685927\n",
      "Iteration 18876, loss = 0.02685765\n",
      "Iteration 18877, loss = 0.02685607\n",
      "Iteration 18878, loss = 0.02685448\n",
      "Iteration 18879, loss = 0.02685288\n",
      "Iteration 18880, loss = 0.02685129\n",
      "Iteration 18881, loss = 0.02684972\n",
      "Iteration 18882, loss = 0.02684811\n",
      "Iteration 18883, loss = 0.02684652\n",
      "Iteration 18884, loss = 0.02684493\n",
      "Iteration 18885, loss = 0.02684332\n",
      "Iteration 18886, loss = 0.02684174\n",
      "Iteration 18887, loss = 0.02684015\n",
      "Iteration 18888, loss = 0.02683856\n",
      "Iteration 18889, loss = 0.02683697\n",
      "Iteration 18890, loss = 0.02683538\n",
      "Iteration 18891, loss = 0.02683380\n",
      "Iteration 18892, loss = 0.02683219\n",
      "Iteration 18893, loss = 0.02683062\n",
      "Iteration 18894, loss = 0.02682902\n",
      "Iteration 18895, loss = 0.02682744\n",
      "Iteration 18896, loss = 0.02682585\n",
      "Iteration 18897, loss = 0.02682426\n",
      "Iteration 18898, loss = 0.02682267\n",
      "Iteration 18899, loss = 0.02682110\n",
      "Iteration 18900, loss = 0.02681950\n",
      "Iteration 18901, loss = 0.02681794\n",
      "Iteration 18902, loss = 0.02681634\n",
      "Iteration 18903, loss = 0.02681475\n",
      "Iteration 18904, loss = 0.02681318\n",
      "Iteration 18905, loss = 0.02681158\n",
      "Iteration 18906, loss = 0.02681000\n",
      "Iteration 18907, loss = 0.02680844\n",
      "Iteration 18908, loss = 0.02680684\n",
      "Iteration 18909, loss = 0.02680528\n",
      "Iteration 18910, loss = 0.02680370\n",
      "Iteration 18911, loss = 0.02680212\n",
      "Iteration 18912, loss = 0.02680056\n",
      "Iteration 18913, loss = 0.02679895\n",
      "Iteration 18914, loss = 0.02679738\n",
      "Iteration 18915, loss = 0.02679580\n",
      "Iteration 18916, loss = 0.02679422\n",
      "Iteration 18917, loss = 0.02679265\n",
      "Iteration 18918, loss = 0.02679107\n",
      "Iteration 18919, loss = 0.02678948\n",
      "Iteration 18920, loss = 0.02678792\n",
      "Iteration 18921, loss = 0.02678632\n",
      "Iteration 18922, loss = 0.02678475\n",
      "Iteration 18923, loss = 0.02678316\n",
      "Iteration 18924, loss = 0.02678159\n",
      "Iteration 18925, loss = 0.02678000\n",
      "Iteration 18926, loss = 0.02677842\n",
      "Iteration 18927, loss = 0.02677685\n",
      "Iteration 18928, loss = 0.02677527\n",
      "Iteration 18929, loss = 0.02677369\n",
      "Iteration 18930, loss = 0.02677212\n",
      "Iteration 18931, loss = 0.02677053\n",
      "Iteration 18932, loss = 0.02676894\n",
      "Iteration 18933, loss = 0.02676737\n",
      "Iteration 18934, loss = 0.02676582\n",
      "Iteration 18935, loss = 0.02676421\n",
      "Iteration 18936, loss = 0.02676262\n",
      "Iteration 18937, loss = 0.02676104\n",
      "Iteration 18938, loss = 0.02675949\n",
      "Iteration 18939, loss = 0.02675789\n",
      "Iteration 18940, loss = 0.02675633\n",
      "Iteration 18941, loss = 0.02675475\n",
      "Iteration 18942, loss = 0.02675316\n",
      "Iteration 18943, loss = 0.02675160\n",
      "Iteration 18944, loss = 0.02675000\n",
      "Iteration 18945, loss = 0.02674845\n",
      "Iteration 18946, loss = 0.02674687\n",
      "Iteration 18947, loss = 0.02674528\n",
      "Iteration 18948, loss = 0.02674371\n",
      "Iteration 18949, loss = 0.02674213\n",
      "Iteration 18950, loss = 0.02674055\n",
      "Iteration 18951, loss = 0.02673897\n",
      "Iteration 18952, loss = 0.02673739\n",
      "Iteration 18953, loss = 0.02673584\n",
      "Iteration 18954, loss = 0.02673426\n",
      "Iteration 18955, loss = 0.02673266\n",
      "Iteration 18956, loss = 0.02673111\n",
      "Iteration 18957, loss = 0.02672952\n",
      "Iteration 18958, loss = 0.02672794\n",
      "Iteration 18959, loss = 0.02672638\n",
      "Iteration 18960, loss = 0.02672479\n",
      "Iteration 18961, loss = 0.02672322\n",
      "Iteration 18962, loss = 0.02672164\n",
      "Iteration 18963, loss = 0.02672006\n",
      "Iteration 18964, loss = 0.02671849\n",
      "Iteration 18965, loss = 0.02671691\n",
      "Iteration 18966, loss = 0.02671536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18967, loss = 0.02671378\n",
      "Iteration 18968, loss = 0.02671220\n",
      "Iteration 18969, loss = 0.02671062\n",
      "Iteration 18970, loss = 0.02670905\n",
      "Iteration 18971, loss = 0.02670749\n",
      "Iteration 18972, loss = 0.02670590\n",
      "Iteration 18973, loss = 0.02670436\n",
      "Iteration 18974, loss = 0.02670276\n",
      "Iteration 18975, loss = 0.02670120\n",
      "Iteration 18976, loss = 0.02669961\n",
      "Iteration 18977, loss = 0.02669804\n",
      "Iteration 18978, loss = 0.02669648\n",
      "Iteration 18979, loss = 0.02669490\n",
      "Iteration 18980, loss = 0.02669332\n",
      "Iteration 18981, loss = 0.02669174\n",
      "Iteration 18982, loss = 0.02669017\n",
      "Iteration 18983, loss = 0.02668862\n",
      "Iteration 18984, loss = 0.02668704\n",
      "Iteration 18985, loss = 0.02668545\n",
      "Iteration 18986, loss = 0.02668387\n",
      "Iteration 18987, loss = 0.02668230\n",
      "Iteration 18988, loss = 0.02668074\n",
      "Iteration 18989, loss = 0.02667917\n",
      "Iteration 18990, loss = 0.02667758\n",
      "Iteration 18991, loss = 0.02667601\n",
      "Iteration 18992, loss = 0.02667445\n",
      "Iteration 18993, loss = 0.02667286\n",
      "Iteration 18994, loss = 0.02667132\n",
      "Iteration 18995, loss = 0.02666973\n",
      "Iteration 18996, loss = 0.02666816\n",
      "Iteration 18997, loss = 0.02666657\n",
      "Iteration 18998, loss = 0.02666504\n",
      "Iteration 18999, loss = 0.02666345\n",
      "Iteration 19000, loss = 0.02666189\n",
      "Iteration 19001, loss = 0.02666031\n",
      "Iteration 19002, loss = 0.02665876\n",
      "Iteration 19003, loss = 0.02665719\n",
      "Iteration 19004, loss = 0.02665562\n",
      "Iteration 19005, loss = 0.02665405\n",
      "Iteration 19006, loss = 0.02665249\n",
      "Iteration 19007, loss = 0.02665093\n",
      "Iteration 19008, loss = 0.02664935\n",
      "Iteration 19009, loss = 0.02664777\n",
      "Iteration 19010, loss = 0.02664623\n",
      "Iteration 19011, loss = 0.02664467\n",
      "Iteration 19012, loss = 0.02664309\n",
      "Iteration 19013, loss = 0.02664156\n",
      "Iteration 19014, loss = 0.02664000\n",
      "Iteration 19015, loss = 0.02663843\n",
      "Iteration 19016, loss = 0.02663687\n",
      "Iteration 19017, loss = 0.02663529\n",
      "Iteration 19018, loss = 0.02663374\n",
      "Iteration 19019, loss = 0.02663217\n",
      "Iteration 19020, loss = 0.02663061\n",
      "Iteration 19021, loss = 0.02662905\n",
      "Iteration 19022, loss = 0.02662750\n",
      "Iteration 19023, loss = 0.02662595\n",
      "Iteration 19024, loss = 0.02662437\n",
      "Iteration 19025, loss = 0.02662282\n",
      "Iteration 19026, loss = 0.02662125\n",
      "Iteration 19027, loss = 0.02661969\n",
      "Iteration 19028, loss = 0.02661812\n",
      "Iteration 19029, loss = 0.02661656\n",
      "Iteration 19030, loss = 0.02661500\n",
      "Iteration 19031, loss = 0.02661343\n",
      "Iteration 19032, loss = 0.02661188\n",
      "Iteration 19033, loss = 0.02661032\n",
      "Iteration 19034, loss = 0.02660874\n",
      "Iteration 19035, loss = 0.02660717\n",
      "Iteration 19036, loss = 0.02660563\n",
      "Iteration 19037, loss = 0.02660405\n",
      "Iteration 19038, loss = 0.02660250\n",
      "Iteration 19039, loss = 0.02660093\n",
      "Iteration 19040, loss = 0.02659937\n",
      "Iteration 19041, loss = 0.02659780\n",
      "Iteration 19042, loss = 0.02659624\n",
      "Iteration 19043, loss = 0.02659469\n",
      "Iteration 19044, loss = 0.02659311\n",
      "Iteration 19045, loss = 0.02659157\n",
      "Iteration 19046, loss = 0.02659004\n",
      "Iteration 19047, loss = 0.02658846\n",
      "Iteration 19048, loss = 0.02658689\n",
      "Iteration 19049, loss = 0.02658535\n",
      "Iteration 19050, loss = 0.02658376\n",
      "Iteration 19051, loss = 0.02658222\n",
      "Iteration 19052, loss = 0.02658068\n",
      "Iteration 19053, loss = 0.02657911\n",
      "Iteration 19054, loss = 0.02657755\n",
      "Iteration 19055, loss = 0.02657601\n",
      "Iteration 19056, loss = 0.02657445\n",
      "Iteration 19057, loss = 0.02657287\n",
      "Iteration 19058, loss = 0.02657133\n",
      "Iteration 19059, loss = 0.02656978\n",
      "Iteration 19060, loss = 0.02656822\n",
      "Iteration 19061, loss = 0.02656668\n",
      "Iteration 19062, loss = 0.02656511\n",
      "Iteration 19063, loss = 0.02656355\n",
      "Iteration 19064, loss = 0.02656200\n",
      "Iteration 19065, loss = 0.02656045\n",
      "Iteration 19066, loss = 0.02655887\n",
      "Iteration 19067, loss = 0.02655733\n",
      "Iteration 19068, loss = 0.02655576\n",
      "Iteration 19069, loss = 0.02655421\n",
      "Iteration 19070, loss = 0.02655265\n",
      "Iteration 19071, loss = 0.02655112\n",
      "Iteration 19072, loss = 0.02654954\n",
      "Iteration 19073, loss = 0.02654798\n",
      "Iteration 19074, loss = 0.02654643\n",
      "Iteration 19075, loss = 0.02654488\n",
      "Iteration 19076, loss = 0.02654333\n",
      "Iteration 19077, loss = 0.02654179\n",
      "Iteration 19078, loss = 0.02654021\n",
      "Iteration 19079, loss = 0.02653865\n",
      "Iteration 19080, loss = 0.02653709\n",
      "Iteration 19081, loss = 0.02653552\n",
      "Iteration 19082, loss = 0.02653399\n",
      "Iteration 19083, loss = 0.02653242\n",
      "Iteration 19084, loss = 0.02653087\n",
      "Iteration 19085, loss = 0.02652934\n",
      "Iteration 19086, loss = 0.02652775\n",
      "Iteration 19087, loss = 0.02652622\n",
      "Iteration 19088, loss = 0.02652466\n",
      "Iteration 19089, loss = 0.02652313\n",
      "Iteration 19090, loss = 0.02652154\n",
      "Iteration 19091, loss = 0.02652000\n",
      "Iteration 19092, loss = 0.02651843\n",
      "Iteration 19093, loss = 0.02651688\n",
      "Iteration 19094, loss = 0.02651533\n",
      "Iteration 19095, loss = 0.02651378\n",
      "Iteration 19096, loss = 0.02651223\n",
      "Iteration 19097, loss = 0.02651070\n",
      "Iteration 19098, loss = 0.02650912\n",
      "Iteration 19099, loss = 0.02650757\n",
      "Iteration 19100, loss = 0.02650602\n",
      "Iteration 19101, loss = 0.02650451\n",
      "Iteration 19102, loss = 0.02650293\n",
      "Iteration 19103, loss = 0.02650138\n",
      "Iteration 19104, loss = 0.02649982\n",
      "Iteration 19105, loss = 0.02649829\n",
      "Iteration 19106, loss = 0.02649675\n",
      "Iteration 19107, loss = 0.02649518\n",
      "Iteration 19108, loss = 0.02649365\n",
      "Iteration 19109, loss = 0.02649210\n",
      "Iteration 19110, loss = 0.02649056\n",
      "Iteration 19111, loss = 0.02648900\n",
      "Iteration 19112, loss = 0.02648747\n",
      "Iteration 19113, loss = 0.02648592\n",
      "Iteration 19114, loss = 0.02648437\n",
      "Iteration 19115, loss = 0.02648282\n",
      "Iteration 19116, loss = 0.02648127\n",
      "Iteration 19117, loss = 0.02647971\n",
      "Iteration 19118, loss = 0.02647818\n",
      "Iteration 19119, loss = 0.02647663\n",
      "Iteration 19120, loss = 0.02647508\n",
      "Iteration 19121, loss = 0.02647354\n",
      "Iteration 19122, loss = 0.02647199\n",
      "Iteration 19123, loss = 0.02647045\n",
      "Iteration 19124, loss = 0.02646889\n",
      "Iteration 19125, loss = 0.02646735\n",
      "Iteration 19126, loss = 0.02646578\n",
      "Iteration 19127, loss = 0.02646424\n",
      "Iteration 19128, loss = 0.02646271\n",
      "Iteration 19129, loss = 0.02646116\n",
      "Iteration 19130, loss = 0.02645961\n",
      "Iteration 19131, loss = 0.02645807\n",
      "Iteration 19132, loss = 0.02645652\n",
      "Iteration 19133, loss = 0.02645497\n",
      "Iteration 19134, loss = 0.02645345\n",
      "Iteration 19135, loss = 0.02645190\n",
      "Iteration 19136, loss = 0.02645035\n",
      "Iteration 19137, loss = 0.02644880\n",
      "Iteration 19138, loss = 0.02644726\n",
      "Iteration 19139, loss = 0.02644571\n",
      "Iteration 19140, loss = 0.02644419\n",
      "Iteration 19141, loss = 0.02644264\n",
      "Iteration 19142, loss = 0.02644109\n",
      "Iteration 19143, loss = 0.02643953\n",
      "Iteration 19144, loss = 0.02643802\n",
      "Iteration 19145, loss = 0.02643646\n",
      "Iteration 19146, loss = 0.02643492\n",
      "Iteration 19147, loss = 0.02643337\n",
      "Iteration 19148, loss = 0.02643183\n",
      "Iteration 19149, loss = 0.02643029\n",
      "Iteration 19150, loss = 0.02642875\n",
      "Iteration 19151, loss = 0.02642720\n",
      "Iteration 19152, loss = 0.02642567\n",
      "Iteration 19153, loss = 0.02642411\n",
      "Iteration 19154, loss = 0.02642260\n",
      "Iteration 19155, loss = 0.02642106\n",
      "Iteration 19156, loss = 0.02641949\n",
      "Iteration 19157, loss = 0.02641796\n",
      "Iteration 19158, loss = 0.02641643\n",
      "Iteration 19159, loss = 0.02641487\n",
      "Iteration 19160, loss = 0.02641332\n",
      "Iteration 19161, loss = 0.02641178\n",
      "Iteration 19162, loss = 0.02641026\n",
      "Iteration 19163, loss = 0.02640870\n",
      "Iteration 19164, loss = 0.02640718\n",
      "Iteration 19165, loss = 0.02640563\n",
      "Iteration 19166, loss = 0.02640408\n",
      "Iteration 19167, loss = 0.02640255\n",
      "Iteration 19168, loss = 0.02640100\n",
      "Iteration 19169, loss = 0.02639950\n",
      "Iteration 19170, loss = 0.02639793\n",
      "Iteration 19171, loss = 0.02639639\n",
      "Iteration 19172, loss = 0.02639487\n",
      "Iteration 19173, loss = 0.02639334\n",
      "Iteration 19174, loss = 0.02639180\n",
      "Iteration 19175, loss = 0.02639027\n",
      "Iteration 19176, loss = 0.02638876\n",
      "Iteration 19177, loss = 0.02638721\n",
      "Iteration 19178, loss = 0.02638567\n",
      "Iteration 19179, loss = 0.02638413\n",
      "Iteration 19180, loss = 0.02638260\n",
      "Iteration 19181, loss = 0.02638107\n",
      "Iteration 19182, loss = 0.02637956\n",
      "Iteration 19183, loss = 0.02637799\n",
      "Iteration 19184, loss = 0.02637645\n",
      "Iteration 19185, loss = 0.02637493\n",
      "Iteration 19186, loss = 0.02637339\n",
      "Iteration 19187, loss = 0.02637187\n",
      "Iteration 19188, loss = 0.02637031\n",
      "Iteration 19189, loss = 0.02636879\n",
      "Iteration 19190, loss = 0.02636724\n",
      "Iteration 19191, loss = 0.02636572\n",
      "Iteration 19192, loss = 0.02636419\n",
      "Iteration 19193, loss = 0.02636265\n",
      "Iteration 19194, loss = 0.02636114\n",
      "Iteration 19195, loss = 0.02635961\n",
      "Iteration 19196, loss = 0.02635806\n",
      "Iteration 19197, loss = 0.02635653\n",
      "Iteration 19198, loss = 0.02635500\n",
      "Iteration 19199, loss = 0.02635347\n",
      "Iteration 19200, loss = 0.02635194\n",
      "Iteration 19201, loss = 0.02635042\n",
      "Iteration 19202, loss = 0.02634888\n",
      "Iteration 19203, loss = 0.02634737\n",
      "Iteration 19204, loss = 0.02634583\n",
      "Iteration 19205, loss = 0.02634430\n",
      "Iteration 19206, loss = 0.02634276\n",
      "Iteration 19207, loss = 0.02634124\n",
      "Iteration 19208, loss = 0.02633969\n",
      "Iteration 19209, loss = 0.02633817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19210, loss = 0.02633663\n",
      "Iteration 19211, loss = 0.02633511\n",
      "Iteration 19212, loss = 0.02633357\n",
      "Iteration 19213, loss = 0.02633204\n",
      "Iteration 19214, loss = 0.02633051\n",
      "Iteration 19215, loss = 0.02632900\n",
      "Iteration 19216, loss = 0.02632746\n",
      "Iteration 19217, loss = 0.02632591\n",
      "Iteration 19218, loss = 0.02632438\n",
      "Iteration 19219, loss = 0.02632287\n",
      "Iteration 19220, loss = 0.02632135\n",
      "Iteration 19221, loss = 0.02631981\n",
      "Iteration 19222, loss = 0.02631829\n",
      "Iteration 19223, loss = 0.02631676\n",
      "Iteration 19224, loss = 0.02631523\n",
      "Iteration 19225, loss = 0.02631371\n",
      "Iteration 19226, loss = 0.02631219\n",
      "Iteration 19227, loss = 0.02631066\n",
      "Iteration 19228, loss = 0.02630915\n",
      "Iteration 19229, loss = 0.02630762\n",
      "Iteration 19230, loss = 0.02630610\n",
      "Iteration 19231, loss = 0.02630458\n",
      "Iteration 19232, loss = 0.02630305\n",
      "Iteration 19233, loss = 0.02630151\n",
      "Iteration 19234, loss = 0.02630000\n",
      "Iteration 19235, loss = 0.02629848\n",
      "Iteration 19236, loss = 0.02629695\n",
      "Iteration 19237, loss = 0.02629544\n",
      "Iteration 19238, loss = 0.02629390\n",
      "Iteration 19239, loss = 0.02629239\n",
      "Iteration 19240, loss = 0.02629088\n",
      "Iteration 19241, loss = 0.02628934\n",
      "Iteration 19242, loss = 0.02628781\n",
      "Iteration 19243, loss = 0.02628630\n",
      "Iteration 19244, loss = 0.02628477\n",
      "Iteration 19245, loss = 0.02628326\n",
      "Iteration 19246, loss = 0.02628173\n",
      "Iteration 19247, loss = 0.02628019\n",
      "Iteration 19248, loss = 0.02627867\n",
      "Iteration 19249, loss = 0.02627715\n",
      "Iteration 19250, loss = 0.02627564\n",
      "Iteration 19251, loss = 0.02627412\n",
      "Iteration 19252, loss = 0.02627261\n",
      "Iteration 19253, loss = 0.02627107\n",
      "Iteration 19254, loss = 0.02626957\n",
      "Iteration 19255, loss = 0.02626807\n",
      "Iteration 19256, loss = 0.02626653\n",
      "Iteration 19257, loss = 0.02626502\n",
      "Iteration 19258, loss = 0.02626350\n",
      "Iteration 19259, loss = 0.02626197\n",
      "Iteration 19260, loss = 0.02626047\n",
      "Iteration 19261, loss = 0.02625894\n",
      "Iteration 19262, loss = 0.02625743\n",
      "Iteration 19263, loss = 0.02625592\n",
      "Iteration 19264, loss = 0.02625441\n",
      "Iteration 19265, loss = 0.02625287\n",
      "Iteration 19266, loss = 0.02625136\n",
      "Iteration 19267, loss = 0.02624985\n",
      "Iteration 19268, loss = 0.02624833\n",
      "Iteration 19269, loss = 0.02624681\n",
      "Iteration 19270, loss = 0.02624531\n",
      "Iteration 19271, loss = 0.02624376\n",
      "Iteration 19272, loss = 0.02624225\n",
      "Iteration 19273, loss = 0.02624073\n",
      "Iteration 19274, loss = 0.02623921\n",
      "Iteration 19275, loss = 0.02623770\n",
      "Iteration 19276, loss = 0.02623618\n",
      "Iteration 19277, loss = 0.02623467\n",
      "Iteration 19278, loss = 0.02623314\n",
      "Iteration 19279, loss = 0.02623163\n",
      "Iteration 19280, loss = 0.02623010\n",
      "Iteration 19281, loss = 0.02622859\n",
      "Iteration 19282, loss = 0.02622706\n",
      "Iteration 19283, loss = 0.02622555\n",
      "Iteration 19284, loss = 0.02622403\n",
      "Iteration 19285, loss = 0.02622251\n",
      "Iteration 19286, loss = 0.02622098\n",
      "Iteration 19287, loss = 0.02621949\n",
      "Iteration 19288, loss = 0.02621796\n",
      "Iteration 19289, loss = 0.02621645\n",
      "Iteration 19290, loss = 0.02621494\n",
      "Iteration 19291, loss = 0.02621343\n",
      "Iteration 19292, loss = 0.02621191\n",
      "Iteration 19293, loss = 0.02621041\n",
      "Iteration 19294, loss = 0.02620889\n",
      "Iteration 19295, loss = 0.02620739\n",
      "Iteration 19296, loss = 0.02620586\n",
      "Iteration 19297, loss = 0.02620435\n",
      "Iteration 19298, loss = 0.02620284\n",
      "Iteration 19299, loss = 0.02620136\n",
      "Iteration 19300, loss = 0.02619983\n",
      "Iteration 19301, loss = 0.02619834\n",
      "Iteration 19302, loss = 0.02619682\n",
      "Iteration 19303, loss = 0.02619531\n",
      "Iteration 19304, loss = 0.02619379\n",
      "Iteration 19305, loss = 0.02619230\n",
      "Iteration 19306, loss = 0.02619078\n",
      "Iteration 19307, loss = 0.02618929\n",
      "Iteration 19308, loss = 0.02618778\n",
      "Iteration 19309, loss = 0.02618626\n",
      "Iteration 19310, loss = 0.02618472\n",
      "Iteration 19311, loss = 0.02618322\n",
      "Iteration 19312, loss = 0.02618173\n",
      "Iteration 19313, loss = 0.02618020\n",
      "Iteration 19314, loss = 0.02617870\n",
      "Iteration 19315, loss = 0.02617719\n",
      "Iteration 19316, loss = 0.02617569\n",
      "Iteration 19317, loss = 0.02617415\n",
      "Iteration 19318, loss = 0.02617265\n",
      "Iteration 19319, loss = 0.02617114\n",
      "Iteration 19320, loss = 0.02616961\n",
      "Iteration 19321, loss = 0.02616811\n",
      "Iteration 19322, loss = 0.02616657\n",
      "Iteration 19323, loss = 0.02616508\n",
      "Iteration 19324, loss = 0.02616355\n",
      "Iteration 19325, loss = 0.02616203\n",
      "Iteration 19326, loss = 0.02616054\n",
      "Iteration 19327, loss = 0.02615902\n",
      "Iteration 19328, loss = 0.02615749\n",
      "Iteration 19329, loss = 0.02615599\n",
      "Iteration 19330, loss = 0.02615446\n",
      "Iteration 19331, loss = 0.02615297\n",
      "Iteration 19332, loss = 0.02615145\n",
      "Iteration 19333, loss = 0.02614995\n",
      "Iteration 19334, loss = 0.02614842\n",
      "Iteration 19335, loss = 0.02614691\n",
      "Iteration 19336, loss = 0.02614539\n",
      "Iteration 19337, loss = 0.02614392\n",
      "Iteration 19338, loss = 0.02614240\n",
      "Iteration 19339, loss = 0.02614090\n",
      "Iteration 19340, loss = 0.02613937\n",
      "Iteration 19341, loss = 0.02613787\n",
      "Iteration 19342, loss = 0.02613638\n",
      "Iteration 19343, loss = 0.02613486\n",
      "Iteration 19344, loss = 0.02613338\n",
      "Iteration 19345, loss = 0.02613186\n",
      "Iteration 19346, loss = 0.02613035\n",
      "Iteration 19347, loss = 0.02612886\n",
      "Iteration 19348, loss = 0.02612734\n",
      "Iteration 19349, loss = 0.02612584\n",
      "Iteration 19350, loss = 0.02612433\n",
      "Iteration 19351, loss = 0.02612284\n",
      "Iteration 19352, loss = 0.02612133\n",
      "Iteration 19353, loss = 0.02611982\n",
      "Iteration 19354, loss = 0.02611834\n",
      "Iteration 19355, loss = 0.02611684\n",
      "Iteration 19356, loss = 0.02611533\n",
      "Iteration 19357, loss = 0.02611384\n",
      "Iteration 19358, loss = 0.02611232\n",
      "Iteration 19359, loss = 0.02611084\n",
      "Iteration 19360, loss = 0.02610934\n",
      "Iteration 19361, loss = 0.02610785\n",
      "Iteration 19362, loss = 0.02610634\n",
      "Iteration 19363, loss = 0.02610486\n",
      "Iteration 19364, loss = 0.02610334\n",
      "Iteration 19365, loss = 0.02610186\n",
      "Iteration 19366, loss = 0.02610036\n",
      "Iteration 19367, loss = 0.02609884\n",
      "Iteration 19368, loss = 0.02609734\n",
      "Iteration 19369, loss = 0.02609585\n",
      "Iteration 19370, loss = 0.02609435\n",
      "Iteration 19371, loss = 0.02609284\n",
      "Iteration 19372, loss = 0.02609137\n",
      "Iteration 19373, loss = 0.02608985\n",
      "Iteration 19374, loss = 0.02608835\n",
      "Iteration 19375, loss = 0.02608683\n",
      "Iteration 19376, loss = 0.02608534\n",
      "Iteration 19377, loss = 0.02608384\n",
      "Iteration 19378, loss = 0.02608235\n",
      "Iteration 19379, loss = 0.02608085\n",
      "Iteration 19380, loss = 0.02607933\n",
      "Iteration 19381, loss = 0.02607784\n",
      "Iteration 19382, loss = 0.02607636\n",
      "Iteration 19383, loss = 0.02607488\n",
      "Iteration 19384, loss = 0.02607335\n",
      "Iteration 19385, loss = 0.02607184\n",
      "Iteration 19386, loss = 0.02607034\n",
      "Iteration 19387, loss = 0.02606886\n",
      "Iteration 19388, loss = 0.02606735\n",
      "Iteration 19389, loss = 0.02606588\n",
      "Iteration 19390, loss = 0.02606438\n",
      "Iteration 19391, loss = 0.02606288\n",
      "Iteration 19392, loss = 0.02606138\n",
      "Iteration 19393, loss = 0.02605990\n",
      "Iteration 19394, loss = 0.02605840\n",
      "Iteration 19395, loss = 0.02605689\n",
      "Iteration 19396, loss = 0.02605539\n",
      "Iteration 19397, loss = 0.02605391\n",
      "Iteration 19398, loss = 0.02605240\n",
      "Iteration 19399, loss = 0.02605093\n",
      "Iteration 19400, loss = 0.02604941\n",
      "Iteration 19401, loss = 0.02604793\n",
      "Iteration 19402, loss = 0.02604642\n",
      "Iteration 19403, loss = 0.02604495\n",
      "Iteration 19404, loss = 0.02604345\n",
      "Iteration 19405, loss = 0.02604195\n",
      "Iteration 19406, loss = 0.02604045\n",
      "Iteration 19407, loss = 0.02603896\n",
      "Iteration 19408, loss = 0.02603748\n",
      "Iteration 19409, loss = 0.02603597\n",
      "Iteration 19410, loss = 0.02603448\n",
      "Iteration 19411, loss = 0.02603301\n",
      "Iteration 19412, loss = 0.02603149\n",
      "Iteration 19413, loss = 0.02603001\n",
      "Iteration 19414, loss = 0.02602853\n",
      "Iteration 19415, loss = 0.02602704\n",
      "Iteration 19416, loss = 0.02602554\n",
      "Iteration 19417, loss = 0.02602405\n",
      "Iteration 19418, loss = 0.02602256\n",
      "Iteration 19419, loss = 0.02602108\n",
      "Iteration 19420, loss = 0.02601956\n",
      "Iteration 19421, loss = 0.02601808\n",
      "Iteration 19422, loss = 0.02601659\n",
      "Iteration 19423, loss = 0.02601510\n",
      "Iteration 19424, loss = 0.02601361\n",
      "Iteration 19425, loss = 0.02601211\n",
      "Iteration 19426, loss = 0.02601063\n",
      "Iteration 19427, loss = 0.02600913\n",
      "Iteration 19428, loss = 0.02600766\n",
      "Iteration 19429, loss = 0.02600615\n",
      "Iteration 19430, loss = 0.02600467\n",
      "Iteration 19431, loss = 0.02600318\n",
      "Iteration 19432, loss = 0.02600170\n",
      "Iteration 19433, loss = 0.02600020\n",
      "Iteration 19434, loss = 0.02599871\n",
      "Iteration 19435, loss = 0.02599723\n",
      "Iteration 19436, loss = 0.02599575\n",
      "Iteration 19437, loss = 0.02599426\n",
      "Iteration 19438, loss = 0.02599275\n",
      "Iteration 19439, loss = 0.02599129\n",
      "Iteration 19440, loss = 0.02598977\n",
      "Iteration 19441, loss = 0.02598829\n",
      "Iteration 19442, loss = 0.02598681\n",
      "Iteration 19443, loss = 0.02598530\n",
      "Iteration 19444, loss = 0.02598381\n",
      "Iteration 19445, loss = 0.02598233\n",
      "Iteration 19446, loss = 0.02598084\n",
      "Iteration 19447, loss = 0.02597936\n",
      "Iteration 19448, loss = 0.02597786\n",
      "Iteration 19449, loss = 0.02597637\n",
      "Iteration 19450, loss = 0.02597489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19451, loss = 0.02597341\n",
      "Iteration 19452, loss = 0.02597192\n",
      "Iteration 19453, loss = 0.02597043\n",
      "Iteration 19454, loss = 0.02596893\n",
      "Iteration 19455, loss = 0.02596745\n",
      "Iteration 19456, loss = 0.02596598\n",
      "Iteration 19457, loss = 0.02596449\n",
      "Iteration 19458, loss = 0.02596300\n",
      "Iteration 19459, loss = 0.02596153\n",
      "Iteration 19460, loss = 0.02596004\n",
      "Iteration 19461, loss = 0.02595856\n",
      "Iteration 19462, loss = 0.02595706\n",
      "Iteration 19463, loss = 0.02595558\n",
      "Iteration 19464, loss = 0.02595410\n",
      "Iteration 19465, loss = 0.02595262\n",
      "Iteration 19466, loss = 0.02595113\n",
      "Iteration 19467, loss = 0.02594964\n",
      "Iteration 19468, loss = 0.02594816\n",
      "Iteration 19469, loss = 0.02594670\n",
      "Iteration 19470, loss = 0.02594516\n",
      "Iteration 19471, loss = 0.02594370\n",
      "Iteration 19472, loss = 0.02594220\n",
      "Iteration 19473, loss = 0.02594072\n",
      "Iteration 19474, loss = 0.02593924\n",
      "Iteration 19475, loss = 0.02593775\n",
      "Iteration 19476, loss = 0.02593626\n",
      "Iteration 19477, loss = 0.02593477\n",
      "Iteration 19478, loss = 0.02593329\n",
      "Iteration 19479, loss = 0.02593182\n",
      "Iteration 19480, loss = 0.02593034\n",
      "Iteration 19481, loss = 0.02592884\n",
      "Iteration 19482, loss = 0.02592734\n",
      "Iteration 19483, loss = 0.02592588\n",
      "Iteration 19484, loss = 0.02592439\n",
      "Iteration 19485, loss = 0.02592292\n",
      "Iteration 19486, loss = 0.02592141\n",
      "Iteration 19487, loss = 0.02591994\n",
      "Iteration 19488, loss = 0.02591848\n",
      "Iteration 19489, loss = 0.02591698\n",
      "Iteration 19490, loss = 0.02591549\n",
      "Iteration 19491, loss = 0.02591403\n",
      "Iteration 19492, loss = 0.02591253\n",
      "Iteration 19493, loss = 0.02591107\n",
      "Iteration 19494, loss = 0.02590958\n",
      "Iteration 19495, loss = 0.02590809\n",
      "Iteration 19496, loss = 0.02590662\n",
      "Iteration 19497, loss = 0.02590515\n",
      "Iteration 19498, loss = 0.02590363\n",
      "Iteration 19499, loss = 0.02590218\n",
      "Iteration 19500, loss = 0.02590071\n",
      "Iteration 19501, loss = 0.02589923\n",
      "Iteration 19502, loss = 0.02589772\n",
      "Iteration 19503, loss = 0.02589625\n",
      "Iteration 19504, loss = 0.02589478\n",
      "Iteration 19505, loss = 0.02589330\n",
      "Iteration 19506, loss = 0.02589182\n",
      "Iteration 19507, loss = 0.02589033\n",
      "Iteration 19508, loss = 0.02588886\n",
      "Iteration 19509, loss = 0.02588737\n",
      "Iteration 19510, loss = 0.02588591\n",
      "Iteration 19511, loss = 0.02588443\n",
      "Iteration 19512, loss = 0.02588295\n",
      "Iteration 19513, loss = 0.02588149\n",
      "Iteration 19514, loss = 0.02587999\n",
      "Iteration 19515, loss = 0.02587855\n",
      "Iteration 19516, loss = 0.02587705\n",
      "Iteration 19517, loss = 0.02587557\n",
      "Iteration 19518, loss = 0.02587411\n",
      "Iteration 19519, loss = 0.02587263\n",
      "Iteration 19520, loss = 0.02587114\n",
      "Iteration 19521, loss = 0.02586967\n",
      "Iteration 19522, loss = 0.02586819\n",
      "Iteration 19523, loss = 0.02586674\n",
      "Iteration 19524, loss = 0.02586525\n",
      "Iteration 19525, loss = 0.02586376\n",
      "Iteration 19526, loss = 0.02586230\n",
      "Iteration 19527, loss = 0.02586082\n",
      "Iteration 19528, loss = 0.02585934\n",
      "Iteration 19529, loss = 0.02585788\n",
      "Iteration 19530, loss = 0.02585640\n",
      "Iteration 19531, loss = 0.02585492\n",
      "Iteration 19532, loss = 0.02585349\n",
      "Iteration 19533, loss = 0.02585198\n",
      "Iteration 19534, loss = 0.02585052\n",
      "Iteration 19535, loss = 0.02584904\n",
      "Iteration 19536, loss = 0.02584758\n",
      "Iteration 19537, loss = 0.02584610\n",
      "Iteration 19538, loss = 0.02584463\n",
      "Iteration 19539, loss = 0.02584315\n",
      "Iteration 19540, loss = 0.02584168\n",
      "Iteration 19541, loss = 0.02584022\n",
      "Iteration 19542, loss = 0.02583874\n",
      "Iteration 19543, loss = 0.02583727\n",
      "Iteration 19544, loss = 0.02583580\n",
      "Iteration 19545, loss = 0.02583431\n",
      "Iteration 19546, loss = 0.02583286\n",
      "Iteration 19547, loss = 0.02583139\n",
      "Iteration 19548, loss = 0.02582992\n",
      "Iteration 19549, loss = 0.02582844\n",
      "Iteration 19550, loss = 0.02582696\n",
      "Iteration 19551, loss = 0.02582551\n",
      "Iteration 19552, loss = 0.02582404\n",
      "Iteration 19553, loss = 0.02582258\n",
      "Iteration 19554, loss = 0.02582109\n",
      "Iteration 19555, loss = 0.02581963\n",
      "Iteration 19556, loss = 0.02581816\n",
      "Iteration 19557, loss = 0.02581671\n",
      "Iteration 19558, loss = 0.02581523\n",
      "Iteration 19559, loss = 0.02581377\n",
      "Iteration 19560, loss = 0.02581230\n",
      "Iteration 19561, loss = 0.02581083\n",
      "Iteration 19562, loss = 0.02580937\n",
      "Iteration 19563, loss = 0.02580791\n",
      "Iteration 19564, loss = 0.02580643\n",
      "Iteration 19565, loss = 0.02580496\n",
      "Iteration 19566, loss = 0.02580351\n",
      "Iteration 19567, loss = 0.02580202\n",
      "Iteration 19568, loss = 0.02580057\n",
      "Iteration 19569, loss = 0.02579910\n",
      "Iteration 19570, loss = 0.02579762\n",
      "Iteration 19571, loss = 0.02579616\n",
      "Iteration 19572, loss = 0.02579468\n",
      "Iteration 19573, loss = 0.02579323\n",
      "Iteration 19574, loss = 0.02579175\n",
      "Iteration 19575, loss = 0.02579029\n",
      "Iteration 19576, loss = 0.02578883\n",
      "Iteration 19577, loss = 0.02578734\n",
      "Iteration 19578, loss = 0.02578588\n",
      "Iteration 19579, loss = 0.02578442\n",
      "Iteration 19580, loss = 0.02578293\n",
      "Iteration 19581, loss = 0.02578147\n",
      "Iteration 19582, loss = 0.02578002\n",
      "Iteration 19583, loss = 0.02577854\n",
      "Iteration 19584, loss = 0.02577709\n",
      "Iteration 19585, loss = 0.02577561\n",
      "Iteration 19586, loss = 0.02577417\n",
      "Iteration 19587, loss = 0.02577268\n",
      "Iteration 19588, loss = 0.02577123\n",
      "Iteration 19589, loss = 0.02576976\n",
      "Iteration 19590, loss = 0.02576830\n",
      "Iteration 19591, loss = 0.02576683\n",
      "Iteration 19592, loss = 0.02576538\n",
      "Iteration 19593, loss = 0.02576391\n",
      "Iteration 19594, loss = 0.02576244\n",
      "Iteration 19595, loss = 0.02576098\n",
      "Iteration 19596, loss = 0.02575953\n",
      "Iteration 19597, loss = 0.02575808\n",
      "Iteration 19598, loss = 0.02575660\n",
      "Iteration 19599, loss = 0.02575515\n",
      "Iteration 19600, loss = 0.02575370\n",
      "Iteration 19601, loss = 0.02575223\n",
      "Iteration 19602, loss = 0.02575077\n",
      "Iteration 19603, loss = 0.02574930\n",
      "Iteration 19604, loss = 0.02574785\n",
      "Iteration 19605, loss = 0.02574639\n",
      "Iteration 19606, loss = 0.02574494\n",
      "Iteration 19607, loss = 0.02574347\n",
      "Iteration 19608, loss = 0.02574202\n",
      "Iteration 19609, loss = 0.02574054\n",
      "Iteration 19610, loss = 0.02573911\n",
      "Iteration 19611, loss = 0.02573765\n",
      "Iteration 19612, loss = 0.02573618\n",
      "Iteration 19613, loss = 0.02573472\n",
      "Iteration 19614, loss = 0.02573325\n",
      "Iteration 19615, loss = 0.02573181\n",
      "Iteration 19616, loss = 0.02573034\n",
      "Iteration 19617, loss = 0.02572889\n",
      "Iteration 19618, loss = 0.02572740\n",
      "Iteration 19619, loss = 0.02572596\n",
      "Iteration 19620, loss = 0.02572450\n",
      "Iteration 19621, loss = 0.02572303\n",
      "Iteration 19622, loss = 0.02572155\n",
      "Iteration 19623, loss = 0.02572009\n",
      "Iteration 19624, loss = 0.02571863\n",
      "Iteration 19625, loss = 0.02571717\n",
      "Iteration 19626, loss = 0.02571571\n",
      "Iteration 19627, loss = 0.02571427\n",
      "Iteration 19628, loss = 0.02571279\n",
      "Iteration 19629, loss = 0.02571136\n",
      "Iteration 19630, loss = 0.02570988\n",
      "Iteration 19631, loss = 0.02570843\n",
      "Iteration 19632, loss = 0.02570695\n",
      "Iteration 19633, loss = 0.02570550\n",
      "Iteration 19634, loss = 0.02570404\n",
      "Iteration 19635, loss = 0.02570258\n",
      "Iteration 19636, loss = 0.02570112\n",
      "Iteration 19637, loss = 0.02569967\n",
      "Iteration 19638, loss = 0.02569820\n",
      "Iteration 19639, loss = 0.02569676\n",
      "Iteration 19640, loss = 0.02569529\n",
      "Iteration 19641, loss = 0.02569383\n",
      "Iteration 19642, loss = 0.02569239\n",
      "Iteration 19643, loss = 0.02569095\n",
      "Iteration 19644, loss = 0.02568946\n",
      "Iteration 19645, loss = 0.02568803\n",
      "Iteration 19646, loss = 0.02568656\n",
      "Iteration 19647, loss = 0.02568510\n",
      "Iteration 19648, loss = 0.02568366\n",
      "Iteration 19649, loss = 0.02568220\n",
      "Iteration 19650, loss = 0.02568075\n",
      "Iteration 19651, loss = 0.02567929\n",
      "Iteration 19652, loss = 0.02567782\n",
      "Iteration 19653, loss = 0.02567637\n",
      "Iteration 19654, loss = 0.02567494\n",
      "Iteration 19655, loss = 0.02567346\n",
      "Iteration 19656, loss = 0.02567203\n",
      "Iteration 19657, loss = 0.02567056\n",
      "Iteration 19658, loss = 0.02566911\n",
      "Iteration 19659, loss = 0.02566764\n",
      "Iteration 19660, loss = 0.02566620\n",
      "Iteration 19661, loss = 0.02566475\n",
      "Iteration 19662, loss = 0.02566328\n",
      "Iteration 19663, loss = 0.02566185\n",
      "Iteration 19664, loss = 0.02566040\n",
      "Iteration 19665, loss = 0.02565892\n",
      "Iteration 19666, loss = 0.02565748\n",
      "Iteration 19667, loss = 0.02565603\n",
      "Iteration 19668, loss = 0.02565459\n",
      "Iteration 19669, loss = 0.02565311\n",
      "Iteration 19670, loss = 0.02565166\n",
      "Iteration 19671, loss = 0.02565022\n",
      "Iteration 19672, loss = 0.02564877\n",
      "Iteration 19673, loss = 0.02564731\n",
      "Iteration 19674, loss = 0.02564586\n",
      "Iteration 19675, loss = 0.02564440\n",
      "Iteration 19676, loss = 0.02564294\n",
      "Iteration 19677, loss = 0.02564150\n",
      "Iteration 19678, loss = 0.02564005\n",
      "Iteration 19679, loss = 0.02563861\n",
      "Iteration 19680, loss = 0.02563715\n",
      "Iteration 19681, loss = 0.02563571\n",
      "Iteration 19682, loss = 0.02563425\n",
      "Iteration 19683, loss = 0.02563280\n",
      "Iteration 19684, loss = 0.02563134\n",
      "Iteration 19685, loss = 0.02562990\n",
      "Iteration 19686, loss = 0.02562846\n",
      "Iteration 19687, loss = 0.02562699\n",
      "Iteration 19688, loss = 0.02562556\n",
      "Iteration 19689, loss = 0.02562409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19690, loss = 0.02562265\n",
      "Iteration 19691, loss = 0.02562121\n",
      "Iteration 19692, loss = 0.02561974\n",
      "Iteration 19693, loss = 0.02561829\n",
      "Iteration 19694, loss = 0.02561686\n",
      "Iteration 19695, loss = 0.02561542\n",
      "Iteration 19696, loss = 0.02561396\n",
      "Iteration 19697, loss = 0.02561250\n",
      "Iteration 19698, loss = 0.02561107\n",
      "Iteration 19699, loss = 0.02560961\n",
      "Iteration 19700, loss = 0.02560815\n",
      "Iteration 19701, loss = 0.02560674\n",
      "Iteration 19702, loss = 0.02560527\n",
      "Iteration 19703, loss = 0.02560383\n",
      "Iteration 19704, loss = 0.02560237\n",
      "Iteration 19705, loss = 0.02560093\n",
      "Iteration 19706, loss = 0.02559949\n",
      "Iteration 19707, loss = 0.02559806\n",
      "Iteration 19708, loss = 0.02559658\n",
      "Iteration 19709, loss = 0.02559516\n",
      "Iteration 19710, loss = 0.02559371\n",
      "Iteration 19711, loss = 0.02559229\n",
      "Iteration 19712, loss = 0.02559082\n",
      "Iteration 19713, loss = 0.02558938\n",
      "Iteration 19714, loss = 0.02558794\n",
      "Iteration 19715, loss = 0.02558650\n",
      "Iteration 19716, loss = 0.02558505\n",
      "Iteration 19717, loss = 0.02558362\n",
      "Iteration 19718, loss = 0.02558216\n",
      "Iteration 19719, loss = 0.02558071\n",
      "Iteration 19720, loss = 0.02557927\n",
      "Iteration 19721, loss = 0.02557781\n",
      "Iteration 19722, loss = 0.02557640\n",
      "Iteration 19723, loss = 0.02557493\n",
      "Iteration 19724, loss = 0.02557349\n",
      "Iteration 19725, loss = 0.02557205\n",
      "Iteration 19726, loss = 0.02557062\n",
      "Iteration 19727, loss = 0.02556919\n",
      "Iteration 19728, loss = 0.02556774\n",
      "Iteration 19729, loss = 0.02556628\n",
      "Iteration 19730, loss = 0.02556485\n",
      "Iteration 19731, loss = 0.02556342\n",
      "Iteration 19732, loss = 0.02556197\n",
      "Iteration 19733, loss = 0.02556054\n",
      "Iteration 19734, loss = 0.02555913\n",
      "Iteration 19735, loss = 0.02555768\n",
      "Iteration 19736, loss = 0.02555623\n",
      "Iteration 19737, loss = 0.02555481\n",
      "Iteration 19738, loss = 0.02555336\n",
      "Iteration 19739, loss = 0.02555193\n",
      "Iteration 19740, loss = 0.02555050\n",
      "Iteration 19741, loss = 0.02554905\n",
      "Iteration 19742, loss = 0.02554761\n",
      "Iteration 19743, loss = 0.02554618\n",
      "Iteration 19744, loss = 0.02554474\n",
      "Iteration 19745, loss = 0.02554331\n",
      "Iteration 19746, loss = 0.02554187\n",
      "Iteration 19747, loss = 0.02554043\n",
      "Iteration 19748, loss = 0.02553900\n",
      "Iteration 19749, loss = 0.02553754\n",
      "Iteration 19750, loss = 0.02553613\n",
      "Iteration 19751, loss = 0.02553469\n",
      "Iteration 19752, loss = 0.02553326\n",
      "Iteration 19753, loss = 0.02553183\n",
      "Iteration 19754, loss = 0.02553038\n",
      "Iteration 19755, loss = 0.02552895\n",
      "Iteration 19756, loss = 0.02552753\n",
      "Iteration 19757, loss = 0.02552609\n",
      "Iteration 19758, loss = 0.02552464\n",
      "Iteration 19759, loss = 0.02552322\n",
      "Iteration 19760, loss = 0.02552177\n",
      "Iteration 19761, loss = 0.02552033\n",
      "Iteration 19762, loss = 0.02551891\n",
      "Iteration 19763, loss = 0.02551746\n",
      "Iteration 19764, loss = 0.02551604\n",
      "Iteration 19765, loss = 0.02551459\n",
      "Iteration 19766, loss = 0.02551316\n",
      "Iteration 19767, loss = 0.02551173\n",
      "Iteration 19768, loss = 0.02551029\n",
      "Iteration 19769, loss = 0.02550887\n",
      "Iteration 19770, loss = 0.02550743\n",
      "Iteration 19771, loss = 0.02550600\n",
      "Iteration 19772, loss = 0.02550457\n",
      "Iteration 19773, loss = 0.02550312\n",
      "Iteration 19774, loss = 0.02550169\n",
      "Iteration 19775, loss = 0.02550026\n",
      "Iteration 19776, loss = 0.02549882\n",
      "Iteration 19777, loss = 0.02549738\n",
      "Iteration 19778, loss = 0.02549596\n",
      "Iteration 19779, loss = 0.02549452\n",
      "Iteration 19780, loss = 0.02549310\n",
      "Iteration 19781, loss = 0.02549166\n",
      "Iteration 19782, loss = 0.02549022\n",
      "Iteration 19783, loss = 0.02548878\n",
      "Iteration 19784, loss = 0.02548736\n",
      "Iteration 19785, loss = 0.02548592\n",
      "Iteration 19786, loss = 0.02548449\n",
      "Iteration 19787, loss = 0.02548308\n",
      "Iteration 19788, loss = 0.02548162\n",
      "Iteration 19789, loss = 0.02548019\n",
      "Iteration 19790, loss = 0.02547877\n",
      "Iteration 19791, loss = 0.02547732\n",
      "Iteration 19792, loss = 0.02547589\n",
      "Iteration 19793, loss = 0.02547445\n",
      "Iteration 19794, loss = 0.02547302\n",
      "Iteration 19795, loss = 0.02547160\n",
      "Iteration 19796, loss = 0.02547017\n",
      "Iteration 19797, loss = 0.02546872\n",
      "Iteration 19798, loss = 0.02546730\n",
      "Iteration 19799, loss = 0.02546587\n",
      "Iteration 19800, loss = 0.02546442\n",
      "Iteration 19801, loss = 0.02546300\n",
      "Iteration 19802, loss = 0.02546156\n",
      "Iteration 19803, loss = 0.02546016\n",
      "Iteration 19804, loss = 0.02545871\n",
      "Iteration 19805, loss = 0.02545728\n",
      "Iteration 19806, loss = 0.02545585\n",
      "Iteration 19807, loss = 0.02545442\n",
      "Iteration 19808, loss = 0.02545298\n",
      "Iteration 19809, loss = 0.02545158\n",
      "Iteration 19810, loss = 0.02545014\n",
      "Iteration 19811, loss = 0.02544873\n",
      "Iteration 19812, loss = 0.02544728\n",
      "Iteration 19813, loss = 0.02544587\n",
      "Iteration 19814, loss = 0.02544443\n",
      "Iteration 19815, loss = 0.02544300\n",
      "Iteration 19816, loss = 0.02544161\n",
      "Iteration 19817, loss = 0.02544016\n",
      "Iteration 19818, loss = 0.02543876\n",
      "Iteration 19819, loss = 0.02543730\n",
      "Iteration 19820, loss = 0.02543588\n",
      "Iteration 19821, loss = 0.02543445\n",
      "Iteration 19822, loss = 0.02543304\n",
      "Iteration 19823, loss = 0.02543159\n",
      "Iteration 19824, loss = 0.02543016\n",
      "Iteration 19825, loss = 0.02542874\n",
      "Iteration 19826, loss = 0.02542730\n",
      "Iteration 19827, loss = 0.02542590\n",
      "Iteration 19828, loss = 0.02542446\n",
      "Iteration 19829, loss = 0.02542305\n",
      "Iteration 19830, loss = 0.02542162\n",
      "Iteration 19831, loss = 0.02542021\n",
      "Iteration 19832, loss = 0.02541877\n",
      "Iteration 19833, loss = 0.02541734\n",
      "Iteration 19834, loss = 0.02541596\n",
      "Iteration 19835, loss = 0.02541452\n",
      "Iteration 19836, loss = 0.02541309\n",
      "Iteration 19837, loss = 0.02541165\n",
      "Iteration 19838, loss = 0.02541026\n",
      "Iteration 19839, loss = 0.02540883\n",
      "Iteration 19840, loss = 0.02540741\n",
      "Iteration 19841, loss = 0.02540600\n",
      "Iteration 19842, loss = 0.02540455\n",
      "Iteration 19843, loss = 0.02540313\n",
      "Iteration 19844, loss = 0.02540172\n",
      "Iteration 19845, loss = 0.02540030\n",
      "Iteration 19846, loss = 0.02539887\n",
      "Iteration 19847, loss = 0.02539744\n",
      "Iteration 19848, loss = 0.02539603\n",
      "Iteration 19849, loss = 0.02539462\n",
      "Iteration 19850, loss = 0.02539318\n",
      "Iteration 19851, loss = 0.02539177\n",
      "Iteration 19852, loss = 0.02539035\n",
      "Iteration 19853, loss = 0.02538892\n",
      "Iteration 19854, loss = 0.02538751\n",
      "Iteration 19855, loss = 0.02538608\n",
      "Iteration 19856, loss = 0.02538466\n",
      "Iteration 19857, loss = 0.02538322\n",
      "Iteration 19858, loss = 0.02538183\n",
      "Iteration 19859, loss = 0.02538041\n",
      "Iteration 19860, loss = 0.02537898\n",
      "Iteration 19861, loss = 0.02537757\n",
      "Iteration 19862, loss = 0.02537614\n",
      "Iteration 19863, loss = 0.02537473\n",
      "Iteration 19864, loss = 0.02537331\n",
      "Iteration 19865, loss = 0.02537189\n",
      "Iteration 19866, loss = 0.02537048\n",
      "Iteration 19867, loss = 0.02536905\n",
      "Iteration 19868, loss = 0.02536763\n",
      "Iteration 19869, loss = 0.02536621\n",
      "Iteration 19870, loss = 0.02536478\n",
      "Iteration 19871, loss = 0.02536338\n",
      "Iteration 19872, loss = 0.02536197\n",
      "Iteration 19873, loss = 0.02536058\n",
      "Iteration 19874, loss = 0.02535913\n",
      "Iteration 19875, loss = 0.02535770\n",
      "Iteration 19876, loss = 0.02535628\n",
      "Iteration 19877, loss = 0.02535489\n",
      "Iteration 19878, loss = 0.02535345\n",
      "Iteration 19879, loss = 0.02535203\n",
      "Iteration 19880, loss = 0.02535061\n",
      "Iteration 19881, loss = 0.02534919\n",
      "Iteration 19882, loss = 0.02534779\n",
      "Iteration 19883, loss = 0.02534636\n",
      "Iteration 19884, loss = 0.02534495\n",
      "Iteration 19885, loss = 0.02534355\n",
      "Iteration 19886, loss = 0.02534211\n",
      "Iteration 19887, loss = 0.02534071\n",
      "Iteration 19888, loss = 0.02533928\n",
      "Iteration 19889, loss = 0.02533787\n",
      "Iteration 19890, loss = 0.02533645\n",
      "Iteration 19891, loss = 0.02533503\n",
      "Iteration 19892, loss = 0.02533362\n",
      "Iteration 19893, loss = 0.02533222\n",
      "Iteration 19894, loss = 0.02533079\n",
      "Iteration 19895, loss = 0.02532939\n",
      "Iteration 19896, loss = 0.02532795\n",
      "Iteration 19897, loss = 0.02532655\n",
      "Iteration 19898, loss = 0.02532514\n",
      "Iteration 19899, loss = 0.02532372\n",
      "Iteration 19900, loss = 0.02532229\n",
      "Iteration 19901, loss = 0.02532089\n",
      "Iteration 19902, loss = 0.02531948\n",
      "Iteration 19903, loss = 0.02531807\n",
      "Iteration 19904, loss = 0.02531666\n",
      "Iteration 19905, loss = 0.02531526\n",
      "Iteration 19906, loss = 0.02531384\n",
      "Iteration 19907, loss = 0.02531242\n",
      "Iteration 19908, loss = 0.02531101\n",
      "Iteration 19909, loss = 0.02530957\n",
      "Iteration 19910, loss = 0.02530818\n",
      "Iteration 19911, loss = 0.02530677\n",
      "Iteration 19912, loss = 0.02530534\n",
      "Iteration 19913, loss = 0.02530393\n",
      "Iteration 19914, loss = 0.02530251\n",
      "Iteration 19915, loss = 0.02530110\n",
      "Iteration 19916, loss = 0.02529969\n",
      "Iteration 19917, loss = 0.02529830\n",
      "Iteration 19918, loss = 0.02529687\n",
      "Iteration 19919, loss = 0.02529547\n",
      "Iteration 19920, loss = 0.02529406\n",
      "Iteration 19921, loss = 0.02529263\n",
      "Iteration 19922, loss = 0.02529122\n",
      "Iteration 19923, loss = 0.02528983\n",
      "Iteration 19924, loss = 0.02528844\n",
      "Iteration 19925, loss = 0.02528700\n",
      "Iteration 19926, loss = 0.02528558\n",
      "Iteration 19927, loss = 0.02528421\n",
      "Iteration 19928, loss = 0.02528278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19929, loss = 0.02528136\n",
      "Iteration 19930, loss = 0.02527997\n",
      "Iteration 19931, loss = 0.02527856\n",
      "Iteration 19932, loss = 0.02527713\n",
      "Iteration 19933, loss = 0.02527574\n",
      "Iteration 19934, loss = 0.02527432\n",
      "Iteration 19935, loss = 0.02527291\n",
      "Iteration 19936, loss = 0.02527150\n",
      "Iteration 19937, loss = 0.02527010\n",
      "Iteration 19938, loss = 0.02526868\n",
      "Iteration 19939, loss = 0.02526727\n",
      "Iteration 19940, loss = 0.02526585\n",
      "Iteration 19941, loss = 0.02526446\n",
      "Iteration 19942, loss = 0.02526304\n",
      "Iteration 19943, loss = 0.02526165\n",
      "Iteration 19944, loss = 0.02526024\n",
      "Iteration 19945, loss = 0.02525882\n",
      "Iteration 19946, loss = 0.02525741\n",
      "Iteration 19947, loss = 0.02525600\n",
      "Iteration 19948, loss = 0.02525460\n",
      "Iteration 19949, loss = 0.02525318\n",
      "Iteration 19950, loss = 0.02525179\n",
      "Iteration 19951, loss = 0.02525038\n",
      "Iteration 19952, loss = 0.02524897\n",
      "Iteration 19953, loss = 0.02524758\n",
      "Iteration 19954, loss = 0.02524616\n",
      "Iteration 19955, loss = 0.02524474\n",
      "Iteration 19956, loss = 0.02524335\n",
      "Iteration 19957, loss = 0.02524194\n",
      "Iteration 19958, loss = 0.02524055\n",
      "Iteration 19959, loss = 0.02523914\n",
      "Iteration 19960, loss = 0.02523773\n",
      "Iteration 19961, loss = 0.02523633\n",
      "Iteration 19962, loss = 0.02523492\n",
      "Iteration 19963, loss = 0.02523352\n",
      "Iteration 19964, loss = 0.02523211\n",
      "Iteration 19965, loss = 0.02523072\n",
      "Iteration 19966, loss = 0.02522930\n",
      "Iteration 19967, loss = 0.02522790\n",
      "Iteration 19968, loss = 0.02522650\n",
      "Iteration 19969, loss = 0.02522511\n",
      "Iteration 19970, loss = 0.02522371\n",
      "Iteration 19971, loss = 0.02522229\n",
      "Iteration 19972, loss = 0.02522089\n",
      "Iteration 19973, loss = 0.02521950\n",
      "Iteration 19974, loss = 0.02521810\n",
      "Iteration 19975, loss = 0.02521668\n",
      "Iteration 19976, loss = 0.02521527\n",
      "Iteration 19977, loss = 0.02521388\n",
      "Iteration 19978, loss = 0.02521247\n",
      "Iteration 19979, loss = 0.02521108\n",
      "Iteration 19980, loss = 0.02520966\n",
      "Iteration 19981, loss = 0.02520827\n",
      "Iteration 19982, loss = 0.02520685\n",
      "Iteration 19983, loss = 0.02520547\n",
      "Iteration 19984, loss = 0.02520405\n",
      "Iteration 19985, loss = 0.02520265\n",
      "Iteration 19986, loss = 0.02520125\n",
      "Iteration 19987, loss = 0.02519985\n",
      "Iteration 19988, loss = 0.02519844\n",
      "Iteration 19989, loss = 0.02519704\n",
      "Iteration 19990, loss = 0.02519564\n",
      "Iteration 19991, loss = 0.02519423\n",
      "Iteration 19992, loss = 0.02519284\n",
      "Iteration 19993, loss = 0.02519144\n",
      "Iteration 19994, loss = 0.02519003\n",
      "Iteration 19995, loss = 0.02518863\n",
      "Iteration 19996, loss = 0.02518723\n",
      "Iteration 19997, loss = 0.02518583\n",
      "Iteration 19998, loss = 0.02518445\n",
      "Iteration 19999, loss = 0.02518305\n",
      "Iteration 20000, loss = 0.02518165\n",
      "Iteration 20001, loss = 0.02518023\n",
      "Iteration 20002, loss = 0.02517887\n",
      "Iteration 20003, loss = 0.02517745\n",
      "Iteration 20004, loss = 0.02517605\n",
      "Iteration 20005, loss = 0.02517464\n",
      "Iteration 20006, loss = 0.02517328\n",
      "Iteration 20007, loss = 0.02517186\n",
      "Iteration 20008, loss = 0.02517046\n",
      "Iteration 20009, loss = 0.02516906\n",
      "Iteration 20010, loss = 0.02516767\n",
      "Iteration 20011, loss = 0.02516628\n",
      "Iteration 20012, loss = 0.02516489\n",
      "Iteration 20013, loss = 0.02516348\n",
      "Iteration 20014, loss = 0.02516209\n",
      "Iteration 20015, loss = 0.02516069\n",
      "Iteration 20016, loss = 0.02515932\n",
      "Iteration 20017, loss = 0.02515791\n",
      "Iteration 20018, loss = 0.02515652\n",
      "Iteration 20019, loss = 0.02515513\n",
      "Iteration 20020, loss = 0.02515374\n",
      "Iteration 20021, loss = 0.02515234\n",
      "Iteration 20022, loss = 0.02515097\n",
      "Iteration 20023, loss = 0.02514954\n",
      "Iteration 20024, loss = 0.02514817\n",
      "Iteration 20025, loss = 0.02514677\n",
      "Iteration 20026, loss = 0.02514538\n",
      "Iteration 20027, loss = 0.02514398\n",
      "Iteration 20028, loss = 0.02514260\n",
      "Iteration 20029, loss = 0.02514120\n",
      "Iteration 20030, loss = 0.02513980\n",
      "Iteration 20031, loss = 0.02513843\n",
      "Iteration 20032, loss = 0.02513704\n",
      "Iteration 20033, loss = 0.02513561\n",
      "Iteration 20034, loss = 0.02513423\n",
      "Iteration 20035, loss = 0.02513283\n",
      "Iteration 20036, loss = 0.02513145\n",
      "Iteration 20037, loss = 0.02513004\n",
      "Iteration 20038, loss = 0.02512865\n",
      "Iteration 20039, loss = 0.02512727\n",
      "Iteration 20040, loss = 0.02512587\n",
      "Iteration 20041, loss = 0.02512447\n",
      "Iteration 20042, loss = 0.02512309\n",
      "Iteration 20043, loss = 0.02512169\n",
      "Iteration 20044, loss = 0.02512029\n",
      "Iteration 20045, loss = 0.02511889\n",
      "Iteration 20046, loss = 0.02511752\n",
      "Iteration 20047, loss = 0.02511612\n",
      "Iteration 20048, loss = 0.02511473\n",
      "Iteration 20049, loss = 0.02511334\n",
      "Iteration 20050, loss = 0.02511192\n",
      "Iteration 20051, loss = 0.02511056\n",
      "Iteration 20052, loss = 0.02510915\n",
      "Iteration 20053, loss = 0.02510776\n",
      "Iteration 20054, loss = 0.02510637\n",
      "Iteration 20055, loss = 0.02510497\n",
      "Iteration 20056, loss = 0.02510359\n",
      "Iteration 20057, loss = 0.02510220\n",
      "Iteration 20058, loss = 0.02510081\n",
      "Iteration 20059, loss = 0.02509941\n",
      "Iteration 20060, loss = 0.02509803\n",
      "Iteration 20061, loss = 0.02509664\n",
      "Iteration 20062, loss = 0.02509526\n",
      "Iteration 20063, loss = 0.02509387\n",
      "Iteration 20064, loss = 0.02509248\n",
      "Iteration 20065, loss = 0.02509109\n",
      "Iteration 20066, loss = 0.02508971\n",
      "Iteration 20067, loss = 0.02508834\n",
      "Iteration 20068, loss = 0.02508694\n",
      "Iteration 20069, loss = 0.02508554\n",
      "Iteration 20070, loss = 0.02508418\n",
      "Iteration 20071, loss = 0.02508281\n",
      "Iteration 20072, loss = 0.02508141\n",
      "Iteration 20073, loss = 0.02508002\n",
      "Iteration 20074, loss = 0.02507864\n",
      "Iteration 20075, loss = 0.02507726\n",
      "Iteration 20076, loss = 0.02507589\n",
      "Iteration 20077, loss = 0.02507451\n",
      "Iteration 20078, loss = 0.02507311\n",
      "Iteration 20079, loss = 0.02507173\n",
      "Iteration 20080, loss = 0.02507034\n",
      "Iteration 20081, loss = 0.02506898\n",
      "Iteration 20082, loss = 0.02506757\n",
      "Iteration 20083, loss = 0.02506619\n",
      "Iteration 20084, loss = 0.02506481\n",
      "Iteration 20085, loss = 0.02506342\n",
      "Iteration 20086, loss = 0.02506205\n",
      "Iteration 20087, loss = 0.02506064\n",
      "Iteration 20088, loss = 0.02505927\n",
      "Iteration 20089, loss = 0.02505788\n",
      "Iteration 20090, loss = 0.02505650\n",
      "Iteration 20091, loss = 0.02505511\n",
      "Iteration 20092, loss = 0.02505372\n",
      "Iteration 20093, loss = 0.02505234\n",
      "Iteration 20094, loss = 0.02505097\n",
      "Iteration 20095, loss = 0.02504957\n",
      "Iteration 20096, loss = 0.02504819\n",
      "Iteration 20097, loss = 0.02504682\n",
      "Iteration 20098, loss = 0.02504543\n",
      "Iteration 20099, loss = 0.02504405\n",
      "Iteration 20100, loss = 0.02504266\n",
      "Iteration 20101, loss = 0.02504126\n",
      "Iteration 20102, loss = 0.02503992\n",
      "Iteration 20103, loss = 0.02503851\n",
      "Iteration 20104, loss = 0.02503713\n",
      "Iteration 20105, loss = 0.02503572\n",
      "Iteration 20106, loss = 0.02503438\n",
      "Iteration 20107, loss = 0.02503298\n",
      "Iteration 20108, loss = 0.02503159\n",
      "Iteration 20109, loss = 0.02503022\n",
      "Iteration 20110, loss = 0.02502884\n",
      "Iteration 20111, loss = 0.02502744\n",
      "Iteration 20112, loss = 0.02502606\n",
      "Iteration 20113, loss = 0.02502468\n",
      "Iteration 20114, loss = 0.02502332\n",
      "Iteration 20115, loss = 0.02502192\n",
      "Iteration 20116, loss = 0.02502054\n",
      "Iteration 20117, loss = 0.02501916\n",
      "Iteration 20118, loss = 0.02501777\n",
      "Iteration 20119, loss = 0.02501640\n",
      "Iteration 20120, loss = 0.02501502\n",
      "Iteration 20121, loss = 0.02501363\n",
      "Iteration 20122, loss = 0.02501225\n",
      "Iteration 20123, loss = 0.02501088\n",
      "Iteration 20124, loss = 0.02500949\n",
      "Iteration 20125, loss = 0.02500811\n",
      "Iteration 20126, loss = 0.02500672\n",
      "Iteration 20127, loss = 0.02500535\n",
      "Iteration 20128, loss = 0.02500397\n",
      "Iteration 20129, loss = 0.02500259\n",
      "Iteration 20130, loss = 0.02500122\n",
      "Iteration 20131, loss = 0.02499984\n",
      "Iteration 20132, loss = 0.02499847\n",
      "Iteration 20133, loss = 0.02499709\n",
      "Iteration 20134, loss = 0.02499572\n",
      "Iteration 20135, loss = 0.02499433\n",
      "Iteration 20136, loss = 0.02499297\n",
      "Iteration 20137, loss = 0.02499159\n",
      "Iteration 20138, loss = 0.02499023\n",
      "Iteration 20139, loss = 0.02498884\n",
      "Iteration 20140, loss = 0.02498746\n",
      "Iteration 20141, loss = 0.02498610\n",
      "Iteration 20142, loss = 0.02498472\n",
      "Iteration 20143, loss = 0.02498333\n",
      "Iteration 20144, loss = 0.02498195\n",
      "Iteration 20145, loss = 0.02498059\n",
      "Iteration 20146, loss = 0.02497921\n",
      "Iteration 20147, loss = 0.02497784\n",
      "Iteration 20148, loss = 0.02497646\n",
      "Iteration 20149, loss = 0.02497510\n",
      "Iteration 20150, loss = 0.02497372\n",
      "Iteration 20151, loss = 0.02497232\n",
      "Iteration 20152, loss = 0.02497096\n",
      "Iteration 20153, loss = 0.02496958\n",
      "Iteration 20154, loss = 0.02496822\n",
      "Iteration 20155, loss = 0.02496684\n",
      "Iteration 20156, loss = 0.02496547\n",
      "Iteration 20157, loss = 0.02496410\n",
      "Iteration 20158, loss = 0.02496272\n",
      "Iteration 20159, loss = 0.02496135\n",
      "Iteration 20160, loss = 0.02495998\n",
      "Iteration 20161, loss = 0.02495860\n",
      "Iteration 20162, loss = 0.02495721\n",
      "Iteration 20163, loss = 0.02495585\n",
      "Iteration 20164, loss = 0.02495449\n",
      "Iteration 20165, loss = 0.02495311\n",
      "Iteration 20166, loss = 0.02495172\n",
      "Iteration 20167, loss = 0.02495034\n",
      "Iteration 20168, loss = 0.02494898\n",
      "Iteration 20169, loss = 0.02494761\n",
      "Iteration 20170, loss = 0.02494625\n",
      "Iteration 20171, loss = 0.02494486\n",
      "Iteration 20172, loss = 0.02494350\n",
      "Iteration 20173, loss = 0.02494212\n",
      "Iteration 20174, loss = 0.02494075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20175, loss = 0.02493939\n",
      "Iteration 20176, loss = 0.02493801\n",
      "Iteration 20177, loss = 0.02493663\n",
      "Iteration 20178, loss = 0.02493527\n",
      "Iteration 20179, loss = 0.02493391\n",
      "Iteration 20180, loss = 0.02493252\n",
      "Iteration 20181, loss = 0.02493114\n",
      "Iteration 20182, loss = 0.02492977\n",
      "Iteration 20183, loss = 0.02492842\n",
      "Iteration 20184, loss = 0.02492703\n",
      "Iteration 20185, loss = 0.02492566\n",
      "Iteration 20186, loss = 0.02492430\n",
      "Iteration 20187, loss = 0.02492293\n",
      "Iteration 20188, loss = 0.02492156\n",
      "Iteration 20189, loss = 0.02492019\n",
      "Iteration 20190, loss = 0.02491881\n",
      "Iteration 20191, loss = 0.02491745\n",
      "Iteration 20192, loss = 0.02491611\n",
      "Iteration 20193, loss = 0.02491471\n",
      "Iteration 20194, loss = 0.02491337\n",
      "Iteration 20195, loss = 0.02491199\n",
      "Iteration 20196, loss = 0.02491062\n",
      "Iteration 20197, loss = 0.02490926\n",
      "Iteration 20198, loss = 0.02490787\n",
      "Iteration 20199, loss = 0.02490652\n",
      "Iteration 20200, loss = 0.02490516\n",
      "Iteration 20201, loss = 0.02490380\n",
      "Iteration 20202, loss = 0.02490243\n",
      "Iteration 20203, loss = 0.02490105\n",
      "Iteration 20204, loss = 0.02489968\n",
      "Iteration 20205, loss = 0.02489831\n",
      "Iteration 20206, loss = 0.02489694\n",
      "Iteration 20207, loss = 0.02489559\n",
      "Iteration 20208, loss = 0.02489422\n",
      "Iteration 20209, loss = 0.02489285\n",
      "Iteration 20210, loss = 0.02489148\n",
      "Iteration 20211, loss = 0.02489012\n",
      "Iteration 20212, loss = 0.02488875\n",
      "Iteration 20213, loss = 0.02488740\n",
      "Iteration 20214, loss = 0.02488603\n",
      "Iteration 20215, loss = 0.02488466\n",
      "Iteration 20216, loss = 0.02488329\n",
      "Iteration 20217, loss = 0.02488192\n",
      "Iteration 20218, loss = 0.02488058\n",
      "Iteration 20219, loss = 0.02487921\n",
      "Iteration 20220, loss = 0.02487784\n",
      "Iteration 20221, loss = 0.02487647\n",
      "Iteration 20222, loss = 0.02487511\n",
      "Iteration 20223, loss = 0.02487375\n",
      "Iteration 20224, loss = 0.02487238\n",
      "Iteration 20225, loss = 0.02487102\n",
      "Iteration 20226, loss = 0.02486964\n",
      "Iteration 20227, loss = 0.02486830\n",
      "Iteration 20228, loss = 0.02486693\n",
      "Iteration 20229, loss = 0.02486558\n",
      "Iteration 20230, loss = 0.02486420\n",
      "Iteration 20231, loss = 0.02486285\n",
      "Iteration 20232, loss = 0.02486150\n",
      "Iteration 20233, loss = 0.02486012\n",
      "Iteration 20234, loss = 0.02485877\n",
      "Iteration 20235, loss = 0.02485741\n",
      "Iteration 20236, loss = 0.02485604\n",
      "Iteration 20237, loss = 0.02485469\n",
      "Iteration 20238, loss = 0.02485329\n",
      "Iteration 20239, loss = 0.02485195\n",
      "Iteration 20240, loss = 0.02485061\n",
      "Iteration 20241, loss = 0.02484924\n",
      "Iteration 20242, loss = 0.02484786\n",
      "Iteration 20243, loss = 0.02484648\n",
      "Iteration 20244, loss = 0.02484515\n",
      "Iteration 20245, loss = 0.02484377\n",
      "Iteration 20246, loss = 0.02484241\n",
      "Iteration 20247, loss = 0.02484105\n",
      "Iteration 20248, loss = 0.02483968\n",
      "Iteration 20249, loss = 0.02483833\n",
      "Iteration 20250, loss = 0.02483695\n",
      "Iteration 20251, loss = 0.02483559\n",
      "Iteration 20252, loss = 0.02483423\n",
      "Iteration 20253, loss = 0.02483288\n",
      "Iteration 20254, loss = 0.02483153\n",
      "Iteration 20255, loss = 0.02483016\n",
      "Iteration 20256, loss = 0.02482880\n",
      "Iteration 20257, loss = 0.02482742\n",
      "Iteration 20258, loss = 0.02482607\n",
      "Iteration 20259, loss = 0.02482471\n",
      "Iteration 20260, loss = 0.02482335\n",
      "Iteration 20261, loss = 0.02482200\n",
      "Iteration 20262, loss = 0.02482063\n",
      "Iteration 20263, loss = 0.02481928\n",
      "Iteration 20264, loss = 0.02481789\n",
      "Iteration 20265, loss = 0.02481655\n",
      "Iteration 20266, loss = 0.02481518\n",
      "Iteration 20267, loss = 0.02481383\n",
      "Iteration 20268, loss = 0.02481248\n",
      "Iteration 20269, loss = 0.02481112\n",
      "Iteration 20270, loss = 0.02480975\n",
      "Iteration 20271, loss = 0.02480838\n",
      "Iteration 20272, loss = 0.02480703\n",
      "Iteration 20273, loss = 0.02480567\n",
      "Iteration 20274, loss = 0.02480431\n",
      "Iteration 20275, loss = 0.02480295\n",
      "Iteration 20276, loss = 0.02480160\n",
      "Iteration 20277, loss = 0.02480024\n",
      "Iteration 20278, loss = 0.02479890\n",
      "Iteration 20279, loss = 0.02479752\n",
      "Iteration 20280, loss = 0.02479618\n",
      "Iteration 20281, loss = 0.02479482\n",
      "Iteration 20282, loss = 0.02479346\n",
      "Iteration 20283, loss = 0.02479211\n",
      "Iteration 20284, loss = 0.02479075\n",
      "Iteration 20285, loss = 0.02478938\n",
      "Iteration 20286, loss = 0.02478804\n",
      "Iteration 20287, loss = 0.02478668\n",
      "Iteration 20288, loss = 0.02478532\n",
      "Iteration 20289, loss = 0.02478397\n",
      "Iteration 20290, loss = 0.02478260\n",
      "Iteration 20291, loss = 0.02478126\n",
      "Iteration 20292, loss = 0.02477991\n",
      "Iteration 20293, loss = 0.02477853\n",
      "Iteration 20294, loss = 0.02477719\n",
      "Iteration 20295, loss = 0.02477582\n",
      "Iteration 20296, loss = 0.02477448\n",
      "Iteration 20297, loss = 0.02477313\n",
      "Iteration 20298, loss = 0.02477177\n",
      "Iteration 20299, loss = 0.02477043\n",
      "Iteration 20300, loss = 0.02476906\n",
      "Iteration 20301, loss = 0.02476769\n",
      "Iteration 20302, loss = 0.02476637\n",
      "Iteration 20303, loss = 0.02476500\n",
      "Iteration 20304, loss = 0.02476363\n",
      "Iteration 20305, loss = 0.02476229\n",
      "Iteration 20306, loss = 0.02476094\n",
      "Iteration 20307, loss = 0.02475957\n",
      "Iteration 20308, loss = 0.02475823\n",
      "Iteration 20309, loss = 0.02475688\n",
      "Iteration 20310, loss = 0.02475553\n",
      "Iteration 20311, loss = 0.02475418\n",
      "Iteration 20312, loss = 0.02475282\n",
      "Iteration 20313, loss = 0.02475147\n",
      "Iteration 20314, loss = 0.02475012\n",
      "Iteration 20315, loss = 0.02474879\n",
      "Iteration 20316, loss = 0.02474742\n",
      "Iteration 20317, loss = 0.02474607\n",
      "Iteration 20318, loss = 0.02474472\n",
      "Iteration 20319, loss = 0.02474337\n",
      "Iteration 20320, loss = 0.02474201\n",
      "Iteration 20321, loss = 0.02474069\n",
      "Iteration 20322, loss = 0.02473932\n",
      "Iteration 20323, loss = 0.02473797\n",
      "Iteration 20324, loss = 0.02473661\n",
      "Iteration 20325, loss = 0.02473526\n",
      "Iteration 20326, loss = 0.02473392\n",
      "Iteration 20327, loss = 0.02473259\n",
      "Iteration 20328, loss = 0.02473122\n",
      "Iteration 20329, loss = 0.02472987\n",
      "Iteration 20330, loss = 0.02472852\n",
      "Iteration 20331, loss = 0.02472718\n",
      "Iteration 20332, loss = 0.02472583\n",
      "Iteration 20333, loss = 0.02472448\n",
      "Iteration 20334, loss = 0.02472312\n",
      "Iteration 20335, loss = 0.02472179\n",
      "Iteration 20336, loss = 0.02472045\n",
      "Iteration 20337, loss = 0.02471909\n",
      "Iteration 20338, loss = 0.02471774\n",
      "Iteration 20339, loss = 0.02471642\n",
      "Iteration 20340, loss = 0.02471506\n",
      "Iteration 20341, loss = 0.02471371\n",
      "Iteration 20342, loss = 0.02471236\n",
      "Iteration 20343, loss = 0.02471102\n",
      "Iteration 20344, loss = 0.02470967\n",
      "Iteration 20345, loss = 0.02470833\n",
      "Iteration 20346, loss = 0.02470700\n",
      "Iteration 20347, loss = 0.02470566\n",
      "Iteration 20348, loss = 0.02470433\n",
      "Iteration 20349, loss = 0.02470296\n",
      "Iteration 20350, loss = 0.02470163\n",
      "Iteration 20351, loss = 0.02470028\n",
      "Iteration 20352, loss = 0.02469893\n",
      "Iteration 20353, loss = 0.02469759\n",
      "Iteration 20354, loss = 0.02469624\n",
      "Iteration 20355, loss = 0.02469490\n",
      "Iteration 20356, loss = 0.02469356\n",
      "Iteration 20357, loss = 0.02469221\n",
      "Iteration 20358, loss = 0.02469086\n",
      "Iteration 20359, loss = 0.02468953\n",
      "Iteration 20360, loss = 0.02468818\n",
      "Iteration 20361, loss = 0.02468684\n",
      "Iteration 20362, loss = 0.02468550\n",
      "Iteration 20363, loss = 0.02468415\n",
      "Iteration 20364, loss = 0.02468282\n",
      "Iteration 20365, loss = 0.02468147\n",
      "Iteration 20366, loss = 0.02468013\n",
      "Iteration 20367, loss = 0.02467878\n",
      "Iteration 20368, loss = 0.02467744\n",
      "Iteration 20369, loss = 0.02467611\n",
      "Iteration 20370, loss = 0.02467475\n",
      "Iteration 20371, loss = 0.02467341\n",
      "Iteration 20372, loss = 0.02467208\n",
      "Iteration 20373, loss = 0.02467072\n",
      "Iteration 20374, loss = 0.02466940\n",
      "Iteration 20375, loss = 0.02466805\n",
      "Iteration 20376, loss = 0.02466670\n",
      "Iteration 20377, loss = 0.02466536\n",
      "Iteration 20378, loss = 0.02466404\n",
      "Iteration 20379, loss = 0.02466269\n",
      "Iteration 20380, loss = 0.02466136\n",
      "Iteration 20381, loss = 0.02466001\n",
      "Iteration 20382, loss = 0.02465869\n",
      "Iteration 20383, loss = 0.02465735\n",
      "Iteration 20384, loss = 0.02465600\n",
      "Iteration 20385, loss = 0.02465467\n",
      "Iteration 20386, loss = 0.02465332\n",
      "Iteration 20387, loss = 0.02465196\n",
      "Iteration 20388, loss = 0.02465063\n",
      "Iteration 20389, loss = 0.02464928\n",
      "Iteration 20390, loss = 0.02464794\n",
      "Iteration 20391, loss = 0.02464661\n",
      "Iteration 20392, loss = 0.02464526\n",
      "Iteration 20393, loss = 0.02464391\n",
      "Iteration 20394, loss = 0.02464260\n",
      "Iteration 20395, loss = 0.02464125\n",
      "Iteration 20396, loss = 0.02463989\n",
      "Iteration 20397, loss = 0.02463858\n",
      "Iteration 20398, loss = 0.02463723\n",
      "Iteration 20399, loss = 0.02463589\n",
      "Iteration 20400, loss = 0.02463453\n",
      "Iteration 20401, loss = 0.02463321\n",
      "Iteration 20402, loss = 0.02463187\n",
      "Iteration 20403, loss = 0.02463052\n",
      "Iteration 20404, loss = 0.02462919\n",
      "Iteration 20405, loss = 0.02462784\n",
      "Iteration 20406, loss = 0.02462649\n",
      "Iteration 20407, loss = 0.02462517\n",
      "Iteration 20408, loss = 0.02462383\n",
      "Iteration 20409, loss = 0.02462249\n",
      "Iteration 20410, loss = 0.02462114\n",
      "Iteration 20411, loss = 0.02461979\n",
      "Iteration 20412, loss = 0.02461844\n",
      "Iteration 20413, loss = 0.02461713\n",
      "Iteration 20414, loss = 0.02461578\n",
      "Iteration 20415, loss = 0.02461444\n",
      "Iteration 20416, loss = 0.02461311\n",
      "Iteration 20417, loss = 0.02461176\n",
      "Iteration 20418, loss = 0.02461042\n",
      "Iteration 20419, loss = 0.02460909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20420, loss = 0.02460774\n",
      "Iteration 20421, loss = 0.02460641\n",
      "Iteration 20422, loss = 0.02460507\n",
      "Iteration 20423, loss = 0.02460374\n",
      "Iteration 20424, loss = 0.02460241\n",
      "Iteration 20425, loss = 0.02460107\n",
      "Iteration 20426, loss = 0.02459974\n",
      "Iteration 20427, loss = 0.02459841\n",
      "Iteration 20428, loss = 0.02459708\n",
      "Iteration 20429, loss = 0.02459575\n",
      "Iteration 20430, loss = 0.02459442\n",
      "Iteration 20431, loss = 0.02459308\n",
      "Iteration 20432, loss = 0.02459175\n",
      "Iteration 20433, loss = 0.02459043\n",
      "Iteration 20434, loss = 0.02458908\n",
      "Iteration 20435, loss = 0.02458775\n",
      "Iteration 20436, loss = 0.02458643\n",
      "Iteration 20437, loss = 0.02458509\n",
      "Iteration 20438, loss = 0.02458375\n",
      "Iteration 20439, loss = 0.02458241\n",
      "Iteration 20440, loss = 0.02458109\n",
      "Iteration 20441, loss = 0.02457974\n",
      "Iteration 20442, loss = 0.02457842\n",
      "Iteration 20443, loss = 0.02457708\n",
      "Iteration 20444, loss = 0.02457572\n",
      "Iteration 20445, loss = 0.02457444\n",
      "Iteration 20446, loss = 0.02457307\n",
      "Iteration 20447, loss = 0.02457172\n",
      "Iteration 20448, loss = 0.02457039\n",
      "Iteration 20449, loss = 0.02456906\n",
      "Iteration 20450, loss = 0.02456776\n",
      "Iteration 20451, loss = 0.02456640\n",
      "Iteration 20452, loss = 0.02456507\n",
      "Iteration 20453, loss = 0.02456374\n",
      "Iteration 20454, loss = 0.02456241\n",
      "Iteration 20455, loss = 0.02456110\n",
      "Iteration 20456, loss = 0.02455974\n",
      "Iteration 20457, loss = 0.02455841\n",
      "Iteration 20458, loss = 0.02455708\n",
      "Iteration 20459, loss = 0.02455575\n",
      "Iteration 20460, loss = 0.02455442\n",
      "Iteration 20461, loss = 0.02455308\n",
      "Iteration 20462, loss = 0.02455176\n",
      "Iteration 20463, loss = 0.02455043\n",
      "Iteration 20464, loss = 0.02454911\n",
      "Iteration 20465, loss = 0.02454778\n",
      "Iteration 20466, loss = 0.02454645\n",
      "Iteration 20467, loss = 0.02454512\n",
      "Iteration 20468, loss = 0.02454379\n",
      "Iteration 20469, loss = 0.02454248\n",
      "Iteration 20470, loss = 0.02454114\n",
      "Iteration 20471, loss = 0.02453980\n",
      "Iteration 20472, loss = 0.02453848\n",
      "Iteration 20473, loss = 0.02453716\n",
      "Iteration 20474, loss = 0.02453582\n",
      "Iteration 20475, loss = 0.02453449\n",
      "Iteration 20476, loss = 0.02453318\n",
      "Iteration 20477, loss = 0.02453187\n",
      "Iteration 20478, loss = 0.02453053\n",
      "Iteration 20479, loss = 0.02452920\n",
      "Iteration 20480, loss = 0.02452787\n",
      "Iteration 20481, loss = 0.02452654\n",
      "Iteration 20482, loss = 0.02452522\n",
      "Iteration 20483, loss = 0.02452391\n",
      "Iteration 20484, loss = 0.02452257\n",
      "Iteration 20485, loss = 0.02452125\n",
      "Iteration 20486, loss = 0.02451995\n",
      "Iteration 20487, loss = 0.02451861\n",
      "Iteration 20488, loss = 0.02451728\n",
      "Iteration 20489, loss = 0.02451596\n",
      "Iteration 20490, loss = 0.02451463\n",
      "Iteration 20491, loss = 0.02451331\n",
      "Iteration 20492, loss = 0.02451199\n",
      "Iteration 20493, loss = 0.02451067\n",
      "Iteration 20494, loss = 0.02450935\n",
      "Iteration 20495, loss = 0.02450803\n",
      "Iteration 20496, loss = 0.02450670\n",
      "Iteration 20497, loss = 0.02450539\n",
      "Iteration 20498, loss = 0.02450404\n",
      "Iteration 20499, loss = 0.02450273\n",
      "Iteration 20500, loss = 0.02450142\n",
      "Iteration 20501, loss = 0.02450008\n",
      "Iteration 20502, loss = 0.02449876\n",
      "Iteration 20503, loss = 0.02449744\n",
      "Iteration 20504, loss = 0.02449612\n",
      "Iteration 20505, loss = 0.02449480\n",
      "Iteration 20506, loss = 0.02449349\n",
      "Iteration 20507, loss = 0.02449216\n",
      "Iteration 20508, loss = 0.02449081\n",
      "Iteration 20509, loss = 0.02448950\n",
      "Iteration 20510, loss = 0.02448818\n",
      "Iteration 20511, loss = 0.02448687\n",
      "Iteration 20512, loss = 0.02448555\n",
      "Iteration 20513, loss = 0.02448421\n",
      "Iteration 20514, loss = 0.02448290\n",
      "Iteration 20515, loss = 0.02448158\n",
      "Iteration 20516, loss = 0.02448024\n",
      "Iteration 20517, loss = 0.02447895\n",
      "Iteration 20518, loss = 0.02447760\n",
      "Iteration 20519, loss = 0.02447629\n",
      "Iteration 20520, loss = 0.02447497\n",
      "Iteration 20521, loss = 0.02447364\n",
      "Iteration 20522, loss = 0.02447235\n",
      "Iteration 20523, loss = 0.02447101\n",
      "Iteration 20524, loss = 0.02446971\n",
      "Iteration 20525, loss = 0.02446836\n",
      "Iteration 20526, loss = 0.02446705\n",
      "Iteration 20527, loss = 0.02446574\n",
      "Iteration 20528, loss = 0.02446441\n",
      "Iteration 20529, loss = 0.02446309\n",
      "Iteration 20530, loss = 0.02446177\n",
      "Iteration 20531, loss = 0.02446045\n",
      "Iteration 20532, loss = 0.02445912\n",
      "Iteration 20533, loss = 0.02445780\n",
      "Iteration 20534, loss = 0.02445648\n",
      "Iteration 20535, loss = 0.02445516\n",
      "Iteration 20536, loss = 0.02445384\n",
      "Iteration 20537, loss = 0.02445253\n",
      "Iteration 20538, loss = 0.02445120\n",
      "Iteration 20539, loss = 0.02444990\n",
      "Iteration 20540, loss = 0.02444857\n",
      "Iteration 20541, loss = 0.02444724\n",
      "Iteration 20542, loss = 0.02444592\n",
      "Iteration 20543, loss = 0.02444462\n",
      "Iteration 20544, loss = 0.02444329\n",
      "Iteration 20545, loss = 0.02444197\n",
      "Iteration 20546, loss = 0.02444065\n",
      "Iteration 20547, loss = 0.02443935\n",
      "Iteration 20548, loss = 0.02443803\n",
      "Iteration 20549, loss = 0.02443670\n",
      "Iteration 20550, loss = 0.02443540\n",
      "Iteration 20551, loss = 0.02443406\n",
      "Iteration 20552, loss = 0.02443274\n",
      "Iteration 20553, loss = 0.02443143\n",
      "Iteration 20554, loss = 0.02443010\n",
      "Iteration 20555, loss = 0.02442880\n",
      "Iteration 20556, loss = 0.02442748\n",
      "Iteration 20557, loss = 0.02442615\n",
      "Iteration 20558, loss = 0.02442485\n",
      "Iteration 20559, loss = 0.02442351\n",
      "Iteration 20560, loss = 0.02442222\n",
      "Iteration 20561, loss = 0.02442090\n",
      "Iteration 20562, loss = 0.02441958\n",
      "Iteration 20563, loss = 0.02441828\n",
      "Iteration 20564, loss = 0.02441694\n",
      "Iteration 20565, loss = 0.02441565\n",
      "Iteration 20566, loss = 0.02441431\n",
      "Iteration 20567, loss = 0.02441301\n",
      "Iteration 20568, loss = 0.02441169\n",
      "Iteration 20569, loss = 0.02441039\n",
      "Iteration 20570, loss = 0.02440906\n",
      "Iteration 20571, loss = 0.02440775\n",
      "Iteration 20572, loss = 0.02440645\n",
      "Iteration 20573, loss = 0.02440513\n",
      "Iteration 20574, loss = 0.02440383\n",
      "Iteration 20575, loss = 0.02440252\n",
      "Iteration 20576, loss = 0.02440120\n",
      "Iteration 20577, loss = 0.02439988\n",
      "Iteration 20578, loss = 0.02439857\n",
      "Iteration 20579, loss = 0.02439727\n",
      "Iteration 20580, loss = 0.02439594\n",
      "Iteration 20581, loss = 0.02439465\n",
      "Iteration 20582, loss = 0.02439333\n",
      "Iteration 20583, loss = 0.02439200\n",
      "Iteration 20584, loss = 0.02439069\n",
      "Iteration 20585, loss = 0.02438940\n",
      "Iteration 20586, loss = 0.02438805\n",
      "Iteration 20587, loss = 0.02438675\n",
      "Iteration 20588, loss = 0.02438544\n",
      "Iteration 20589, loss = 0.02438414\n",
      "Iteration 20590, loss = 0.02438284\n",
      "Iteration 20591, loss = 0.02438150\n",
      "Iteration 20592, loss = 0.02438020\n",
      "Iteration 20593, loss = 0.02437888\n",
      "Iteration 20594, loss = 0.02437758\n",
      "Iteration 20595, loss = 0.02437628\n",
      "Iteration 20596, loss = 0.02437496\n",
      "Iteration 20597, loss = 0.02437366\n",
      "Iteration 20598, loss = 0.02437232\n",
      "Iteration 20599, loss = 0.02437104\n",
      "Iteration 20600, loss = 0.02436973\n",
      "Iteration 20601, loss = 0.02436841\n",
      "Iteration 20602, loss = 0.02436709\n",
      "Iteration 20603, loss = 0.02436578\n",
      "Iteration 20604, loss = 0.02436448\n",
      "Iteration 20605, loss = 0.02436316\n",
      "Iteration 20606, loss = 0.02436187\n",
      "Iteration 20607, loss = 0.02436055\n",
      "Iteration 20608, loss = 0.02435925\n",
      "Iteration 20609, loss = 0.02435793\n",
      "Iteration 20610, loss = 0.02435663\n",
      "Iteration 20611, loss = 0.02435531\n",
      "Iteration 20612, loss = 0.02435402\n",
      "Iteration 20613, loss = 0.02435269\n",
      "Iteration 20614, loss = 0.02435139\n",
      "Iteration 20615, loss = 0.02435008\n",
      "Iteration 20616, loss = 0.02434877\n",
      "Iteration 20617, loss = 0.02434749\n",
      "Iteration 20618, loss = 0.02434618\n",
      "Iteration 20619, loss = 0.02434488\n",
      "Iteration 20620, loss = 0.02434358\n",
      "Iteration 20621, loss = 0.02434225\n",
      "Iteration 20622, loss = 0.02434097\n",
      "Iteration 20623, loss = 0.02433966\n",
      "Iteration 20624, loss = 0.02433836\n",
      "Iteration 20625, loss = 0.02433707\n",
      "Iteration 20626, loss = 0.02433575\n",
      "Iteration 20627, loss = 0.02433444\n",
      "Iteration 20628, loss = 0.02433314\n",
      "Iteration 20629, loss = 0.02433183\n",
      "Iteration 20630, loss = 0.02433053\n",
      "Iteration 20631, loss = 0.02432923\n",
      "Iteration 20632, loss = 0.02432794\n",
      "Iteration 20633, loss = 0.02432662\n",
      "Iteration 20634, loss = 0.02432532\n",
      "Iteration 20635, loss = 0.02432401\n",
      "Iteration 20636, loss = 0.02432271\n",
      "Iteration 20637, loss = 0.02432142\n",
      "Iteration 20638, loss = 0.02432011\n",
      "Iteration 20639, loss = 0.02431881\n",
      "Iteration 20640, loss = 0.02431750\n",
      "Iteration 20641, loss = 0.02431620\n",
      "Iteration 20642, loss = 0.02431490\n",
      "Iteration 20643, loss = 0.02431361\n",
      "Iteration 20644, loss = 0.02431232\n",
      "Iteration 20645, loss = 0.02431099\n",
      "Iteration 20646, loss = 0.02430970\n",
      "Iteration 20647, loss = 0.02430840\n",
      "Iteration 20648, loss = 0.02430710\n",
      "Iteration 20649, loss = 0.02430580\n",
      "Iteration 20650, loss = 0.02430450\n",
      "Iteration 20651, loss = 0.02430322\n",
      "Iteration 20652, loss = 0.02430190\n",
      "Iteration 20653, loss = 0.02430060\n",
      "Iteration 20654, loss = 0.02429929\n",
      "Iteration 20655, loss = 0.02429801\n",
      "Iteration 20656, loss = 0.02429669\n",
      "Iteration 20657, loss = 0.02429539\n",
      "Iteration 20658, loss = 0.02429409\n",
      "Iteration 20659, loss = 0.02429279\n",
      "Iteration 20660, loss = 0.02429147\n",
      "Iteration 20661, loss = 0.02429022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20662, loss = 0.02428888\n",
      "Iteration 20663, loss = 0.02428760\n",
      "Iteration 20664, loss = 0.02428630\n",
      "Iteration 20665, loss = 0.02428500\n",
      "Iteration 20666, loss = 0.02428370\n",
      "Iteration 20667, loss = 0.02428240\n",
      "Iteration 20668, loss = 0.02428111\n",
      "Iteration 20669, loss = 0.02427982\n",
      "Iteration 20670, loss = 0.02427851\n",
      "Iteration 20671, loss = 0.02427723\n",
      "Iteration 20672, loss = 0.02427593\n",
      "Iteration 20673, loss = 0.02427463\n",
      "Iteration 20674, loss = 0.02427333\n",
      "Iteration 20675, loss = 0.02427204\n",
      "Iteration 20676, loss = 0.02427074\n",
      "Iteration 20677, loss = 0.02426946\n",
      "Iteration 20678, loss = 0.02426818\n",
      "Iteration 20679, loss = 0.02426687\n",
      "Iteration 20680, loss = 0.02426558\n",
      "Iteration 20681, loss = 0.02426428\n",
      "Iteration 20682, loss = 0.02426297\n",
      "Iteration 20683, loss = 0.02426170\n",
      "Iteration 20684, loss = 0.02426040\n",
      "Iteration 20685, loss = 0.02425909\n",
      "Iteration 20686, loss = 0.02425779\n",
      "Iteration 20687, loss = 0.02425650\n",
      "Iteration 20688, loss = 0.02425520\n",
      "Iteration 20689, loss = 0.02425391\n",
      "Iteration 20690, loss = 0.02425261\n",
      "Iteration 20691, loss = 0.02425131\n",
      "Iteration 20692, loss = 0.02425001\n",
      "Iteration 20693, loss = 0.02424871\n",
      "Iteration 20694, loss = 0.02424741\n",
      "Iteration 20695, loss = 0.02424612\n",
      "Iteration 20696, loss = 0.02424482\n",
      "Iteration 20697, loss = 0.02424352\n",
      "Iteration 20698, loss = 0.02424222\n",
      "Iteration 20699, loss = 0.02424093\n",
      "Iteration 20700, loss = 0.02423964\n",
      "Iteration 20701, loss = 0.02423833\n",
      "Iteration 20702, loss = 0.02423704\n",
      "Iteration 20703, loss = 0.02423576\n",
      "Iteration 20704, loss = 0.02423445\n",
      "Iteration 20705, loss = 0.02423316\n",
      "Iteration 20706, loss = 0.02423186\n",
      "Iteration 20707, loss = 0.02423058\n",
      "Iteration 20708, loss = 0.02422927\n",
      "Iteration 20709, loss = 0.02422797\n",
      "Iteration 20710, loss = 0.02422667\n",
      "Iteration 20711, loss = 0.02422538\n",
      "Iteration 20712, loss = 0.02422410\n",
      "Iteration 20713, loss = 0.02422278\n",
      "Iteration 20714, loss = 0.02422149\n",
      "Iteration 20715, loss = 0.02422019\n",
      "Iteration 20716, loss = 0.02421890\n",
      "Iteration 20717, loss = 0.02421760\n",
      "Iteration 20718, loss = 0.02421630\n",
      "Iteration 20719, loss = 0.02421503\n",
      "Iteration 20720, loss = 0.02421373\n",
      "Iteration 20721, loss = 0.02421242\n",
      "Iteration 20722, loss = 0.02421114\n",
      "Iteration 20723, loss = 0.02420983\n",
      "Iteration 20724, loss = 0.02420854\n",
      "Iteration 20725, loss = 0.02420726\n",
      "Iteration 20726, loss = 0.02420595\n",
      "Iteration 20727, loss = 0.02420465\n",
      "Iteration 20728, loss = 0.02420336\n",
      "Iteration 20729, loss = 0.02420207\n",
      "Iteration 20730, loss = 0.02420079\n",
      "Iteration 20731, loss = 0.02419950\n",
      "Iteration 20732, loss = 0.02419820\n",
      "Iteration 20733, loss = 0.02419691\n",
      "Iteration 20734, loss = 0.02419560\n",
      "Iteration 20735, loss = 0.02419432\n",
      "Iteration 20736, loss = 0.02419303\n",
      "Iteration 20737, loss = 0.02419174\n",
      "Iteration 20738, loss = 0.02419043\n",
      "Iteration 20739, loss = 0.02418914\n",
      "Iteration 20740, loss = 0.02418783\n",
      "Iteration 20741, loss = 0.02418654\n",
      "Iteration 20742, loss = 0.02418527\n",
      "Iteration 20743, loss = 0.02418394\n",
      "Iteration 20744, loss = 0.02418266\n",
      "Iteration 20745, loss = 0.02418137\n",
      "Iteration 20746, loss = 0.02418009\n",
      "Iteration 20747, loss = 0.02417879\n",
      "Iteration 20748, loss = 0.02417749\n",
      "Iteration 20749, loss = 0.02417621\n",
      "Iteration 20750, loss = 0.02417492\n",
      "Iteration 20751, loss = 0.02417363\n",
      "Iteration 20752, loss = 0.02417235\n",
      "Iteration 20753, loss = 0.02417105\n",
      "Iteration 20754, loss = 0.02416976\n",
      "Iteration 20755, loss = 0.02416850\n",
      "Iteration 20756, loss = 0.02416720\n",
      "Iteration 20757, loss = 0.02416590\n",
      "Iteration 20758, loss = 0.02416464\n",
      "Iteration 20759, loss = 0.02416334\n",
      "Iteration 20760, loss = 0.02416204\n",
      "Iteration 20761, loss = 0.02416076\n",
      "Iteration 20762, loss = 0.02415946\n",
      "Iteration 20763, loss = 0.02415818\n",
      "Iteration 20764, loss = 0.02415688\n",
      "Iteration 20765, loss = 0.02415559\n",
      "Iteration 20766, loss = 0.02415431\n",
      "Iteration 20767, loss = 0.02415301\n",
      "Iteration 20768, loss = 0.02415173\n",
      "Iteration 20769, loss = 0.02415044\n",
      "Iteration 20770, loss = 0.02414915\n",
      "Iteration 20771, loss = 0.02414787\n",
      "Iteration 20772, loss = 0.02414658\n",
      "Iteration 20773, loss = 0.02414529\n",
      "Iteration 20774, loss = 0.02414401\n",
      "Iteration 20775, loss = 0.02414272\n",
      "Iteration 20776, loss = 0.02414146\n",
      "Iteration 20777, loss = 0.02414015\n",
      "Iteration 20778, loss = 0.02413887\n",
      "Iteration 20779, loss = 0.02413759\n",
      "Iteration 20780, loss = 0.02413629\n",
      "Iteration 20781, loss = 0.02413502\n",
      "Iteration 20782, loss = 0.02413372\n",
      "Iteration 20783, loss = 0.02413246\n",
      "Iteration 20784, loss = 0.02413117\n",
      "Iteration 20785, loss = 0.02412988\n",
      "Iteration 20786, loss = 0.02412861\n",
      "Iteration 20787, loss = 0.02412732\n",
      "Iteration 20788, loss = 0.02412603\n",
      "Iteration 20789, loss = 0.02412474\n",
      "Iteration 20790, loss = 0.02412346\n",
      "Iteration 20791, loss = 0.02412217\n",
      "Iteration 20792, loss = 0.02412090\n",
      "Iteration 20793, loss = 0.02411961\n",
      "Iteration 20794, loss = 0.02411831\n",
      "Iteration 20795, loss = 0.02411702\n",
      "Iteration 20796, loss = 0.02411576\n",
      "Iteration 20797, loss = 0.02411448\n",
      "Iteration 20798, loss = 0.02411318\n",
      "Iteration 20799, loss = 0.02411189\n",
      "Iteration 20800, loss = 0.02411060\n",
      "Iteration 20801, loss = 0.02410932\n",
      "Iteration 20802, loss = 0.02410805\n",
      "Iteration 20803, loss = 0.02410674\n",
      "Iteration 20804, loss = 0.02410546\n",
      "Iteration 20805, loss = 0.02410417\n",
      "Iteration 20806, loss = 0.02410291\n",
      "Iteration 20807, loss = 0.02410163\n",
      "Iteration 20808, loss = 0.02410032\n",
      "Iteration 20809, loss = 0.02409905\n",
      "Iteration 20810, loss = 0.02409776\n",
      "Iteration 20811, loss = 0.02409649\n",
      "Iteration 20812, loss = 0.02409519\n",
      "Iteration 20813, loss = 0.02409393\n",
      "Iteration 20814, loss = 0.02409265\n",
      "Iteration 20815, loss = 0.02409136\n",
      "Iteration 20816, loss = 0.02409009\n",
      "Iteration 20817, loss = 0.02408882\n",
      "Iteration 20818, loss = 0.02408752\n",
      "Iteration 20819, loss = 0.02408623\n",
      "Iteration 20820, loss = 0.02408497\n",
      "Iteration 20821, loss = 0.02408368\n",
      "Iteration 20822, loss = 0.02408241\n",
      "Iteration 20823, loss = 0.02408113\n",
      "Iteration 20824, loss = 0.02407984\n",
      "Iteration 20825, loss = 0.02407857\n",
      "Iteration 20826, loss = 0.02407732\n",
      "Iteration 20827, loss = 0.02407602\n",
      "Iteration 20828, loss = 0.02407474\n",
      "Iteration 20829, loss = 0.02407345\n",
      "Iteration 20830, loss = 0.02407219\n",
      "Iteration 20831, loss = 0.02407091\n",
      "Iteration 20832, loss = 0.02406963\n",
      "Iteration 20833, loss = 0.02406837\n",
      "Iteration 20834, loss = 0.02406710\n",
      "Iteration 20835, loss = 0.02406581\n",
      "Iteration 20836, loss = 0.02406453\n",
      "Iteration 20837, loss = 0.02406326\n",
      "Iteration 20838, loss = 0.02406198\n",
      "Iteration 20839, loss = 0.02406072\n",
      "Iteration 20840, loss = 0.02405942\n",
      "Iteration 20841, loss = 0.02405816\n",
      "Iteration 20842, loss = 0.02405688\n",
      "Iteration 20843, loss = 0.02405560\n",
      "Iteration 20844, loss = 0.02405435\n",
      "Iteration 20845, loss = 0.02405307\n",
      "Iteration 20846, loss = 0.02405180\n",
      "Iteration 20847, loss = 0.02405052\n",
      "Iteration 20848, loss = 0.02404924\n",
      "Iteration 20849, loss = 0.02404797\n",
      "Iteration 20850, loss = 0.02404670\n",
      "Iteration 20851, loss = 0.02404542\n",
      "Iteration 20852, loss = 0.02404416\n",
      "Iteration 20853, loss = 0.02404288\n",
      "Iteration 20854, loss = 0.02404161\n",
      "Iteration 20855, loss = 0.02404036\n",
      "Iteration 20856, loss = 0.02403906\n",
      "Iteration 20857, loss = 0.02403780\n",
      "Iteration 20858, loss = 0.02403655\n",
      "Iteration 20859, loss = 0.02403526\n",
      "Iteration 20860, loss = 0.02403398\n",
      "Iteration 20861, loss = 0.02403273\n",
      "Iteration 20862, loss = 0.02403143\n",
      "Iteration 20863, loss = 0.02403017\n",
      "Iteration 20864, loss = 0.02402889\n",
      "Iteration 20865, loss = 0.02402762\n",
      "Iteration 20866, loss = 0.02402635\n",
      "Iteration 20867, loss = 0.02402506\n",
      "Iteration 20868, loss = 0.02402380\n",
      "Iteration 20869, loss = 0.02402254\n",
      "Iteration 20870, loss = 0.02402126\n",
      "Iteration 20871, loss = 0.02401998\n",
      "Iteration 20872, loss = 0.02401870\n",
      "Iteration 20873, loss = 0.02401743\n",
      "Iteration 20874, loss = 0.02401616\n",
      "Iteration 20875, loss = 0.02401490\n",
      "Iteration 20876, loss = 0.02401363\n",
      "Iteration 20877, loss = 0.02401233\n",
      "Iteration 20878, loss = 0.02401109\n",
      "Iteration 20879, loss = 0.02400980\n",
      "Iteration 20880, loss = 0.02400852\n",
      "Iteration 20881, loss = 0.02400726\n",
      "Iteration 20882, loss = 0.02400599\n",
      "Iteration 20883, loss = 0.02400472\n",
      "Iteration 20884, loss = 0.02400346\n",
      "Iteration 20885, loss = 0.02400218\n",
      "Iteration 20886, loss = 0.02400090\n",
      "Iteration 20887, loss = 0.02399965\n",
      "Iteration 20888, loss = 0.02399837\n",
      "Iteration 20889, loss = 0.02399712\n",
      "Iteration 20890, loss = 0.02399582\n",
      "Iteration 20891, loss = 0.02399457\n",
      "Iteration 20892, loss = 0.02399329\n",
      "Iteration 20893, loss = 0.02399205\n",
      "Iteration 20894, loss = 0.02399075\n",
      "Iteration 20895, loss = 0.02398947\n",
      "Iteration 20896, loss = 0.02398820\n",
      "Iteration 20897, loss = 0.02398694\n",
      "Iteration 20898, loss = 0.02398567\n",
      "Iteration 20899, loss = 0.02398440\n",
      "Iteration 20900, loss = 0.02398313\n",
      "Iteration 20901, loss = 0.02398185\n",
      "Iteration 20902, loss = 0.02398058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20903, loss = 0.02397933\n",
      "Iteration 20904, loss = 0.02397805\n",
      "Iteration 20905, loss = 0.02397680\n",
      "Iteration 20906, loss = 0.02397554\n",
      "Iteration 20907, loss = 0.02397426\n",
      "Iteration 20908, loss = 0.02397301\n",
      "Iteration 20909, loss = 0.02397173\n",
      "Iteration 20910, loss = 0.02397047\n",
      "Iteration 20911, loss = 0.02396922\n",
      "Iteration 20912, loss = 0.02396794\n",
      "Iteration 20913, loss = 0.02396670\n",
      "Iteration 20914, loss = 0.02396542\n",
      "Iteration 20915, loss = 0.02396418\n",
      "Iteration 20916, loss = 0.02396290\n",
      "Iteration 20917, loss = 0.02396165\n",
      "Iteration 20918, loss = 0.02396038\n",
      "Iteration 20919, loss = 0.02395911\n",
      "Iteration 20920, loss = 0.02395786\n",
      "Iteration 20921, loss = 0.02395662\n",
      "Iteration 20922, loss = 0.02395532\n",
      "Iteration 20923, loss = 0.02395406\n",
      "Iteration 20924, loss = 0.02395279\n",
      "Iteration 20925, loss = 0.02395153\n",
      "Iteration 20926, loss = 0.02395028\n",
      "Iteration 20927, loss = 0.02394901\n",
      "Iteration 20928, loss = 0.02394774\n",
      "Iteration 20929, loss = 0.02394647\n",
      "Iteration 20930, loss = 0.02394521\n",
      "Iteration 20931, loss = 0.02394395\n",
      "Iteration 20932, loss = 0.02394269\n",
      "Iteration 20933, loss = 0.02394142\n",
      "Iteration 20934, loss = 0.02394015\n",
      "Iteration 20935, loss = 0.02393892\n",
      "Iteration 20936, loss = 0.02393763\n",
      "Iteration 20937, loss = 0.02393637\n",
      "Iteration 20938, loss = 0.02393510\n",
      "Iteration 20939, loss = 0.02393386\n",
      "Iteration 20940, loss = 0.02393260\n",
      "Iteration 20941, loss = 0.02393132\n",
      "Iteration 20942, loss = 0.02393006\n",
      "Iteration 20943, loss = 0.02392880\n",
      "Iteration 20944, loss = 0.02392754\n",
      "Iteration 20945, loss = 0.02392630\n",
      "Iteration 20946, loss = 0.02392502\n",
      "Iteration 20947, loss = 0.02392375\n",
      "Iteration 20948, loss = 0.02392250\n",
      "Iteration 20949, loss = 0.02392123\n",
      "Iteration 20950, loss = 0.02391999\n",
      "Iteration 20951, loss = 0.02391873\n",
      "Iteration 20952, loss = 0.02391745\n",
      "Iteration 20953, loss = 0.02391622\n",
      "Iteration 20954, loss = 0.02391493\n",
      "Iteration 20955, loss = 0.02391366\n",
      "Iteration 20956, loss = 0.02391240\n",
      "Iteration 20957, loss = 0.02391114\n",
      "Iteration 20958, loss = 0.02390988\n",
      "Iteration 20959, loss = 0.02390863\n",
      "Iteration 20960, loss = 0.02390738\n",
      "Iteration 20961, loss = 0.02390610\n",
      "Iteration 20962, loss = 0.02390484\n",
      "Iteration 20963, loss = 0.02390358\n",
      "Iteration 20964, loss = 0.02390232\n",
      "Iteration 20965, loss = 0.02390108\n",
      "Iteration 20966, loss = 0.02389982\n",
      "Iteration 20967, loss = 0.02389856\n",
      "Iteration 20968, loss = 0.02389729\n",
      "Iteration 20969, loss = 0.02389604\n",
      "Iteration 20970, loss = 0.02389478\n",
      "Iteration 20971, loss = 0.02389353\n",
      "Iteration 20972, loss = 0.02389228\n",
      "Iteration 20973, loss = 0.02389102\n",
      "Iteration 20974, loss = 0.02388977\n",
      "Iteration 20975, loss = 0.02388852\n",
      "Iteration 20976, loss = 0.02388724\n",
      "Iteration 20977, loss = 0.02388600\n",
      "Iteration 20978, loss = 0.02388473\n",
      "Iteration 20979, loss = 0.02388347\n",
      "Iteration 20980, loss = 0.02388222\n",
      "Iteration 20981, loss = 0.02388097\n",
      "Iteration 20982, loss = 0.02387971\n",
      "Iteration 20983, loss = 0.02387847\n",
      "Iteration 20984, loss = 0.02387720\n",
      "Iteration 20985, loss = 0.02387594\n",
      "Iteration 20986, loss = 0.02387467\n",
      "Iteration 20987, loss = 0.02387343\n",
      "Iteration 20988, loss = 0.02387218\n",
      "Iteration 20989, loss = 0.02387093\n",
      "Iteration 20990, loss = 0.02386968\n",
      "Iteration 20991, loss = 0.02386840\n",
      "Iteration 20992, loss = 0.02386716\n",
      "Iteration 20993, loss = 0.02386590\n",
      "Iteration 20994, loss = 0.02386465\n",
      "Iteration 20995, loss = 0.02386338\n",
      "Iteration 20996, loss = 0.02386213\n",
      "Iteration 20997, loss = 0.02386088\n",
      "Iteration 20998, loss = 0.02385962\n",
      "Iteration 20999, loss = 0.02385837\n",
      "Iteration 21000, loss = 0.02385711\n",
      "Iteration 21001, loss = 0.02385585\n",
      "Iteration 21002, loss = 0.02385459\n",
      "Iteration 21003, loss = 0.02385335\n",
      "Iteration 21004, loss = 0.02385207\n",
      "Iteration 21005, loss = 0.02385083\n",
      "Iteration 21006, loss = 0.02384958\n",
      "Iteration 21007, loss = 0.02384831\n",
      "Iteration 21008, loss = 0.02384708\n",
      "Iteration 21009, loss = 0.02384582\n",
      "Iteration 21010, loss = 0.02384456\n",
      "Iteration 21011, loss = 0.02384330\n",
      "Iteration 21012, loss = 0.02384204\n",
      "Iteration 21013, loss = 0.02384078\n",
      "Iteration 21014, loss = 0.02383954\n",
      "Iteration 21015, loss = 0.02383829\n",
      "Iteration 21016, loss = 0.02383704\n",
      "Iteration 21017, loss = 0.02383578\n",
      "Iteration 21018, loss = 0.02383453\n",
      "Iteration 21019, loss = 0.02383327\n",
      "Iteration 21020, loss = 0.02383202\n",
      "Iteration 21021, loss = 0.02383077\n",
      "Iteration 21022, loss = 0.02382952\n",
      "Iteration 21023, loss = 0.02382829\n",
      "Iteration 21024, loss = 0.02382701\n",
      "Iteration 21025, loss = 0.02382578\n",
      "Iteration 21026, loss = 0.02382451\n",
      "Iteration 21027, loss = 0.02382327\n",
      "Iteration 21028, loss = 0.02382200\n",
      "Iteration 21029, loss = 0.02382075\n",
      "Iteration 21030, loss = 0.02381950\n",
      "Iteration 21031, loss = 0.02381825\n",
      "Iteration 21032, loss = 0.02381700\n",
      "Iteration 21033, loss = 0.02381574\n",
      "Iteration 21034, loss = 0.02381451\n",
      "Iteration 21035, loss = 0.02381324\n",
      "Iteration 21036, loss = 0.02381199\n",
      "Iteration 21037, loss = 0.02381076\n",
      "Iteration 21038, loss = 0.02380947\n",
      "Iteration 21039, loss = 0.02380822\n",
      "Iteration 21040, loss = 0.02380698\n",
      "Iteration 21041, loss = 0.02380571\n",
      "Iteration 21042, loss = 0.02380447\n",
      "Iteration 21043, loss = 0.02380320\n",
      "Iteration 21044, loss = 0.02380195\n",
      "Iteration 21045, loss = 0.02380071\n",
      "Iteration 21046, loss = 0.02379945\n",
      "Iteration 21047, loss = 0.02379818\n",
      "Iteration 21048, loss = 0.02379694\n",
      "Iteration 21049, loss = 0.02379568\n",
      "Iteration 21050, loss = 0.02379444\n",
      "Iteration 21051, loss = 0.02379319\n",
      "Iteration 21052, loss = 0.02379193\n",
      "Iteration 21053, loss = 0.02379068\n",
      "Iteration 21054, loss = 0.02378943\n",
      "Iteration 21055, loss = 0.02378820\n",
      "Iteration 21056, loss = 0.02378693\n",
      "Iteration 21057, loss = 0.02378568\n",
      "Iteration 21058, loss = 0.02378444\n",
      "Iteration 21059, loss = 0.02378318\n",
      "Iteration 21060, loss = 0.02378193\n",
      "Iteration 21061, loss = 0.02378067\n",
      "Iteration 21062, loss = 0.02377943\n",
      "Iteration 21063, loss = 0.02377818\n",
      "Iteration 21064, loss = 0.02377693\n",
      "Iteration 21065, loss = 0.02377570\n",
      "Iteration 21066, loss = 0.02377443\n",
      "Iteration 21067, loss = 0.02377319\n",
      "Iteration 21068, loss = 0.02377195\n",
      "Iteration 21069, loss = 0.02377069\n",
      "Iteration 21070, loss = 0.02376945\n",
      "Iteration 21071, loss = 0.02376821\n",
      "Iteration 21072, loss = 0.02376695\n",
      "Iteration 21073, loss = 0.02376570\n",
      "Iteration 21074, loss = 0.02376446\n",
      "Iteration 21075, loss = 0.02376321\n",
      "Iteration 21076, loss = 0.02376196\n",
      "Iteration 21077, loss = 0.02376074\n",
      "Iteration 21078, loss = 0.02375948\n",
      "Iteration 21079, loss = 0.02375823\n",
      "Iteration 21080, loss = 0.02375699\n",
      "Iteration 21081, loss = 0.02375575\n",
      "Iteration 21082, loss = 0.02375449\n",
      "Iteration 21083, loss = 0.02375326\n",
      "Iteration 21084, loss = 0.02375202\n",
      "Iteration 21085, loss = 0.02375077\n",
      "Iteration 21086, loss = 0.02374953\n",
      "Iteration 21087, loss = 0.02374828\n",
      "Iteration 21088, loss = 0.02374707\n",
      "Iteration 21089, loss = 0.02374581\n",
      "Iteration 21090, loss = 0.02374455\n",
      "Iteration 21091, loss = 0.02374332\n",
      "Iteration 21092, loss = 0.02374208\n",
      "Iteration 21093, loss = 0.02374083\n",
      "Iteration 21094, loss = 0.02373960\n",
      "Iteration 21095, loss = 0.02373834\n",
      "Iteration 21096, loss = 0.02373711\n",
      "Iteration 21097, loss = 0.02373588\n",
      "Iteration 21098, loss = 0.02373463\n",
      "Iteration 21099, loss = 0.02373338\n",
      "Iteration 21100, loss = 0.02373215\n",
      "Iteration 21101, loss = 0.02373090\n",
      "Iteration 21102, loss = 0.02372967\n",
      "Iteration 21103, loss = 0.02372843\n",
      "Iteration 21104, loss = 0.02372718\n",
      "Iteration 21105, loss = 0.02372596\n",
      "Iteration 21106, loss = 0.02372471\n",
      "Iteration 21107, loss = 0.02372347\n",
      "Iteration 21108, loss = 0.02372223\n",
      "Iteration 21109, loss = 0.02372098\n",
      "Iteration 21110, loss = 0.02371974\n",
      "Iteration 21111, loss = 0.02371852\n",
      "Iteration 21112, loss = 0.02371726\n",
      "Iteration 21113, loss = 0.02371603\n",
      "Iteration 21114, loss = 0.02371479\n",
      "Iteration 21115, loss = 0.02371355\n",
      "Iteration 21116, loss = 0.02371231\n",
      "Iteration 21117, loss = 0.02371106\n",
      "Iteration 21118, loss = 0.02370980\n",
      "Iteration 21119, loss = 0.02370858\n",
      "Iteration 21120, loss = 0.02370735\n",
      "Iteration 21121, loss = 0.02370611\n",
      "Iteration 21122, loss = 0.02370487\n",
      "Iteration 21123, loss = 0.02370361\n",
      "Iteration 21124, loss = 0.02370239\n",
      "Iteration 21125, loss = 0.02370116\n",
      "Iteration 21126, loss = 0.02369991\n",
      "Iteration 21127, loss = 0.02369868\n",
      "Iteration 21128, loss = 0.02369744\n",
      "Iteration 21129, loss = 0.02369622\n",
      "Iteration 21130, loss = 0.02369497\n",
      "Iteration 21131, loss = 0.02369373\n",
      "Iteration 21132, loss = 0.02369248\n",
      "Iteration 21133, loss = 0.02369126\n",
      "Iteration 21134, loss = 0.02369002\n",
      "Iteration 21135, loss = 0.02368879\n",
      "Iteration 21136, loss = 0.02368755\n",
      "Iteration 21137, loss = 0.02368631\n",
      "Iteration 21138, loss = 0.02368509\n",
      "Iteration 21139, loss = 0.02368384\n",
      "Iteration 21140, loss = 0.02368260\n",
      "Iteration 21141, loss = 0.02368137\n",
      "Iteration 21142, loss = 0.02368013\n",
      "Iteration 21143, loss = 0.02367890\n",
      "Iteration 21144, loss = 0.02367763\n",
      "Iteration 21145, loss = 0.02367642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21146, loss = 0.02367518\n",
      "Iteration 21147, loss = 0.02367394\n",
      "Iteration 21148, loss = 0.02367269\n",
      "Iteration 21149, loss = 0.02367146\n",
      "Iteration 21150, loss = 0.02367024\n",
      "Iteration 21151, loss = 0.02366898\n",
      "Iteration 21152, loss = 0.02366776\n",
      "Iteration 21153, loss = 0.02366653\n",
      "Iteration 21154, loss = 0.02366528\n",
      "Iteration 21155, loss = 0.02366406\n",
      "Iteration 21156, loss = 0.02366282\n",
      "Iteration 21157, loss = 0.02366158\n",
      "Iteration 21158, loss = 0.02366032\n",
      "Iteration 21159, loss = 0.02365911\n",
      "Iteration 21160, loss = 0.02365787\n",
      "Iteration 21161, loss = 0.02365663\n",
      "Iteration 21162, loss = 0.02365538\n",
      "Iteration 21163, loss = 0.02365415\n",
      "Iteration 21164, loss = 0.02365291\n",
      "Iteration 21165, loss = 0.02365168\n",
      "Iteration 21166, loss = 0.02365046\n",
      "Iteration 21167, loss = 0.02364921\n",
      "Iteration 21168, loss = 0.02364798\n",
      "Iteration 21169, loss = 0.02364675\n",
      "Iteration 21170, loss = 0.02364552\n",
      "Iteration 21171, loss = 0.02364427\n",
      "Iteration 21172, loss = 0.02364305\n",
      "Iteration 21173, loss = 0.02364181\n",
      "Iteration 21174, loss = 0.02364058\n",
      "Iteration 21175, loss = 0.02363936\n",
      "Iteration 21176, loss = 0.02363813\n",
      "Iteration 21177, loss = 0.02363688\n",
      "Iteration 21178, loss = 0.02363564\n",
      "Iteration 21179, loss = 0.02363441\n",
      "Iteration 21180, loss = 0.02363318\n",
      "Iteration 21181, loss = 0.02363195\n",
      "Iteration 21182, loss = 0.02363073\n",
      "Iteration 21183, loss = 0.02362950\n",
      "Iteration 21184, loss = 0.02362826\n",
      "Iteration 21185, loss = 0.02362704\n",
      "Iteration 21186, loss = 0.02362581\n",
      "Iteration 21187, loss = 0.02362459\n",
      "Iteration 21188, loss = 0.02362335\n",
      "Iteration 21189, loss = 0.02362210\n",
      "Iteration 21190, loss = 0.02362088\n",
      "Iteration 21191, loss = 0.02361966\n",
      "Iteration 21192, loss = 0.02361844\n",
      "Iteration 21193, loss = 0.02361720\n",
      "Iteration 21194, loss = 0.02361597\n",
      "Iteration 21195, loss = 0.02361476\n",
      "Iteration 21196, loss = 0.02361351\n",
      "Iteration 21197, loss = 0.02361230\n",
      "Iteration 21198, loss = 0.02361106\n",
      "Iteration 21199, loss = 0.02360984\n",
      "Iteration 21200, loss = 0.02360862\n",
      "Iteration 21201, loss = 0.02360738\n",
      "Iteration 21202, loss = 0.02360616\n",
      "Iteration 21203, loss = 0.02360492\n",
      "Iteration 21204, loss = 0.02360370\n",
      "Iteration 21205, loss = 0.02360248\n",
      "Iteration 21206, loss = 0.02360124\n",
      "Iteration 21207, loss = 0.02360001\n",
      "Iteration 21208, loss = 0.02359880\n",
      "Iteration 21209, loss = 0.02359757\n",
      "Iteration 21210, loss = 0.02359634\n",
      "Iteration 21211, loss = 0.02359511\n",
      "Iteration 21212, loss = 0.02359388\n",
      "Iteration 21213, loss = 0.02359265\n",
      "Iteration 21214, loss = 0.02359146\n",
      "Iteration 21215, loss = 0.02359019\n",
      "Iteration 21216, loss = 0.02358897\n",
      "Iteration 21217, loss = 0.02358773\n",
      "Iteration 21218, loss = 0.02358651\n",
      "Iteration 21219, loss = 0.02358528\n",
      "Iteration 21220, loss = 0.02358405\n",
      "Iteration 21221, loss = 0.02358282\n",
      "Iteration 21222, loss = 0.02358161\n",
      "Iteration 21223, loss = 0.02358036\n",
      "Iteration 21224, loss = 0.02357913\n",
      "Iteration 21225, loss = 0.02357790\n",
      "Iteration 21226, loss = 0.02357668\n",
      "Iteration 21227, loss = 0.02357544\n",
      "Iteration 21228, loss = 0.02357422\n",
      "Iteration 21229, loss = 0.02357300\n",
      "Iteration 21230, loss = 0.02357178\n",
      "Iteration 21231, loss = 0.02357055\n",
      "Iteration 21232, loss = 0.02356932\n",
      "Iteration 21233, loss = 0.02356810\n",
      "Iteration 21234, loss = 0.02356685\n",
      "Iteration 21235, loss = 0.02356564\n",
      "Iteration 21236, loss = 0.02356441\n",
      "Iteration 21237, loss = 0.02356318\n",
      "Iteration 21238, loss = 0.02356195\n",
      "Iteration 21239, loss = 0.02356073\n",
      "Iteration 21240, loss = 0.02355951\n",
      "Iteration 21241, loss = 0.02355826\n",
      "Iteration 21242, loss = 0.02355704\n",
      "Iteration 21243, loss = 0.02355583\n",
      "Iteration 21244, loss = 0.02355459\n",
      "Iteration 21245, loss = 0.02355336\n",
      "Iteration 21246, loss = 0.02355215\n",
      "Iteration 21247, loss = 0.02355094\n",
      "Iteration 21248, loss = 0.02354968\n",
      "Iteration 21249, loss = 0.02354849\n",
      "Iteration 21250, loss = 0.02354725\n",
      "Iteration 21251, loss = 0.02354603\n",
      "Iteration 21252, loss = 0.02354480\n",
      "Iteration 21253, loss = 0.02354358\n",
      "Iteration 21254, loss = 0.02354236\n",
      "Iteration 21255, loss = 0.02354116\n",
      "Iteration 21256, loss = 0.02353992\n",
      "Iteration 21257, loss = 0.02353869\n",
      "Iteration 21258, loss = 0.02353747\n",
      "Iteration 21259, loss = 0.02353626\n",
      "Iteration 21260, loss = 0.02353502\n",
      "Iteration 21261, loss = 0.02353378\n",
      "Iteration 21262, loss = 0.02353256\n",
      "Iteration 21263, loss = 0.02353135\n",
      "Iteration 21264, loss = 0.02353012\n",
      "Iteration 21265, loss = 0.02352892\n",
      "Iteration 21266, loss = 0.02352767\n",
      "Iteration 21267, loss = 0.02352646\n",
      "Iteration 21268, loss = 0.02352523\n",
      "Iteration 21269, loss = 0.02352402\n",
      "Iteration 21270, loss = 0.02352280\n",
      "Iteration 21271, loss = 0.02352156\n",
      "Iteration 21272, loss = 0.02352035\n",
      "Iteration 21273, loss = 0.02351913\n",
      "Iteration 21274, loss = 0.02351792\n",
      "Iteration 21275, loss = 0.02351670\n",
      "Iteration 21276, loss = 0.02351546\n",
      "Iteration 21277, loss = 0.02351427\n",
      "Iteration 21278, loss = 0.02351303\n",
      "Iteration 21279, loss = 0.02351182\n",
      "Iteration 21280, loss = 0.02351060\n",
      "Iteration 21281, loss = 0.02350938\n",
      "Iteration 21282, loss = 0.02350816\n",
      "Iteration 21283, loss = 0.02350695\n",
      "Iteration 21284, loss = 0.02350572\n",
      "Iteration 21285, loss = 0.02350451\n",
      "Iteration 21286, loss = 0.02350331\n",
      "Iteration 21287, loss = 0.02350205\n",
      "Iteration 21288, loss = 0.02350085\n",
      "Iteration 21289, loss = 0.02349963\n",
      "Iteration 21290, loss = 0.02349840\n",
      "Iteration 21291, loss = 0.02349720\n",
      "Iteration 21292, loss = 0.02349597\n",
      "Iteration 21293, loss = 0.02349474\n",
      "Iteration 21294, loss = 0.02349353\n",
      "Iteration 21295, loss = 0.02349229\n",
      "Iteration 21296, loss = 0.02349107\n",
      "Iteration 21297, loss = 0.02348986\n",
      "Iteration 21298, loss = 0.02348865\n",
      "Iteration 21299, loss = 0.02348742\n",
      "Iteration 21300, loss = 0.02348621\n",
      "Iteration 21301, loss = 0.02348498\n",
      "Iteration 21302, loss = 0.02348377\n",
      "Iteration 21303, loss = 0.02348256\n",
      "Iteration 21304, loss = 0.02348133\n",
      "Iteration 21305, loss = 0.02348011\n",
      "Iteration 21306, loss = 0.02347892\n",
      "Iteration 21307, loss = 0.02347769\n",
      "Iteration 21308, loss = 0.02347646\n",
      "Iteration 21309, loss = 0.02347524\n",
      "Iteration 21310, loss = 0.02347402\n",
      "Iteration 21311, loss = 0.02347283\n",
      "Iteration 21312, loss = 0.02347159\n",
      "Iteration 21313, loss = 0.02347039\n",
      "Iteration 21314, loss = 0.02346916\n",
      "Iteration 21315, loss = 0.02346796\n",
      "Iteration 21316, loss = 0.02346673\n",
      "Iteration 21317, loss = 0.02346551\n",
      "Iteration 21318, loss = 0.02346429\n",
      "Iteration 21319, loss = 0.02346308\n",
      "Iteration 21320, loss = 0.02346187\n",
      "Iteration 21321, loss = 0.02346065\n",
      "Iteration 21322, loss = 0.02345945\n",
      "Iteration 21323, loss = 0.02345821\n",
      "Iteration 21324, loss = 0.02345700\n",
      "Iteration 21325, loss = 0.02345578\n",
      "Iteration 21326, loss = 0.02345455\n",
      "Iteration 21327, loss = 0.02345336\n",
      "Iteration 21328, loss = 0.02345215\n",
      "Iteration 21329, loss = 0.02345093\n",
      "Iteration 21330, loss = 0.02344971\n",
      "Iteration 21331, loss = 0.02344850\n",
      "Iteration 21332, loss = 0.02344728\n",
      "Iteration 21333, loss = 0.02344607\n",
      "Iteration 21334, loss = 0.02344484\n",
      "Iteration 21335, loss = 0.02344365\n",
      "Iteration 21336, loss = 0.02344244\n",
      "Iteration 21337, loss = 0.02344123\n",
      "Iteration 21338, loss = 0.02344001\n",
      "Iteration 21339, loss = 0.02343881\n",
      "Iteration 21340, loss = 0.02343760\n",
      "Iteration 21341, loss = 0.02343639\n",
      "Iteration 21342, loss = 0.02343517\n",
      "Iteration 21343, loss = 0.02343396\n",
      "Iteration 21344, loss = 0.02343272\n",
      "Iteration 21345, loss = 0.02343153\n",
      "Iteration 21346, loss = 0.02343032\n",
      "Iteration 21347, loss = 0.02342909\n",
      "Iteration 21348, loss = 0.02342790\n",
      "Iteration 21349, loss = 0.02342668\n",
      "Iteration 21350, loss = 0.02342546\n",
      "Iteration 21351, loss = 0.02342425\n",
      "Iteration 21352, loss = 0.02342305\n",
      "Iteration 21353, loss = 0.02342183\n",
      "Iteration 21354, loss = 0.02342060\n",
      "Iteration 21355, loss = 0.02341941\n",
      "Iteration 21356, loss = 0.02341819\n",
      "Iteration 21357, loss = 0.02341698\n",
      "Iteration 21358, loss = 0.02341576\n",
      "Iteration 21359, loss = 0.02341457\n",
      "Iteration 21360, loss = 0.02341334\n",
      "Iteration 21361, loss = 0.02341217\n",
      "Iteration 21362, loss = 0.02341096\n",
      "Iteration 21363, loss = 0.02340974\n",
      "Iteration 21364, loss = 0.02340853\n",
      "Iteration 21365, loss = 0.02340733\n",
      "Iteration 21366, loss = 0.02340612\n",
      "Iteration 21367, loss = 0.02340492\n",
      "Iteration 21368, loss = 0.02340371\n",
      "Iteration 21369, loss = 0.02340250\n",
      "Iteration 21370, loss = 0.02340130\n",
      "Iteration 21371, loss = 0.02340010\n",
      "Iteration 21372, loss = 0.02339888\n",
      "Iteration 21373, loss = 0.02339768\n",
      "Iteration 21374, loss = 0.02339646\n",
      "Iteration 21375, loss = 0.02339528\n",
      "Iteration 21376, loss = 0.02339406\n",
      "Iteration 21377, loss = 0.02339284\n",
      "Iteration 21378, loss = 0.02339164\n",
      "Iteration 21379, loss = 0.02339044\n",
      "Iteration 21380, loss = 0.02338922\n",
      "Iteration 21381, loss = 0.02338803\n",
      "Iteration 21382, loss = 0.02338683\n",
      "Iteration 21383, loss = 0.02338563\n",
      "Iteration 21384, loss = 0.02338442\n",
      "Iteration 21385, loss = 0.02338321\n",
      "Iteration 21386, loss = 0.02338201\n",
      "Iteration 21387, loss = 0.02338083\n",
      "Iteration 21388, loss = 0.02337962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21389, loss = 0.02337840\n",
      "Iteration 21390, loss = 0.02337721\n",
      "Iteration 21391, loss = 0.02337600\n",
      "Iteration 21392, loss = 0.02337481\n",
      "Iteration 21393, loss = 0.02337358\n",
      "Iteration 21394, loss = 0.02337239\n",
      "Iteration 21395, loss = 0.02337118\n",
      "Iteration 21396, loss = 0.02336997\n",
      "Iteration 21397, loss = 0.02336877\n",
      "Iteration 21398, loss = 0.02336756\n",
      "Iteration 21399, loss = 0.02336635\n",
      "Iteration 21400, loss = 0.02336516\n",
      "Iteration 21401, loss = 0.02336395\n",
      "Iteration 21402, loss = 0.02336274\n",
      "Iteration 21403, loss = 0.02336154\n",
      "Iteration 21404, loss = 0.02336032\n",
      "Iteration 21405, loss = 0.02335913\n",
      "Iteration 21406, loss = 0.02335791\n",
      "Iteration 21407, loss = 0.02335671\n",
      "Iteration 21408, loss = 0.02335550\n",
      "Iteration 21409, loss = 0.02335431\n",
      "Iteration 21410, loss = 0.02335309\n",
      "Iteration 21411, loss = 0.02335188\n",
      "Iteration 21412, loss = 0.02335068\n",
      "Iteration 21413, loss = 0.02334950\n",
      "Iteration 21414, loss = 0.02334829\n",
      "Iteration 21415, loss = 0.02334706\n",
      "Iteration 21416, loss = 0.02334586\n",
      "Iteration 21417, loss = 0.02334467\n",
      "Iteration 21418, loss = 0.02334345\n",
      "Iteration 21419, loss = 0.02334226\n",
      "Iteration 21420, loss = 0.02334105\n",
      "Iteration 21421, loss = 0.02333985\n",
      "Iteration 21422, loss = 0.02333863\n",
      "Iteration 21423, loss = 0.02333743\n",
      "Iteration 21424, loss = 0.02333623\n",
      "Iteration 21425, loss = 0.02333503\n",
      "Iteration 21426, loss = 0.02333382\n",
      "Iteration 21427, loss = 0.02333262\n",
      "Iteration 21428, loss = 0.02333141\n",
      "Iteration 21429, loss = 0.02333021\n",
      "Iteration 21430, loss = 0.02332901\n",
      "Iteration 21431, loss = 0.02332780\n",
      "Iteration 21432, loss = 0.02332661\n",
      "Iteration 21433, loss = 0.02332540\n",
      "Iteration 21434, loss = 0.02332421\n",
      "Iteration 21435, loss = 0.02332300\n",
      "Iteration 21436, loss = 0.02332180\n",
      "Iteration 21437, loss = 0.02332059\n",
      "Iteration 21438, loss = 0.02331940\n",
      "Iteration 21439, loss = 0.02331819\n",
      "Iteration 21440, loss = 0.02331698\n",
      "Iteration 21441, loss = 0.02331578\n",
      "Iteration 21442, loss = 0.02331459\n",
      "Iteration 21443, loss = 0.02331338\n",
      "Iteration 21444, loss = 0.02331218\n",
      "Iteration 21445, loss = 0.02331097\n",
      "Iteration 21446, loss = 0.02330978\n",
      "Iteration 21447, loss = 0.02330858\n",
      "Iteration 21448, loss = 0.02330738\n",
      "Iteration 21449, loss = 0.02330618\n",
      "Iteration 21450, loss = 0.02330497\n",
      "Iteration 21451, loss = 0.02330377\n",
      "Iteration 21452, loss = 0.02330258\n",
      "Iteration 21453, loss = 0.02330138\n",
      "Iteration 21454, loss = 0.02330020\n",
      "Iteration 21455, loss = 0.02329897\n",
      "Iteration 21456, loss = 0.02329777\n",
      "Iteration 21457, loss = 0.02329657\n",
      "Iteration 21458, loss = 0.02329538\n",
      "Iteration 21459, loss = 0.02329417\n",
      "Iteration 21460, loss = 0.02329300\n",
      "Iteration 21461, loss = 0.02329179\n",
      "Iteration 21462, loss = 0.02329059\n",
      "Iteration 21463, loss = 0.02328938\n",
      "Iteration 21464, loss = 0.02328818\n",
      "Iteration 21465, loss = 0.02328699\n",
      "Iteration 21466, loss = 0.02328577\n",
      "Iteration 21467, loss = 0.02328458\n",
      "Iteration 21468, loss = 0.02328337\n",
      "Iteration 21469, loss = 0.02328218\n",
      "Iteration 21470, loss = 0.02328099\n",
      "Iteration 21471, loss = 0.02327978\n",
      "Iteration 21472, loss = 0.02327856\n",
      "Iteration 21473, loss = 0.02327738\n",
      "Iteration 21474, loss = 0.02327617\n",
      "Iteration 21475, loss = 0.02327499\n",
      "Iteration 21476, loss = 0.02327378\n",
      "Iteration 21477, loss = 0.02327260\n",
      "Iteration 21478, loss = 0.02327139\n",
      "Iteration 21479, loss = 0.02327017\n",
      "Iteration 21480, loss = 0.02326900\n",
      "Iteration 21481, loss = 0.02326782\n",
      "Iteration 21482, loss = 0.02326660\n",
      "Iteration 21483, loss = 0.02326541\n",
      "Iteration 21484, loss = 0.02326421\n",
      "Iteration 21485, loss = 0.02326301\n",
      "Iteration 21486, loss = 0.02326182\n",
      "Iteration 21487, loss = 0.02326063\n",
      "Iteration 21488, loss = 0.02325944\n",
      "Iteration 21489, loss = 0.02325823\n",
      "Iteration 21490, loss = 0.02325703\n",
      "Iteration 21491, loss = 0.02325584\n",
      "Iteration 21492, loss = 0.02325466\n",
      "Iteration 21493, loss = 0.02325346\n",
      "Iteration 21494, loss = 0.02325226\n",
      "Iteration 21495, loss = 0.02325107\n",
      "Iteration 21496, loss = 0.02324987\n",
      "Iteration 21497, loss = 0.02324866\n",
      "Iteration 21498, loss = 0.02324749\n",
      "Iteration 21499, loss = 0.02324629\n",
      "Iteration 21500, loss = 0.02324509\n",
      "Iteration 21501, loss = 0.02324390\n",
      "Iteration 21502, loss = 0.02324270\n",
      "Iteration 21503, loss = 0.02324151\n",
      "Iteration 21504, loss = 0.02324033\n",
      "Iteration 21505, loss = 0.02323911\n",
      "Iteration 21506, loss = 0.02323793\n",
      "Iteration 21507, loss = 0.02323674\n",
      "Iteration 21508, loss = 0.02323554\n",
      "Iteration 21509, loss = 0.02323436\n",
      "Iteration 21510, loss = 0.02323316\n",
      "Iteration 21511, loss = 0.02323195\n",
      "Iteration 21512, loss = 0.02323078\n",
      "Iteration 21513, loss = 0.02322958\n",
      "Iteration 21514, loss = 0.02322838\n",
      "Iteration 21515, loss = 0.02322717\n",
      "Iteration 21516, loss = 0.02322600\n",
      "Iteration 21517, loss = 0.02322480\n",
      "Iteration 21518, loss = 0.02322360\n",
      "Iteration 21519, loss = 0.02322241\n",
      "Iteration 21520, loss = 0.02322121\n",
      "Iteration 21521, loss = 0.02322002\n",
      "Iteration 21522, loss = 0.02321885\n",
      "Iteration 21523, loss = 0.02321764\n",
      "Iteration 21524, loss = 0.02321646\n",
      "Iteration 21525, loss = 0.02321525\n",
      "Iteration 21526, loss = 0.02321406\n",
      "Iteration 21527, loss = 0.02321287\n",
      "Iteration 21528, loss = 0.02321171\n",
      "Iteration 21529, loss = 0.02321049\n",
      "Iteration 21530, loss = 0.02320930\n",
      "Iteration 21531, loss = 0.02320810\n",
      "Iteration 21532, loss = 0.02320692\n",
      "Iteration 21533, loss = 0.02320574\n",
      "Iteration 21534, loss = 0.02320454\n",
      "Iteration 21535, loss = 0.02320335\n",
      "Iteration 21536, loss = 0.02320216\n",
      "Iteration 21537, loss = 0.02320098\n",
      "Iteration 21538, loss = 0.02319979\n",
      "Iteration 21539, loss = 0.02319860\n",
      "Iteration 21540, loss = 0.02319741\n",
      "Iteration 21541, loss = 0.02319622\n",
      "Iteration 21542, loss = 0.02319504\n",
      "Iteration 21543, loss = 0.02319385\n",
      "Iteration 21544, loss = 0.02319266\n",
      "Iteration 21545, loss = 0.02319148\n",
      "Iteration 21546, loss = 0.02319030\n",
      "Iteration 21547, loss = 0.02318910\n",
      "Iteration 21548, loss = 0.02318793\n",
      "Iteration 21549, loss = 0.02318673\n",
      "Iteration 21550, loss = 0.02318554\n",
      "Iteration 21551, loss = 0.02318435\n",
      "Iteration 21552, loss = 0.02318318\n",
      "Iteration 21553, loss = 0.02318198\n",
      "Iteration 21554, loss = 0.02318080\n",
      "Iteration 21555, loss = 0.02317962\n",
      "Iteration 21556, loss = 0.02317843\n",
      "Iteration 21557, loss = 0.02317726\n",
      "Iteration 21558, loss = 0.02317606\n",
      "Iteration 21559, loss = 0.02317487\n",
      "Iteration 21560, loss = 0.02317369\n",
      "Iteration 21561, loss = 0.02317250\n",
      "Iteration 21562, loss = 0.02317131\n",
      "Iteration 21563, loss = 0.02317012\n",
      "Iteration 21564, loss = 0.02316895\n",
      "Iteration 21565, loss = 0.02316775\n",
      "Iteration 21566, loss = 0.02316656\n",
      "Iteration 21567, loss = 0.02316537\n",
      "Iteration 21568, loss = 0.02316418\n",
      "Iteration 21569, loss = 0.02316301\n",
      "Iteration 21570, loss = 0.02316181\n",
      "Iteration 21571, loss = 0.02316062\n",
      "Iteration 21572, loss = 0.02315943\n",
      "Iteration 21573, loss = 0.02315825\n",
      "Iteration 21574, loss = 0.02315707\n",
      "Iteration 21575, loss = 0.02315588\n",
      "Iteration 21576, loss = 0.02315472\n",
      "Iteration 21577, loss = 0.02315350\n",
      "Iteration 21578, loss = 0.02315234\n",
      "Iteration 21579, loss = 0.02315113\n",
      "Iteration 21580, loss = 0.02314995\n",
      "Iteration 21581, loss = 0.02314877\n",
      "Iteration 21582, loss = 0.02314757\n",
      "Iteration 21583, loss = 0.02314641\n",
      "Iteration 21584, loss = 0.02314520\n",
      "Iteration 21585, loss = 0.02314404\n",
      "Iteration 21586, loss = 0.02314283\n",
      "Iteration 21587, loss = 0.02314165\n",
      "Iteration 21588, loss = 0.02314046\n",
      "Iteration 21589, loss = 0.02313928\n",
      "Iteration 21590, loss = 0.02313812\n",
      "Iteration 21591, loss = 0.02313691\n",
      "Iteration 21592, loss = 0.02313573\n",
      "Iteration 21593, loss = 0.02313455\n",
      "Iteration 21594, loss = 0.02313336\n",
      "Iteration 21595, loss = 0.02313218\n",
      "Iteration 21596, loss = 0.02313100\n",
      "Iteration 21597, loss = 0.02312982\n",
      "Iteration 21598, loss = 0.02312863\n",
      "Iteration 21599, loss = 0.02312746\n",
      "Iteration 21600, loss = 0.02312628\n",
      "Iteration 21601, loss = 0.02312508\n",
      "Iteration 21602, loss = 0.02312390\n",
      "Iteration 21603, loss = 0.02312272\n",
      "Iteration 21604, loss = 0.02312155\n",
      "Iteration 21605, loss = 0.02312036\n",
      "Iteration 21606, loss = 0.02311917\n",
      "Iteration 21607, loss = 0.02311799\n",
      "Iteration 21608, loss = 0.02311682\n",
      "Iteration 21609, loss = 0.02311564\n",
      "Iteration 21610, loss = 0.02311446\n",
      "Iteration 21611, loss = 0.02311328\n",
      "Iteration 21612, loss = 0.02311210\n",
      "Iteration 21613, loss = 0.02311092\n",
      "Iteration 21614, loss = 0.02310974\n",
      "Iteration 21615, loss = 0.02310856\n",
      "Iteration 21616, loss = 0.02310739\n",
      "Iteration 21617, loss = 0.02310620\n",
      "Iteration 21618, loss = 0.02310504\n",
      "Iteration 21619, loss = 0.02310384\n",
      "Iteration 21620, loss = 0.02310267\n",
      "Iteration 21621, loss = 0.02310150\n",
      "Iteration 21622, loss = 0.02310032\n",
      "Iteration 21623, loss = 0.02309916\n",
      "Iteration 21624, loss = 0.02309795\n",
      "Iteration 21625, loss = 0.02309680\n",
      "Iteration 21626, loss = 0.02309561\n",
      "Iteration 21627, loss = 0.02309442\n",
      "Iteration 21628, loss = 0.02309324\n",
      "Iteration 21629, loss = 0.02309207\n",
      "Iteration 21630, loss = 0.02309088\n",
      "Iteration 21631, loss = 0.02308972\n",
      "Iteration 21632, loss = 0.02308854\n",
      "Iteration 21633, loss = 0.02308736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21634, loss = 0.02308619\n",
      "Iteration 21635, loss = 0.02308501\n",
      "Iteration 21636, loss = 0.02308383\n",
      "Iteration 21637, loss = 0.02308265\n",
      "Iteration 21638, loss = 0.02308148\n",
      "Iteration 21639, loss = 0.02308030\n",
      "Iteration 21640, loss = 0.02307913\n",
      "Iteration 21641, loss = 0.02307794\n",
      "Iteration 21642, loss = 0.02307679\n",
      "Iteration 21643, loss = 0.02307559\n",
      "Iteration 21644, loss = 0.02307441\n",
      "Iteration 21645, loss = 0.02307324\n",
      "Iteration 21646, loss = 0.02307205\n",
      "Iteration 21647, loss = 0.02307087\n",
      "Iteration 21648, loss = 0.02306971\n",
      "Iteration 21649, loss = 0.02306852\n",
      "Iteration 21650, loss = 0.02306734\n",
      "Iteration 21651, loss = 0.02306615\n",
      "Iteration 21652, loss = 0.02306498\n",
      "Iteration 21653, loss = 0.02306381\n",
      "Iteration 21654, loss = 0.02306261\n",
      "Iteration 21655, loss = 0.02306145\n",
      "Iteration 21656, loss = 0.02306027\n",
      "Iteration 21657, loss = 0.02305909\n",
      "Iteration 21658, loss = 0.02305792\n",
      "Iteration 21659, loss = 0.02305675\n",
      "Iteration 21660, loss = 0.02305557\n",
      "Iteration 21661, loss = 0.02305440\n",
      "Iteration 21662, loss = 0.02305323\n",
      "Iteration 21663, loss = 0.02305206\n",
      "Iteration 21664, loss = 0.02305089\n",
      "Iteration 21665, loss = 0.02304972\n",
      "Iteration 21666, loss = 0.02304854\n",
      "Iteration 21667, loss = 0.02304737\n",
      "Iteration 21668, loss = 0.02304619\n",
      "Iteration 21669, loss = 0.02304501\n",
      "Iteration 21670, loss = 0.02304384\n",
      "Iteration 21671, loss = 0.02304268\n",
      "Iteration 21672, loss = 0.02304150\n",
      "Iteration 21673, loss = 0.02304032\n",
      "Iteration 21674, loss = 0.02303916\n",
      "Iteration 21675, loss = 0.02303797\n",
      "Iteration 21676, loss = 0.02303680\n",
      "Iteration 21677, loss = 0.02303562\n",
      "Iteration 21678, loss = 0.02303445\n",
      "Iteration 21679, loss = 0.02303328\n",
      "Iteration 21680, loss = 0.02303209\n",
      "Iteration 21681, loss = 0.02303094\n",
      "Iteration 21682, loss = 0.02302976\n",
      "Iteration 21683, loss = 0.02302858\n",
      "Iteration 21684, loss = 0.02302740\n",
      "Iteration 21685, loss = 0.02302623\n",
      "Iteration 21686, loss = 0.02302507\n",
      "Iteration 21687, loss = 0.02302389\n",
      "Iteration 21688, loss = 0.02302272\n",
      "Iteration 21689, loss = 0.02302154\n",
      "Iteration 21690, loss = 0.02302036\n",
      "Iteration 21691, loss = 0.02301919\n",
      "Iteration 21692, loss = 0.02301802\n",
      "Iteration 21693, loss = 0.02301685\n",
      "Iteration 21694, loss = 0.02301567\n",
      "Iteration 21695, loss = 0.02301450\n",
      "Iteration 21696, loss = 0.02301330\n",
      "Iteration 21697, loss = 0.02301213\n",
      "Iteration 21698, loss = 0.02301096\n",
      "Iteration 21699, loss = 0.02300979\n",
      "Iteration 21700, loss = 0.02300861\n",
      "Iteration 21701, loss = 0.02300746\n",
      "Iteration 21702, loss = 0.02300628\n",
      "Iteration 21703, loss = 0.02300510\n",
      "Iteration 21704, loss = 0.02300393\n",
      "Iteration 21705, loss = 0.02300276\n",
      "Iteration 21706, loss = 0.02300160\n",
      "Iteration 21707, loss = 0.02300042\n",
      "Iteration 21708, loss = 0.02299925\n",
      "Iteration 21709, loss = 0.02299809\n",
      "Iteration 21710, loss = 0.02299692\n",
      "Iteration 21711, loss = 0.02299573\n",
      "Iteration 21712, loss = 0.02299457\n",
      "Iteration 21713, loss = 0.02299339\n",
      "Iteration 21714, loss = 0.02299223\n",
      "Iteration 21715, loss = 0.02299107\n",
      "Iteration 21716, loss = 0.02298989\n",
      "Iteration 21717, loss = 0.02298871\n",
      "Iteration 21718, loss = 0.02298754\n",
      "Iteration 21719, loss = 0.02298638\n",
      "Iteration 21720, loss = 0.02298521\n",
      "Iteration 21721, loss = 0.02298403\n",
      "Iteration 21722, loss = 0.02298285\n",
      "Iteration 21723, loss = 0.02298169\n",
      "Iteration 21724, loss = 0.02298053\n",
      "Iteration 21725, loss = 0.02297936\n",
      "Iteration 21726, loss = 0.02297819\n",
      "Iteration 21727, loss = 0.02297701\n",
      "Iteration 21728, loss = 0.02297586\n",
      "Iteration 21729, loss = 0.02297469\n",
      "Iteration 21730, loss = 0.02297351\n",
      "Iteration 21731, loss = 0.02297234\n",
      "Iteration 21732, loss = 0.02297118\n",
      "Iteration 21733, loss = 0.02297000\n",
      "Iteration 21734, loss = 0.02296884\n",
      "Iteration 21735, loss = 0.02296768\n",
      "Iteration 21736, loss = 0.02296650\n",
      "Iteration 21737, loss = 0.02296535\n",
      "Iteration 21738, loss = 0.02296417\n",
      "Iteration 21739, loss = 0.02296299\n",
      "Iteration 21740, loss = 0.02296183\n",
      "Iteration 21741, loss = 0.02296066\n",
      "Iteration 21742, loss = 0.02295950\n",
      "Iteration 21743, loss = 0.02295832\n",
      "Iteration 21744, loss = 0.02295716\n",
      "Iteration 21745, loss = 0.02295599\n",
      "Iteration 21746, loss = 0.02295482\n",
      "Iteration 21747, loss = 0.02295366\n",
      "Iteration 21748, loss = 0.02295250\n",
      "Iteration 21749, loss = 0.02295133\n",
      "Iteration 21750, loss = 0.02295017\n",
      "Iteration 21751, loss = 0.02294901\n",
      "Iteration 21752, loss = 0.02294785\n",
      "Iteration 21753, loss = 0.02294668\n",
      "Iteration 21754, loss = 0.02294553\n",
      "Iteration 21755, loss = 0.02294435\n",
      "Iteration 21756, loss = 0.02294320\n",
      "Iteration 21757, loss = 0.02294202\n",
      "Iteration 21758, loss = 0.02294085\n",
      "Iteration 21759, loss = 0.02293970\n",
      "Iteration 21760, loss = 0.02293851\n",
      "Iteration 21761, loss = 0.02293737\n",
      "Iteration 21762, loss = 0.02293620\n",
      "Iteration 21763, loss = 0.02293504\n",
      "Iteration 21764, loss = 0.02293387\n",
      "Iteration 21765, loss = 0.02293269\n",
      "Iteration 21766, loss = 0.02293155\n",
      "Iteration 21767, loss = 0.02293037\n",
      "Iteration 21768, loss = 0.02292919\n",
      "Iteration 21769, loss = 0.02292802\n",
      "Iteration 21770, loss = 0.02292686\n",
      "Iteration 21771, loss = 0.02292570\n",
      "Iteration 21772, loss = 0.02292455\n",
      "Iteration 21773, loss = 0.02292339\n",
      "Iteration 21774, loss = 0.02292220\n",
      "Iteration 21775, loss = 0.02292105\n",
      "Iteration 21776, loss = 0.02291988\n",
      "Iteration 21777, loss = 0.02291871\n",
      "Iteration 21778, loss = 0.02291755\n",
      "Iteration 21779, loss = 0.02291639\n",
      "Iteration 21780, loss = 0.02291523\n",
      "Iteration 21781, loss = 0.02291405\n",
      "Iteration 21782, loss = 0.02291291\n",
      "Iteration 21783, loss = 0.02291175\n",
      "Iteration 21784, loss = 0.02291058\n",
      "Iteration 21785, loss = 0.02290941\n",
      "Iteration 21786, loss = 0.02290826\n",
      "Iteration 21787, loss = 0.02290710\n",
      "Iteration 21788, loss = 0.02290593\n",
      "Iteration 21789, loss = 0.02290476\n",
      "Iteration 21790, loss = 0.02290360\n",
      "Iteration 21791, loss = 0.02290244\n",
      "Iteration 21792, loss = 0.02290128\n",
      "Iteration 21793, loss = 0.02290012\n",
      "Iteration 21794, loss = 0.02289896\n",
      "Iteration 21795, loss = 0.02289780\n",
      "Iteration 21796, loss = 0.02289663\n",
      "Iteration 21797, loss = 0.02289548\n",
      "Iteration 21798, loss = 0.02289430\n",
      "Iteration 21799, loss = 0.02289315\n",
      "Iteration 21800, loss = 0.02289200\n",
      "Iteration 21801, loss = 0.02289082\n",
      "Iteration 21802, loss = 0.02288967\n",
      "Iteration 21803, loss = 0.02288850\n",
      "Iteration 21804, loss = 0.02288735\n",
      "Iteration 21805, loss = 0.02288619\n",
      "Iteration 21806, loss = 0.02288502\n",
      "Iteration 21807, loss = 0.02288386\n",
      "Iteration 21808, loss = 0.02288269\n",
      "Iteration 21809, loss = 0.02288152\n",
      "Iteration 21810, loss = 0.02288037\n",
      "Iteration 21811, loss = 0.02287919\n",
      "Iteration 21812, loss = 0.02287803\n",
      "Iteration 21813, loss = 0.02287686\n",
      "Iteration 21814, loss = 0.02287570\n",
      "Iteration 21815, loss = 0.02287453\n",
      "Iteration 21816, loss = 0.02287339\n",
      "Iteration 21817, loss = 0.02287221\n",
      "Iteration 21818, loss = 0.02287104\n",
      "Iteration 21819, loss = 0.02286990\n",
      "Iteration 21820, loss = 0.02286873\n",
      "Iteration 21821, loss = 0.02286755\n",
      "Iteration 21822, loss = 0.02286642\n",
      "Iteration 21823, loss = 0.02286525\n",
      "Iteration 21824, loss = 0.02286407\n",
      "Iteration 21825, loss = 0.02286294\n",
      "Iteration 21826, loss = 0.02286179\n",
      "Iteration 21827, loss = 0.02286062\n",
      "Iteration 21828, loss = 0.02285947\n",
      "Iteration 21829, loss = 0.02285831\n",
      "Iteration 21830, loss = 0.02285717\n",
      "Iteration 21831, loss = 0.02285601\n",
      "Iteration 21832, loss = 0.02285485\n",
      "Iteration 21833, loss = 0.02285370\n",
      "Iteration 21834, loss = 0.02285255\n",
      "Iteration 21835, loss = 0.02285139\n",
      "Iteration 21836, loss = 0.02285024\n",
      "Iteration 21837, loss = 0.02284907\n",
      "Iteration 21838, loss = 0.02284793\n",
      "Iteration 21839, loss = 0.02284676\n",
      "Iteration 21840, loss = 0.02284560\n",
      "Iteration 21841, loss = 0.02284445\n",
      "Iteration 21842, loss = 0.02284331\n",
      "Iteration 21843, loss = 0.02284213\n",
      "Iteration 21844, loss = 0.02284098\n",
      "Iteration 21845, loss = 0.02283983\n",
      "Iteration 21846, loss = 0.02283867\n",
      "Iteration 21847, loss = 0.02283752\n",
      "Iteration 21848, loss = 0.02283638\n",
      "Iteration 21849, loss = 0.02283522\n",
      "Iteration 21850, loss = 0.02283407\n",
      "Iteration 21851, loss = 0.02283290\n",
      "Iteration 21852, loss = 0.02283176\n",
      "Iteration 21853, loss = 0.02283059\n",
      "Iteration 21854, loss = 0.02282944\n",
      "Iteration 21855, loss = 0.02282830\n",
      "Iteration 21856, loss = 0.02282715\n",
      "Iteration 21857, loss = 0.02282598\n",
      "Iteration 21858, loss = 0.02282483\n",
      "Iteration 21859, loss = 0.02282367\n",
      "Iteration 21860, loss = 0.02282252\n",
      "Iteration 21861, loss = 0.02282136\n",
      "Iteration 21862, loss = 0.02282020\n",
      "Iteration 21863, loss = 0.02281904\n",
      "Iteration 21864, loss = 0.02281790\n",
      "Iteration 21865, loss = 0.02281674\n",
      "Iteration 21866, loss = 0.02281560\n",
      "Iteration 21867, loss = 0.02281442\n",
      "Iteration 21868, loss = 0.02281328\n",
      "Iteration 21869, loss = 0.02281213\n",
      "Iteration 21870, loss = 0.02281097\n",
      "Iteration 21871, loss = 0.02280981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21872, loss = 0.02280867\n",
      "Iteration 21873, loss = 0.02280751\n",
      "Iteration 21874, loss = 0.02280636\n",
      "Iteration 21875, loss = 0.02280520\n",
      "Iteration 21876, loss = 0.02280406\n",
      "Iteration 21877, loss = 0.02280290\n",
      "Iteration 21878, loss = 0.02280175\n",
      "Iteration 21879, loss = 0.02280059\n",
      "Iteration 21880, loss = 0.02279943\n",
      "Iteration 21881, loss = 0.02279827\n",
      "Iteration 21882, loss = 0.02279713\n",
      "Iteration 21883, loss = 0.02279598\n",
      "Iteration 21884, loss = 0.02279483\n",
      "Iteration 21885, loss = 0.02279367\n",
      "Iteration 21886, loss = 0.02279253\n",
      "Iteration 21887, loss = 0.02279137\n",
      "Iteration 21888, loss = 0.02279021\n",
      "Iteration 21889, loss = 0.02278906\n",
      "Iteration 21890, loss = 0.02278792\n",
      "Iteration 21891, loss = 0.02278676\n",
      "Iteration 21892, loss = 0.02278561\n",
      "Iteration 21893, loss = 0.02278447\n",
      "Iteration 21894, loss = 0.02278332\n",
      "Iteration 21895, loss = 0.02278217\n",
      "Iteration 21896, loss = 0.02278103\n",
      "Iteration 21897, loss = 0.02277988\n",
      "Iteration 21898, loss = 0.02277874\n",
      "Iteration 21899, loss = 0.02277758\n",
      "Iteration 21900, loss = 0.02277644\n",
      "Iteration 21901, loss = 0.02277529\n",
      "Iteration 21902, loss = 0.02277415\n",
      "Iteration 21903, loss = 0.02277299\n",
      "Iteration 21904, loss = 0.02277183\n",
      "Iteration 21905, loss = 0.02277069\n",
      "Iteration 21906, loss = 0.02276953\n",
      "Iteration 21907, loss = 0.02276838\n",
      "Iteration 21908, loss = 0.02276723\n",
      "Iteration 21909, loss = 0.02276608\n",
      "Iteration 21910, loss = 0.02276495\n",
      "Iteration 21911, loss = 0.02276379\n",
      "Iteration 21912, loss = 0.02276265\n",
      "Iteration 21913, loss = 0.02276151\n",
      "Iteration 21914, loss = 0.02276035\n",
      "Iteration 21915, loss = 0.02275919\n",
      "Iteration 21916, loss = 0.02275804\n",
      "Iteration 21917, loss = 0.02275690\n",
      "Iteration 21918, loss = 0.02275575\n",
      "Iteration 21919, loss = 0.02275460\n",
      "Iteration 21920, loss = 0.02275345\n",
      "Iteration 21921, loss = 0.02275231\n",
      "Iteration 21922, loss = 0.02275114\n",
      "Iteration 21923, loss = 0.02275001\n",
      "Iteration 21924, loss = 0.02274885\n",
      "Iteration 21925, loss = 0.02274772\n",
      "Iteration 21926, loss = 0.02274657\n",
      "Iteration 21927, loss = 0.02274544\n",
      "Iteration 21928, loss = 0.02274426\n",
      "Iteration 21929, loss = 0.02274313\n",
      "Iteration 21930, loss = 0.02274198\n",
      "Iteration 21931, loss = 0.02274084\n",
      "Iteration 21932, loss = 0.02273970\n",
      "Iteration 21933, loss = 0.02273854\n",
      "Iteration 21934, loss = 0.02273740\n",
      "Iteration 21935, loss = 0.02273626\n",
      "Iteration 21936, loss = 0.02273512\n",
      "Iteration 21937, loss = 0.02273395\n",
      "Iteration 21938, loss = 0.02273283\n",
      "Iteration 21939, loss = 0.02273168\n",
      "Iteration 21940, loss = 0.02273054\n",
      "Iteration 21941, loss = 0.02272939\n",
      "Iteration 21942, loss = 0.02272824\n",
      "Iteration 21943, loss = 0.02272712\n",
      "Iteration 21944, loss = 0.02272596\n",
      "Iteration 21945, loss = 0.02272481\n",
      "Iteration 21946, loss = 0.02272370\n",
      "Iteration 21947, loss = 0.02272254\n",
      "Iteration 21948, loss = 0.02272138\n",
      "Iteration 21949, loss = 0.02272026\n",
      "Iteration 21950, loss = 0.02271910\n",
      "Iteration 21951, loss = 0.02271796\n",
      "Iteration 21952, loss = 0.02271682\n",
      "Iteration 21953, loss = 0.02271568\n",
      "Iteration 21954, loss = 0.02271452\n",
      "Iteration 21955, loss = 0.02271338\n",
      "Iteration 21956, loss = 0.02271224\n",
      "Iteration 21957, loss = 0.02271109\n",
      "Iteration 21958, loss = 0.02270995\n",
      "Iteration 21959, loss = 0.02270880\n",
      "Iteration 21960, loss = 0.02270768\n",
      "Iteration 21961, loss = 0.02270654\n",
      "Iteration 21962, loss = 0.02270538\n",
      "Iteration 21963, loss = 0.02270425\n",
      "Iteration 21964, loss = 0.02270310\n",
      "Iteration 21965, loss = 0.02270197\n",
      "Iteration 21966, loss = 0.02270082\n",
      "Iteration 21967, loss = 0.02269968\n",
      "Iteration 21968, loss = 0.02269853\n",
      "Iteration 21969, loss = 0.02269740\n",
      "Iteration 21970, loss = 0.02269626\n",
      "Iteration 21971, loss = 0.02269511\n",
      "Iteration 21972, loss = 0.02269397\n",
      "Iteration 21973, loss = 0.02269284\n",
      "Iteration 21974, loss = 0.02269170\n",
      "Iteration 21975, loss = 0.02269054\n",
      "Iteration 21976, loss = 0.02268940\n",
      "Iteration 21977, loss = 0.02268826\n",
      "Iteration 21978, loss = 0.02268712\n",
      "Iteration 21979, loss = 0.02268597\n",
      "Iteration 21980, loss = 0.02268482\n",
      "Iteration 21981, loss = 0.02268370\n",
      "Iteration 21982, loss = 0.02268255\n",
      "Iteration 21983, loss = 0.02268142\n",
      "Iteration 21984, loss = 0.02268026\n",
      "Iteration 21985, loss = 0.02267913\n",
      "Iteration 21986, loss = 0.02267799\n",
      "Iteration 21987, loss = 0.02267683\n",
      "Iteration 21988, loss = 0.02267572\n",
      "Iteration 21989, loss = 0.02267457\n",
      "Iteration 21990, loss = 0.02267344\n",
      "Iteration 21991, loss = 0.02267228\n",
      "Iteration 21992, loss = 0.02267114\n",
      "Iteration 21993, loss = 0.02267000\n",
      "Iteration 21994, loss = 0.02266889\n",
      "Iteration 21995, loss = 0.02266774\n",
      "Iteration 21996, loss = 0.02266660\n",
      "Iteration 21997, loss = 0.02266546\n",
      "Iteration 21998, loss = 0.02266433\n",
      "Iteration 21999, loss = 0.02266318\n",
      "Iteration 22000, loss = 0.02266207\n",
      "Iteration 22001, loss = 0.02266090\n",
      "Iteration 22002, loss = 0.02265978\n",
      "Iteration 22003, loss = 0.02265862\n",
      "Iteration 22004, loss = 0.02265750\n",
      "Iteration 22005, loss = 0.02265637\n",
      "Iteration 22006, loss = 0.02265521\n",
      "Iteration 22007, loss = 0.02265407\n",
      "Iteration 22008, loss = 0.02265296\n",
      "Iteration 22009, loss = 0.02265182\n",
      "Iteration 22010, loss = 0.02265067\n",
      "Iteration 22011, loss = 0.02264953\n",
      "Iteration 22012, loss = 0.02264838\n",
      "Iteration 22013, loss = 0.02264726\n",
      "Iteration 22014, loss = 0.02264611\n",
      "Iteration 22015, loss = 0.02264497\n",
      "Iteration 22016, loss = 0.02264383\n",
      "Iteration 22017, loss = 0.02264268\n",
      "Iteration 22018, loss = 0.02264155\n",
      "Iteration 22019, loss = 0.02264041\n",
      "Iteration 22020, loss = 0.02263928\n",
      "Iteration 22021, loss = 0.02263813\n",
      "Iteration 22022, loss = 0.02263700\n",
      "Iteration 22023, loss = 0.02263587\n",
      "Iteration 22024, loss = 0.02263473\n",
      "Iteration 22025, loss = 0.02263359\n",
      "Iteration 22026, loss = 0.02263245\n",
      "Iteration 22027, loss = 0.02263132\n",
      "Iteration 22028, loss = 0.02263018\n",
      "Iteration 22029, loss = 0.02262906\n",
      "Iteration 22030, loss = 0.02262791\n",
      "Iteration 22031, loss = 0.02262679\n",
      "Iteration 22032, loss = 0.02262566\n",
      "Iteration 22033, loss = 0.02262453\n",
      "Iteration 22034, loss = 0.02262339\n",
      "Iteration 22035, loss = 0.02262226\n",
      "Iteration 22036, loss = 0.02262113\n",
      "Iteration 22037, loss = 0.02262000\n",
      "Iteration 22038, loss = 0.02261888\n",
      "Iteration 22039, loss = 0.02261773\n",
      "Iteration 22040, loss = 0.02261661\n",
      "Iteration 22041, loss = 0.02261546\n",
      "Iteration 22042, loss = 0.02261434\n",
      "Iteration 22043, loss = 0.02261320\n",
      "Iteration 22044, loss = 0.02261208\n",
      "Iteration 22045, loss = 0.02261093\n",
      "Iteration 22046, loss = 0.02260981\n",
      "Iteration 22047, loss = 0.02260867\n",
      "Iteration 22048, loss = 0.02260754\n",
      "Iteration 22049, loss = 0.02260639\n",
      "Iteration 22050, loss = 0.02260528\n",
      "Iteration 22051, loss = 0.02260414\n",
      "Iteration 22052, loss = 0.02260300\n",
      "Iteration 22053, loss = 0.02260188\n",
      "Iteration 22054, loss = 0.02260075\n",
      "Iteration 22055, loss = 0.02259961\n",
      "Iteration 22056, loss = 0.02259851\n",
      "Iteration 22057, loss = 0.02259733\n",
      "Iteration 22058, loss = 0.02259621\n",
      "Iteration 22059, loss = 0.02259509\n",
      "Iteration 22060, loss = 0.02259394\n",
      "Iteration 22061, loss = 0.02259282\n",
      "Iteration 22062, loss = 0.02259167\n",
      "Iteration 22063, loss = 0.02259053\n",
      "Iteration 22064, loss = 0.02258940\n",
      "Iteration 22065, loss = 0.02258826\n",
      "Iteration 22066, loss = 0.02258714\n",
      "Iteration 22067, loss = 0.02258598\n",
      "Iteration 22068, loss = 0.02258485\n",
      "Iteration 22069, loss = 0.02258372\n",
      "Iteration 22070, loss = 0.02258258\n",
      "Iteration 22071, loss = 0.02258145\n",
      "Iteration 22072, loss = 0.02258033\n",
      "Iteration 22073, loss = 0.02257917\n",
      "Iteration 22074, loss = 0.02257805\n",
      "Iteration 22075, loss = 0.02257693\n",
      "Iteration 22076, loss = 0.02257579\n",
      "Iteration 22077, loss = 0.02257468\n",
      "Iteration 22078, loss = 0.02257355\n",
      "Iteration 22079, loss = 0.02257240\n",
      "Iteration 22080, loss = 0.02257128\n",
      "Iteration 22081, loss = 0.02257014\n",
      "Iteration 22082, loss = 0.02256902\n",
      "Iteration 22083, loss = 0.02256789\n",
      "Iteration 22084, loss = 0.02256677\n",
      "Iteration 22085, loss = 0.02256564\n",
      "Iteration 22086, loss = 0.02256449\n",
      "Iteration 22087, loss = 0.02256337\n",
      "Iteration 22088, loss = 0.02256224\n",
      "Iteration 22089, loss = 0.02256111\n",
      "Iteration 22090, loss = 0.02255997\n",
      "Iteration 22091, loss = 0.02255886\n",
      "Iteration 22092, loss = 0.02255772\n",
      "Iteration 22093, loss = 0.02255658\n",
      "Iteration 22094, loss = 0.02255545\n",
      "Iteration 22095, loss = 0.02255433\n",
      "Iteration 22096, loss = 0.02255319\n",
      "Iteration 22097, loss = 0.02255206\n",
      "Iteration 22098, loss = 0.02255093\n",
      "Iteration 22099, loss = 0.02254980\n",
      "Iteration 22100, loss = 0.02254867\n",
      "Iteration 22101, loss = 0.02254753\n",
      "Iteration 22102, loss = 0.02254640\n",
      "Iteration 22103, loss = 0.02254526\n",
      "Iteration 22104, loss = 0.02254413\n",
      "Iteration 22105, loss = 0.02254301\n",
      "Iteration 22106, loss = 0.02254187\n",
      "Iteration 22107, loss = 0.02254076\n",
      "Iteration 22108, loss = 0.02253962\n",
      "Iteration 22109, loss = 0.02253850\n",
      "Iteration 22110, loss = 0.02253737\n",
      "Iteration 22111, loss = 0.02253623\n",
      "Iteration 22112, loss = 0.02253510\n",
      "Iteration 22113, loss = 0.02253398\n",
      "Iteration 22114, loss = 0.02253286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22115, loss = 0.02253173\n",
      "Iteration 22116, loss = 0.02253061\n",
      "Iteration 22117, loss = 0.02252947\n",
      "Iteration 22118, loss = 0.02252834\n",
      "Iteration 22119, loss = 0.02252722\n",
      "Iteration 22120, loss = 0.02252609\n",
      "Iteration 22121, loss = 0.02252498\n",
      "Iteration 22122, loss = 0.02252386\n",
      "Iteration 22123, loss = 0.02252271\n",
      "Iteration 22124, loss = 0.02252159\n",
      "Iteration 22125, loss = 0.02252047\n",
      "Iteration 22126, loss = 0.02251932\n",
      "Iteration 22127, loss = 0.02251820\n",
      "Iteration 22128, loss = 0.02251707\n",
      "Iteration 22129, loss = 0.02251594\n",
      "Iteration 22130, loss = 0.02251482\n",
      "Iteration 22131, loss = 0.02251368\n",
      "Iteration 22132, loss = 0.02251256\n",
      "Iteration 22133, loss = 0.02251143\n",
      "Iteration 22134, loss = 0.02251030\n",
      "Iteration 22135, loss = 0.02250916\n",
      "Iteration 22136, loss = 0.02250804\n",
      "Iteration 22137, loss = 0.02250691\n",
      "Iteration 22138, loss = 0.02250579\n",
      "Iteration 22139, loss = 0.02250467\n",
      "Iteration 22140, loss = 0.02250353\n",
      "Iteration 22141, loss = 0.02250240\n",
      "Iteration 22142, loss = 0.02250129\n",
      "Iteration 22143, loss = 0.02250015\n",
      "Iteration 22144, loss = 0.02249904\n",
      "Iteration 22145, loss = 0.02249792\n",
      "Iteration 22146, loss = 0.02249678\n",
      "Iteration 22147, loss = 0.02249565\n",
      "Iteration 22148, loss = 0.02249454\n",
      "Iteration 22149, loss = 0.02249339\n",
      "Iteration 22150, loss = 0.02249227\n",
      "Iteration 22151, loss = 0.02249114\n",
      "Iteration 22152, loss = 0.02249001\n",
      "Iteration 22153, loss = 0.02248890\n",
      "Iteration 22154, loss = 0.02248777\n",
      "Iteration 22155, loss = 0.02248664\n",
      "Iteration 22156, loss = 0.02248550\n",
      "Iteration 22157, loss = 0.02248438\n",
      "Iteration 22158, loss = 0.02248324\n",
      "Iteration 22159, loss = 0.02248213\n",
      "Iteration 22160, loss = 0.02248100\n",
      "Iteration 22161, loss = 0.02247988\n",
      "Iteration 22162, loss = 0.02247876\n",
      "Iteration 22163, loss = 0.02247763\n",
      "Iteration 22164, loss = 0.02247650\n",
      "Iteration 22165, loss = 0.02247539\n",
      "Iteration 22166, loss = 0.02247425\n",
      "Iteration 22167, loss = 0.02247314\n",
      "Iteration 22168, loss = 0.02247200\n",
      "Iteration 22169, loss = 0.02247088\n",
      "Iteration 22170, loss = 0.02246977\n",
      "Iteration 22171, loss = 0.02246866\n",
      "Iteration 22172, loss = 0.02246753\n",
      "Iteration 22173, loss = 0.02246641\n",
      "Iteration 22174, loss = 0.02246528\n",
      "Iteration 22175, loss = 0.02246417\n",
      "Iteration 22176, loss = 0.02246304\n",
      "Iteration 22177, loss = 0.02246193\n",
      "Iteration 22178, loss = 0.02246079\n",
      "Iteration 22179, loss = 0.02245968\n",
      "Iteration 22180, loss = 0.02245855\n",
      "Iteration 22181, loss = 0.02245744\n",
      "Iteration 22182, loss = 0.02245632\n",
      "Iteration 22183, loss = 0.02245520\n",
      "Iteration 22184, loss = 0.02245406\n",
      "Iteration 22185, loss = 0.02245296\n",
      "Iteration 22186, loss = 0.02245184\n",
      "Iteration 22187, loss = 0.02245072\n",
      "Iteration 22188, loss = 0.02244959\n",
      "Iteration 22189, loss = 0.02244848\n",
      "Iteration 22190, loss = 0.02244734\n",
      "Iteration 22191, loss = 0.02244622\n",
      "Iteration 22192, loss = 0.02244511\n",
      "Iteration 22193, loss = 0.02244397\n",
      "Iteration 22194, loss = 0.02244283\n",
      "Iteration 22195, loss = 0.02244173\n",
      "Iteration 22196, loss = 0.02244060\n",
      "Iteration 22197, loss = 0.02243948\n",
      "Iteration 22198, loss = 0.02243836\n",
      "Iteration 22199, loss = 0.02243722\n",
      "Iteration 22200, loss = 0.02243610\n",
      "Iteration 22201, loss = 0.02243498\n",
      "Iteration 22202, loss = 0.02243387\n",
      "Iteration 22203, loss = 0.02243275\n",
      "Iteration 22204, loss = 0.02243163\n",
      "Iteration 22205, loss = 0.02243053\n",
      "Iteration 22206, loss = 0.02242939\n",
      "Iteration 22207, loss = 0.02242827\n",
      "Iteration 22208, loss = 0.02242714\n",
      "Iteration 22209, loss = 0.02242603\n",
      "Iteration 22210, loss = 0.02242491\n",
      "Iteration 22211, loss = 0.02242380\n",
      "Iteration 22212, loss = 0.02242267\n",
      "Iteration 22213, loss = 0.02242154\n",
      "Iteration 22214, loss = 0.02242042\n",
      "Iteration 22215, loss = 0.02241932\n",
      "Iteration 22216, loss = 0.02241820\n",
      "Iteration 22217, loss = 0.02241706\n",
      "Iteration 22218, loss = 0.02241596\n",
      "Iteration 22219, loss = 0.02241483\n",
      "Iteration 22220, loss = 0.02241371\n",
      "Iteration 22221, loss = 0.02241260\n",
      "Iteration 22222, loss = 0.02241147\n",
      "Iteration 22223, loss = 0.02241036\n",
      "Iteration 22224, loss = 0.02240924\n",
      "Iteration 22225, loss = 0.02240813\n",
      "Iteration 22226, loss = 0.02240700\n",
      "Iteration 22227, loss = 0.02240588\n",
      "Iteration 22228, loss = 0.02240478\n",
      "Iteration 22229, loss = 0.02240366\n",
      "Iteration 22230, loss = 0.02240253\n",
      "Iteration 22231, loss = 0.02240142\n",
      "Iteration 22232, loss = 0.02240030\n",
      "Iteration 22233, loss = 0.02239919\n",
      "Iteration 22234, loss = 0.02239807\n",
      "Iteration 22235, loss = 0.02239695\n",
      "Iteration 22236, loss = 0.02239584\n",
      "Iteration 22237, loss = 0.02239472\n",
      "Iteration 22238, loss = 0.02239361\n",
      "Iteration 22239, loss = 0.02239250\n",
      "Iteration 22240, loss = 0.02239138\n",
      "Iteration 22241, loss = 0.02239027\n",
      "Iteration 22242, loss = 0.02238915\n",
      "Iteration 22243, loss = 0.02238804\n",
      "Iteration 22244, loss = 0.02238694\n",
      "Iteration 22245, loss = 0.02238581\n",
      "Iteration 22246, loss = 0.02238470\n",
      "Iteration 22247, loss = 0.02238359\n",
      "Iteration 22248, loss = 0.02238247\n",
      "Iteration 22249, loss = 0.02238136\n",
      "Iteration 22250, loss = 0.02238025\n",
      "Iteration 22251, loss = 0.02237913\n",
      "Iteration 22252, loss = 0.02237802\n",
      "Iteration 22253, loss = 0.02237690\n",
      "Iteration 22254, loss = 0.02237579\n",
      "Iteration 22255, loss = 0.02237468\n",
      "Iteration 22256, loss = 0.02237355\n",
      "Iteration 22257, loss = 0.02237243\n",
      "Iteration 22258, loss = 0.02237133\n",
      "Iteration 22259, loss = 0.02237020\n",
      "Iteration 22260, loss = 0.02236911\n",
      "Iteration 22261, loss = 0.02236798\n",
      "Iteration 22262, loss = 0.02236687\n",
      "Iteration 22263, loss = 0.02236575\n",
      "Iteration 22264, loss = 0.02236463\n",
      "Iteration 22265, loss = 0.02236354\n",
      "Iteration 22266, loss = 0.02236242\n",
      "Iteration 22267, loss = 0.02236132\n",
      "Iteration 22268, loss = 0.02236019\n",
      "Iteration 22269, loss = 0.02235907\n",
      "Iteration 22270, loss = 0.02235797\n",
      "Iteration 22271, loss = 0.02235684\n",
      "Iteration 22272, loss = 0.02235574\n",
      "Iteration 22273, loss = 0.02235461\n",
      "Iteration 22274, loss = 0.02235350\n",
      "Iteration 22275, loss = 0.02235238\n",
      "Iteration 22276, loss = 0.02235126\n",
      "Iteration 22277, loss = 0.02235016\n",
      "Iteration 22278, loss = 0.02234905\n",
      "Iteration 22279, loss = 0.02234793\n",
      "Iteration 22280, loss = 0.02234682\n",
      "Iteration 22281, loss = 0.02234570\n",
      "Iteration 22282, loss = 0.02234459\n",
      "Iteration 22283, loss = 0.02234348\n",
      "Iteration 22284, loss = 0.02234236\n",
      "Iteration 22285, loss = 0.02234126\n",
      "Iteration 22286, loss = 0.02234012\n",
      "Iteration 22287, loss = 0.02233903\n",
      "Iteration 22288, loss = 0.02233790\n",
      "Iteration 22289, loss = 0.02233681\n",
      "Iteration 22290, loss = 0.02233569\n",
      "Iteration 22291, loss = 0.02233455\n",
      "Iteration 22292, loss = 0.02233347\n",
      "Iteration 22293, loss = 0.02233233\n",
      "Iteration 22294, loss = 0.02233123\n",
      "Iteration 22295, loss = 0.02233010\n",
      "Iteration 22296, loss = 0.02232901\n",
      "Iteration 22297, loss = 0.02232789\n",
      "Iteration 22298, loss = 0.02232679\n",
      "Iteration 22299, loss = 0.02232567\n",
      "Iteration 22300, loss = 0.02232456\n",
      "Iteration 22301, loss = 0.02232346\n",
      "Iteration 22302, loss = 0.02232234\n",
      "Iteration 22303, loss = 0.02232122\n",
      "Iteration 22304, loss = 0.02232013\n",
      "Iteration 22305, loss = 0.02231901\n",
      "Iteration 22306, loss = 0.02231791\n",
      "Iteration 22307, loss = 0.02231680\n",
      "Iteration 22308, loss = 0.02231569\n",
      "Iteration 22309, loss = 0.02231458\n",
      "Iteration 22310, loss = 0.02231349\n",
      "Iteration 22311, loss = 0.02231236\n",
      "Iteration 22312, loss = 0.02231125\n",
      "Iteration 22313, loss = 0.02231015\n",
      "Iteration 22314, loss = 0.02230902\n",
      "Iteration 22315, loss = 0.02230792\n",
      "Iteration 22316, loss = 0.02230682\n",
      "Iteration 22317, loss = 0.02230570\n",
      "Iteration 22318, loss = 0.02230458\n",
      "Iteration 22319, loss = 0.02230349\n",
      "Iteration 22320, loss = 0.02230236\n",
      "Iteration 22321, loss = 0.02230126\n",
      "Iteration 22322, loss = 0.02230016\n",
      "Iteration 22323, loss = 0.02229905\n",
      "Iteration 22324, loss = 0.02229794\n",
      "Iteration 22325, loss = 0.02229681\n",
      "Iteration 22326, loss = 0.02229573\n",
      "Iteration 22327, loss = 0.02229462\n",
      "Iteration 22328, loss = 0.02229349\n",
      "Iteration 22329, loss = 0.02229238\n",
      "Iteration 22330, loss = 0.02229128\n",
      "Iteration 22331, loss = 0.02229017\n",
      "Iteration 22332, loss = 0.02228907\n",
      "Iteration 22333, loss = 0.02228795\n",
      "Iteration 22334, loss = 0.02228685\n",
      "Iteration 22335, loss = 0.02228573\n",
      "Iteration 22336, loss = 0.02228464\n",
      "Iteration 22337, loss = 0.02228352\n",
      "Iteration 22338, loss = 0.02228241\n",
      "Iteration 22339, loss = 0.02228131\n",
      "Iteration 22340, loss = 0.02228018\n",
      "Iteration 22341, loss = 0.02227909\n",
      "Iteration 22342, loss = 0.02227800\n",
      "Iteration 22343, loss = 0.02227688\n",
      "Iteration 22344, loss = 0.02227575\n",
      "Iteration 22345, loss = 0.02227466\n",
      "Iteration 22346, loss = 0.02227356\n",
      "Iteration 22347, loss = 0.02227244\n",
      "Iteration 22348, loss = 0.02227134\n",
      "Iteration 22349, loss = 0.02227024\n",
      "Iteration 22350, loss = 0.02226913\n",
      "Iteration 22351, loss = 0.02226800\n",
      "Iteration 22352, loss = 0.02226690\n",
      "Iteration 22353, loss = 0.02226582\n",
      "Iteration 22354, loss = 0.02226471\n",
      "Iteration 22355, loss = 0.02226359\n",
      "Iteration 22356, loss = 0.02226248\n",
      "Iteration 22357, loss = 0.02226138\n",
      "Iteration 22358, loss = 0.02226027\n",
      "Iteration 22359, loss = 0.02225917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22360, loss = 0.02225806\n",
      "Iteration 22361, loss = 0.02225695\n",
      "Iteration 22362, loss = 0.02225583\n",
      "Iteration 22363, loss = 0.02225474\n",
      "Iteration 22364, loss = 0.02225362\n",
      "Iteration 22365, loss = 0.02225252\n",
      "Iteration 22366, loss = 0.02225142\n",
      "Iteration 22367, loss = 0.02225031\n",
      "Iteration 22368, loss = 0.02224921\n",
      "Iteration 22369, loss = 0.02224810\n",
      "Iteration 22370, loss = 0.02224699\n",
      "Iteration 22371, loss = 0.02224589\n",
      "Iteration 22372, loss = 0.02224478\n",
      "Iteration 22373, loss = 0.02224368\n",
      "Iteration 22374, loss = 0.02224257\n",
      "Iteration 22375, loss = 0.02224146\n",
      "Iteration 22376, loss = 0.02224036\n",
      "Iteration 22377, loss = 0.02223926\n",
      "Iteration 22378, loss = 0.02223815\n",
      "Iteration 22379, loss = 0.02223703\n",
      "Iteration 22380, loss = 0.02223595\n",
      "Iteration 22381, loss = 0.02223483\n",
      "Iteration 22382, loss = 0.02223375\n",
      "Iteration 22383, loss = 0.02223263\n",
      "Iteration 22384, loss = 0.02223152\n",
      "Iteration 22385, loss = 0.02223041\n",
      "Iteration 22386, loss = 0.02222931\n",
      "Iteration 22387, loss = 0.02222820\n",
      "Iteration 22388, loss = 0.02222709\n",
      "Iteration 22389, loss = 0.02222602\n",
      "Iteration 22390, loss = 0.02222490\n",
      "Iteration 22391, loss = 0.02222378\n",
      "Iteration 22392, loss = 0.02222268\n",
      "Iteration 22393, loss = 0.02222159\n",
      "Iteration 22394, loss = 0.02222048\n",
      "Iteration 22395, loss = 0.02221938\n",
      "Iteration 22396, loss = 0.02221829\n",
      "Iteration 22397, loss = 0.02221716\n",
      "Iteration 22398, loss = 0.02221608\n",
      "Iteration 22399, loss = 0.02221497\n",
      "Iteration 22400, loss = 0.02221385\n",
      "Iteration 22401, loss = 0.02221276\n",
      "Iteration 22402, loss = 0.02221166\n",
      "Iteration 22403, loss = 0.02221056\n",
      "Iteration 22404, loss = 0.02220946\n",
      "Iteration 22405, loss = 0.02220837\n",
      "Iteration 22406, loss = 0.02220726\n",
      "Iteration 22407, loss = 0.02220615\n",
      "Iteration 22408, loss = 0.02220505\n",
      "Iteration 22409, loss = 0.02220395\n",
      "Iteration 22410, loss = 0.02220287\n",
      "Iteration 22411, loss = 0.02220176\n",
      "Iteration 22412, loss = 0.02220066\n",
      "Iteration 22413, loss = 0.02219956\n",
      "Iteration 22414, loss = 0.02219845\n",
      "Iteration 22415, loss = 0.02219736\n",
      "Iteration 22416, loss = 0.02219628\n",
      "Iteration 22417, loss = 0.02219516\n",
      "Iteration 22418, loss = 0.02219407\n",
      "Iteration 22419, loss = 0.02219295\n",
      "Iteration 22420, loss = 0.02219187\n",
      "Iteration 22421, loss = 0.02219077\n",
      "Iteration 22422, loss = 0.02218967\n",
      "Iteration 22423, loss = 0.02218857\n",
      "Iteration 22424, loss = 0.02218747\n",
      "Iteration 22425, loss = 0.02218637\n",
      "Iteration 22426, loss = 0.02218525\n",
      "Iteration 22427, loss = 0.02218417\n",
      "Iteration 22428, loss = 0.02218305\n",
      "Iteration 22429, loss = 0.02218196\n",
      "Iteration 22430, loss = 0.02218086\n",
      "Iteration 22431, loss = 0.02217974\n",
      "Iteration 22432, loss = 0.02217865\n",
      "Iteration 22433, loss = 0.02217755\n",
      "Iteration 22434, loss = 0.02217644\n",
      "Iteration 22435, loss = 0.02217534\n",
      "Iteration 22436, loss = 0.02217422\n",
      "Iteration 22437, loss = 0.02217315\n",
      "Iteration 22438, loss = 0.02217205\n",
      "Iteration 22439, loss = 0.02217094\n",
      "Iteration 22440, loss = 0.02216983\n",
      "Iteration 22441, loss = 0.02216872\n",
      "Iteration 22442, loss = 0.02216764\n",
      "Iteration 22443, loss = 0.02216656\n",
      "Iteration 22444, loss = 0.02216544\n",
      "Iteration 22445, loss = 0.02216436\n",
      "Iteration 22446, loss = 0.02216326\n",
      "Iteration 22447, loss = 0.02216216\n",
      "Iteration 22448, loss = 0.02216105\n",
      "Iteration 22449, loss = 0.02215995\n",
      "Iteration 22450, loss = 0.02215885\n",
      "Iteration 22451, loss = 0.02215774\n",
      "Iteration 22452, loss = 0.02215667\n",
      "Iteration 22453, loss = 0.02215556\n",
      "Iteration 22454, loss = 0.02215446\n",
      "Iteration 22455, loss = 0.02215335\n",
      "Iteration 22456, loss = 0.02215224\n",
      "Iteration 22457, loss = 0.02215116\n",
      "Iteration 22458, loss = 0.02215006\n",
      "Iteration 22459, loss = 0.02214896\n",
      "Iteration 22460, loss = 0.02214786\n",
      "Iteration 22461, loss = 0.02214676\n",
      "Iteration 22462, loss = 0.02214567\n",
      "Iteration 22463, loss = 0.02214456\n",
      "Iteration 22464, loss = 0.02214347\n",
      "Iteration 22465, loss = 0.02214238\n",
      "Iteration 22466, loss = 0.02214129\n",
      "Iteration 22467, loss = 0.02214019\n",
      "Iteration 22468, loss = 0.02213910\n",
      "Iteration 22469, loss = 0.02213801\n",
      "Iteration 22470, loss = 0.02213692\n",
      "Iteration 22471, loss = 0.02213582\n",
      "Iteration 22472, loss = 0.02213474\n",
      "Iteration 22473, loss = 0.02213364\n",
      "Iteration 22474, loss = 0.02213255\n",
      "Iteration 22475, loss = 0.02213146\n",
      "Iteration 22476, loss = 0.02213036\n",
      "Iteration 22477, loss = 0.02212928\n",
      "Iteration 22478, loss = 0.02212818\n",
      "Iteration 22479, loss = 0.02212709\n",
      "Iteration 22480, loss = 0.02212600\n",
      "Iteration 22481, loss = 0.02212489\n",
      "Iteration 22482, loss = 0.02212381\n",
      "Iteration 22483, loss = 0.02212271\n",
      "Iteration 22484, loss = 0.02212163\n",
      "Iteration 22485, loss = 0.02212052\n",
      "Iteration 22486, loss = 0.02211944\n",
      "Iteration 22487, loss = 0.02211834\n",
      "Iteration 22488, loss = 0.02211727\n",
      "Iteration 22489, loss = 0.02211618\n",
      "Iteration 22490, loss = 0.02211506\n",
      "Iteration 22491, loss = 0.02211397\n",
      "Iteration 22492, loss = 0.02211289\n",
      "Iteration 22493, loss = 0.02211179\n",
      "Iteration 22494, loss = 0.02211070\n",
      "Iteration 22495, loss = 0.02210962\n",
      "Iteration 22496, loss = 0.02210852\n",
      "Iteration 22497, loss = 0.02210744\n",
      "Iteration 22498, loss = 0.02210633\n",
      "Iteration 22499, loss = 0.02210525\n",
      "Iteration 22500, loss = 0.02210417\n",
      "Iteration 22501, loss = 0.02210308\n",
      "Iteration 22502, loss = 0.02210198\n",
      "Iteration 22503, loss = 0.02210090\n",
      "Iteration 22504, loss = 0.02209980\n",
      "Iteration 22505, loss = 0.02209871\n",
      "Iteration 22506, loss = 0.02209762\n",
      "Iteration 22507, loss = 0.02209653\n",
      "Iteration 22508, loss = 0.02209544\n",
      "Iteration 22509, loss = 0.02209435\n",
      "Iteration 22510, loss = 0.02209325\n",
      "Iteration 22511, loss = 0.02209217\n",
      "Iteration 22512, loss = 0.02209107\n",
      "Iteration 22513, loss = 0.02208999\n",
      "Iteration 22514, loss = 0.02208889\n",
      "Iteration 22515, loss = 0.02208779\n",
      "Iteration 22516, loss = 0.02208672\n",
      "Iteration 22517, loss = 0.02208562\n",
      "Iteration 22518, loss = 0.02208453\n",
      "Iteration 22519, loss = 0.02208342\n",
      "Iteration 22520, loss = 0.02208233\n",
      "Iteration 22521, loss = 0.02208123\n",
      "Iteration 22522, loss = 0.02208013\n",
      "Iteration 22523, loss = 0.02207906\n",
      "Iteration 22524, loss = 0.02207796\n",
      "Iteration 22525, loss = 0.02207686\n",
      "Iteration 22526, loss = 0.02207577\n",
      "Iteration 22527, loss = 0.02207467\n",
      "Iteration 22528, loss = 0.02207359\n",
      "Iteration 22529, loss = 0.02207250\n",
      "Iteration 22530, loss = 0.02207139\n",
      "Iteration 22531, loss = 0.02207032\n",
      "Iteration 22532, loss = 0.02206922\n",
      "Iteration 22533, loss = 0.02206813\n",
      "Iteration 22534, loss = 0.02206705\n",
      "Iteration 22535, loss = 0.02206596\n",
      "Iteration 22536, loss = 0.02206486\n",
      "Iteration 22537, loss = 0.02206376\n",
      "Iteration 22538, loss = 0.02206268\n",
      "Iteration 22539, loss = 0.02206159\n",
      "Iteration 22540, loss = 0.02206050\n",
      "Iteration 22541, loss = 0.02205942\n",
      "Iteration 22542, loss = 0.02205833\n",
      "Iteration 22543, loss = 0.02205725\n",
      "Iteration 22544, loss = 0.02205615\n",
      "Iteration 22545, loss = 0.02205507\n",
      "Iteration 22546, loss = 0.02205397\n",
      "Iteration 22547, loss = 0.02205289\n",
      "Iteration 22548, loss = 0.02205180\n",
      "Iteration 22549, loss = 0.02205072\n",
      "Iteration 22550, loss = 0.02204964\n",
      "Iteration 22551, loss = 0.02204854\n",
      "Iteration 22552, loss = 0.02204746\n",
      "Iteration 22553, loss = 0.02204638\n",
      "Iteration 22554, loss = 0.02204529\n",
      "Iteration 22555, loss = 0.02204421\n",
      "Iteration 22556, loss = 0.02204314\n",
      "Iteration 22557, loss = 0.02204205\n",
      "Iteration 22558, loss = 0.02204095\n",
      "Iteration 22559, loss = 0.02203985\n",
      "Iteration 22560, loss = 0.02203878\n",
      "Iteration 22561, loss = 0.02203769\n",
      "Iteration 22562, loss = 0.02203661\n",
      "Iteration 22563, loss = 0.02203551\n",
      "Iteration 22564, loss = 0.02203443\n",
      "Iteration 22565, loss = 0.02203334\n",
      "Iteration 22566, loss = 0.02203226\n",
      "Iteration 22567, loss = 0.02203118\n",
      "Iteration 22568, loss = 0.02203008\n",
      "Iteration 22569, loss = 0.02202901\n",
      "Iteration 22570, loss = 0.02202792\n",
      "Iteration 22571, loss = 0.02202681\n",
      "Iteration 22572, loss = 0.02202575\n",
      "Iteration 22573, loss = 0.02202466\n",
      "Iteration 22574, loss = 0.02202357\n",
      "Iteration 22575, loss = 0.02202247\n",
      "Iteration 22576, loss = 0.02202138\n",
      "Iteration 22577, loss = 0.02202031\n",
      "Iteration 22578, loss = 0.02201922\n",
      "Iteration 22579, loss = 0.02201813\n",
      "Iteration 22580, loss = 0.02201704\n",
      "Iteration 22581, loss = 0.02201594\n",
      "Iteration 22582, loss = 0.02201487\n",
      "Iteration 22583, loss = 0.02201376\n",
      "Iteration 22584, loss = 0.02201270\n",
      "Iteration 22585, loss = 0.02201159\n",
      "Iteration 22586, loss = 0.02201050\n",
      "Iteration 22587, loss = 0.02200942\n",
      "Iteration 22588, loss = 0.02200832\n",
      "Iteration 22589, loss = 0.02200724\n",
      "Iteration 22590, loss = 0.02200615\n",
      "Iteration 22591, loss = 0.02200507\n",
      "Iteration 22592, loss = 0.02200398\n",
      "Iteration 22593, loss = 0.02200289\n",
      "Iteration 22594, loss = 0.02200181\n",
      "Iteration 22595, loss = 0.02200071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22596, loss = 0.02199964\n",
      "Iteration 22597, loss = 0.02199855\n",
      "Iteration 22598, loss = 0.02199746\n",
      "Iteration 22599, loss = 0.02199638\n",
      "Iteration 22600, loss = 0.02199529\n",
      "Iteration 22601, loss = 0.02199421\n",
      "Iteration 22602, loss = 0.02199313\n",
      "Iteration 22603, loss = 0.02199205\n",
      "Iteration 22604, loss = 0.02199096\n",
      "Iteration 22605, loss = 0.02198989\n",
      "Iteration 22606, loss = 0.02198881\n",
      "Iteration 22607, loss = 0.02198771\n",
      "Iteration 22608, loss = 0.02198664\n",
      "Iteration 22609, loss = 0.02198557\n",
      "Iteration 22610, loss = 0.02198447\n",
      "Iteration 22611, loss = 0.02198338\n",
      "Iteration 22612, loss = 0.02198230\n",
      "Iteration 22613, loss = 0.02198121\n",
      "Iteration 22614, loss = 0.02198014\n",
      "Iteration 22615, loss = 0.02197905\n",
      "Iteration 22616, loss = 0.02197798\n",
      "Iteration 22617, loss = 0.02197686\n",
      "Iteration 22618, loss = 0.02197580\n",
      "Iteration 22619, loss = 0.02197470\n",
      "Iteration 22620, loss = 0.02197362\n",
      "Iteration 22621, loss = 0.02197254\n",
      "Iteration 22622, loss = 0.02197147\n",
      "Iteration 22623, loss = 0.02197038\n",
      "Iteration 22624, loss = 0.02196928\n",
      "Iteration 22625, loss = 0.02196821\n",
      "Iteration 22626, loss = 0.02196712\n",
      "Iteration 22627, loss = 0.02196604\n",
      "Iteration 22628, loss = 0.02196496\n",
      "Iteration 22629, loss = 0.02196387\n",
      "Iteration 22630, loss = 0.02196278\n",
      "Iteration 22631, loss = 0.02196171\n",
      "Iteration 22632, loss = 0.02196063\n",
      "Iteration 22633, loss = 0.02195954\n",
      "Iteration 22634, loss = 0.02195846\n",
      "Iteration 22635, loss = 0.02195738\n",
      "Iteration 22636, loss = 0.02195629\n",
      "Iteration 22637, loss = 0.02195522\n",
      "Iteration 22638, loss = 0.02195416\n",
      "Iteration 22639, loss = 0.02195308\n",
      "Iteration 22640, loss = 0.02195197\n",
      "Iteration 22641, loss = 0.02195089\n",
      "Iteration 22642, loss = 0.02194981\n",
      "Iteration 22643, loss = 0.02194872\n",
      "Iteration 22644, loss = 0.02194764\n",
      "Iteration 22645, loss = 0.02194656\n",
      "Iteration 22646, loss = 0.02194547\n",
      "Iteration 22647, loss = 0.02194438\n",
      "Iteration 22648, loss = 0.02194329\n",
      "Iteration 22649, loss = 0.02194223\n",
      "Iteration 22650, loss = 0.02194112\n",
      "Iteration 22651, loss = 0.02194005\n",
      "Iteration 22652, loss = 0.02193895\n",
      "Iteration 22653, loss = 0.02193787\n",
      "Iteration 22654, loss = 0.02193679\n",
      "Iteration 22655, loss = 0.02193572\n",
      "Iteration 22656, loss = 0.02193462\n",
      "Iteration 22657, loss = 0.02193353\n",
      "Iteration 22658, loss = 0.02193243\n",
      "Iteration 22659, loss = 0.02193137\n",
      "Iteration 22660, loss = 0.02193028\n",
      "Iteration 22661, loss = 0.02192919\n",
      "Iteration 22662, loss = 0.02192810\n",
      "Iteration 22663, loss = 0.02192701\n",
      "Iteration 22664, loss = 0.02192594\n",
      "Iteration 22665, loss = 0.02192485\n",
      "Iteration 22666, loss = 0.02192377\n",
      "Iteration 22667, loss = 0.02192268\n",
      "Iteration 22668, loss = 0.02192160\n",
      "Iteration 22669, loss = 0.02192052\n",
      "Iteration 22670, loss = 0.02191941\n",
      "Iteration 22671, loss = 0.02191835\n",
      "Iteration 22672, loss = 0.02191725\n",
      "Iteration 22673, loss = 0.02191615\n",
      "Iteration 22674, loss = 0.02191509\n",
      "Iteration 22675, loss = 0.02191400\n",
      "Iteration 22676, loss = 0.02191291\n",
      "Iteration 22677, loss = 0.02191183\n",
      "Iteration 22678, loss = 0.02191076\n",
      "Iteration 22679, loss = 0.02190966\n",
      "Iteration 22680, loss = 0.02190857\n",
      "Iteration 22681, loss = 0.02190750\n",
      "Iteration 22682, loss = 0.02190643\n",
      "Iteration 22683, loss = 0.02190533\n",
      "Iteration 22684, loss = 0.02190427\n",
      "Iteration 22685, loss = 0.02190320\n",
      "Iteration 22686, loss = 0.02190210\n",
      "Iteration 22687, loss = 0.02190102\n",
      "Iteration 22688, loss = 0.02189997\n",
      "Iteration 22689, loss = 0.02189885\n",
      "Iteration 22690, loss = 0.02189778\n",
      "Iteration 22691, loss = 0.02189669\n",
      "Iteration 22692, loss = 0.02189560\n",
      "Iteration 22693, loss = 0.02189452\n",
      "Iteration 22694, loss = 0.02189342\n",
      "Iteration 22695, loss = 0.02189236\n",
      "Iteration 22696, loss = 0.02189127\n",
      "Iteration 22697, loss = 0.02189019\n",
      "Iteration 22698, loss = 0.02188909\n",
      "Iteration 22699, loss = 0.02188800\n",
      "Iteration 22700, loss = 0.02188694\n",
      "Iteration 22701, loss = 0.02188586\n",
      "Iteration 22702, loss = 0.02188477\n",
      "Iteration 22703, loss = 0.02188369\n",
      "Iteration 22704, loss = 0.02188262\n",
      "Iteration 22705, loss = 0.02188156\n",
      "Iteration 22706, loss = 0.02188046\n",
      "Iteration 22707, loss = 0.02187939\n",
      "Iteration 22708, loss = 0.02187829\n",
      "Iteration 22709, loss = 0.02187723\n",
      "Iteration 22710, loss = 0.02187615\n",
      "Iteration 22711, loss = 0.02187507\n",
      "Iteration 22712, loss = 0.02187399\n",
      "Iteration 22713, loss = 0.02187291\n",
      "Iteration 22714, loss = 0.02187185\n",
      "Iteration 22715, loss = 0.02187077\n",
      "Iteration 22716, loss = 0.02186970\n",
      "Iteration 22717, loss = 0.02186862\n",
      "Iteration 22718, loss = 0.02186754\n",
      "Iteration 22719, loss = 0.02186647\n",
      "Iteration 22720, loss = 0.02186538\n",
      "Iteration 22721, loss = 0.02186432\n",
      "Iteration 22722, loss = 0.02186324\n",
      "Iteration 22723, loss = 0.02186214\n",
      "Iteration 22724, loss = 0.02186109\n",
      "Iteration 22725, loss = 0.02185999\n",
      "Iteration 22726, loss = 0.02185893\n",
      "Iteration 22727, loss = 0.02185785\n",
      "Iteration 22728, loss = 0.02185679\n",
      "Iteration 22729, loss = 0.02185568\n",
      "Iteration 22730, loss = 0.02185461\n",
      "Iteration 22731, loss = 0.02185353\n",
      "Iteration 22732, loss = 0.02185246\n",
      "Iteration 22733, loss = 0.02185138\n",
      "Iteration 22734, loss = 0.02185029\n",
      "Iteration 22735, loss = 0.02184920\n",
      "Iteration 22736, loss = 0.02184813\n",
      "Iteration 22737, loss = 0.02184706\n",
      "Iteration 22738, loss = 0.02184596\n",
      "Iteration 22739, loss = 0.02184488\n",
      "Iteration 22740, loss = 0.02184381\n",
      "Iteration 22741, loss = 0.02184273\n",
      "Iteration 22742, loss = 0.02184164\n",
      "Iteration 22743, loss = 0.02184057\n",
      "Iteration 22744, loss = 0.02183947\n",
      "Iteration 22745, loss = 0.02183841\n",
      "Iteration 22746, loss = 0.02183731\n",
      "Iteration 22747, loss = 0.02183625\n",
      "Iteration 22748, loss = 0.02183518\n",
      "Iteration 22749, loss = 0.02183411\n",
      "Iteration 22750, loss = 0.02183301\n",
      "Iteration 22751, loss = 0.02183194\n",
      "Iteration 22752, loss = 0.02183087\n",
      "Iteration 22753, loss = 0.02182980\n",
      "Iteration 22754, loss = 0.02182873\n",
      "Iteration 22755, loss = 0.02182766\n",
      "Iteration 22756, loss = 0.02182657\n",
      "Iteration 22757, loss = 0.02182550\n",
      "Iteration 22758, loss = 0.02182442\n",
      "Iteration 22759, loss = 0.02182335\n",
      "Iteration 22760, loss = 0.02182227\n",
      "Iteration 22761, loss = 0.02182118\n",
      "Iteration 22762, loss = 0.02182012\n",
      "Iteration 22763, loss = 0.02181904\n",
      "Iteration 22764, loss = 0.02181796\n",
      "Iteration 22765, loss = 0.02181686\n",
      "Iteration 22766, loss = 0.02181581\n",
      "Iteration 22767, loss = 0.02181474\n",
      "Iteration 22768, loss = 0.02181367\n",
      "Iteration 22769, loss = 0.02181259\n",
      "Iteration 22770, loss = 0.02181151\n",
      "Iteration 22771, loss = 0.02181044\n",
      "Iteration 22772, loss = 0.02180938\n",
      "Iteration 22773, loss = 0.02180829\n",
      "Iteration 22774, loss = 0.02180722\n",
      "Iteration 22775, loss = 0.02180616\n",
      "Iteration 22776, loss = 0.02180507\n",
      "Iteration 22777, loss = 0.02180399\n",
      "Iteration 22778, loss = 0.02180291\n",
      "Iteration 22779, loss = 0.02180184\n",
      "Iteration 22780, loss = 0.02180077\n",
      "Iteration 22781, loss = 0.02179968\n",
      "Iteration 22782, loss = 0.02179863\n",
      "Iteration 22783, loss = 0.02179753\n",
      "Iteration 22784, loss = 0.02179648\n",
      "Iteration 22785, loss = 0.02179541\n",
      "Iteration 22786, loss = 0.02179433\n",
      "Iteration 22787, loss = 0.02179327\n",
      "Iteration 22788, loss = 0.02179218\n",
      "Iteration 22789, loss = 0.02179113\n",
      "Iteration 22790, loss = 0.02179006\n",
      "Iteration 22791, loss = 0.02178899\n",
      "Iteration 22792, loss = 0.02178792\n",
      "Iteration 22793, loss = 0.02178684\n",
      "Iteration 22794, loss = 0.02178577\n",
      "Iteration 22795, loss = 0.02178470\n",
      "Iteration 22796, loss = 0.02178366\n",
      "Iteration 22797, loss = 0.02178258\n",
      "Iteration 22798, loss = 0.02178148\n",
      "Iteration 22799, loss = 0.02178042\n",
      "Iteration 22800, loss = 0.02177933\n",
      "Iteration 22801, loss = 0.02177828\n",
      "Iteration 22802, loss = 0.02177718\n",
      "Iteration 22803, loss = 0.02177611\n",
      "Iteration 22804, loss = 0.02177505\n",
      "Iteration 22805, loss = 0.02177398\n",
      "Iteration 22806, loss = 0.02177290\n",
      "Iteration 22807, loss = 0.02177183\n",
      "Iteration 22808, loss = 0.02177073\n",
      "Iteration 22809, loss = 0.02176966\n",
      "Iteration 22810, loss = 0.02176860\n",
      "Iteration 22811, loss = 0.02176751\n",
      "Iteration 22812, loss = 0.02176644\n",
      "Iteration 22813, loss = 0.02176539\n",
      "Iteration 22814, loss = 0.02176430\n",
      "Iteration 22815, loss = 0.02176322\n",
      "Iteration 22816, loss = 0.02176215\n",
      "Iteration 22817, loss = 0.02176107\n",
      "Iteration 22818, loss = 0.02176001\n",
      "Iteration 22819, loss = 0.02175893\n",
      "Iteration 22820, loss = 0.02175786\n",
      "Iteration 22821, loss = 0.02175677\n",
      "Iteration 22822, loss = 0.02175574\n",
      "Iteration 22823, loss = 0.02175465\n",
      "Iteration 22824, loss = 0.02175358\n",
      "Iteration 22825, loss = 0.02175253\n",
      "Iteration 22826, loss = 0.02175146\n",
      "Iteration 22827, loss = 0.02175038\n",
      "Iteration 22828, loss = 0.02174932\n",
      "Iteration 22829, loss = 0.02174825\n",
      "Iteration 22830, loss = 0.02174719\n",
      "Iteration 22831, loss = 0.02174611\n",
      "Iteration 22832, loss = 0.02174505\n",
      "Iteration 22833, loss = 0.02174399\n",
      "Iteration 22834, loss = 0.02174291\n",
      "Iteration 22835, loss = 0.02174185\n",
      "Iteration 22836, loss = 0.02174079\n",
      "Iteration 22837, loss = 0.02173971\n",
      "Iteration 22838, loss = 0.02173866\n",
      "Iteration 22839, loss = 0.02173758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22840, loss = 0.02173654\n",
      "Iteration 22841, loss = 0.02173545\n",
      "Iteration 22842, loss = 0.02173438\n",
      "Iteration 22843, loss = 0.02173330\n",
      "Iteration 22844, loss = 0.02173225\n",
      "Iteration 22845, loss = 0.02173116\n",
      "Iteration 22846, loss = 0.02173012\n",
      "Iteration 22847, loss = 0.02172904\n",
      "Iteration 22848, loss = 0.02172797\n",
      "Iteration 22849, loss = 0.02172690\n",
      "Iteration 22850, loss = 0.02172583\n",
      "Iteration 22851, loss = 0.02172477\n",
      "Iteration 22852, loss = 0.02172368\n",
      "Iteration 22853, loss = 0.02172261\n",
      "Iteration 22854, loss = 0.02172157\n",
      "Iteration 22855, loss = 0.02172048\n",
      "Iteration 22856, loss = 0.02171941\n",
      "Iteration 22857, loss = 0.02171834\n",
      "Iteration 22858, loss = 0.02171725\n",
      "Iteration 22859, loss = 0.02171622\n",
      "Iteration 22860, loss = 0.02171513\n",
      "Iteration 22861, loss = 0.02171404\n",
      "Iteration 22862, loss = 0.02171299\n",
      "Iteration 22863, loss = 0.02171191\n",
      "Iteration 22864, loss = 0.02171082\n",
      "Iteration 22865, loss = 0.02170977\n",
      "Iteration 22866, loss = 0.02170871\n",
      "Iteration 22867, loss = 0.02170761\n",
      "Iteration 22868, loss = 0.02170657\n",
      "Iteration 22869, loss = 0.02170548\n",
      "Iteration 22870, loss = 0.02170443\n",
      "Iteration 22871, loss = 0.02170335\n",
      "Iteration 22872, loss = 0.02170229\n",
      "Iteration 22873, loss = 0.02170121\n",
      "Iteration 22874, loss = 0.02170013\n",
      "Iteration 22875, loss = 0.02169909\n",
      "Iteration 22876, loss = 0.02169799\n",
      "Iteration 22877, loss = 0.02169695\n",
      "Iteration 22878, loss = 0.02169587\n",
      "Iteration 22879, loss = 0.02169481\n",
      "Iteration 22880, loss = 0.02169373\n",
      "Iteration 22881, loss = 0.02169267\n",
      "Iteration 22882, loss = 0.02169160\n",
      "Iteration 22883, loss = 0.02169053\n",
      "Iteration 22884, loss = 0.02168947\n",
      "Iteration 22885, loss = 0.02168839\n",
      "Iteration 22886, loss = 0.02168734\n",
      "Iteration 22887, loss = 0.02168627\n",
      "Iteration 22888, loss = 0.02168517\n",
      "Iteration 22889, loss = 0.02168410\n",
      "Iteration 22890, loss = 0.02168303\n",
      "Iteration 22891, loss = 0.02168197\n",
      "Iteration 22892, loss = 0.02168091\n",
      "Iteration 22893, loss = 0.02167982\n",
      "Iteration 22894, loss = 0.02167877\n",
      "Iteration 22895, loss = 0.02167769\n",
      "Iteration 22896, loss = 0.02167662\n",
      "Iteration 22897, loss = 0.02167555\n",
      "Iteration 22898, loss = 0.02167449\n",
      "Iteration 22899, loss = 0.02167344\n",
      "Iteration 22900, loss = 0.02167235\n",
      "Iteration 22901, loss = 0.02167130\n",
      "Iteration 22902, loss = 0.02167024\n",
      "Iteration 22903, loss = 0.02166918\n",
      "Iteration 22904, loss = 0.02166811\n",
      "Iteration 22905, loss = 0.02166705\n",
      "Iteration 22906, loss = 0.02166598\n",
      "Iteration 22907, loss = 0.02166493\n",
      "Iteration 22908, loss = 0.02166387\n",
      "Iteration 22909, loss = 0.02166279\n",
      "Iteration 22910, loss = 0.02166174\n",
      "Iteration 22911, loss = 0.02166067\n",
      "Iteration 22912, loss = 0.02165961\n",
      "Iteration 22913, loss = 0.02165856\n",
      "Iteration 22914, loss = 0.02165749\n",
      "Iteration 22915, loss = 0.02165642\n",
      "Iteration 22916, loss = 0.02165538\n",
      "Iteration 22917, loss = 0.02165432\n",
      "Iteration 22918, loss = 0.02165324\n",
      "Iteration 22919, loss = 0.02165216\n",
      "Iteration 22920, loss = 0.02165113\n",
      "Iteration 22921, loss = 0.02165004\n",
      "Iteration 22922, loss = 0.02164899\n",
      "Iteration 22923, loss = 0.02164792\n",
      "Iteration 22924, loss = 0.02164685\n",
      "Iteration 22925, loss = 0.02164577\n",
      "Iteration 22926, loss = 0.02164472\n",
      "Iteration 22927, loss = 0.02164367\n",
      "Iteration 22928, loss = 0.02164257\n",
      "Iteration 22929, loss = 0.02164152\n",
      "Iteration 22930, loss = 0.02164044\n",
      "Iteration 22931, loss = 0.02163939\n",
      "Iteration 22932, loss = 0.02163832\n",
      "Iteration 22933, loss = 0.02163727\n",
      "Iteration 22934, loss = 0.02163618\n",
      "Iteration 22935, loss = 0.02163514\n",
      "Iteration 22936, loss = 0.02163409\n",
      "Iteration 22937, loss = 0.02163302\n",
      "Iteration 22938, loss = 0.02163197\n",
      "Iteration 22939, loss = 0.02163091\n",
      "Iteration 22940, loss = 0.02162985\n",
      "Iteration 22941, loss = 0.02162880\n",
      "Iteration 22942, loss = 0.02162773\n",
      "Iteration 22943, loss = 0.02162668\n",
      "Iteration 22944, loss = 0.02162563\n",
      "Iteration 22945, loss = 0.02162458\n",
      "Iteration 22946, loss = 0.02162352\n",
      "Iteration 22947, loss = 0.02162246\n",
      "Iteration 22948, loss = 0.02162141\n",
      "Iteration 22949, loss = 0.02162036\n",
      "Iteration 22950, loss = 0.02161931\n",
      "Iteration 22951, loss = 0.02161824\n",
      "Iteration 22952, loss = 0.02161720\n",
      "Iteration 22953, loss = 0.02161614\n",
      "Iteration 22954, loss = 0.02161510\n",
      "Iteration 22955, loss = 0.02161403\n",
      "Iteration 22956, loss = 0.02161298\n",
      "Iteration 22957, loss = 0.02161193\n",
      "Iteration 22958, loss = 0.02161086\n",
      "Iteration 22959, loss = 0.02160981\n",
      "Iteration 22960, loss = 0.02160877\n",
      "Iteration 22961, loss = 0.02160769\n",
      "Iteration 22962, loss = 0.02160664\n",
      "Iteration 22963, loss = 0.02160559\n",
      "Iteration 22964, loss = 0.02160453\n",
      "Iteration 22965, loss = 0.02160346\n",
      "Iteration 22966, loss = 0.02160240\n",
      "Iteration 22967, loss = 0.02160135\n",
      "Iteration 22968, loss = 0.02160029\n",
      "Iteration 22969, loss = 0.02159924\n",
      "Iteration 22970, loss = 0.02159818\n",
      "Iteration 22971, loss = 0.02159713\n",
      "Iteration 22972, loss = 0.02159606\n",
      "Iteration 22973, loss = 0.02159501\n",
      "Iteration 22974, loss = 0.02159395\n",
      "Iteration 22975, loss = 0.02159290\n",
      "Iteration 22976, loss = 0.02159185\n",
      "Iteration 22977, loss = 0.02159080\n",
      "Iteration 22978, loss = 0.02158973\n",
      "Iteration 22979, loss = 0.02158868\n",
      "Iteration 22980, loss = 0.02158760\n",
      "Iteration 22981, loss = 0.02158655\n",
      "Iteration 22982, loss = 0.02158549\n",
      "Iteration 22983, loss = 0.02158443\n",
      "Iteration 22984, loss = 0.02158336\n",
      "Iteration 22985, loss = 0.02158231\n",
      "Iteration 22986, loss = 0.02158124\n",
      "Iteration 22987, loss = 0.02158016\n",
      "Iteration 22988, loss = 0.02157912\n",
      "Iteration 22989, loss = 0.02157807\n",
      "Iteration 22990, loss = 0.02157701\n",
      "Iteration 22991, loss = 0.02157593\n",
      "Iteration 22992, loss = 0.02157489\n",
      "Iteration 22993, loss = 0.02157383\n",
      "Iteration 22994, loss = 0.02157277\n",
      "Iteration 22995, loss = 0.02157171\n",
      "Iteration 22996, loss = 0.02157064\n",
      "Iteration 22997, loss = 0.02156962\n",
      "Iteration 22998, loss = 0.02156854\n",
      "Iteration 22999, loss = 0.02156748\n",
      "Iteration 23000, loss = 0.02156641\n",
      "Iteration 23001, loss = 0.02156537\n",
      "Iteration 23002, loss = 0.02156431\n",
      "Iteration 23003, loss = 0.02156325\n",
      "Iteration 23004, loss = 0.02156220\n",
      "Iteration 23005, loss = 0.02156111\n",
      "Iteration 23006, loss = 0.02156004\n",
      "Iteration 23007, loss = 0.02155902\n",
      "Iteration 23008, loss = 0.02155795\n",
      "Iteration 23009, loss = 0.02155687\n",
      "Iteration 23010, loss = 0.02155584\n",
      "Iteration 23011, loss = 0.02155476\n",
      "Iteration 23012, loss = 0.02155372\n",
      "Iteration 23013, loss = 0.02155264\n",
      "Iteration 23014, loss = 0.02155159\n",
      "Iteration 23015, loss = 0.02155052\n",
      "Iteration 23016, loss = 0.02154950\n",
      "Iteration 23017, loss = 0.02154841\n",
      "Iteration 23018, loss = 0.02154737\n",
      "Iteration 23019, loss = 0.02154631\n",
      "Iteration 23020, loss = 0.02154525\n",
      "Iteration 23021, loss = 0.02154418\n",
      "Iteration 23022, loss = 0.02154313\n",
      "Iteration 23023, loss = 0.02154207\n",
      "Iteration 23024, loss = 0.02154102\n",
      "Iteration 23025, loss = 0.02153994\n",
      "Iteration 23026, loss = 0.02153890\n",
      "Iteration 23027, loss = 0.02153783\n",
      "Iteration 23028, loss = 0.02153676\n",
      "Iteration 23029, loss = 0.02153570\n",
      "Iteration 23030, loss = 0.02153463\n",
      "Iteration 23031, loss = 0.02153359\n",
      "Iteration 23032, loss = 0.02153250\n",
      "Iteration 23033, loss = 0.02153147\n",
      "Iteration 23034, loss = 0.02153041\n",
      "Iteration 23035, loss = 0.02152934\n",
      "Iteration 23036, loss = 0.02152826\n",
      "Iteration 23037, loss = 0.02152722\n",
      "Iteration 23038, loss = 0.02152617\n",
      "Iteration 23039, loss = 0.02152511\n",
      "Iteration 23040, loss = 0.02152406\n",
      "Iteration 23041, loss = 0.02152300\n",
      "Iteration 23042, loss = 0.02152195\n",
      "Iteration 23043, loss = 0.02152092\n",
      "Iteration 23044, loss = 0.02151986\n",
      "Iteration 23045, loss = 0.02151880\n",
      "Iteration 23046, loss = 0.02151774\n",
      "Iteration 23047, loss = 0.02151669\n",
      "Iteration 23048, loss = 0.02151564\n",
      "Iteration 23049, loss = 0.02151457\n",
      "Iteration 23050, loss = 0.02151352\n",
      "Iteration 23051, loss = 0.02151244\n",
      "Iteration 23052, loss = 0.02151139\n",
      "Iteration 23053, loss = 0.02151036\n",
      "Iteration 23054, loss = 0.02150930\n",
      "Iteration 23055, loss = 0.02150823\n",
      "Iteration 23056, loss = 0.02150716\n",
      "Iteration 23057, loss = 0.02150612\n",
      "Iteration 23058, loss = 0.02150506\n",
      "Iteration 23059, loss = 0.02150401\n",
      "Iteration 23060, loss = 0.02150297\n",
      "Iteration 23061, loss = 0.02150192\n",
      "Iteration 23062, loss = 0.02150085\n",
      "Iteration 23063, loss = 0.02149981\n",
      "Iteration 23064, loss = 0.02149876\n",
      "Iteration 23065, loss = 0.02149768\n",
      "Iteration 23066, loss = 0.02149664\n",
      "Iteration 23067, loss = 0.02149558\n",
      "Iteration 23068, loss = 0.02149454\n",
      "Iteration 23069, loss = 0.02149347\n",
      "Iteration 23070, loss = 0.02149242\n",
      "Iteration 23071, loss = 0.02149137\n",
      "Iteration 23072, loss = 0.02149032\n",
      "Iteration 23073, loss = 0.02148926\n",
      "Iteration 23074, loss = 0.02148821\n",
      "Iteration 23075, loss = 0.02148717\n",
      "Iteration 23076, loss = 0.02148611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23077, loss = 0.02148504\n",
      "Iteration 23078, loss = 0.02148402\n",
      "Iteration 23079, loss = 0.02148293\n",
      "Iteration 23080, loss = 0.02148188\n",
      "Iteration 23081, loss = 0.02148082\n",
      "Iteration 23082, loss = 0.02147978\n",
      "Iteration 23083, loss = 0.02147872\n",
      "Iteration 23084, loss = 0.02147769\n",
      "Iteration 23085, loss = 0.02147661\n",
      "Iteration 23086, loss = 0.02147556\n",
      "Iteration 23087, loss = 0.02147450\n",
      "Iteration 23088, loss = 0.02147347\n",
      "Iteration 23089, loss = 0.02147241\n",
      "Iteration 23090, loss = 0.02147135\n",
      "Iteration 23091, loss = 0.02147028\n",
      "Iteration 23092, loss = 0.02146925\n",
      "Iteration 23093, loss = 0.02146817\n",
      "Iteration 23094, loss = 0.02146711\n",
      "Iteration 23095, loss = 0.02146605\n",
      "Iteration 23096, loss = 0.02146500\n",
      "Iteration 23097, loss = 0.02146398\n",
      "Iteration 23098, loss = 0.02146290\n",
      "Iteration 23099, loss = 0.02146185\n",
      "Iteration 23100, loss = 0.02146078\n",
      "Iteration 23101, loss = 0.02145975\n",
      "Iteration 23102, loss = 0.02145870\n",
      "Iteration 23103, loss = 0.02145766\n",
      "Iteration 23104, loss = 0.02145657\n",
      "Iteration 23105, loss = 0.02145554\n",
      "Iteration 23106, loss = 0.02145450\n",
      "Iteration 23107, loss = 0.02145344\n",
      "Iteration 23108, loss = 0.02145239\n",
      "Iteration 23109, loss = 0.02145134\n",
      "Iteration 23110, loss = 0.02145029\n",
      "Iteration 23111, loss = 0.02144923\n",
      "Iteration 23112, loss = 0.02144820\n",
      "Iteration 23113, loss = 0.02144716\n",
      "Iteration 23114, loss = 0.02144611\n",
      "Iteration 23115, loss = 0.02144505\n",
      "Iteration 23116, loss = 0.02144402\n",
      "Iteration 23117, loss = 0.02144298\n",
      "Iteration 23118, loss = 0.02144194\n",
      "Iteration 23119, loss = 0.02144089\n",
      "Iteration 23120, loss = 0.02143985\n",
      "Iteration 23121, loss = 0.02143882\n",
      "Iteration 23122, loss = 0.02143778\n",
      "Iteration 23123, loss = 0.02143672\n",
      "Iteration 23124, loss = 0.02143569\n",
      "Iteration 23125, loss = 0.02143463\n",
      "Iteration 23126, loss = 0.02143361\n",
      "Iteration 23127, loss = 0.02143255\n",
      "Iteration 23128, loss = 0.02143151\n",
      "Iteration 23129, loss = 0.02143048\n",
      "Iteration 23130, loss = 0.02142942\n",
      "Iteration 23131, loss = 0.02142839\n",
      "Iteration 23132, loss = 0.02142734\n",
      "Iteration 23133, loss = 0.02142629\n",
      "Iteration 23134, loss = 0.02142525\n",
      "Iteration 23135, loss = 0.02142420\n",
      "Iteration 23136, loss = 0.02142315\n",
      "Iteration 23137, loss = 0.02142211\n",
      "Iteration 23138, loss = 0.02142107\n",
      "Iteration 23139, loss = 0.02142001\n",
      "Iteration 23140, loss = 0.02141899\n",
      "Iteration 23141, loss = 0.02141793\n",
      "Iteration 23142, loss = 0.02141688\n",
      "Iteration 23143, loss = 0.02141583\n",
      "Iteration 23144, loss = 0.02141479\n",
      "Iteration 23145, loss = 0.02141374\n",
      "Iteration 23146, loss = 0.02141270\n",
      "Iteration 23147, loss = 0.02141165\n",
      "Iteration 23148, loss = 0.02141061\n",
      "Iteration 23149, loss = 0.02140958\n",
      "Iteration 23150, loss = 0.02140853\n",
      "Iteration 23151, loss = 0.02140747\n",
      "Iteration 23152, loss = 0.02140645\n",
      "Iteration 23153, loss = 0.02140541\n",
      "Iteration 23154, loss = 0.02140435\n",
      "Iteration 23155, loss = 0.02140332\n",
      "Iteration 23156, loss = 0.02140227\n",
      "Iteration 23157, loss = 0.02140124\n",
      "Iteration 23158, loss = 0.02140019\n",
      "Iteration 23159, loss = 0.02139914\n",
      "Iteration 23160, loss = 0.02139812\n",
      "Iteration 23161, loss = 0.02139706\n",
      "Iteration 23162, loss = 0.02139601\n",
      "Iteration 23163, loss = 0.02139496\n",
      "Iteration 23164, loss = 0.02139391\n",
      "Iteration 23165, loss = 0.02139288\n",
      "Iteration 23166, loss = 0.02139182\n",
      "Iteration 23167, loss = 0.02139079\n",
      "Iteration 23168, loss = 0.02138973\n",
      "Iteration 23169, loss = 0.02138868\n",
      "Iteration 23170, loss = 0.02138766\n",
      "Iteration 23171, loss = 0.02138661\n",
      "Iteration 23172, loss = 0.02138557\n",
      "Iteration 23173, loss = 0.02138455\n",
      "Iteration 23174, loss = 0.02138348\n",
      "Iteration 23175, loss = 0.02138243\n",
      "Iteration 23176, loss = 0.02138141\n",
      "Iteration 23177, loss = 0.02138037\n",
      "Iteration 23178, loss = 0.02137934\n",
      "Iteration 23179, loss = 0.02137828\n",
      "Iteration 23180, loss = 0.02137725\n",
      "Iteration 23181, loss = 0.02137622\n",
      "Iteration 23182, loss = 0.02137517\n",
      "Iteration 23183, loss = 0.02137414\n",
      "Iteration 23184, loss = 0.02137310\n",
      "Iteration 23185, loss = 0.02137206\n",
      "Iteration 23186, loss = 0.02137102\n",
      "Iteration 23187, loss = 0.02136999\n",
      "Iteration 23188, loss = 0.02136896\n",
      "Iteration 23189, loss = 0.02136791\n",
      "Iteration 23190, loss = 0.02136689\n",
      "Iteration 23191, loss = 0.02136585\n",
      "Iteration 23192, loss = 0.02136481\n",
      "Iteration 23193, loss = 0.02136378\n",
      "Iteration 23194, loss = 0.02136273\n",
      "Iteration 23195, loss = 0.02136172\n",
      "Iteration 23196, loss = 0.02136067\n",
      "Iteration 23197, loss = 0.02135963\n",
      "Iteration 23198, loss = 0.02135860\n",
      "Iteration 23199, loss = 0.02135758\n",
      "Iteration 23200, loss = 0.02135653\n",
      "Iteration 23201, loss = 0.02135549\n",
      "Iteration 23202, loss = 0.02135447\n",
      "Iteration 23203, loss = 0.02135341\n",
      "Iteration 23204, loss = 0.02135238\n",
      "Iteration 23205, loss = 0.02135135\n",
      "Iteration 23206, loss = 0.02135031\n",
      "Iteration 23207, loss = 0.02134927\n",
      "Iteration 23208, loss = 0.02134823\n",
      "Iteration 23209, loss = 0.02134720\n",
      "Iteration 23210, loss = 0.02134615\n",
      "Iteration 23211, loss = 0.02134512\n",
      "Iteration 23212, loss = 0.02134409\n",
      "Iteration 23213, loss = 0.02134306\n",
      "Iteration 23214, loss = 0.02134203\n",
      "Iteration 23215, loss = 0.02134099\n",
      "Iteration 23216, loss = 0.02133993\n",
      "Iteration 23217, loss = 0.02133892\n",
      "Iteration 23218, loss = 0.02133787\n",
      "Iteration 23219, loss = 0.02133684\n",
      "Iteration 23220, loss = 0.02133579\n",
      "Iteration 23221, loss = 0.02133478\n",
      "Iteration 23222, loss = 0.02133375\n",
      "Iteration 23223, loss = 0.02133268\n",
      "Iteration 23224, loss = 0.02133164\n",
      "Iteration 23225, loss = 0.02133061\n",
      "Iteration 23226, loss = 0.02132960\n",
      "Iteration 23227, loss = 0.02132854\n",
      "Iteration 23228, loss = 0.02132751\n",
      "Iteration 23229, loss = 0.02132646\n",
      "Iteration 23230, loss = 0.02132542\n",
      "Iteration 23231, loss = 0.02132440\n",
      "Iteration 23232, loss = 0.02132334\n",
      "Iteration 23233, loss = 0.02132234\n",
      "Iteration 23234, loss = 0.02132128\n",
      "Iteration 23235, loss = 0.02132026\n",
      "Iteration 23236, loss = 0.02131922\n",
      "Iteration 23237, loss = 0.02131818\n",
      "Iteration 23238, loss = 0.02131716\n",
      "Iteration 23239, loss = 0.02131611\n",
      "Iteration 23240, loss = 0.02131507\n",
      "Iteration 23241, loss = 0.02131404\n",
      "Iteration 23242, loss = 0.02131301\n",
      "Iteration 23243, loss = 0.02131196\n",
      "Iteration 23244, loss = 0.02131096\n",
      "Iteration 23245, loss = 0.02130992\n",
      "Iteration 23246, loss = 0.02130889\n",
      "Iteration 23247, loss = 0.02130784\n",
      "Iteration 23248, loss = 0.02130682\n",
      "Iteration 23249, loss = 0.02130579\n",
      "Iteration 23250, loss = 0.02130473\n",
      "Iteration 23251, loss = 0.02130372\n",
      "Iteration 23252, loss = 0.02130269\n",
      "Iteration 23253, loss = 0.02130166\n",
      "Iteration 23254, loss = 0.02130063\n",
      "Iteration 23255, loss = 0.02129959\n",
      "Iteration 23256, loss = 0.02129858\n",
      "Iteration 23257, loss = 0.02129754\n",
      "Iteration 23258, loss = 0.02129653\n",
      "Iteration 23259, loss = 0.02129549\n",
      "Iteration 23260, loss = 0.02129447\n",
      "Iteration 23261, loss = 0.02129345\n",
      "Iteration 23262, loss = 0.02129242\n",
      "Iteration 23263, loss = 0.02129138\n",
      "Iteration 23264, loss = 0.02129036\n",
      "Iteration 23265, loss = 0.02128935\n",
      "Iteration 23266, loss = 0.02128830\n",
      "Iteration 23267, loss = 0.02128729\n",
      "Iteration 23268, loss = 0.02128626\n",
      "Iteration 23269, loss = 0.02128522\n",
      "Iteration 23270, loss = 0.02128421\n",
      "Iteration 23271, loss = 0.02128317\n",
      "Iteration 23272, loss = 0.02128213\n",
      "Iteration 23273, loss = 0.02128111\n",
      "Iteration 23274, loss = 0.02128009\n",
      "Iteration 23275, loss = 0.02127904\n",
      "Iteration 23276, loss = 0.02127803\n",
      "Iteration 23277, loss = 0.02127700\n",
      "Iteration 23278, loss = 0.02127596\n",
      "Iteration 23279, loss = 0.02127492\n",
      "Iteration 23280, loss = 0.02127391\n",
      "Iteration 23281, loss = 0.02127287\n",
      "Iteration 23282, loss = 0.02127182\n",
      "Iteration 23283, loss = 0.02127082\n",
      "Iteration 23284, loss = 0.02126978\n",
      "Iteration 23285, loss = 0.02126873\n",
      "Iteration 23286, loss = 0.02126770\n",
      "Iteration 23287, loss = 0.02126668\n",
      "Iteration 23288, loss = 0.02126565\n",
      "Iteration 23289, loss = 0.02126462\n",
      "Iteration 23290, loss = 0.02126358\n",
      "Iteration 23291, loss = 0.02126254\n",
      "Iteration 23292, loss = 0.02126151\n",
      "Iteration 23293, loss = 0.02126050\n",
      "Iteration 23294, loss = 0.02125948\n",
      "Iteration 23295, loss = 0.02125844\n",
      "Iteration 23296, loss = 0.02125740\n",
      "Iteration 23297, loss = 0.02125638\n",
      "Iteration 23298, loss = 0.02125533\n",
      "Iteration 23299, loss = 0.02125432\n",
      "Iteration 23300, loss = 0.02125328\n",
      "Iteration 23301, loss = 0.02125227\n",
      "Iteration 23302, loss = 0.02125122\n",
      "Iteration 23303, loss = 0.02125021\n",
      "Iteration 23304, loss = 0.02124918\n",
      "Iteration 23305, loss = 0.02124813\n",
      "Iteration 23306, loss = 0.02124713\n",
      "Iteration 23307, loss = 0.02124606\n",
      "Iteration 23308, loss = 0.02124506\n",
      "Iteration 23309, loss = 0.02124401\n",
      "Iteration 23310, loss = 0.02124301\n",
      "Iteration 23311, loss = 0.02124196\n",
      "Iteration 23312, loss = 0.02124095\n",
      "Iteration 23313, loss = 0.02123993\n",
      "Iteration 23314, loss = 0.02123889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23315, loss = 0.02123788\n",
      "Iteration 23316, loss = 0.02123684\n",
      "Iteration 23317, loss = 0.02123585\n",
      "Iteration 23318, loss = 0.02123481\n",
      "Iteration 23319, loss = 0.02123378\n",
      "Iteration 23320, loss = 0.02123274\n",
      "Iteration 23321, loss = 0.02123172\n",
      "Iteration 23322, loss = 0.02123072\n",
      "Iteration 23323, loss = 0.02122968\n",
      "Iteration 23324, loss = 0.02122865\n",
      "Iteration 23325, loss = 0.02122761\n",
      "Iteration 23326, loss = 0.02122660\n",
      "Iteration 23327, loss = 0.02122557\n",
      "Iteration 23328, loss = 0.02122453\n",
      "Iteration 23329, loss = 0.02122350\n",
      "Iteration 23330, loss = 0.02122249\n",
      "Iteration 23331, loss = 0.02122148\n",
      "Iteration 23332, loss = 0.02122041\n",
      "Iteration 23333, loss = 0.02121940\n",
      "Iteration 23334, loss = 0.02121839\n",
      "Iteration 23335, loss = 0.02121736\n",
      "Iteration 23336, loss = 0.02121632\n",
      "Iteration 23337, loss = 0.02121532\n",
      "Iteration 23338, loss = 0.02121428\n",
      "Iteration 23339, loss = 0.02121326\n",
      "Iteration 23340, loss = 0.02121223\n",
      "Iteration 23341, loss = 0.02121122\n",
      "Iteration 23342, loss = 0.02121019\n",
      "Iteration 23343, loss = 0.02120916\n",
      "Iteration 23344, loss = 0.02120814\n",
      "Iteration 23345, loss = 0.02120710\n",
      "Iteration 23346, loss = 0.02120610\n",
      "Iteration 23347, loss = 0.02120504\n",
      "Iteration 23348, loss = 0.02120403\n",
      "Iteration 23349, loss = 0.02120302\n",
      "Iteration 23350, loss = 0.02120198\n",
      "Iteration 23351, loss = 0.02120098\n",
      "Iteration 23352, loss = 0.02119993\n",
      "Iteration 23353, loss = 0.02119891\n",
      "Iteration 23354, loss = 0.02119790\n",
      "Iteration 23355, loss = 0.02119685\n",
      "Iteration 23356, loss = 0.02119586\n",
      "Iteration 23357, loss = 0.02119481\n",
      "Iteration 23358, loss = 0.02119383\n",
      "Iteration 23359, loss = 0.02119279\n",
      "Iteration 23360, loss = 0.02119176\n",
      "Iteration 23361, loss = 0.02119075\n",
      "Iteration 23362, loss = 0.02118971\n",
      "Iteration 23363, loss = 0.02118871\n",
      "Iteration 23364, loss = 0.02118769\n",
      "Iteration 23365, loss = 0.02118667\n",
      "Iteration 23366, loss = 0.02118567\n",
      "Iteration 23367, loss = 0.02118464\n",
      "Iteration 23368, loss = 0.02118362\n",
      "Iteration 23369, loss = 0.02118260\n",
      "Iteration 23370, loss = 0.02118160\n",
      "Iteration 23371, loss = 0.02118058\n",
      "Iteration 23372, loss = 0.02117957\n",
      "Iteration 23373, loss = 0.02117855\n",
      "Iteration 23374, loss = 0.02117754\n",
      "Iteration 23375, loss = 0.02117651\n",
      "Iteration 23376, loss = 0.02117551\n",
      "Iteration 23377, loss = 0.02117449\n",
      "Iteration 23378, loss = 0.02117346\n",
      "Iteration 23379, loss = 0.02117246\n",
      "Iteration 23380, loss = 0.02117144\n",
      "Iteration 23381, loss = 0.02117041\n",
      "Iteration 23382, loss = 0.02116940\n",
      "Iteration 23383, loss = 0.02116839\n",
      "Iteration 23384, loss = 0.02116737\n",
      "Iteration 23385, loss = 0.02116636\n",
      "Iteration 23386, loss = 0.02116531\n",
      "Iteration 23387, loss = 0.02116432\n",
      "Iteration 23388, loss = 0.02116330\n",
      "Iteration 23389, loss = 0.02116228\n",
      "Iteration 23390, loss = 0.02116125\n",
      "Iteration 23391, loss = 0.02116025\n",
      "Iteration 23392, loss = 0.02115923\n",
      "Iteration 23393, loss = 0.02115820\n",
      "Iteration 23394, loss = 0.02115718\n",
      "Iteration 23395, loss = 0.02115619\n",
      "Iteration 23396, loss = 0.02115515\n",
      "Iteration 23397, loss = 0.02115415\n",
      "Iteration 23398, loss = 0.02115313\n",
      "Iteration 23399, loss = 0.02115210\n",
      "Iteration 23400, loss = 0.02115110\n",
      "Iteration 23401, loss = 0.02115009\n",
      "Iteration 23402, loss = 0.02114907\n",
      "Iteration 23403, loss = 0.02114804\n",
      "Iteration 23404, loss = 0.02114704\n",
      "Iteration 23405, loss = 0.02114601\n",
      "Iteration 23406, loss = 0.02114502\n",
      "Iteration 23407, loss = 0.02114399\n",
      "Iteration 23408, loss = 0.02114296\n",
      "Iteration 23409, loss = 0.02114196\n",
      "Iteration 23410, loss = 0.02114095\n",
      "Iteration 23411, loss = 0.02113993\n",
      "Iteration 23412, loss = 0.02113893\n",
      "Iteration 23413, loss = 0.02113791\n",
      "Iteration 23414, loss = 0.02113691\n",
      "Iteration 23415, loss = 0.02113588\n",
      "Iteration 23416, loss = 0.02113488\n",
      "Iteration 23417, loss = 0.02113384\n",
      "Iteration 23418, loss = 0.02113286\n",
      "Iteration 23419, loss = 0.02113183\n",
      "Iteration 23420, loss = 0.02113082\n",
      "Iteration 23421, loss = 0.02112981\n",
      "Iteration 23422, loss = 0.02112879\n",
      "Iteration 23423, loss = 0.02112777\n",
      "Iteration 23424, loss = 0.02112677\n",
      "Iteration 23425, loss = 0.02112576\n",
      "Iteration 23426, loss = 0.02112474\n",
      "Iteration 23427, loss = 0.02112372\n",
      "Iteration 23428, loss = 0.02112270\n",
      "Iteration 23429, loss = 0.02112171\n",
      "Iteration 23430, loss = 0.02112068\n",
      "Iteration 23431, loss = 0.02111968\n",
      "Iteration 23432, loss = 0.02111863\n",
      "Iteration 23433, loss = 0.02111765\n",
      "Iteration 23434, loss = 0.02111661\n",
      "Iteration 23435, loss = 0.02111562\n",
      "Iteration 23436, loss = 0.02111460\n",
      "Iteration 23437, loss = 0.02111356\n",
      "Iteration 23438, loss = 0.02111256\n",
      "Iteration 23439, loss = 0.02111154\n",
      "Iteration 23440, loss = 0.02111054\n",
      "Iteration 23441, loss = 0.02110951\n",
      "Iteration 23442, loss = 0.02110852\n",
      "Iteration 23443, loss = 0.02110749\n",
      "Iteration 23444, loss = 0.02110648\n",
      "Iteration 23445, loss = 0.02110548\n",
      "Iteration 23446, loss = 0.02110446\n",
      "Iteration 23447, loss = 0.02110345\n",
      "Iteration 23448, loss = 0.02110244\n",
      "Iteration 23449, loss = 0.02110144\n",
      "Iteration 23450, loss = 0.02110042\n",
      "Iteration 23451, loss = 0.02109943\n",
      "Iteration 23452, loss = 0.02109840\n",
      "Iteration 23453, loss = 0.02109739\n",
      "Iteration 23454, loss = 0.02109639\n",
      "Iteration 23455, loss = 0.02109536\n",
      "Iteration 23456, loss = 0.02109437\n",
      "Iteration 23457, loss = 0.02109335\n",
      "Iteration 23458, loss = 0.02109232\n",
      "Iteration 23459, loss = 0.02109132\n",
      "Iteration 23460, loss = 0.02109031\n",
      "Iteration 23461, loss = 0.02108930\n",
      "Iteration 23462, loss = 0.02108831\n",
      "Iteration 23463, loss = 0.02108730\n",
      "Iteration 23464, loss = 0.02108629\n",
      "Iteration 23465, loss = 0.02108529\n",
      "Iteration 23466, loss = 0.02108429\n",
      "Iteration 23467, loss = 0.02108330\n",
      "Iteration 23468, loss = 0.02108228\n",
      "Iteration 23469, loss = 0.02108128\n",
      "Iteration 23470, loss = 0.02108029\n",
      "Iteration 23471, loss = 0.02107928\n",
      "Iteration 23472, loss = 0.02107827\n",
      "Iteration 23473, loss = 0.02107727\n",
      "Iteration 23474, loss = 0.02107628\n",
      "Iteration 23475, loss = 0.02107526\n",
      "Iteration 23476, loss = 0.02107425\n",
      "Iteration 23477, loss = 0.02107328\n",
      "Iteration 23478, loss = 0.02107225\n",
      "Iteration 23479, loss = 0.02107124\n",
      "Iteration 23480, loss = 0.02107023\n",
      "Iteration 23481, loss = 0.02106924\n",
      "Iteration 23482, loss = 0.02106823\n",
      "Iteration 23483, loss = 0.02106724\n",
      "Iteration 23484, loss = 0.02106622\n",
      "Iteration 23485, loss = 0.02106521\n",
      "Iteration 23486, loss = 0.02106421\n",
      "Iteration 23487, loss = 0.02106320\n",
      "Iteration 23488, loss = 0.02106221\n",
      "Iteration 23489, loss = 0.02106120\n",
      "Iteration 23490, loss = 0.02106020\n",
      "Iteration 23491, loss = 0.02105918\n",
      "Iteration 23492, loss = 0.02105818\n",
      "Iteration 23493, loss = 0.02105718\n",
      "Iteration 23494, loss = 0.02105619\n",
      "Iteration 23495, loss = 0.02105518\n",
      "Iteration 23496, loss = 0.02105418\n",
      "Iteration 23497, loss = 0.02105319\n",
      "Iteration 23498, loss = 0.02105217\n",
      "Iteration 23499, loss = 0.02105118\n",
      "Iteration 23500, loss = 0.02105018\n",
      "Iteration 23501, loss = 0.02104918\n",
      "Iteration 23502, loss = 0.02104817\n",
      "Iteration 23503, loss = 0.02104717\n",
      "Iteration 23504, loss = 0.02104617\n",
      "Iteration 23505, loss = 0.02104516\n",
      "Iteration 23506, loss = 0.02104415\n",
      "Iteration 23507, loss = 0.02104315\n",
      "Iteration 23508, loss = 0.02104216\n",
      "Iteration 23509, loss = 0.02104114\n",
      "Iteration 23510, loss = 0.02104014\n",
      "Iteration 23511, loss = 0.02103913\n",
      "Iteration 23512, loss = 0.02103814\n",
      "Iteration 23513, loss = 0.02103712\n",
      "Iteration 23514, loss = 0.02103615\n",
      "Iteration 23515, loss = 0.02103512\n",
      "Iteration 23516, loss = 0.02103413\n",
      "Iteration 23517, loss = 0.02103312\n",
      "Iteration 23518, loss = 0.02103213\n",
      "Iteration 23519, loss = 0.02103110\n",
      "Iteration 23520, loss = 0.02103014\n",
      "Iteration 23521, loss = 0.02102911\n",
      "Iteration 23522, loss = 0.02102812\n",
      "Iteration 23523, loss = 0.02102712\n",
      "Iteration 23524, loss = 0.02102612\n",
      "Iteration 23525, loss = 0.02102513\n",
      "Iteration 23526, loss = 0.02102412\n",
      "Iteration 23527, loss = 0.02102312\n",
      "Iteration 23528, loss = 0.02102213\n",
      "Iteration 23529, loss = 0.02102113\n",
      "Iteration 23530, loss = 0.02102012\n",
      "Iteration 23531, loss = 0.02101912\n",
      "Iteration 23532, loss = 0.02101813\n",
      "Iteration 23533, loss = 0.02101714\n",
      "Iteration 23534, loss = 0.02101615\n",
      "Iteration 23535, loss = 0.02101516\n",
      "Iteration 23536, loss = 0.02101416\n",
      "Iteration 23537, loss = 0.02101315\n",
      "Iteration 23538, loss = 0.02101218\n",
      "Iteration 23539, loss = 0.02101118\n",
      "Iteration 23540, loss = 0.02101020\n",
      "Iteration 23541, loss = 0.02100920\n",
      "Iteration 23542, loss = 0.02100821\n",
      "Iteration 23543, loss = 0.02100721\n",
      "Iteration 23544, loss = 0.02100623\n",
      "Iteration 23545, loss = 0.02100525\n",
      "Iteration 23546, loss = 0.02100424\n",
      "Iteration 23547, loss = 0.02100327\n",
      "Iteration 23548, loss = 0.02100226\n",
      "Iteration 23549, loss = 0.02100128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23550, loss = 0.02100028\n",
      "Iteration 23551, loss = 0.02099929\n",
      "Iteration 23552, loss = 0.02099829\n",
      "Iteration 23553, loss = 0.02099729\n",
      "Iteration 23554, loss = 0.02099629\n",
      "Iteration 23555, loss = 0.02099530\n",
      "Iteration 23556, loss = 0.02099429\n",
      "Iteration 23557, loss = 0.02099330\n",
      "Iteration 23558, loss = 0.02099232\n",
      "Iteration 23559, loss = 0.02099131\n",
      "Iteration 23560, loss = 0.02099032\n",
      "Iteration 23561, loss = 0.02098930\n",
      "Iteration 23562, loss = 0.02098833\n",
      "Iteration 23563, loss = 0.02098732\n",
      "Iteration 23564, loss = 0.02098634\n",
      "Iteration 23565, loss = 0.02098533\n",
      "Iteration 23566, loss = 0.02098435\n",
      "Iteration 23567, loss = 0.02098335\n",
      "Iteration 23568, loss = 0.02098237\n",
      "Iteration 23569, loss = 0.02098137\n",
      "Iteration 23570, loss = 0.02098038\n",
      "Iteration 23571, loss = 0.02097940\n",
      "Iteration 23572, loss = 0.02097841\n",
      "Iteration 23573, loss = 0.02097742\n",
      "Iteration 23574, loss = 0.02097643\n",
      "Iteration 23575, loss = 0.02097543\n",
      "Iteration 23576, loss = 0.02097445\n",
      "Iteration 23577, loss = 0.02097346\n",
      "Iteration 23578, loss = 0.02097248\n",
      "Iteration 23579, loss = 0.02097150\n",
      "Iteration 23580, loss = 0.02097050\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Best parameters found:  {'activation': 'logistic', 'hidden_layer_sizes': (30,), 'solver': 'sgd'}\n"
     ]
    }
   ],
   "source": [
    "# search best parameter\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(30,)],\n",
    "    'activation': ['logistic', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "}\n",
    "\n",
    "# model\n",
    "opts = dict(verbose=1, tol=1e-6, max_iter=int(1e6))\n",
    "nn_mlp_model = MLPClassifier(**opts)\n",
    "\n",
    "# grid search\n",
    "grid_search = GridSearchCV(nn_mlp_model, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# predict\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_nn_mlp_best = best_model.predict(X_test_scaled)\n",
    "# running time : 37m 43s\n",
    "# Iteration 23580, loss = 0.02097050\n",
    "# Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
    "# Best parameters found:  {'activation': 'logistic', 'hidden_layer_sizes': (30,), 'solver': 'sgd'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9ab87610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:38:55.219945Z",
     "start_time": "2024-05-11T20:38:55.191798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">NN_MLP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    standard      \n",
       "      NN_MLP      \n",
       "      metric value\n",
       "0   accuracy  0.95\n",
       "1  precision  0.97\n",
       "2     recall  0.95\n",
       "3   F1-score  0.95"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "nn_mlp_eva_best = func.evaluate_model(y_test, y_pred_nn_mlp_best)\n",
    "# change to df\n",
    "nn_mlp_df_best = pd.DataFrame(list(nn_mlp_eva_best.items()), columns=[\n",
    "                         [\"standard\", \"standard\"], [\"NN_MLP\", \"NN_MLP\"], ['metric', 'value']])\n",
    "nn_mlp_df_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de679327",
   "metadata": {},
   "source": [
    "### NN--MLP conclusion\n",
    "可以發現 best 的結果就已經是最好的 <br>\n",
    "因此使用 best 為最佳模型 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb474640",
   "metadata": {},
   "source": [
    "## NN--SEQ\n",
    "神經網路 (Neural Network) -- 簡單的線性堆疊模型 <br>\n",
    "一層一層的建立神經網路 <br>\n",
    "接著列出現在構建的網路長甚麼樣子 <br>\n",
    "建模之後以 測試集 進行評估 <br>\n",
    "並且列出 4 種指標以供觀察 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdd06288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:21:42.645851Z",
     "start_time": "2024-05-11T20:21:42.246409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ACER\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                262208    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 40)                1320      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 265608 (1.01 MB)\n",
      "Trainable params: 265608 (1.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# change 1~3 to 0~2\n",
    "y_train_nn = y_train\n",
    "y_test_nn = y_test\n",
    "\n",
    "# model\n",
    "nn_seq_model = Sequential()\n",
    "nn_seq_model.add(Dense(64, activation='relu',\n",
    "                 input_shape=(X_train_scaled.shape[1],)))\n",
    "nn_seq_model.add(Dense(32, activation='relu'))\n",
    "# avoid overfitting\n",
    "nn_seq_model.add(Dropout(0.2))\n",
    "nn_seq_model.add(Dense(40, activation='softmax'))\n",
    "\n",
    "# compile\n",
    "# set learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "nn_seq_model.compile(optimizer=optimizer,\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# check model\n",
    "nn_seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cb95b7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:21:46.775125Z",
     "start_time": "2024-05-11T20:21:44.077106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ACER\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ACER\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = nn_seq_model.fit(X_train_scaled, y_train_nn,\n",
    "                           epochs=50, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "890eef99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:21:47.571016Z",
     "start_time": "2024-05-11T20:21:47.314727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.2628282904624939\n",
      "accuracy : 0.9125000238418579\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "test_loss, test_accuracy = nn_seq_model.evaluate(\n",
    "    X_test_scaled, y_test_nn, verbose=0)\n",
    "y_pred_prob = nn_seq_model.predict(X_test_scaled, verbose=0)\n",
    "y_pred_nn_seq = np.argmax(y_pred_prob, axis=1)\n",
    "print(\"loss :\", test_loss)\n",
    "print(\"accuracy :\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b9eb949f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:39:21.684926Z",
     "start_time": "2024-05-11T20:39:21.658017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">NN_SEQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    standard      \n",
       "      NN_SEQ      \n",
       "      metric value\n",
       "0   accuracy  0.91\n",
       "1  precision  0.94\n",
       "2     recall  0.91\n",
       "3   F1-score  0.91"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "nn_seq_eva = func.evaluate_model(y_test_nn, y_pred_nn_seq)\n",
    "# change to df\n",
    "nn_seq_df = pd.DataFrame(list(nn_seq_eva.items()), columns=[\n",
    "                         [\"standard\", \"standard\"], [\"NN_SEQ\", \"NN_SEQ\"], ['metric', 'value']])\n",
    "nn_seq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5edb73b",
   "metadata": {},
   "source": [
    "## compare evaluation of standardize data\n",
    "把上述所有的評估指標組合一起觀察 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "35e281be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:39:23.462639Z",
     "start_time": "2024-05-11T20:39:23.446347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">standard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>MLR</th>\n",
       "      <th>SVM</th>\n",
       "      <th>NN_MLP</th>\n",
       "      <th>NN_SEQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-score</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          standard                    \n",
       "               MLR   SVM NN_MLP NN_SEQ\n",
       "             value value  value  value\n",
       "metric                                \n",
       "accuracy      0.99  0.99   0.95   0.91\n",
       "precision     0.99  0.99   0.97   0.94\n",
       "recall        0.99  0.99   0.95   0.91\n",
       "F1-score      0.99  0.99   0.95   0.91"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine df together\n",
    "mlr_df_copy = mlr_df.copy()\n",
    "svm_df_copy = svm_df.copy()\n",
    "nn_mlp_df_best_copy = nn_mlp_df_best.copy()\n",
    "nn_seq_df_copy = nn_seq_df.copy()\n",
    "# del same col\n",
    "svm_df_copy.drop((\"standard\", \"SVM\", \"metric\"), axis=1, inplace=True)\n",
    "nn_mlp_df_best_copy.drop((\"standard\", \"NN_MLP\", \"metric\"), axis=1, inplace=True)\n",
    "nn_seq_df_copy.drop((\"standard\", \"NN_SEQ\", \"metric\"), axis=1, inplace=True)\n",
    "\n",
    "# combine\n",
    "final_df_standard = pd.concat(\n",
    "    [mlr_df_copy, svm_df_copy, nn_mlp_df_best_copy, nn_seq_df_copy], axis=1)\n",
    "# set \"metric\" to index\n",
    "final_df_standard.set_index((\"standard\", \"MLR\", \"metric\"), inplace=True)\n",
    "final_df_standard = final_df_standard.rename_axis(\"metric\")\n",
    "\n",
    "# show\n",
    "final_df_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958d362",
   "metadata": {},
   "source": [
    "## model of standardize data conclusion\n",
    "可以看出總體來說 MLR、NN_MLP 模型的表現是最好的 <br>\n",
    "不過考慮到運行時間 <br>\n",
    "MLR 似乎會是比較好的選項 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946cc94",
   "metadata": {},
   "source": [
    "# modeling pca data\n",
    "把 PCA 後的資料建模 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0842a5",
   "metadata": {},
   "source": [
    "## MLR\n",
    "多元羅吉斯回歸 (Multinomial Logistic Regression) <br>\n",
    "建模之後以 測試集 進行評估 <br>\n",
    "並且列出 4 種指標以供觀察 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdbd441",
   "metadata": {},
   "source": [
    "### default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e35eb62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:25:14.003635Z",
     "start_time": "2024-05-11T20:25:13.798365Z"
    }
   },
   "outputs": [],
   "source": [
    "# MLR model & hyperparametersmodel\n",
    "mlr_opts = dict(multi_class='auto', tol=1e-6, max_iter=int(1e6), verbose=1)\n",
    "mlr_model = LogisticRegression(**mlr_opts)\n",
    "mlr_model.fit(X_train_pca, y_train)\n",
    "# predict\n",
    "y_pred_mlr = mlr_model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89dfb684",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:34:23.250104Z",
     "start_time": "2024-05-11T20:34:23.225633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">PCA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">MLR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PCA      \n",
       "         MLR      \n",
       "      metric value\n",
       "0   accuracy  0.99\n",
       "1  precision  0.99\n",
       "2     recall  0.99\n",
       "3   F1-score  0.99"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "mlr_eva = func.evaluate_model(y_test, y_pred_mlr)\n",
    "# change to df\n",
    "mlr_df = pd.DataFrame(list(mlr_eva.items()), columns=[\n",
    "                      [\"PCA\", \"PCA\"], [\"MLR\", \"MLR\"], ['metric', 'value']])\n",
    "mlr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63646b2e",
   "metadata": {},
   "source": [
    "### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6389a852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:25:44.092738Z",
     "start_time": "2024-05-11T20:25:39.827378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters found:  {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "# search best parameter\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1],\n",
    "    'solver': ['newton-cg', 'lbfgs'],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# model\n",
    "mlr_opts = dict(multi_class='auto', tol=1e-6, max_iter=int(1e6), verbose=1)\n",
    "mlr_model = LogisticRegression(**mlr_opts)\n",
    "\n",
    "# grid search\n",
    "grid_search = GridSearchCV(mlr_model, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# print\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# predict\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_mlr_best = best_model.predict(X_test_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7679072",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:34:26.764345Z",
     "start_time": "2024-05-11T20:34:26.738230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">PCA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">MLR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PCA      \n",
       "         MLR      \n",
       "      metric value\n",
       "0   accuracy  0.99\n",
       "1  precision  0.99\n",
       "2     recall  0.99\n",
       "3   F1-score  0.99"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "mlr_eva_best = func.evaluate_model(y_test, y_pred_mlr_best)\n",
    "# change to df\n",
    "mlr_df_best = pd.DataFrame(list(mlr_eva_best.items()), columns=[\n",
    "                      [\"PCA\", \"PCA\"], [\"MLR\", \"MLR\"], ['metric', 'value']])\n",
    "mlr_df_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57280bd3",
   "metadata": {},
   "source": [
    "### MLR conclusion\n",
    "可以發現 2個的結果相同 <br>\n",
    "為了避免麻煩 <br>\n",
    "因此使用 default 為最佳模型 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc36e85",
   "metadata": {},
   "source": [
    "## SVM\n",
    "支援向量機 (Support Vector Machine) <br>\n",
    "建模之後以 測試集 進行評估 <br>\n",
    "並且列出 4 種指標以供觀察 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5aab17",
   "metadata": {},
   "source": [
    "### default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7025592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:27:46.328816Z",
     "start_time": "2024-05-11T20:27:46.304574Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVM model & hyperparameter\n",
    "svm_opts = dict(C=1, tol=1e-6, max_iter=int(1e6))\n",
    "svm_model = SVC(kernel='linear', **svm_opts)\n",
    "svm_model.fit(X_train_pca, y_train)\n",
    "# predict\n",
    "y_pred_svm = svm_model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ec413a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:34:53.321810Z",
     "start_time": "2024-05-11T20:34:53.287706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">PCA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">SVM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PCA      \n",
       "         SVM      \n",
       "      metric value\n",
       "0   accuracy  0.99\n",
       "1  precision  0.99\n",
       "2     recall  0.99\n",
       "3   F1-score  0.99"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "svm_eva = func.evaluate_model(y_test, y_pred_svm)\n",
    "# change to df\n",
    "svm_df = pd.DataFrame(list(svm_eva.items()), columns=[\n",
    "                      [\"PCA\", \"PCA\"], [\"SVM\", \"SVM\"], ['metric', 'value']])\n",
    "svm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db91be4",
   "metadata": {},
   "source": [
    "### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3b7bde1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:28:57.407618Z",
     "start_time": "2024-05-11T20:28:57.038066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best parameters found:  {'C': 0.08, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# search best parameter\n",
    "param_grid = {\n",
    "    'C': [0.08, 0.1, 1, 5],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "}\n",
    "\n",
    "# model\n",
    "svm_opts = dict(tol=1e-6, max_iter=int(1e6))\n",
    "svm_model = SVC(**svm_opts)\n",
    "\n",
    "# grid search\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# print\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# predict\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_svm_best = best_model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "677f32ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:29:06.685253Z",
     "start_time": "2024-05-11T20:29:06.663478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">PCA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">SVM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PCA      \n",
       "         SVM      \n",
       "      metric value\n",
       "0   accuracy  0.99\n",
       "1  precision  0.99\n",
       "2     recall  0.99\n",
       "3   F1-score  0.99"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "svm_eva_best = func.evaluate_model(y_test, y_pred_svm_best)\n",
    "# change to df\n",
    "svm_df_best = pd.DataFrame(list(svm_eva_best.items()), columns=[\n",
    "                      [\"PCA\", \"PCA\"], [\"SVM\", \"SVM\"], ['metric', 'value']])\n",
    "svm_df_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296a59a",
   "metadata": {},
   "source": [
    "### SVM conclusion\n",
    "可以發現 2個的結果相同 <br>\n",
    "為了避免麻煩 <br>\n",
    "因此使用 default 為最佳模型 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6a0c6",
   "metadata": {},
   "source": [
    "## NN--MLP\n",
    "神經網路 (Neural Network) -- 多層感知機 (Multilayer perceptron) <br>\n",
    "建模之後以 測試集 進行評估 <br>\n",
    "並且列出 4 種指標以供觀察 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd211d",
   "metadata": {},
   "source": [
    "### default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb850fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:29:47.543333Z",
     "start_time": "2024-05-11T20:29:46.463176Z"
    }
   },
   "outputs": [],
   "source": [
    "# NN-MLP model & hyperparameter\n",
    "hidden_layers = (30,)\n",
    "opts = dict(hidden_layer_sizes=hidden_layers, verbose=0,\n",
    "            activation='relu', tol=1e-6, max_iter=int(1e6))\n",
    "nn_mlp_model = MLPClassifier(solver='adam', **opts)\n",
    "nn_mlp_model.fit(X_train_pca, y_train)\n",
    "# predict\n",
    "y_pred_nn_mlp = nn_mlp_model.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62ef9761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:29:48.344887Z",
     "start_time": "2024-05-11T20:29:48.324041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">NN_MLP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      NN_MLP      \n",
       "      metric value\n",
       "0   accuracy  0.90\n",
       "1  precision  0.96\n",
       "2     recall  0.90\n",
       "3   F1-score  0.91"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "nn_mlp_eva = func.evaluate_model(y_test, y_pred_nn_mlp)\n",
    "# change to df\n",
    "nn_mlp_df = pd.DataFrame(list(nn_mlp_eva.items()), columns=[\n",
    "                         [\"PCA\", \"PCA\"], [\"NN_MLP\", \"NN_MLP\"], ['metric', 'value']])\n",
    "nn_mlp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6b337",
   "metadata": {},
   "source": [
    "### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "348a0c48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:31:14.274294Z",
     "start_time": "2024-05-11T20:30:21.237973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best parameters found:  {'activation': 'relu', 'hidden_layer_sizes': (30,), 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "# search best parameter\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(30,)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "}\n",
    "\n",
    "# model\n",
    "opts = dict(verbose=1, tol=1e-6, max_iter=int(1e6))\n",
    "nn_mlp_model = MLPClassifier(**opts)\n",
    "\n",
    "# grid search\n",
    "grid_search = GridSearchCV(nn_mlp_model, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# print\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# predict\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_nn_mlp_best = best_model.predict(X_test_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e05dfcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:32:12.296236Z",
     "start_time": "2024-05-11T20:32:12.268961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">PCA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">NN_MLP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PCA      \n",
       "      NN_MLP      \n",
       "      metric value\n",
       "0   accuracy  0.95\n",
       "1  precision  0.97\n",
       "2     recall  0.95\n",
       "3   F1-score  0.95"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "nn_mlp_eva_best = func.evaluate_model(y_test, y_pred_nn_mlp_best)\n",
    "# change to df\n",
    "nn_mlp_df_best = pd.DataFrame(list(nn_mlp_eva_best.items()), columns=[\n",
    "                         [\"PCA\", \"PCA\"], [\"NN_MLP\", \"NN_MLP\"], ['metric', 'value']])\n",
    "nn_mlp_df_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec04ed",
   "metadata": {},
   "source": [
    "### NN--MLP conclusion\n",
    "可以發現 best 的結果就已經是最好的 <br>\n",
    "因此使用 best 為最佳模型 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65581fee",
   "metadata": {},
   "source": [
    "## NN--SEQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d20c0052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:33:31.095696Z",
     "start_time": "2024-05-11T20:33:31.039358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 64)                1728      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 40)                1320      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5128 (20.03 KB)\n",
      "Trainable params: 5128 (20.03 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# change 1~3 to 0~2\n",
    "y_train_nn = y_train\n",
    "y_test_nn = y_test\n",
    "\n",
    "# model\n",
    "nn_seq_model = Sequential()\n",
    "nn_seq_model.add(Dense(64, activation='relu',\n",
    "                 input_shape=(X_train_pca.shape[1],)))\n",
    "nn_seq_model.add(Dense(32, activation='relu'))\n",
    "# avoid overfitting\n",
    "nn_seq_model.add(Dropout(0.2))\n",
    "nn_seq_model.add(Dense(40, activation='softmax'))\n",
    "\n",
    "# compile\n",
    "# set learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "nn_seq_model.compile(optimizer=optimizer,\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# check model\n",
    "nn_seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab67801e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:33:51.261616Z",
     "start_time": "2024-05-11T20:33:44.216870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0949 - accuracy: 0.9648 - val_loss: 0.5425 - val_accuracy: 0.8750\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1008 - accuracy: 0.9570 - val_loss: 0.5467 - val_accuracy: 0.8906\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1108 - accuracy: 0.9648 - val_loss: 0.5488 - val_accuracy: 0.8906\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0675 - accuracy: 0.9844 - val_loss: 0.5563 - val_accuracy: 0.9062\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0642 - accuracy: 0.9844 - val_loss: 0.5565 - val_accuracy: 0.9062\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0903 - accuracy: 0.9805 - val_loss: 0.5522 - val_accuracy: 0.9062\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0781 - accuracy: 0.9805 - val_loss: 0.5611 - val_accuracy: 0.8750\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0852 - accuracy: 0.9805 - val_loss: 0.5621 - val_accuracy: 0.8750\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0810 - accuracy: 0.9727 - val_loss: 0.5668 - val_accuracy: 0.8750\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0868 - accuracy: 0.9688 - val_loss: 0.5667 - val_accuracy: 0.8750\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0626 - accuracy: 0.9883 - val_loss: 0.5578 - val_accuracy: 0.8750\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0578 - accuracy: 0.9844 - val_loss: 0.5621 - val_accuracy: 0.8906\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0763 - accuracy: 0.9844 - val_loss: 0.5712 - val_accuracy: 0.8750\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0686 - accuracy: 0.9844 - val_loss: 0.5828 - val_accuracy: 0.8594\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0641 - accuracy: 0.9805 - val_loss: 0.5944 - val_accuracy: 0.8594\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0656 - accuracy: 0.9883 - val_loss: 0.5854 - val_accuracy: 0.8594\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0736 - accuracy: 0.9844 - val_loss: 0.5763 - val_accuracy: 0.8594\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0565 - accuracy: 0.9805 - val_loss: 0.5744 - val_accuracy: 0.8594\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0526 - accuracy: 0.9883 - val_loss: 0.5708 - val_accuracy: 0.8750\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9883 - val_loss: 0.5605 - val_accuracy: 0.8906\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0453 - accuracy: 0.9883 - val_loss: 0.5561 - val_accuracy: 0.8906\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0414 - accuracy: 0.9922 - val_loss: 0.5368 - val_accuracy: 0.8906\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0458 - accuracy: 0.9961 - val_loss: 0.5302 - val_accuracy: 0.8750\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0669 - accuracy: 0.9805 - val_loss: 0.5541 - val_accuracy: 0.8750\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0537 - accuracy: 0.9883 - val_loss: 0.5637 - val_accuracy: 0.8594\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.9766 - val_loss: 0.5632 - val_accuracy: 0.8906\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0822 - accuracy: 0.9727 - val_loss: 0.5637 - val_accuracy: 0.9062\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0679 - accuracy: 0.9805 - val_loss: 0.5570 - val_accuracy: 0.9062\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0581 - accuracy: 0.9844 - val_loss: 0.5489 - val_accuracy: 0.8750\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0703 - accuracy: 0.9883 - val_loss: 0.5517 - val_accuracy: 0.8750\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0538 - accuracy: 0.9883 - val_loss: 0.5554 - val_accuracy: 0.8750\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9922 - val_loss: 0.5519 - val_accuracy: 0.8750\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0886 - accuracy: 0.9766 - val_loss: 0.5447 - val_accuracy: 0.8750\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0414 - accuracy: 0.9961 - val_loss: 0.5456 - val_accuracy: 0.8750\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.9766 - val_loss: 0.5265 - val_accuracy: 0.8750\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.9844 - val_loss: 0.5114 - val_accuracy: 0.8750\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0430 - accuracy: 0.9922 - val_loss: 0.4977 - val_accuracy: 0.8750\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0510 - accuracy: 0.9805 - val_loss: 0.5058 - val_accuracy: 0.8750\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9766 - val_loss: 0.5123 - val_accuracy: 0.8750\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0460 - accuracy: 0.9883 - val_loss: 0.5079 - val_accuracy: 0.8750\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0679 - accuracy: 0.9766 - val_loss: 0.5175 - val_accuracy: 0.8750\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9727 - val_loss: 0.5381 - val_accuracy: 0.8750\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0458 - accuracy: 0.9766 - val_loss: 0.5533 - val_accuracy: 0.8750\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0696 - accuracy: 0.9688 - val_loss: 0.5579 - val_accuracy: 0.8750\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0516 - accuracy: 0.9844 - val_loss: 0.5646 - val_accuracy: 0.8750\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0583 - accuracy: 0.9922 - val_loss: 0.5614 - val_accuracy: 0.8750\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 0.5511 - val_accuracy: 0.8750\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0566 - accuracy: 0.9844 - val_loss: 0.5530 - val_accuracy: 0.8750\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 0.9805 - val_loss: 0.5406 - val_accuracy: 0.8750\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0434 - accuracy: 0.9883 - val_loss: 0.5234 - val_accuracy: 0.8906\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 0.9883 - val_loss: 0.5103 - val_accuracy: 0.8906\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0618 - accuracy: 0.9766 - val_loss: 0.5073 - val_accuracy: 0.8906\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0329 - accuracy: 0.9922 - val_loss: 0.5048 - val_accuracy: 0.8906\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0435 - accuracy: 0.9883 - val_loss: 0.5107 - val_accuracy: 0.8906\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0622 - accuracy: 0.9883 - val_loss: 0.5211 - val_accuracy: 0.8750\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 0.9922 - val_loss: 0.5321 - val_accuracy: 0.8750\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0544 - accuracy: 0.9727 - val_loss: 0.5363 - val_accuracy: 0.8750\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0415 - accuracy: 0.9883 - val_loss: 0.5390 - val_accuracy: 0.8750\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0568 - accuracy: 0.9805 - val_loss: 0.5366 - val_accuracy: 0.8750\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0329 - accuracy: 0.9922 - val_loss: 0.5307 - val_accuracy: 0.8750\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9922 - val_loss: 0.5189 - val_accuracy: 0.8750\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0598 - accuracy: 0.9727 - val_loss: 0.5020 - val_accuracy: 0.8750\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0580 - accuracy: 0.9844 - val_loss: 0.5065 - val_accuracy: 0.8906\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.9805 - val_loss: 0.5033 - val_accuracy: 0.9062\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0605 - accuracy: 0.9844 - val_loss: 0.5132 - val_accuracy: 0.9062\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0432 - accuracy: 0.9883 - val_loss: 0.5281 - val_accuracy: 0.9062\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.9922 - val_loss: 0.5353 - val_accuracy: 0.8906\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0324 - accuracy: 0.9883 - val_loss: 0.5392 - val_accuracy: 0.8906\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0476 - accuracy: 0.9922 - val_loss: 0.5342 - val_accuracy: 0.8906\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0382 - accuracy: 0.9922 - val_loss: 0.5405 - val_accuracy: 0.8906\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.9922 - val_loss: 0.5402 - val_accuracy: 0.8906\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0505 - accuracy: 0.9883 - val_loss: 0.5314 - val_accuracy: 0.8906\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0538 - accuracy: 0.9844 - val_loss: 0.5319 - val_accuracy: 0.8906\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0287 - accuracy: 0.9922 - val_loss: 0.5331 - val_accuracy: 0.8906\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.5381 - val_accuracy: 0.8906\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.9883 - val_loss: 0.5074 - val_accuracy: 0.8906\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0224 - accuracy: 0.9961 - val_loss: 0.4933 - val_accuracy: 0.8906\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0436 - accuracy: 0.9922 - val_loss: 0.4881 - val_accuracy: 0.8906\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9844 - val_loss: 0.4803 - val_accuracy: 0.8906\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0452 - accuracy: 0.9844 - val_loss: 0.4614 - val_accuracy: 0.8906\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0493 - accuracy: 0.9688 - val_loss: 0.4742 - val_accuracy: 0.8906\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0444 - accuracy: 0.9883 - val_loss: 0.5107 - val_accuracy: 0.8906\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9961 - val_loss: 0.5348 - val_accuracy: 0.8906\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.9922 - val_loss: 0.5474 - val_accuracy: 0.8906\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0530 - accuracy: 0.9805 - val_loss: 0.5308 - val_accuracy: 0.8906\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0463 - accuracy: 0.9883 - val_loss: 0.5195 - val_accuracy: 0.8906\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.9961 - val_loss: 0.5065 - val_accuracy: 0.8906\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0323 - accuracy: 0.9961 - val_loss: 0.4978 - val_accuracy: 0.8906\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0330 - accuracy: 0.9922 - val_loss: 0.5005 - val_accuracy: 0.8906\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0298 - accuracy: 0.9883 - val_loss: 0.4886 - val_accuracy: 0.8906\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.9883 - val_loss: 0.4778 - val_accuracy: 0.8906\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9922 - val_loss: 0.4684 - val_accuracy: 0.8906\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.9922 - val_loss: 0.4739 - val_accuracy: 0.8906\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.9883 - val_loss: 0.4815 - val_accuracy: 0.8906\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0460 - accuracy: 0.9844 - val_loss: 0.5021 - val_accuracy: 0.8906\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0544 - accuracy: 0.9844 - val_loss: 0.5132 - val_accuracy: 0.8906\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0289 - accuracy: 0.9922 - val_loss: 0.5165 - val_accuracy: 0.8906\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 0.9883 - val_loss: 0.5290 - val_accuracy: 0.8906\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0427 - accuracy: 0.9844 - val_loss: 0.5329 - val_accuracy: 0.8906\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0313 - accuracy: 0.9883 - val_loss: 0.5223 - val_accuracy: 0.8906\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0506 - accuracy: 0.9844 - val_loss: 0.5053 - val_accuracy: 0.8906\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0790 - accuracy: 0.9727 - val_loss: 0.4891 - val_accuracy: 0.8906\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9883 - val_loss: 0.4816 - val_accuracy: 0.8906\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.9883 - val_loss: 0.4857 - val_accuracy: 0.8906\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0497 - accuracy: 0.9805 - val_loss: 0.4863 - val_accuracy: 0.8906\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0374 - accuracy: 0.9922 - val_loss: 0.4831 - val_accuracy: 0.8906\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0367 - accuracy: 0.9883 - val_loss: 0.4753 - val_accuracy: 0.8906\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.9961 - val_loss: 0.4749 - val_accuracy: 0.8906\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0443 - accuracy: 0.9883 - val_loss: 0.4737 - val_accuracy: 0.8906\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0339 - accuracy: 0.9922 - val_loss: 0.4789 - val_accuracy: 0.8906\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.9883 - val_loss: 0.4907 - val_accuracy: 0.8906\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.4992 - val_accuracy: 0.8906\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.9961 - val_loss: 0.4975 - val_accuracy: 0.8906\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0437 - accuracy: 0.9844 - val_loss: 0.5065 - val_accuracy: 0.8906\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0235 - accuracy: 0.9961 - val_loss: 0.5187 - val_accuracy: 0.8906\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0310 - accuracy: 0.9922 - val_loss: 0.5304 - val_accuracy: 0.8906\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9922 - val_loss: 0.5333 - val_accuracy: 0.8906\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9883 - val_loss: 0.5316 - val_accuracy: 0.8906\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9727 - val_loss: 0.5146 - val_accuracy: 0.8906\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0366 - accuracy: 0.9922 - val_loss: 0.4866 - val_accuracy: 0.8906\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.9922 - val_loss: 0.4862 - val_accuracy: 0.8906\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 0.9922 - val_loss: 0.4911 - val_accuracy: 0.8906\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.9961 - val_loss: 0.4922 - val_accuracy: 0.8906\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9844 - val_loss: 0.4895 - val_accuracy: 0.8906\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0268 - accuracy: 0.9922 - val_loss: 0.4659 - val_accuracy: 0.8906\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.9922 - val_loss: 0.4527 - val_accuracy: 0.9062\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0476 - accuracy: 0.9883 - val_loss: 0.4562 - val_accuracy: 0.9062\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 0.9844 - val_loss: 0.4459 - val_accuracy: 0.9062\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0377 - accuracy: 0.9961 - val_loss: 0.4339 - val_accuracy: 0.9062\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0419 - accuracy: 0.9883 - val_loss: 0.4320 - val_accuracy: 0.9062\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0521 - accuracy: 0.9844 - val_loss: 0.4325 - val_accuracy: 0.9062\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.9805 - val_loss: 0.4278 - val_accuracy: 0.9062\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0273 - accuracy: 0.9922 - val_loss: 0.4234 - val_accuracy: 0.9219\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.9922 - val_loss: 0.4203 - val_accuracy: 0.9062\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 0.9766 - val_loss: 0.4357 - val_accuracy: 0.9062\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0494 - accuracy: 0.9805 - val_loss: 0.4431 - val_accuracy: 0.9062\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9844 - val_loss: 0.4431 - val_accuracy: 0.9062\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9961 - val_loss: 0.4528 - val_accuracy: 0.9062\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.4592 - val_accuracy: 0.9062\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.4648 - val_accuracy: 0.9062\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0524 - accuracy: 0.9766 - val_loss: 0.4560 - val_accuracy: 0.9062\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.9844 - val_loss: 0.4480 - val_accuracy: 0.9062\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9727 - val_loss: 0.4238 - val_accuracy: 0.8906\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9922 - val_loss: 0.4315 - val_accuracy: 0.8906\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.9961 - val_loss: 0.4367 - val_accuracy: 0.8906\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9844 - val_loss: 0.4488 - val_accuracy: 0.8906\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0290 - accuracy: 0.9961 - val_loss: 0.4686 - val_accuracy: 0.8906\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.9922 - val_loss: 0.4754 - val_accuracy: 0.8906\n",
      "Epoch 149/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0382 - accuracy: 0.9883 - val_loss: 0.4837 - val_accuracy: 0.8906\n",
      "Epoch 150/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9961 - val_loss: 0.4952 - val_accuracy: 0.8906\n",
      "Epoch 151/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.9922 - val_loss: 0.5127 - val_accuracy: 0.8906\n",
      "Epoch 152/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0382 - accuracy: 0.9883 - val_loss: 0.5154 - val_accuracy: 0.8906\n",
      "Epoch 153/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.9961 - val_loss: 0.5184 - val_accuracy: 0.9062\n",
      "Epoch 154/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.9961 - val_loss: 0.5199 - val_accuracy: 0.9062\n",
      "Epoch 155/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0465 - accuracy: 0.9805 - val_loss: 0.5120 - val_accuracy: 0.9062\n",
      "Epoch 156/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0239 - accuracy: 0.9961 - val_loss: 0.5106 - val_accuracy: 0.8750\n",
      "Epoch 157/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9961 - val_loss: 0.5042 - val_accuracy: 0.8750\n",
      "Epoch 158/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.9961 - val_loss: 0.4939 - val_accuracy: 0.8906\n",
      "Epoch 159/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.9961 - val_loss: 0.4891 - val_accuracy: 0.8906\n",
      "Epoch 160/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.9961 - val_loss: 0.4685 - val_accuracy: 0.8906\n",
      "Epoch 161/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.9844 - val_loss: 0.4535 - val_accuracy: 0.8906\n",
      "Epoch 162/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0272 - accuracy: 0.9922 - val_loss: 0.4485 - val_accuracy: 0.8906\n",
      "Epoch 163/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.4581 - val_accuracy: 0.9062\n",
      "Epoch 164/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0449 - accuracy: 0.9844 - val_loss: 0.4580 - val_accuracy: 0.9062\n",
      "Epoch 165/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0208 - accuracy: 0.9961 - val_loss: 0.4604 - val_accuracy: 0.9062\n",
      "Epoch 166/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.9961 - val_loss: 0.4707 - val_accuracy: 0.9062\n",
      "Epoch 167/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.9922 - val_loss: 0.4747 - val_accuracy: 0.9219\n",
      "Epoch 168/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.4704 - val_accuracy: 0.9219\n",
      "Epoch 169/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.4744 - val_accuracy: 0.9219\n",
      "Epoch 170/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.9922 - val_loss: 0.4792 - val_accuracy: 0.9219\n",
      "Epoch 171/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.4827 - val_accuracy: 0.9062\n",
      "Epoch 172/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.9844 - val_loss: 0.4922 - val_accuracy: 0.9062\n",
      "Epoch 173/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.9883 - val_loss: 0.4867 - val_accuracy: 0.8906\n",
      "Epoch 174/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.9922 - val_loss: 0.5036 - val_accuracy: 0.8906\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 0.9922 - val_loss: 0.5059 - val_accuracy: 0.8750\n",
      "Epoch 176/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0196 - accuracy: 0.9961 - val_loss: 0.5005 - val_accuracy: 0.8750\n",
      "Epoch 177/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9961 - val_loss: 0.5030 - val_accuracy: 0.8750\n",
      "Epoch 178/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9961 - val_loss: 0.5075 - val_accuracy: 0.8750\n",
      "Epoch 179/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.5104 - val_accuracy: 0.8750\n",
      "Epoch 180/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9961 - val_loss: 0.5134 - val_accuracy: 0.8750\n",
      "Epoch 181/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9883 - val_loss: 0.5167 - val_accuracy: 0.8750\n",
      "Epoch 182/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.4976 - val_accuracy: 0.8906\n",
      "Epoch 183/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9961 - val_loss: 0.4849 - val_accuracy: 0.8906\n",
      "Epoch 184/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.9961 - val_loss: 0.4773 - val_accuracy: 0.8906\n",
      "Epoch 185/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.4811 - val_accuracy: 0.8906\n",
      "Epoch 186/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.9922 - val_loss: 0.4735 - val_accuracy: 0.9062\n",
      "Epoch 187/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.9922 - val_loss: 0.4778 - val_accuracy: 0.9062\n",
      "Epoch 188/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9922 - val_loss: 0.4734 - val_accuracy: 0.9062\n",
      "Epoch 189/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0320 - accuracy: 0.9883 - val_loss: 0.4719 - val_accuracy: 0.9062\n",
      "Epoch 190/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.4650 - val_accuracy: 0.9062\n",
      "Epoch 191/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0611 - accuracy: 0.9844 - val_loss: 0.4573 - val_accuracy: 0.9062\n",
      "Epoch 192/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0263 - accuracy: 0.9922 - val_loss: 0.4473 - val_accuracy: 0.9062\n",
      "Epoch 193/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0229 - accuracy: 0.9922 - val_loss: 0.4721 - val_accuracy: 0.9062\n",
      "Epoch 194/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0156 - accuracy: 0.9961 - val_loss: 0.4855 - val_accuracy: 0.9062\n",
      "Epoch 195/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0240 - accuracy: 0.9883 - val_loss: 0.4881 - val_accuracy: 0.9062\n",
      "Epoch 196/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.9883 - val_loss: 0.4735 - val_accuracy: 0.9062\n",
      "Epoch 197/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9922 - val_loss: 0.4495 - val_accuracy: 0.9062\n",
      "Epoch 198/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9961 - val_loss: 0.4413 - val_accuracy: 0.9062\n",
      "Epoch 199/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0205 - accuracy: 0.9922 - val_loss: 0.4369 - val_accuracy: 0.9062\n",
      "Epoch 200/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.9961 - val_loss: 0.4488 - val_accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = nn_seq_model.fit(X_train_pca, y_train_nn,\n",
    "                           epochs=200, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1a2ec00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:33:53.040297Z",
     "start_time": "2024-05-11T20:33:52.835584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 0.2821381986141205\n",
      "accuracy : 0.9125000238418579\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "test_loss, test_accuracy = nn_seq_model.evaluate(\n",
    "    X_test_pca, y_test_nn, verbose=0)\n",
    "y_pred_prob = nn_seq_model.predict(X_test_pca, verbose=0)\n",
    "y_pred_nn_seq = np.argmax(y_pred_prob, axis=1)\n",
    "print(\"loss :\", test_loss)\n",
    "print(\"accuracy :\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c467c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:33:56.091834Z",
     "start_time": "2024-05-11T20:33:56.070734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">PCA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">NN_SEQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-score</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PCA      \n",
       "      NN_SEQ      \n",
       "      metric value\n",
       "0   accuracy  0.91\n",
       "1  precision  0.94\n",
       "2     recall  0.91\n",
       "3   F1-score  0.91"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "nn_seq_eva = func.evaluate_model(y_test_nn, y_pred_nn_seq)\n",
    "# change to df\n",
    "nn_seq_df = pd.DataFrame(list(nn_seq_eva.items()), columns=[\n",
    "                         [\"PCA\", \"PCA\"], [\"NN_SEQ\", \"NN_SEQ\"], ['metric', 'value']])\n",
    "nn_seq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d48999",
   "metadata": {},
   "source": [
    "## compare evaluation of pca data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "19c2368a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:36:27.108435Z",
     "start_time": "2024-05-11T20:36:27.080171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">PCA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>MLR</th>\n",
       "      <th>SVM</th>\n",
       "      <th>NN_MLP</th>\n",
       "      <th>NN_SEQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-score</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            PCA                    \n",
       "            MLR   SVM NN_MLP NN_SEQ\n",
       "          value value  value  value\n",
       "metric                             \n",
       "accuracy   0.99  0.99   0.95   0.91\n",
       "precision  0.99  0.99   0.97   0.94\n",
       "recall     0.99  0.99   0.95   0.91\n",
       "F1-score   0.99  0.99   0.95   0.91"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine df together\n",
    "mlr_df_copy = mlr_df.copy()\n",
    "svm_df_copy = svm_df.copy()\n",
    "nn_mlp_df_best_copy = nn_mlp_df_best.copy()\n",
    "nn_seq_df_copy = nn_seq_df.copy()\n",
    "# del same col\n",
    "svm_df_copy.drop((\"PCA\", \"SVM\", \"metric\"), axis=1, inplace=True)\n",
    "nn_mlp_df_best_copy.drop((\"PCA\", \"NN_MLP\", \"metric\"), axis=1, inplace=True)\n",
    "nn_seq_df_copy.drop((\"PCA\", \"NN_SEQ\", \"metric\"), axis=1, inplace=True)\n",
    "\n",
    "# combine\n",
    "final_df_pca = pd.concat(\n",
    "    [mlr_df_copy, svm_df_copy, nn_mlp_df_best_copy, nn_seq_df_copy], axis=1)\n",
    "# set \"metric\" to index\n",
    "final_df_pca.set_index((\"PCA\", \"MLR\", \"metric\"), inplace=True)\n",
    "final_df_pca = final_df.rename_axis(\"metric\")\n",
    "\n",
    "# show\n",
    "final_df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd79c6c",
   "metadata": {},
   "source": [
    "## model of pca data conclusion\n",
    "可以看出總體來說 MLR、SVM 模型的表現是最好的 <br>\n",
    "這 2 者相較於神經網路 <br>\n",
    "不僅可以得到非常高的評估指標分數 <br>\n",
    "同時運行時間也比較短 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306f3f1",
   "metadata": {},
   "source": [
    "# final conclusion\n",
    "把上述所有的評估指標組合一起觀察 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7fc0b205",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T20:39:33.618234Z",
     "start_time": "2024-05-11T20:39:33.601698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">standard</th>\n",
       "      <th colspan=\"4\" halign=\"left\">PCA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>MLR</th>\n",
       "      <th>SVM</th>\n",
       "      <th>NN_MLP</th>\n",
       "      <th>NN_SEQ</th>\n",
       "      <th>MLR</th>\n",
       "      <th>SVM</th>\n",
       "      <th>NN_MLP</th>\n",
       "      <th>NN_SEQ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-score</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          standard                       PCA                    \n",
       "               MLR   SVM NN_MLP NN_SEQ   MLR   SVM NN_MLP NN_SEQ\n",
       "             value value  value  value value value  value  value\n",
       "metric                                                          \n",
       "accuracy      0.99  0.99   0.95   0.91  0.99  0.99   0.95   0.91\n",
       "precision     0.99  0.99   0.97   0.94  0.99  0.99   0.97   0.94\n",
       "recall        0.99  0.99   0.95   0.91  0.99  0.99   0.95   0.91\n",
       "F1-score      0.99  0.99   0.95   0.91  0.99  0.99   0.95   0.91"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final conclusion\n",
    "final_df = pd.concat(\n",
    "    [final_df_standard, final_df_pca], axis=1)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d34a01",
   "metadata": {},
   "source": [
    "可以發現到其實資料集進行標準化與加上 PCA 之後並沒有很大的差別 <br>\n",
    "猜測有可能是因為資料集不夠大所以造成沒有太大的差異 <br>\n",
    "另外也只需要用到比較基礎的模型就能夠得到還不錯的效果 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86113ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "458.119px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "365.854px",
    "left": "1517.67px",
    "right": "20px",
    "top": "80px",
    "width": "689.333px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
